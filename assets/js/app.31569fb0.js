(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(n){function e(e){for(var r,a,s=e[0],c=e[1],l=e[2],u=0,p=[];u<s.length;u++)a=s[u],Object.prototype.hasOwnProperty.call(o,a)&&o[a]&&p.push(o[a][0]),o[a]=0;for(r in c)Object.prototype.hasOwnProperty.call(c,r)&&(n[r]=c[r]);for(d&&d(e);p.length;)p.shift()();return i.push.apply(i,l||[]),t()}function t(){for(var n,e=0;e<i.length;e++){for(var t=i[e],r=!0,s=1;s<t.length;s++){var c=t[s];0!==o[c]&&(r=!1)}r&&(i.splice(e--,1),n=a(a.s=t[0]))}return n}var r={},o={1:0},i=[];function a(e){if(r[e])return r[e].exports;var t=r[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,a),t.l=!0,t.exports}a.e=function(n){var e=[],t=o[n];if(0!==t)if(t)e.push(t[2]);else{var r=new Promise((function(e,r){t=o[n]=[e,r]}));e.push(t[2]=r);var i,s=document.createElement("script");s.charset="utf-8",s.timeout=120,a.nc&&s.setAttribute("nonce",a.nc),s.src=function(n){return a.p+"assets/js/"+({}[n]||n)+"."+{2:"df69d635",3:"c76bdb66",4:"ce850d9b",5:"1faa936b",6:"5a088ba5",7:"8f4bed74",8:"b1937d38",9:"ebe72a00",10:"788e911f",11:"76e4b896",12:"88aeacb8",13:"ef864632",14:"ddfa3e6b",15:"68f8ce44",16:"aff0328f",17:"125748af",18:"fa6cd919",19:"4bac67a7",20:"c78801f0",21:"64cc6926",22:"cacbe2f9",23:"70f001e8",24:"e4a1dc80",25:"607f30bb",26:"60c54e7d",27:"48c44b2e",28:"eb44c0ba",29:"3f35baff",30:"5ceb6f39",31:"df68a33c",32:"0870e048",33:"3b446daa",34:"fd564325",35:"0e1dceb3",36:"6ea040e9",37:"d0f64292",38:"da592564",39:"f2413731",40:"7c003d2a",41:"61dee31e",42:"dc1cfeb4",43:"3fe14024",44:"b66c9969",45:"281f7712",46:"fb40bb01",47:"311785d3",48:"0070f89f",49:"0a752342",50:"d46282a9",51:"5a52f209",52:"1ec82ee7",53:"01e08d71",54:"0ef033a3",55:"602b8614",56:"3a886341",57:"feae1a28",58:"6d2f12be",59:"9325c1da",60:"a265bf7c",61:"30146ded",62:"662b760a",63:"5d59c7f0",64:"faabd330",65:"9ced9383",66:"2ff8ca8d",67:"9000451e",68:"c63a2ef0",69:"8c8e119e",70:"de3ec280",71:"0009b157",72:"31a06f9d",73:"e84721d3",74:"8da44d3d",75:"0c64e4b0",76:"3db2aba9",77:"b00b1560",78:"53a1c343",79:"1218161f",80:"b85cb7a2",81:"d4f058f7",82:"bd0f843c",83:"b6f1d5c6",84:"e3469377",85:"1f3e7bd0",86:"179e9bc8",87:"9c3f44b4",88:"79659ee4",89:"bea5895d",90:"a7bf2270",91:"82f3a339",92:"89864afc",93:"6a775a5a",94:"dede140a",95:"e182e5da",96:"bca74f0c",97:"0b7a1a08",98:"33ae82c8",99:"4f690b51",100:"5869e407",101:"4735f8c8",102:"0afc9257",103:"deaaedc9",104:"e5de9bfa",105:"04f52d11",106:"a0c4adf4",107:"72cf41ff",108:"2185b4f1",109:"a1966f5b",110:"f0666616",111:"2d52e7dc",112:"7705a466",113:"2f1ab226",114:"4aa4d6ae",115:"4a9765c7",116:"31d2b63f",117:"d2af2299",118:"70b7c00b",119:"50227395",120:"b2f1f2eb",121:"e9e3e5bf",122:"8821ca4c",123:"5c481679",124:"90aefb34",125:"3c2d8a2e",126:"ca38566c",127:"74131025",128:"da48cef5",129:"3122af0d",130:"1e22d289",131:"706af342",132:"50c671ee",133:"db98fa8a",134:"7fe8cf44",135:"6219a109",136:"27f3f774",137:"b4e57291",138:"82aaf97d",139:"cab10f5a",140:"4b42a53f",141:"ba14455a",142:"4acedada",143:"e89a5e92",144:"27ae0dda",145:"6bbfb61f",146:"55df497b",147:"40936f4a",148:"4be699c3",149:"fc77592a",150:"e7fb1666",151:"40ff52a0",152:"fda432cf",153:"1b165d6b",154:"3bb5bcb3",155:"6cd504f2",156:"b1ff07fd",157:"57a6101a",158:"0a18c4db",159:"89815b25",160:"9a45fd99",161:"82e463d5",162:"5a06785b",163:"08555c19",164:"9f890f94",165:"2e875427",166:"3f418177",167:"06c2cc48",168:"3e5efe52",169:"637e7376",170:"319e96c6",171:"a09af050",172:"23d78708",173:"c057313f",174:"8d168208",175:"d81d3c21",176:"8ba38854",177:"620927ce",178:"3c001e58",179:"0acd475a",180:"e910d853",181:"7798460e",182:"3c1fc210",183:"8cc80600",184:"fff56765",185:"c63f91e5",186:"7870decd",187:"3cb205f3",188:"3215c6b1",189:"5e324d91",190:"71194c3e",191:"3d513d7f",192:"66f1b9c4",193:"cfe9beef",194:"d54494f6",195:"91f8acc9",196:"a727b0fb",197:"dcd6487d",198:"4734520f",199:"4946dc06",200:"72242046",201:"87f62686",202:"38ace3e6",203:"2827a086",204:"4c8342e0",205:"e1de7f41",206:"94f84999",207:"95ca2dd7",208:"f293b55a",209:"056b496b",210:"7f232344",211:"748e6034",212:"06e705a7",213:"a9f317f4",214:"65b257a7",215:"1260a609",216:"c1b39ca2",217:"084bb897",218:"02aa4cd9",219:"35339213",220:"1c043025",221:"af0028af",222:"43ea3237",223:"8c59c2c8",224:"ceff003f",225:"d7aa9f36",226:"7d7574e8",227:"3786dac4",228:"67433714",229:"874cf521",230:"63255993",231:"2771635e",232:"1a846631",233:"7950c15f",234:"9c9d2947",235:"3423e660",236:"2d15acb2",237:"5e3e145a",238:"ffb20d88",239:"2d0bd5a9",240:"987df996",241:"9e67aa11",242:"20c57ba1",243:"b3b4e01f"}[n]+".js"}(n);var c=new Error;i=function(e){s.onerror=s.onload=null,clearTimeout(l);var t=o[n];if(0!==t){if(t){var r=e&&("load"===e.type?"missing":e.type),i=e&&e.target&&e.target.src;c.message="Loading chunk "+n+" failed.\n("+r+": "+i+")",c.name="ChunkLoadError",c.type=r,c.request=i,t[1](c)}o[n]=void 0}};var l=setTimeout((function(){i({type:"timeout",target:s})}),12e4);s.onerror=s.onload=i,document.head.appendChild(s)}return Promise.all(e)},a.m=n,a.c=r,a.d=function(n,e,t){a.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},a.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},a.t=function(n,e){if(1&e&&(n=a(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(a.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var r in n)a.d(t,r,function(e){return n[e]}.bind(null,r));return t},a.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return a.d(e,"a",e),e},a.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},a.p="/",a.oe=function(n){throw console.error(n),n};var s=window.webpackJsonp=window.webpackJsonp||[],c=s.push.bind(s);s.push=e,s=s.slice();for(var l=0;l<s.length;l++)e(s[l]);var d=c;i.push([233,0]),t()}([function(n,e){var t=function(n){return n&&n.Math==Math&&n};n.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||Function("return this")()},function(n,e,t){var r=t(0),o=t(39).f,i=t(23),a=t(14),s=t(110),c=t(115),l=t(101);n.exports=function(n,e){var t,d,u,p,m,g=n.target,h=n.global,b=n.stat;if(t=h?r:b?r[g]||s(g,{}):(r[g]||{}).prototype)for(d in e){if(p=e[d],u=n.noTargetGet?(m=o(t,d))&&m.value:t[d],!l(h?d:g+(b?".":"#")+d,n.forced)&&void 0!==u){if(typeof p==typeof u)continue;c(p,u)}(n.sham||u&&u.sham)&&i(p,"sham",!0),a(t,d,p,n)}}},function(n,e,t){var r=t(62),o=Function.prototype,i=o.bind,a=o.call,s=r&&i.bind(a,a);n.exports=r?function(n){return n&&s(n)}:function(n){return n&&function(){return a.apply(n,arguments)}}},function(n,e){n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e){n.exports=function(n){return"function"==typeof n}},function(n,e,t){var r=t(118),o=t(14),i=t(249);r||o(Object.prototype,"toString",i,{unsafe:!0})},function(n,e,t){var r=t(0),o=t(78),i=t(10),a=t(79),s=t(111),c=t(152),l=o("wks"),d=r.Symbol,u=d&&d.for,p=c?d:d&&d.withoutSetter||a;n.exports=function(n){if(!i(l,n)||!s&&"string"!=typeof l[n]){var e="Symbol."+n;s&&i(d,n)?l[n]=d[n]:l[n]=c&&u?u(e):p(e)}return l[n]}},function(n,e,t){var r=t(0),o=t(8),i=r.String,a=r.TypeError;n.exports=function(n){if(o(n))return n;throw a(i(n)+" is not an object")}},function(n,e,t){var r=t(4);n.exports=function(n){return"object"==typeof n?null!==n:r(n)}},function(n,e,t){var r=t(3);n.exports=!r((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e,t){var r=t(2),o=t(15),i=r({}.hasOwnProperty);n.exports=Object.hasOwn||function(n,e){return i(o(n),e)}},function(n,e,t){var r=t(62),o=Function.prototype.call;n.exports=r?o.bind(o):function(){return o.apply(o,arguments)}},function(n,e,t){var r=t(0),o=t(75),i=r.String;n.exports=function(n){if("Symbol"===o(n))throw TypeError("Cannot convert a Symbol value to a string");return i(n)}},function(n,e,t){var r=t(0),o=t(9),i=t(154),a=t(153),s=t(7),c=t(81),l=r.TypeError,d=Object.defineProperty,u=Object.getOwnPropertyDescriptor;e.f=o?a?function(n,e,t){if(s(n),e=c(e),s(t),"function"==typeof n&&"prototype"===e&&"value"in t&&"writable"in t&&!t.writable){var r=u(n,e);r&&r.writable&&(n[e]=t.value,t={configurable:"configurable"in t?t.configurable:r.configurable,enumerable:"enumerable"in t?t.enumerable:r.enumerable,writable:!1})}return d(n,e,t)}:d:function(n,e,t){if(s(n),e=c(e),s(t),i)try{return d(n,e,t)}catch(n){}if("get"in t||"set"in t)throw l("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e,t){var r=t(0),o=t(4),i=t(10),a=t(23),s=t(110),c=t(87),l=t(38),d=t(65).CONFIGURABLE,u=l.get,p=l.enforce,m=String(String).split("String");(n.exports=function(n,e,t,c){var l,u=!!c&&!!c.unsafe,g=!!c&&!!c.enumerable,h=!!c&&!!c.noTargetGet,b=c&&void 0!==c.name?c.name:e;o(t)&&("Symbol("===String(b).slice(0,7)&&(b="["+String(b).replace(/^Symbol\(([^)]*)\)/,"$1")+"]"),(!i(t,"name")||d&&t.name!==b)&&a(t,"name",b),(l=p(t)).source||(l.source=m.join("string"==typeof b?b:""))),n!==r?(u?!h&&n[e]&&(g=!0):delete n[e],g?n[e]=t:a(n,e,t)):g?n[e]=t:s(e,t)})(Function.prototype,"toString",(function(){return o(this)&&u(this).source||c(this)}))},function(n,e,t){var r=t(0),o=t(17),i=r.Object;n.exports=function(n){return i(o(n))}},function(n,e,t){var r=t(0),o=t(4),i=function(n){return o(n)?n:void 0};n.exports=function(n,e){return arguments.length<2?i(r[n]):r[n]&&r[n][e]}},function(n,e,t){var r=t(0).TypeError;n.exports=function(n){if(null==n)throw r("Can't call method on "+n);return n}},function(n,e,t){"use strict";var r=t(1),o=t(92);r({target:"RegExp",proto:!0,forced:/./.exec!==o},{exec:o})},function(n,e,t){var r=t(52);n.exports=function(n){return r(n.length)}},function(n,e,t){var r=t(61),o=t(17);n.exports=function(n){return r(o(n))}},function(n,e,t){var r=t(2),o=r({}.toString),i=r("".slice);n.exports=function(n){return i(o(n),8,-1)}},function(n,e){n.exports=!1},function(n,e,t){var r=t(9),o=t(13),i=t(49);n.exports=r?function(n,e,t){return o.f(n,e,i(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e,t){"use strict";var r=t(171).charAt,o=t(12),i=t(38),a=t(158),s=i.set,c=i.getterFor("String Iterator");a(String,"String",(function(n){s(this,{type:"String Iterator",string:o(n),index:0})}),(function(){var n,e=c(this),t=e.string,o=e.index;return o>=t.length?{value:void 0,done:!0}:(n=r(t,o),e.index+=n.length,{value:n,done:!1})}))},function(n,e,t){var r=t(14),o=t(265),i=Error.prototype;i.toString!==o&&r(i,"toString",o)},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){var r=t(16);n.exports=r("navigator","userAgent")||""},function(n,e,t){var r=t(187),o="object"==typeof self&&self&&self.Object===Object&&self,i=r||o||Function("return this")();n.exports=i},function(n,e,t){"use strict";function r(n,e,t,r,o,i,a,s){var c,l="function"==typeof n?n.options:n;if(e&&(l.render=e,l.staticRenderFns=t,l._compiled=!0),r&&(l.functional=!0),i&&(l._scopeId="data-v-"+i),a?(c=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),o&&o.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(a)},l._ssrRegister=c):o&&(c=s?function(){o.call(this,(l.functional?this.parent:this).$root.$options.shadowRoot)}:o),c)if(l.functional){l._injectStyles=c;var d=l.render;l.render=function(n,e){return c.call(e),d(n,e)}}else{var u=l.beforeCreate;l.beforeCreate=u?[].concat(u,c):[c]}return{exports:n,options:l}}t.d(e,"a",(function(){return r}))},function(n,e,t){"use strict";var r=t(1),o=t(56).filter;r({target:"Array",proto:!0,forced:!t(68)("filter")},{filter:function(n){return o(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var r=t(0),o=t(172),i=t(173),a=t(140),s=t(23),c=t(6),l=c("iterator"),d=c("toStringTag"),u=a.values,p=function(n,e){if(n){if(n[l]!==u)try{s(n,l,u)}catch(e){n[l]=u}if(n[d]||s(n,d,e),o[e])for(var t in a)if(n[t]!==a[t])try{s(n,t,a[t])}catch(e){n[t]=a[t]}}};for(var m in o)p(r[m]&&r[m].prototype,m);p(i,"DOMTokenList")},function(n,e,t){"use strict";var r=t(1),o=t(177);r({target:"Array",proto:!0,forced:[].forEach!=o},{forEach:o})},function(n,e,t){var r=t(0),o=t(172),i=t(173),a=t(177),s=t(23),c=function(n){if(n&&n.forEach!==a)try{s(n,"forEach",a)}catch(e){n.forEach=a}};for(var l in o)o[l]&&c(r[l]&&r[l].prototype);c(i)},function(n,e,t){var r,o=t(7),i=t(141),a=t(113),s=t(63),c=t(157),l=t(80),d=t(86),u=d("IE_PROTO"),p=function(){},m=function(n){return"<script>"+n+"<\/script>"},g=function(n){n.write(m("")),n.close();var e=n.parentWindow.Object;return n=null,e},h=function(){try{r=new ActiveXObject("htmlfile")}catch(n){}var n,e;h="undefined"!=typeof document?document.domain&&r?g(r):((e=l("iframe")).style.display="none",c.appendChild(e),e.src=String("javascript:"),(n=e.contentWindow.document).open(),n.write(m("document.F=Object")),n.close(),n.F):g(r);for(var t=a.length;t--;)delete h.prototype[a[t]];return h()};s[u]=!0,n.exports=Object.create||function(n,e){var t;return null!==n?(p.prototype=o(n),t=new p,p.prototype=null,t[u]=n):t=h(),void 0===e?t:i.f(t,e)}},function(n,e,t){var r=t(2);n.exports=r({}.isPrototypeOf)},function(n,e,t){var r=t(62),o=Function.prototype,i=o.apply,a=o.call;n.exports="object"==typeof Reflect&&Reflect.apply||(r?a.bind(i):function(){return a.apply(i,arguments)})},function(n,e,t){var r=t(0),o=t(4),i=t(83),a=r.TypeError;n.exports=function(n){if(o(n))return n;throw a(i(n)+" is not a function")}},function(n,e,t){var r,o,i,a=t(235),s=t(0),c=t(2),l=t(8),d=t(23),u=t(10),p=t(109),m=t(86),g=t(63),h=s.TypeError,b=s.WeakMap;if(a||p.state){var f=p.state||(p.state=new b),v=c(f.get),k=c(f.has),y=c(f.set);r=function(n,e){if(k(f,n))throw new h("Object already initialized");return e.facade=n,y(f,n,e),e},o=function(n){return v(f,n)||{}},i=function(n){return k(f,n)}}else{var x=m("state");g[x]=!0,r=function(n,e){if(u(n,x))throw new h("Object already initialized");return e.facade=n,d(n,x,e),e},o=function(n){return u(n,x)?n[x]:{}},i=function(n){return u(n,x)}}n.exports={set:r,get:o,has:i,enforce:function(n){return i(n)?o(n):r(n,{})},getterFor:function(n){return function(e){var t;if(!l(e)||(t=o(e)).type!==n)throw h("Incompatible receiver, "+n+" required");return t}}}},function(n,e,t){var r=t(9),o=t(11),i=t(114),a=t(49),s=t(20),c=t(81),l=t(10),d=t(154),u=Object.getOwnPropertyDescriptor;e.f=r?u:function(n,e){if(n=s(n),e=c(e),d)try{return u(n,e)}catch(n){}if(l(n,e))return a(!o(i.f,n,e),n[e])}},function(n,e,t){var r=t(1),o=t(0),i=t(36),a=t(261),s=o.WebAssembly,c=7!==Error("e",{cause:7}).cause,l=function(n,e){var t={};t[n]=a(n,e,c),r({global:!0,forced:c},t)},d=function(n,e){if(s&&s[n]){var t={};t[n]=a("WebAssembly."+n,e,c),r({target:"WebAssembly",stat:!0,forced:c},t)}};l("Error",(function(n){return function(e){return i(n,this,arguments)}})),l("EvalError",(function(n){return function(e){return i(n,this,arguments)}})),l("RangeError",(function(n){return function(e){return i(n,this,arguments)}})),l("ReferenceError",(function(n){return function(e){return i(n,this,arguments)}})),l("SyntaxError",(function(n){return function(e){return i(n,this,arguments)}})),l("TypeError",(function(n){return function(e){return i(n,this,arguments)}})),l("URIError",(function(n){return function(e){return i(n,this,arguments)}})),d("CompileError",(function(n){return function(e){return i(n,this,arguments)}})),d("LinkError",(function(n){return function(e){return i(n,this,arguments)}})),d("RuntimeError",(function(n){return function(e){return i(n,this,arguments)}}))},function(n,e,t){var r=t(285),o=t(288);n.exports=function(n,e){var t=o(n,e);return r(t)?t:void 0}},function(n,e,t){var r=t(1),o=t(0),i=t(36),a=t(4),s=t(27),c=t(67),l=t(143),d=/MSIE .\./.test(s),u=o.Function,p=function(n){return function(e,t){var r=l(arguments.length,1)>2,o=a(e)?e:u(e),s=r?c(arguments,2):void 0;return n(r?function(){i(o,this,s)}:o,t)}};r({global:!0,bind:!0,forced:d},{setTimeout:p(o.setTimeout),setInterval:p(o.setInterval)})},function(n,e,t){"use strict";t.d(e,"e",(function(){return r})),t.d(e,"b",(function(){return i})),t.d(e,"j",(function(){return a})),t.d(e,"g",(function(){return c})),t.d(e,"h",(function(){return l})),t.d(e,"i",(function(){return d})),t.d(e,"c",(function(){return u})),t.d(e,"f",(function(){return p})),t.d(e,"l",(function(){return m})),t.d(e,"m",(function(){return g})),t.d(e,"d",(function(){return b})),t.d(e,"k",(function(){return f})),t.d(e,"n",(function(){return v})),t.d(e,"a",(function(){return y}));t(18),t(45),t(135),t(74),t(133),t(108),t(44),t(32),t(5),t(33),t(30),t(77),t(73),t(151),t(72),t(207),t(25),t(139);var r=/#.*$/,o=/\.(md|html)$/,i=/\/$/,a=/^[a-z]+:/i;function s(n){return decodeURI(n).replace(r,"").replace(o,"")}function c(n){return a.test(n)}function l(n){return/^mailto:/.test(n)}function d(n){return/^tel:/.test(n)}function u(n){if(c(n))return n;var e=n.match(r),t=e?e[0]:"",o=s(n);return i.test(o)?n:o+".html"+t}function p(n,e){var t=n.hash,o=function(n){var e=n.match(r);if(e)return e[0]}(e);return(!o||t===o)&&s(n.path)===s(e)}function m(n,e,t){if(c(e))return{type:"external",path:e};t&&(e=function(n,e,t){var r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;var o=e.split("/");t&&o[o.length-1]||o.pop();for(var i=n.replace(/^\//,"").split("/"),a=0;a<i.length;a++){var s=i[a];".."===s?o.pop():"."!==s&&o.push(s)}""!==o[0]&&o.unshift("");return o.join("/")}(e,t));for(var r=s(e),o=0;o<n.length;o++)if(s(n[o].regularPath)===r)return Object.assign({},n[o],{type:"page",path:u(n[o].path)});return console.error('[vuepress] No matching page found for sidebar item "'.concat(e,'"')),{}}function g(n,e,t,r){var o=t.pages,i=t.themeConfig,a=r&&i.locales&&i.locales[r]||i;if("auto"===(n.frontmatter.sidebar||a.sidebar||i.sidebar))return h(n);var s=a.sidebar||i.sidebar;if(s){var c=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(var t in e)if(0===(r=n,/(\.html|\/)$/.test(r)?r:r+"/").indexOf(encodeURI(t)))return{base:t,config:e[t]};var r;return{}}(e,s),l=c.base,d=c.config;return"auto"===d?h(n):d?d.map((function(n){return function n(e,t,r){var o=arguments.length>3&&void 0!==arguments[3]?arguments[3]:1;if("string"==typeof e)return m(t,e,r);if(Array.isArray(e))return Object.assign(m(t,e[0],r),{title:e[1]});o>3&&console.error("[vuepress] detected a too deep nested sidebar group.");var i=e.children||[];return 0===i.length&&e.path?Object.assign(m(t,e.path,r),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,initialOpenGroupIndex:e.initialOpenGroupIndex,children:i.map((function(e){return n(e,t,r,o+1)})),collapsable:!1!==e.collapsable}}(n,o,l)})):[]}return[]}function h(n){var e=b(n.headers||[]);return[{type:"group",collapsable:!1,title:n.title,path:null,children:e.map((function(e){return{type:"auto",title:e.title,basePath:n.path,path:n.path+"#"+e.slug,children:e.children||[]}}))}]}function b(n){var e;return(n=n.map((function(n){return Object.assign({},n)}))).forEach((function(n){2===n.level?e=n:e&&(e.children||(e.children=[])).push(n)})),n.filter((function(n){return 2===n.level}))}function f(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function v(n){return Object.prototype.toString.call(n).match(/\[object (.*?)\]/)[1].toLowerCase()}function k(n){var e=n.frontmatter.date||n.lastUpdated||new Date,t=new Date(e);return"Invalid Date"==t&&e&&(t=new Date(e.replace(/-/g,"/"))),t.getTime()}function y(n,e){return k(e)-k(n)}},function(n,e,t){"use strict";var r=t(1),o=t(56).map;r({target:"Array",proto:!0,forced:!t(68)("map")},{map:function(n){return o(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){"use strict";var r=t(36),o=t(11),i=t(2),a=t(106),s=t(3),c=t(7),l=t(4),d=t(46),u=t(52),p=t(12),m=t(17),g=t(122),h=t(48),b=t(266),f=t(107),v=t(6)("replace"),k=Math.max,y=Math.min,x=i([].concat),w=i([].push),S=i("".indexOf),_=i("".slice),E="$0"==="a".replace(/./,"$0"),T=!!/./[v]&&""===/./[v]("a","$0");a("replace",(function(n,e,t){var i=T?"$":"$0";return[function(n,t){var r=m(this),i=null==n?void 0:h(n,v);return i?o(i,n,r,t):o(e,p(r),n,t)},function(n,o){var a=c(this),s=p(n);if("string"==typeof o&&-1===S(o,i)&&-1===S(o,"$<")){var m=t(e,a,s,o);if(m.done)return m.value}var h=l(o);h||(o=p(o));var v=a.global;if(v){var E=a.unicode;a.lastIndex=0}for(var T=[];;){var I=f(a,s);if(null===I)break;if(w(T,I),!v)break;""===p(I[0])&&(a.lastIndex=g(s,u(a.lastIndex),E))}for(var j,A="",C=0,D=0;D<T.length;D++){for(var z=p((I=T[D])[0]),M=k(y(d(I.index),s.length),0),P=[],O=1;O<I.length;O++)w(P,void 0===(j=I[O])?j:String(j));var R=I.groups;if(h){var N=x([z],P,M,s);void 0!==R&&w(N,R);var L=p(r(o,void 0,N))}else L=b(z,s,M,P,R,o);M>=C&&(A+=_(s,C,M)+L,C=M+z.length)}return A+_(s,C)}]}),!!s((function(){var n=/./;return n.exec=function(){var n=[];return n.groups={a:"7"},n},"7"!=="".replace(n,"$<a>")}))||!E||T)},function(n,e){var t=Math.ceil,r=Math.floor;n.exports=function(n){var e=+n;return e!=e||0===e?0:(e>0?r:t)(e)}},function(n,e,t){"use strict";var r=t(3);n.exports=function(n,e){var t=[][n];return!!t&&r((function(){t.call(null,e||function(){return 1},1)}))}},function(n,e,t){var r=t(37);n.exports=function(n,e){var t=n[e];return null==t?void 0:r(t)}},function(n,e){n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){var r,o,i=t(0),a=t(27),s=i.process,c=i.Deno,l=s&&s.versions||c&&c.version,d=l&&l.v8;d&&(o=(r=d.split("."))[0]>0&&r[0]<4?1:+(r[0]+r[1])),!o&&a&&(!(r=a.match(/Edge\/(\d+)/))||r[1]>=74)&&(r=a.match(/Chrome\/(\d+)/))&&(o=+r[1]),n.exports=o},function(n,e,t){var r=t(46),o=Math.min;n.exports=function(n){return n>0?o(r(n),9007199254740991):0}},function(n,e,t){"use strict";var r=t(81),o=t(13),i=t(49);n.exports=function(n,e,t){var a=r(e);a in n?o.f(n,a,i(0,t)):n[a]=t}},function(n,e,t){"use strict";var r=t(1),o=t(0),i=t(59),a=t(88),s=t(8),c=t(85),l=t(19),d=t(20),u=t(53),p=t(6),m=t(68),g=t(67),h=m("slice"),b=p("species"),f=o.Array,v=Math.max;r({target:"Array",proto:!0,forced:!h},{slice:function(n,e){var t,r,o,p=d(this),m=l(p),h=c(n,m),k=c(void 0===e?m:e,m);if(i(p)&&(t=p.constructor,(a(t)&&(t===f||i(t.prototype))||s(t)&&null===(t=t[b]))&&(t=void 0),t===f||void 0===t))return g(p,h,k);for(r=new(void 0===t?f:t)(v(k-h,0)),o=0;h<k;h++,o++)h in p&&u(r,o,p[h]);return r.length=o,r}})},function(n,e,t){var r=t(2),o=t(37),i=t(62),a=r(r.bind);n.exports=function(n,e){return o(n),void 0===e?n:i?a(n,e):function(){return n.apply(e,arguments)}}},function(n,e,t){var r=t(55),o=t(2),i=t(61),a=t(15),s=t(19),c=t(103),l=o([].push),d=function(n){var e=1==n,t=2==n,o=3==n,d=4==n,u=6==n,p=7==n,m=5==n||u;return function(g,h,b,f){for(var v,k,y=a(g),x=i(y),w=r(h,b),S=s(x),_=0,E=f||c,T=e?E(g,S):t||p?E(g,0):void 0;S>_;_++)if((m||_ in x)&&(k=w(v=x[_],_,y),n))if(e)T[_]=k;else if(k)switch(n){case 3:return!0;case 5:return v;case 6:return _;case 2:l(T,v)}else switch(n){case 4:return!1;case 7:l(T,v)}return u?-1:o||d?d:T}};n.exports={forEach:d(0),map:d(1),filter:d(2),some:d(3),every:d(4),find:d(5),findIndex:d(6),filterReject:d(7)}},function(n,e,t){var r=t(156),o=t(113).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return r(n,o)}},function(n,e,t){var r=t(13).f,o=t(10),i=t(6)("toStringTag");n.exports=function(n,e,t){n&&!t&&(n=n.prototype),n&&!o(n,i)&&r(n,i,{configurable:!0,value:e})}},function(n,e,t){var r=t(21);n.exports=Array.isArray||function(n){return"Array"==r(n)}},function(n,e,t){var r=t(69),o=t(270),i=t(271),a=r?r.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":a&&a in Object(n)?o(n):i(n)}},function(n,e,t){var r=t(0),o=t(2),i=t(3),a=t(21),s=r.Object,c=o("".split);n.exports=i((function(){return!s("z").propertyIsEnumerable(0)}))?function(n){return"String"==a(n)?c(n,""):s(n)}:s},function(n,e,t){var r=t(3);n.exports=!r((function(){var n=function(){}.bind();return"function"!=typeof n||n.hasOwnProperty("prototype")}))},function(n,e){n.exports={}},function(n,e){n.exports={}},function(n,e,t){var r=t(9),o=t(10),i=Function.prototype,a=r&&Object.getOwnPropertyDescriptor,s=o(i,"name"),c=s&&"something"===function(){}.name,l=s&&(!r||r&&a(i,"name").configurable);n.exports={EXISTS:s,PROPER:c,CONFIGURABLE:l}},function(n,e,t){var r=t(2),o=t(7),i=t(236);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=r(Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set))(t,[]),e=t instanceof Array}catch(n){}return function(t,r){return o(t),i(r),e?n(t,r):t.__proto__=r,t}}():void 0)},function(n,e,t){var r=t(2);n.exports=r([].slice)},function(n,e,t){var r=t(3),o=t(6),i=t(51),a=o("species");n.exports=function(n){return i>=51||!r((function(){var e=[];return(e.constructor={})[a]=function(){return{foo:1}},1!==e[n](Boolean).foo}))}},function(n,e,t){var r=t(28).Symbol;n.exports=r},function(n,e,t){"use strict";t.d(e,"a",(function(){return i}));t(77);var r=t(71);t(76),t(91),t(5),t(121),t(24),t(31),t(182);var o=t(98);t(40),t(25);function i(n){return function(n){if(Array.isArray(n))return Object(r.a)(n)}(n)||function(n){if("undefined"!=typeof Symbol&&null!=n[Symbol.iterator]||null!=n["@@iterator"])return Array.from(n)}(n)||Object(o.a)(n)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){"use strict";function r(n,e){(null==e||e>n.length)&&(e=n.length);for(var t=0,r=new Array(e);t<e;t++)r[t]=n[t];return r}t.d(e,"a",(function(){return r}))},function(n,e,t){"use strict";var r=t(1),o=t(0),i=t(3),a=t(59),s=t(8),c=t(15),l=t(19),d=t(53),u=t(103),p=t(68),m=t(6),g=t(51),h=m("isConcatSpreadable"),b=o.TypeError,f=g>=51||!i((function(){var n=[];return n[h]=!1,n.concat()[0]!==n})),v=p("concat"),k=function(n){if(!s(n))return!1;var e=n[h];return void 0!==e?!!e:a(n)};r({target:"Array",proto:!0,forced:!f||!v},{concat:function(n){var e,t,r,o,i,a=c(this),s=u(a,0),p=0;for(e=-1,r=arguments.length;e<r;e++)if(k(i=-1===e?a:arguments[e])){if(p+(o=l(i))>9007199254740991)throw b("Maximum allowed index exceeded");for(t=0;t<o;t++,p++)t in i&&d(s,p,i[t])}else{if(p>=9007199254740991)throw b("Maximum allowed index exceeded");d(s,p++,i)}return s.length=p,s}})},function(n,e,t){"use strict";var r=t(1),o=t(2),i=t(112).indexOf,a=t(47),s=o([].indexOf),c=!!s&&1/s([1],1,-0)<0,l=a("indexOf");r({target:"Array",proto:!0,forced:c||!l},{indexOf:function(n){var e=arguments.length>1?arguments[1]:void 0;return c?s(this,n,e)||0:i(this,n,e)}})},function(n,e,t){"use strict";t(18);var r,o,i=t(1),a=t(0),s=t(11),c=t(2),l=t(4),d=t(8),u=(r=!1,(o=/[ac]/).exec=function(){return r=!0,/./.exec.apply(this,arguments)},!0===o.test("abc")&&r),p=a.Error,m=c(/./.test);i({target:"RegExp",proto:!0,forced:!u},{test:function(n){var e=this.exec;if(!l(e))return m(this,n);var t=s(e,this,n);if(null!==t&&!d(t))throw new p("RegExp exec method returned something other than an Object or null");return!!t}})},function(n,e,t){var r=t(0),o=t(118),i=t(4),a=t(21),s=t(6)("toStringTag"),c=r.Object,l="Arguments"==a(function(){return arguments}());n.exports=o?a:function(n){var e,t,r;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=c(n),s))?t:l?a(e):"Object"==(r=a(e))&&i(e.callee)?"Arguments":r}},function(n,e,t){"use strict";var r=t(1),o=t(0),i=t(16),a=t(36),s=t(11),c=t(2),l=t(22),d=t(9),u=t(111),p=t(3),m=t(10),g=t(59),h=t(4),b=t(8),f=t(35),v=t(82),k=t(7),y=t(15),x=t(20),w=t(81),S=t(12),_=t(49),E=t(34),T=t(84),I=t(57),j=t(179),A=t(116),C=t(39),D=t(13),z=t(141),M=t(114),P=t(67),O=t(14),R=t(78),N=t(86),L=t(63),q=t(79),B=t(6),U=t(180),$=t(181),G=t(58),F=t(38),H=t(56).forEach,K=N("hidden"),J=B("toPrimitive"),V=F.set,W=F.getterFor("Symbol"),Q=Object.prototype,Y=o.Symbol,X=Y&&Y.prototype,Z=o.TypeError,nn=o.QObject,en=i("JSON","stringify"),tn=C.f,rn=D.f,on=j.f,an=M.f,sn=c([].push),cn=R("symbols"),ln=R("op-symbols"),dn=R("string-to-symbol-registry"),un=R("symbol-to-string-registry"),pn=R("wks"),mn=!nn||!nn.prototype||!nn.prototype.findChild,gn=d&&p((function(){return 7!=E(rn({},"a",{get:function(){return rn(this,"a",{value:7}).a}})).a}))?function(n,e,t){var r=tn(Q,e);r&&delete Q[e],rn(n,e,t),r&&n!==Q&&rn(Q,e,r)}:rn,hn=function(n,e){var t=cn[n]=E(X);return V(t,{type:"Symbol",tag:n,description:e}),d||(t.description=e),t},bn=function(n,e,t){n===Q&&bn(ln,e,t),k(n);var r=w(e);return k(t),m(cn,r)?(t.enumerable?(m(n,K)&&n[K][r]&&(n[K][r]=!1),t=E(t,{enumerable:_(0,!1)})):(m(n,K)||rn(n,K,_(1,{})),n[K][r]=!0),gn(n,r,t)):rn(n,r,t)},fn=function(n,e){k(n);var t=x(e),r=T(t).concat(xn(t));return H(r,(function(e){d&&!s(vn,t,e)||bn(n,e,t[e])})),n},vn=function(n){var e=w(n),t=s(an,this,e);return!(this===Q&&m(cn,e)&&!m(ln,e))&&(!(t||!m(this,e)||!m(cn,e)||m(this,K)&&this[K][e])||t)},kn=function(n,e){var t=x(n),r=w(e);if(t!==Q||!m(cn,r)||m(ln,r)){var o=tn(t,r);return!o||!m(cn,r)||m(t,K)&&t[K][r]||(o.enumerable=!0),o}},yn=function(n){var e=on(x(n)),t=[];return H(e,(function(n){m(cn,n)||m(L,n)||sn(t,n)})),t},xn=function(n){var e=n===Q,t=on(e?ln:x(n)),r=[];return H(t,(function(n){!m(cn,n)||e&&!m(Q,n)||sn(r,cn[n])})),r};(u||(O(X=(Y=function(){if(f(X,this))throw Z("Symbol is not a constructor");var n=arguments.length&&void 0!==arguments[0]?S(arguments[0]):void 0,e=q(n),t=function(n){this===Q&&s(t,ln,n),m(this,K)&&m(this[K],e)&&(this[K][e]=!1),gn(this,e,_(1,n))};return d&&mn&&gn(Q,e,{configurable:!0,set:t}),hn(e,n)}).prototype,"toString",(function(){return W(this).tag})),O(Y,"withoutSetter",(function(n){return hn(q(n),n)})),M.f=vn,D.f=bn,z.f=fn,C.f=kn,I.f=j.f=yn,A.f=xn,U.f=function(n){return hn(B(n),n)},d&&(rn(X,"description",{configurable:!0,get:function(){return W(this).description}}),l||O(Q,"propertyIsEnumerable",vn,{unsafe:!0}))),r({global:!0,wrap:!0,forced:!u,sham:!u},{Symbol:Y}),H(T(pn),(function(n){$(n)})),r({target:"Symbol",stat:!0,forced:!u},{for:function(n){var e=S(n);if(m(dn,e))return dn[e];var t=Y(e);return dn[e]=t,un[t]=e,t},keyFor:function(n){if(!v(n))throw Z(n+" is not a symbol");if(m(un,n))return un[n]},useSetter:function(){mn=!0},useSimple:function(){mn=!1}}),r({target:"Object",stat:!0,forced:!u,sham:!d},{create:function(n,e){return void 0===e?E(n):fn(E(n),e)},defineProperty:bn,defineProperties:fn,getOwnPropertyDescriptor:kn}),r({target:"Object",stat:!0,forced:!u},{getOwnPropertyNames:yn,getOwnPropertySymbols:xn}),r({target:"Object",stat:!0,forced:p((function(){A.f(1)}))},{getOwnPropertySymbols:function(n){return A.f(y(n))}}),en)&&r({target:"JSON",stat:!0,forced:!u||p((function(){var n=Y();return"[null]"!=en([n])||"{}"!=en({a:n})||"{}"!=en(Object(n))}))},{stringify:function(n,e,t){var r=P(arguments),o=e;if((b(e)||void 0!==n)&&!v(n))return g(e)||(e=function(n,e){if(h(o)&&(e=s(o,this,n,e)),!v(e))return e}),r[1]=e,a(en,null,r)}});if(!X[J]){var wn=X.valueOf;O(X,J,(function(n){return s(wn,this)}))}G(Y,"Symbol"),L[K]=!0},function(n,e,t){t(1)({target:"Array",stat:!0},{isArray:t(59)})},function(n,e,t){var r=t(22),o=t(109);(n.exports=function(n,e){return o[n]||(o[n]=void 0!==e?e:{})})("versions",[]).push({version:"3.21.1",mode:r?"pure":"global",copyright:"© 2014-2022 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.21.1/LICENSE",source:"https://github.com/zloirock/core-js"})},function(n,e,t){var r=t(2),o=0,i=Math.random(),a=r(1..toString);n.exports=function(n){return"Symbol("+(void 0===n?"":n)+")_"+a(++o+i,36)}},function(n,e,t){var r=t(0),o=t(8),i=r.document,a=o(i)&&o(i.createElement);n.exports=function(n){return a?i.createElement(n):{}}},function(n,e,t){var r=t(155),o=t(82);n.exports=function(n){var e=r(n,"string");return o(e)?e:e+""}},function(n,e,t){var r=t(0),o=t(16),i=t(4),a=t(35),s=t(152),c=r.Object;n.exports=s?function(n){return"symbol"==typeof n}:function(n){var e=o("Symbol");return i(e)&&a(e.prototype,c(n))}},function(n,e,t){var r=t(0).String;n.exports=function(n){try{return r(n)}catch(n){return"Object"}}},function(n,e,t){var r=t(156),o=t(113);n.exports=Object.keys||function(n){return r(n,o)}},function(n,e,t){var r=t(46),o=Math.max,i=Math.min;n.exports=function(n,e){var t=r(n);return t<0?o(t+e,0):i(t,e)}},function(n,e,t){var r=t(78),o=t(79),i=r("keys");n.exports=function(n){return i[n]||(i[n]=o(n))}},function(n,e,t){var r=t(2),o=t(4),i=t(109),a=r(Function.toString);o(i.inspectSource)||(i.inspectSource=function(n){return a(n)}),n.exports=i.inspectSource},function(n,e,t){var r=t(2),o=t(3),i=t(4),a=t(75),s=t(16),c=t(87),l=function(){},d=[],u=s("Reflect","construct"),p=/^\s*(?:class|function)\b/,m=r(p.exec),g=!p.exec(l),h=function(n){if(!i(n))return!1;try{return u(l,d,n),!0}catch(n){return!1}},b=function(n){if(!i(n))return!1;switch(a(n)){case"AsyncFunction":case"GeneratorFunction":case"AsyncGeneratorFunction":return!1}try{return g||!!m(p,c(n))}catch(n){return!0}};b.sham=!0,n.exports=!u||o((function(){var n;return h(h.call)||!h(Object)||!h((function(){n=!0}))||n}))?b:h},function(n,e,t){var r=t(21),o=t(0);n.exports="process"==r(o.process)},function(n,e,t){"use strict";t.d(e,"a",(function(){return o}));t(5);function r(n,e,t,r,o,i,a){try{var s=n[i](a),c=s.value}catch(n){return void t(n)}s.done?e(c):Promise.resolve(c).then(r,o)}function o(n){return function(){var e=this,t=arguments;return new Promise((function(o,i){var a=n.apply(e,t);function s(n){r(a,o,i,s,c,"next",n)}function c(n){r(a,o,i,s,c,"throw",n)}s(void 0)}))}}},function(n,e,t){"use strict";var r=t(1),o=t(9),i=t(0),a=t(2),s=t(10),c=t(4),l=t(35),d=t(12),u=t(13).f,p=t(115),m=i.Symbol,g=m&&m.prototype;if(o&&c(m)&&(!("description"in g)||void 0!==m().description)){var h={},b=function(){var n=arguments.length<1||void 0===arguments[0]?void 0:d(arguments[0]),e=l(g,this)?new m(n):void 0===n?m():m(n);return""===n&&(h[e]=!0),e};p(b,m),b.prototype=g,g.constructor=b;var f="Symbol(test)"==String(m("test")),v=a(g.toString),k=a(g.valueOf),y=/^Symbol\((.*)\)[^)]+$/,x=a("".replace),w=a("".slice);u(g,"description",{configurable:!0,get:function(){var n=k(this),e=v(n);if(s(h,n))return"";var t=f?w(e,7,-1):x(e,y,"$1");return""===t?void 0:t}}),r({global:!0,forced:!0},{Symbol:b})}},function(n,e,t){"use strict";var r,o,i=t(11),a=t(2),s=t(12),c=t(145),l=t(105),d=t(78),u=t(34),p=t(38).get,m=t(220),g=t(226),h=d("native-string-replace",String.prototype.replace),b=RegExp.prototype.exec,f=b,v=a("".charAt),k=a("".indexOf),y=a("".replace),x=a("".slice),w=(o=/b*/g,i(b,r=/a/,"a"),i(b,o,"a"),0!==r.lastIndex||0!==o.lastIndex),S=l.BROKEN_CARET,_=void 0!==/()??/.exec("")[1];(w||_||S||m||g)&&(f=function(n){var e,t,r,o,a,l,d,m=this,g=p(m),E=s(n),T=g.raw;if(T)return T.lastIndex=m.lastIndex,e=i(f,T,E),m.lastIndex=T.lastIndex,e;var I=g.groups,j=S&&m.sticky,A=i(c,m),C=m.source,D=0,z=E;if(j&&(A=y(A,"y",""),-1===k(A,"g")&&(A+="g"),z=x(E,m.lastIndex),m.lastIndex>0&&(!m.multiline||m.multiline&&"\n"!==v(E,m.lastIndex-1))&&(C="(?: "+C+")",z=" "+z,D++),t=new RegExp("^(?:"+C+")",A)),_&&(t=new RegExp("^"+C+"$(?!\\s)",A)),w&&(r=m.lastIndex),o=i(b,j?t:m,z),j?o?(o.input=x(o.input,D),o[0]=x(o[0],D),o.index=m.lastIndex,m.lastIndex+=o[0].length):m.lastIndex=0:w&&o&&(m.lastIndex=m.global?o.index+o[0].length:r),_&&o&&o.length>1&&i(h,o[0],t,(function(){for(a=1;a<arguments.length-2;a++)void 0===arguments[a]&&(o[a]=void 0)})),o&&I)for(o.groups=l=u(null),a=0;a<I.length;a++)l[(d=I[a])[0]]=o[d[1]];return o}),n.exports=f},function(n,e,t){var r=t(275),o=t(276),i=t(277),a=t(278),s=t(279);function c(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}c.prototype.clear=r,c.prototype.delete=o,c.prototype.get=i,c.prototype.has=a,c.prototype.set=s,n.exports=c},function(n,e,t){var r=t(189);n.exports=function(n,e){for(var t=n.length;t--;)if(r(n[t][0],e))return t;return-1}},function(n,e,t){var r=t(41)(Object,"create");n.exports=r},function(n,e,t){var r=t(297);n.exports=function(n,e){var t=n.__data__;return r(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var r=t(130);n.exports=function(n){if("string"==typeof n||r(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e,t){"use strict";t.d(e,"a",(function(){return o}));t(54),t(5),t(104),t(182),t(24),t(18),t(74);var r=t(71);function o(n,e){if(n){if("string"==typeof n)return Object(r.a)(n,e);var t=Object.prototype.toString.call(n).slice(8,-1);return"Object"===t&&n.constructor&&(t=n.constructor.name),"Map"===t||"Set"===t?Array.from(n):"Arguments"===t||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t)?Object(r.a)(n,e):void 0}}},function(n,e,t){var r,o;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(o="function"==typeof(r=function(){var n,e,t={version:"0.2.0"},r=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function o(n,e,t){return n<e?e:n>t?t:n}function i(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(r[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=o(n,r.minimum,1),t.status=1===n?null:n;var c=t.render(!e),l=c.querySelector(r.barSelector),d=r.speed,u=r.easing;return c.offsetWidth,a((function(e){""===r.positionUsing&&(r.positionUsing=t.getPositioningCSS()),s(l,function(n,e,t){var o;return(o="translate3d"===r.positionUsing?{transform:"translate3d("+i(n)+"%,0,0)"}:"translate"===r.positionUsing?{transform:"translate("+i(n)+"%,0)"}:{"margin-left":i(n)+"%"}).transition="all "+e+"ms "+t,o}(n,d,u)),1===n?(s(c,{transition:"none",opacity:1}),c.offsetWidth,setTimeout((function(){s(c,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),d)}),d)):setTimeout(e,d)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),r.trickleSpeed)};return r.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*o(Math.random()*e,.1,.95)),e=o(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*r.trickleRate)},n=0,e=0,t.promise=function(r){return r&&"resolved"!==r.state()?(0===e&&t.start(),n++,e++,r.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");l(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=r.template;var o,a=e.querySelector(r.barSelector),c=n?"-100":i(t.status||0),d=document.querySelector(r.parent);return s(a,{transition:"all 0 linear",transform:"translate3d("+c+"%,0,0)"}),r.showSpinner||(o=e.querySelector(r.spinnerSelector))&&p(o),d!=document.body&&l(d,"nprogress-custom-parent"),d.appendChild(e),e},t.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(r.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&p(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var a=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),s=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var r,o=n.length,i=e.charAt(0).toUpperCase()+e.slice(1);o--;)if((r=n[o]+i)in t)return r;return e}(t))}function r(n,e,r){e=t(e),n.style[e]=r}return function(n,e){var t,o,i=arguments;if(2==i.length)for(t in e)void 0!==(o=e[t])&&e.hasOwnProperty(t)&&r(n,t,o);else r(n,i[1],i[2])}}();function c(n,e){return("string"==typeof n?n:u(n)).indexOf(" "+e+" ")>=0}function l(n,e){var t=u(n),r=t+e;c(t,e)||(n.className=r.substring(1))}function d(n,e){var t,r=u(n);c(n,e)&&(t=r.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function u(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function p(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?r.call(e,t,e,n):r)||(n.exports=o)},function(n,e,t){var r=t(1),o=t(15),i=t(84);r({target:"Object",stat:!0,forced:t(3)((function(){i(1)}))},{keys:function(n){return i(o(n))}})},function(n,e,t){var r=t(3),o=t(4),i=/#|\.prototype\./,a=function(n,e){var t=c[s(n)];return t==d||t!=l&&(o(e)?r(e):!!e)},s=a.normalize=function(n){return String(n).replace(i,".").toLowerCase()},c=a.data={},l=a.NATIVE="N",d=a.POLYFILL="P";n.exports=a},function(n,e,t){var r=t(75),o=t(48),i=t(64),a=t(6)("iterator");n.exports=function(n){if(null!=n)return o(n,a)||o(n,"@@iterator")||i[r(n)]}},function(n,e,t){var r=t(250);n.exports=function(n,e){return new(r(n))(0===e?0:e)}},function(n,e,t){var r=t(9),o=t(65).EXISTS,i=t(2),a=t(13).f,s=Function.prototype,c=i(s.toString),l=/function\b(?:\s|\/\*[\S\s]*?\*\/|\/\/[^\n\r]*[\n\r]+)*([^\s(/]*)/,d=i(l.exec);r&&!o&&a(s,"name",{configurable:!0,get:function(){try{return d(l,c(this))[1]}catch(n){return""}}})},function(n,e,t){var r=t(3),o=t(0).RegExp,i=r((function(){var n=o("a","y");return n.lastIndex=2,null!=n.exec("abcd")})),a=i||r((function(){return!o("a","y").sticky})),s=i||r((function(){var n=o("^r","gy");return n.lastIndex=2,null!=n.exec("str")}));n.exports={BROKEN_CARET:s,MISSED_STICKY:a,UNSUPPORTED_Y:i}},function(n,e,t){"use strict";t(18);var r=t(2),o=t(14),i=t(92),a=t(3),s=t(6),c=t(23),l=s("species"),d=RegExp.prototype;n.exports=function(n,e,t,u){var p=s(n),m=!a((function(){var e={};return e[p]=function(){return 7},7!=""[n](e)})),g=m&&!a((function(){var e=!1,t=/a/;return"split"===n&&((t={}).constructor={},t.constructor[l]=function(){return t},t.flags="",t[p]=/./[p]),t.exec=function(){return e=!0,null},t[p](""),!e}));if(!m||!g||t){var h=r(/./[p]),b=e(p,""[n],(function(n,e,t,o,a){var s=r(n),c=e.exec;return c===i||c===d.exec?m&&!a?{done:!0,value:h(e,t,o)}:{done:!0,value:s(t,e,o)}:{done:!1}}));o(String.prototype,n,b[0]),o(d,p,b[1])}u&&c(d[p],"sham",!0)}},function(n,e,t){var r=t(0),o=t(11),i=t(7),a=t(4),s=t(21),c=t(92),l=r.TypeError;n.exports=function(n,e){var t=n.exec;if(a(t)){var r=o(t,n,e);return null!==r&&i(r),r}if("RegExp"===s(n))return o(c,n,e);throw l("RegExp#exec called on incompatible receiver")}},function(n,e,t){"use strict";var r=t(1),o=t(2),i=t(61),a=t(20),s=t(47),c=o([].join),l=i!=Object,d=s("join",",");r({target:"Array",proto:!0,forced:l||!d},{join:function(n){return c(a(this),void 0===n?",":n)}})},function(n,e,t){var r=t(0),o=t(110),i=r["__core-js_shared__"]||o("__core-js_shared__",{});n.exports=i},function(n,e,t){var r=t(0),o=Object.defineProperty;n.exports=function(n,e){try{o(r,n,{value:e,configurable:!0,writable:!0})}catch(t){r[n]=e}return e}},function(n,e,t){var r=t(51),o=t(3);n.exports=!!Object.getOwnPropertySymbols&&!o((function(){var n=Symbol();return!String(n)||!(Object(n)instanceof Symbol)||!Symbol.sham&&r&&r<41}))},function(n,e,t){var r=t(20),o=t(85),i=t(19),a=function(n){return function(e,t,a){var s,c=r(e),l=i(c),d=o(a,l);if(n&&t!=t){for(;l>d;)if((s=c[d++])!=s)return!0}else for(;l>d;d++)if((n||d in c)&&c[d]===t)return n||d||0;return!n&&-1}};n.exports={includes:a(!0),indexOf:a(!1)}},function(n,e){n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){"use strict";var r={}.propertyIsEnumerable,o=Object.getOwnPropertyDescriptor,i=o&&!r.call({1:2},1);e.f=i?function(n){var e=o(this,n);return!!e&&e.enumerable}:r},function(n,e,t){var r=t(10),o=t(159),i=t(39),a=t(13);n.exports=function(n,e,t){for(var s=o(e),c=a.f,l=i.f,d=0;d<s.length;d++){var u=s[d];r(n,u)||t&&r(t,u)||c(n,u,l(e,u))}}},function(n,e){e.f=Object.getOwnPropertySymbols},function(n,e,t){var r=t(0),o=t(10),i=t(4),a=t(15),s=t(86),c=t(161),l=s("IE_PROTO"),d=r.Object,u=d.prototype;n.exports=c?d.getPrototypeOf:function(n){var e=a(n);if(o(e,l))return e[l];var t=e.constructor;return i(t)&&e instanceof t?t.prototype:e instanceof d?u:null}},function(n,e,t){var r={};r[t(6)("toStringTag")]="z",n.exports="[object z]"===String(r)},function(n,e,t){var r=t(7),o=t(166),i=t(6)("species");n.exports=function(n,e){var t,a=r(n).constructor;return void 0===a||null==(t=r(a)[i])?e:o(t)}},function(n,e,t){var r=t(0),o=t(85),i=t(19),a=t(53),s=r.Array,c=Math.max;n.exports=function(n,e,t){for(var r=i(n),l=o(e,r),d=o(void 0===t?r:t,r),u=s(c(d-l,0)),p=0;l<d;l++,p++)a(u,p,n[l]);return u.length=p,u}},function(n,e,t){t(181)("iterator")},function(n,e,t){"use strict";var r=t(171).charAt;n.exports=function(n,e,t){return e+(t?r(n,e).length:1)}},function(n,e,t){var r=t(269),o=t(50),i=Object.prototype,a=i.hasOwnProperty,s=i.propertyIsEnumerable,c=r(function(){return arguments}())?r:function(n){return o(n)&&a.call(n,"callee")&&!s.call(n,"callee")};n.exports=c},function(n,e,t){var r=t(41)(t(28),"Map");n.exports=r},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var r=t(289),o=t(296),i=t(298),a=t(299),s=t(300);function c(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}c.prototype.clear=r,c.prototype.delete=o,c.prototype.get=i,c.prototype.has=a,c.prototype.set=s,n.exports=c},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var r=t(26),o=t(130),i=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,a=/^\w*$/;n.exports=function(n,e){if(r(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!o(n))||(a.test(n)||!i.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var r=t(60),o=t(50);n.exports=function(n){return"symbol"==typeof n||o(n)&&"[object Symbol]"==r(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){"use strict";t.d(e,"a",(function(){return o}));t(77);t(76),t(91),t(5),t(121),t(24),t(31);var r=t(98);t(40),t(25);function o(n,e){return function(n){if(Array.isArray(n))return n}(n)||function(n,e){var t=null==n?null:"undefined"!=typeof Symbol&&n[Symbol.iterator]||n["@@iterator"];if(null!=t){var r,o,i=[],a=!0,s=!1;try{for(t=t.call(n);!(a=(r=t.next()).done)&&(i.push(r.value),!e||i.length!==e);a=!0);}catch(n){s=!0,o=n}finally{try{a||null==t.return||t.return()}finally{if(s)throw o}}return i}}(n,e)||Object(r.a)(n,e)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){"use strict";var r=t(36),o=t(11),i=t(2),a=t(106),s=t(144),c=t(7),l=t(17),d=t(119),u=t(122),p=t(52),m=t(12),g=t(48),h=t(120),b=t(107),f=t(92),v=t(105),k=t(3),y=v.UNSUPPORTED_Y,x=Math.min,w=[].push,S=i(/./.exec),_=i(w),E=i("".slice);a("split",(function(n,e,t){var i;return i="c"=="abbc".split(/(b)*/)[1]||4!="test".split(/(?:)/,-1).length||2!="ab".split(/(?:ab)*/).length||4!=".".split(/(.?)(.?)/).length||".".split(/()()/).length>1||"".split(/.?/).length?function(n,t){var i=m(l(this)),a=void 0===t?4294967295:t>>>0;if(0===a)return[];if(void 0===n)return[i];if(!s(n))return o(e,i,n,a);for(var c,d,u,p=[],g=(n.ignoreCase?"i":"")+(n.multiline?"m":"")+(n.unicode?"u":"")+(n.sticky?"y":""),b=0,v=new RegExp(n.source,g+"g");(c=o(f,v,i))&&!((d=v.lastIndex)>b&&(_(p,E(i,b,c.index)),c.length>1&&c.index<i.length&&r(w,p,h(c,1)),u=c[0].length,b=d,p.length>=a));)v.lastIndex===c.index&&v.lastIndex++;return b===i.length?!u&&S(v,"")||_(p,""):_(p,E(i,b)),p.length>a?h(p,0,a):p}:"0".split(void 0,0).length?function(n,t){return void 0===n&&0===t?[]:o(e,this,n,t)}:e,[function(e,t){var r=l(this),a=null==e?void 0:g(e,n);return a?o(a,e,r,t):o(i,m(r),e,t)},function(n,r){var o=c(this),a=m(n),s=t(i,o,a,r,i!==e);if(s.done)return s.value;var l=d(o,RegExp),g=o.unicode,h=(o.ignoreCase?"i":"")+(o.multiline?"m":"")+(o.unicode?"u":"")+(y?"g":"y"),f=new l(y?"^(?:"+o.source+")":o,h),v=void 0===r?4294967295:r>>>0;if(0===v)return[];if(0===a.length)return null===b(f,a)?[a]:[];for(var k=0,w=0,S=[];w<a.length;){f.lastIndex=y?0:w;var T,I=b(f,y?E(a,w):a);if(null===I||(T=x(p(f.lastIndex+(y?w:0)),a.length))===k)w=u(a,w,g);else{if(_(S,E(a,k,w)),S.length===v)return S;for(var j=1;j<=I.length-1;j++)if(_(S,I[j]),S.length===v)return S;w=k=T}}return _(S,E(a,k)),S}]}),!!k((function(){var n=/(?:)/,e=n.exec;n.exec=function(){return e.apply(this,arguments)};var t="ab".split(n);return 2!==t.length||"a"!==t[0]||"b"!==t[1]})),y)},function(n,e,t){"use strict";var r=t(1),o=t(56).some;r({target:"Array",proto:!0,forced:!t(47)("some")},{some:function(n){return o(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){"use strict";var r=t(11),o=t(106),i=t(7),a=t(52),s=t(12),c=t(17),l=t(48),d=t(122),u=t(107);o("match",(function(n,e,t){return[function(e){var t=c(this),o=null==e?void 0:l(e,n);return o?r(o,e,t):new RegExp(e)[n](s(t))},function(n){var r=i(this),o=s(n),c=t(e,r,o);if(c.done)return c.value;if(!r.global)return u(r,o);var l=r.unicode;r.lastIndex=0;for(var p,m=[],g=0;null!==(p=u(r,o));){var h=s(p[0]);m[g]=h,""===h&&(r.lastIndex=d(o,a(r.lastIndex),l)),g++}return 0===g?null:m}]}))},function(n,e,t){var r=t(6),o=t(34),i=t(13),a=r("unscopables"),s=Array.prototype;null==s[a]&&i.f(s,a,{configurable:!0,value:o(null)}),n.exports=function(n){s[a][n]=!0}},function(n,e,t){var r=function(n){"use strict";var e=Object.prototype,t=e.hasOwnProperty,r="function"==typeof Symbol?Symbol:{},o=r.iterator||"@@iterator",i=r.asyncIterator||"@@asyncIterator",a=r.toStringTag||"@@toStringTag";function s(n,e,t){return Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}),n[e]}try{s({},"")}catch(n){s=function(n,e,t){return n[e]=t}}function c(n,e,t,r){var o=e&&e.prototype instanceof u?e:u,i=Object.create(o.prototype),a=new S(r||[]);return i._invoke=function(n,e,t){var r="suspendedStart";return function(o,i){if("executing"===r)throw new Error("Generator is already running");if("completed"===r){if("throw"===o)throw i;return E()}for(t.method=o,t.arg=i;;){var a=t.delegate;if(a){var s=y(a,t);if(s){if(s===d)continue;return s}}if("next"===t.method)t.sent=t._sent=t.arg;else if("throw"===t.method){if("suspendedStart"===r)throw r="completed",t.arg;t.dispatchException(t.arg)}else"return"===t.method&&t.abrupt("return",t.arg);r="executing";var c=l(n,e,t);if("normal"===c.type){if(r=t.done?"completed":"suspendedYield",c.arg===d)continue;return{value:c.arg,done:t.done}}"throw"===c.type&&(r="completed",t.method="throw",t.arg=c.arg)}}}(n,t,a),i}function l(n,e,t){try{return{type:"normal",arg:n.call(e,t)}}catch(n){return{type:"throw",arg:n}}}n.wrap=c;var d={};function u(){}function p(){}function m(){}var g={};s(g,o,(function(){return this}));var h=Object.getPrototypeOf,b=h&&h(h(_([])));b&&b!==e&&t.call(b,o)&&(g=b);var f=m.prototype=u.prototype=Object.create(g);function v(n){["next","throw","return"].forEach((function(e){s(n,e,(function(n){return this._invoke(e,n)}))}))}function k(n,e){var r;this._invoke=function(o,i){function a(){return new e((function(r,a){!function r(o,i,a,s){var c=l(n[o],n,i);if("throw"!==c.type){var d=c.arg,u=d.value;return u&&"object"==typeof u&&t.call(u,"__await")?e.resolve(u.__await).then((function(n){r("next",n,a,s)}),(function(n){r("throw",n,a,s)})):e.resolve(u).then((function(n){d.value=n,a(d)}),(function(n){return r("throw",n,a,s)}))}s(c.arg)}(o,i,r,a)}))}return r=r?r.then(a,a):a()}}function y(n,e){var t=n.iterator[e.method];if(void 0===t){if(e.delegate=null,"throw"===e.method){if(n.iterator.return&&(e.method="return",e.arg=void 0,y(n,e),"throw"===e.method))return d;e.method="throw",e.arg=new TypeError("The iterator does not provide a 'throw' method")}return d}var r=l(t,n.iterator,e.arg);if("throw"===r.type)return e.method="throw",e.arg=r.arg,e.delegate=null,d;var o=r.arg;return o?o.done?(e[n.resultName]=o.value,e.next=n.nextLoc,"return"!==e.method&&(e.method="next",e.arg=void 0),e.delegate=null,d):o:(e.method="throw",e.arg=new TypeError("iterator result is not an object"),e.delegate=null,d)}function x(n){var e={tryLoc:n[0]};1 in n&&(e.catchLoc=n[1]),2 in n&&(e.finallyLoc=n[2],e.afterLoc=n[3]),this.tryEntries.push(e)}function w(n){var e=n.completion||{};e.type="normal",delete e.arg,n.completion=e}function S(n){this.tryEntries=[{tryLoc:"root"}],n.forEach(x,this),this.reset(!0)}function _(n){if(n){var e=n[o];if(e)return e.call(n);if("function"==typeof n.next)return n;if(!isNaN(n.length)){var r=-1,i=function e(){for(;++r<n.length;)if(t.call(n,r))return e.value=n[r],e.done=!1,e;return e.value=void 0,e.done=!0,e};return i.next=i}}return{next:E}}function E(){return{value:void 0,done:!0}}return p.prototype=m,s(f,"constructor",m),s(m,"constructor",p),p.displayName=s(m,a,"GeneratorFunction"),n.isGeneratorFunction=function(n){var e="function"==typeof n&&n.constructor;return!!e&&(e===p||"GeneratorFunction"===(e.displayName||e.name))},n.mark=function(n){return Object.setPrototypeOf?Object.setPrototypeOf(n,m):(n.__proto__=m,s(n,a,"GeneratorFunction")),n.prototype=Object.create(f),n},n.awrap=function(n){return{__await:n}},v(k.prototype),s(k.prototype,i,(function(){return this})),n.AsyncIterator=k,n.async=function(e,t,r,o,i){void 0===i&&(i=Promise);var a=new k(c(e,t,r,o),i);return n.isGeneratorFunction(t)?a:a.next().then((function(n){return n.done?n.value:a.next()}))},v(f),s(f,a,"Generator"),s(f,o,(function(){return this})),s(f,"toString",(function(){return"[object Generator]"})),n.keys=function(n){var e=[];for(var t in n)e.push(t);return e.reverse(),function t(){for(;e.length;){var r=e.pop();if(r in n)return t.value=r,t.done=!1,t}return t.done=!0,t}},n.values=_,S.prototype={constructor:S,reset:function(n){if(this.prev=0,this.next=0,this.sent=this._sent=void 0,this.done=!1,this.delegate=null,this.method="next",this.arg=void 0,this.tryEntries.forEach(w),!n)for(var e in this)"t"===e.charAt(0)&&t.call(this,e)&&!isNaN(+e.slice(1))&&(this[e]=void 0)},stop:function(){this.done=!0;var n=this.tryEntries[0].completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(n){if(this.done)throw n;var e=this;function r(t,r){return a.type="throw",a.arg=n,e.next=t,r&&(e.method="next",e.arg=void 0),!!r}for(var o=this.tryEntries.length-1;o>=0;--o){var i=this.tryEntries[o],a=i.completion;if("root"===i.tryLoc)return r("end");if(i.tryLoc<=this.prev){var s=t.call(i,"catchLoc"),c=t.call(i,"finallyLoc");if(s&&c){if(this.prev<i.catchLoc)return r(i.catchLoc,!0);if(this.prev<i.finallyLoc)return r(i.finallyLoc)}else if(s){if(this.prev<i.catchLoc)return r(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return r(i.finallyLoc)}}}},abrupt:function(n,e){for(var r=this.tryEntries.length-1;r>=0;--r){var o=this.tryEntries[r];if(o.tryLoc<=this.prev&&t.call(o,"finallyLoc")&&this.prev<o.finallyLoc){var i=o;break}}i&&("break"===n||"continue"===n)&&i.tryLoc<=e&&e<=i.finallyLoc&&(i=null);var a=i?i.completion:{};return a.type=n,a.arg=e,i?(this.method="next",this.next=i.finallyLoc,d):this.complete(a)},complete:function(n,e){if("throw"===n.type)throw n.arg;return"break"===n.type||"continue"===n.type?this.next=n.arg:"return"===n.type?(this.rval=this.arg=n.arg,this.method="return",this.next="end"):"normal"===n.type&&e&&(this.next=e),d},finish:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.finallyLoc===n)return this.complete(t.completion,t.afterLoc),w(t),d}},catch:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.tryLoc===n){var r=t.completion;if("throw"===r.type){var o=r.arg;w(t)}return o}}throw new Error("illegal catch attempt")},delegateYield:function(n,e,t){return this.delegate={iterator:_(n),resultName:e,nextLoc:t},"next"===this.method&&(this.arg=void 0),d}},n}(n.exports);try{regeneratorRuntime=r}catch(n){"object"==typeof globalThis?globalThis.regeneratorRuntime=r:Function("r","regeneratorRuntime = r")(r)}},function(n,e,t){var r=t(1),o=t(9),i=t(13).f;r({target:"Object",stat:!0,forced:Object.defineProperty!==i,sham:!o},{defineProperty:i})},function(n,e,t){"use strict";var r=t(2),o=t(65).PROPER,i=t(14),a=t(7),s=t(35),c=t(12),l=t(3),d=t(145),u=RegExp.prototype,p=u.toString,m=r(d),g=l((function(){return"/a/b"!=p.call({source:"a",flags:"b"})})),h=o&&"toString"!=p.name;(g||h)&&i(RegExp.prototype,"toString",(function(){var n=a(this),e=c(n.source),t=n.flags;return"/"+e+"/"+c(void 0===t&&s(u,n)&&!("flags"in u)?m(n):t)}),{unsafe:!0})},function(n,e,t){"use strict";var r=t(20),o=t(136),i=t(64),a=t(38),s=t(13).f,c=t(158),l=t(22),d=t(9),u=a.set,p=a.getterFor("Array Iterator");n.exports=c(Array,"Array",(function(n,e){u(this,{type:"Array Iterator",target:r(n),index:0,kind:e})}),(function(){var n=p(this),e=n.target,t=n.kind,r=n.index++;return!e||r>=e.length?(n.target=void 0,{value:void 0,done:!0}):"keys"==t?{value:r,done:!1}:"values"==t?{value:e[r],done:!1}:{value:[r,e[r]],done:!1}}),"values");var m=i.Arguments=i.Array;if(o("keys"),o("values"),o("entries"),!l&&d&&"values"!==m.name)try{s(m,"name",{value:"values"})}catch(n){}},function(n,e,t){var r=t(9),o=t(153),i=t(13),a=t(7),s=t(20),c=t(84);e.f=r&&!o?Object.defineProperties:function(n,e){a(n);for(var t,r=s(e),o=c(e),l=o.length,d=0;l>d;)i.f(n,t=o[d++],r[t]);return n}},function(n,e,t){var r=t(0),o=t(11),i=t(37),a=t(7),s=t(83),c=t(102),l=r.TypeError;n.exports=function(n,e){var t=arguments.length<2?c(n):e;if(i(t))return a(o(t,n));throw l(s(n)+" is not iterable")}},function(n,e,t){var r=t(0).TypeError;n.exports=function(n,e){if(n<e)throw r("Not enough arguments");return n}},function(n,e,t){var r=t(8),o=t(21),i=t(6)("match");n.exports=function(n){var e;return r(n)&&(void 0!==(e=n[i])?!!e:"RegExp"==o(n))}},function(n,e,t){"use strict";var r=t(7);n.exports=function(){var n=r(this),e="";return n.global&&(e+="g"),n.ignoreCase&&(e+="i"),n.multiline&&(e+="m"),n.dotAll&&(e+="s"),n.unicode&&(e+="u"),n.sticky&&(e+="y"),e}},function(n,e,t){var r=t(4),o=t(8),i=t(66);n.exports=function(n,e,t){var a,s;return i&&r(a=e.constructor)&&a!==t&&o(s=a.prototype)&&s!==t.prototype&&i(n,s),n}},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e){var t=/^\s+|\s+$/g,r=/^[-+]0x[0-9a-f]+$/i,o=/^0b[01]+$/i,i=/^0o[0-7]+$/i,a=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,c="object"==typeof self&&self&&self.Object===Object&&self,l=s||c||Function("return this")(),d=Object.prototype.toString,u=Math.max,p=Math.min,m=function(){return l.Date.now()};function g(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function h(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==d.call(n)}(n))return NaN;if(g(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=g(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var s=o.test(n);return s||i.test(n)?a(n.slice(2),s?2:8):r.test(n)?NaN:+n}n.exports=function(n,e,t){var r,o,i,a,s,c,l=0,d=!1,b=!1,f=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function v(e){var t=r,i=o;return r=o=void 0,l=e,a=n.apply(i,t)}function k(n){return l=n,s=setTimeout(x,e),d?v(n):a}function y(n){var t=n-c;return void 0===c||t>=e||t<0||b&&n-l>=i}function x(){var n=m();if(y(n))return w(n);s=setTimeout(x,function(n){var t=e-(n-c);return b?p(t,i-(n-l)):t}(n))}function w(n){return s=void 0,f&&r?v(n):(r=o=void 0,a)}function S(){var n=m(),t=y(n);if(r=arguments,o=this,c=n,t){if(void 0===s)return k(c);if(b)return s=setTimeout(x,e),v(c)}return void 0===s&&(s=setTimeout(x,e)),a}return e=h(e)||0,g(t)&&(d=!!t.leading,i=(b="maxWait"in t)?u(h(t.maxWait)||0,e):i,f="trailing"in t?!!t.trailing:f),S.cancel=function(){void 0!==s&&clearTimeout(s),l=0,r=c=o=s=void 0},S.flush=function(){return void 0===s?a:w(m())},S}},function(n,e,t){var r=t(2),o=t(17),i=t(12),a=t(150),s=r("".replace),c="["+a+"]",l=RegExp("^"+c+c+"*"),d=RegExp(c+c+"*$"),u=function(n){return function(e){var t=i(o(e));return 1&n&&(t=s(t,l,"")),2&n&&(t=s(t,d,"")),t}};n.exports={start:u(1),end:u(2),trim:u(3)}},function(n,e){n.exports="\t\n\v\f\r                　\u2028\u2029\ufeff"},function(n,e,t){var r=t(2),o=t(14),i=Date.prototype,a=r(i.toString),s=r(i.getTime);"Invalid Date"!=String(new Date(NaN))&&o(i,"toString",(function(){var n=s(this);return n==n?a(this):"Invalid Date"}))},function(n,e,t){var r=t(111);n.exports=r&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){var r=t(9),o=t(3);n.exports=r&&o((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(n,e,t){var r=t(9),o=t(3),i=t(80);n.exports=!r&&!o((function(){return 7!=Object.defineProperty(i("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){var r=t(0),o=t(11),i=t(8),a=t(82),s=t(48),c=t(234),l=t(6),d=r.TypeError,u=l("toPrimitive");n.exports=function(n,e){if(!i(n)||a(n))return n;var t,r=s(n,u);if(r){if(void 0===e&&(e="default"),t=o(r,n,e),!i(t)||a(t))return t;throw d("Can't convert object to primitive value")}return void 0===e&&(e="number"),c(n,e)}},function(n,e,t){var r=t(2),o=t(10),i=t(20),a=t(112).indexOf,s=t(63),c=r([].push);n.exports=function(n,e){var t,r=i(n),l=0,d=[];for(t in r)!o(s,t)&&o(r,t)&&c(d,t);for(;e.length>l;)o(r,t=e[l++])&&(~a(d,t)||c(d,t));return d}},function(n,e,t){var r=t(16);n.exports=r("document","documentElement")},function(n,e,t){"use strict";var r=t(1),o=t(11),i=t(22),a=t(65),s=t(4),c=t(221),l=t(117),d=t(66),u=t(58),p=t(23),m=t(14),g=t(6),h=t(64),b=t(160),f=a.PROPER,v=a.CONFIGURABLE,k=b.IteratorPrototype,y=b.BUGGY_SAFARI_ITERATORS,x=g("iterator"),w=function(){return this};n.exports=function(n,e,t,a,g,b,S){c(t,e,a);var _,E,T,I=function(n){if(n===g&&z)return z;if(!y&&n in C)return C[n];switch(n){case"keys":case"values":case"entries":return function(){return new t(this,n)}}return function(){return new t(this)}},j=e+" Iterator",A=!1,C=n.prototype,D=C[x]||C["@@iterator"]||g&&C[g],z=!y&&D||I(g),M="Array"==e&&C.entries||D;if(M&&(_=l(M.call(new n)))!==Object.prototype&&_.next&&(i||l(_)===k||(d?d(_,k):s(_[x])||m(_,x,w)),u(_,j,!0,!0),i&&(h[j]=w)),f&&"values"==g&&D&&"values"!==D.name&&(!i&&v?p(C,"name","values"):(A=!0,z=function(){return o(D,this)})),g)if(E={values:I("values"),keys:b?z:I("keys"),entries:I("entries")},S)for(T in E)(y||A||!(T in C))&&m(C,T,E[T]);else r({target:e,proto:!0,forced:y||A},E);return i&&!S||C[x]===z||m(C,x,z,{name:g}),h[e]=z,E}},function(n,e,t){var r=t(16),o=t(2),i=t(57),a=t(116),s=t(7),c=o([].concat);n.exports=r("Reflect","ownKeys")||function(n){var e=i.f(s(n)),t=a.f;return t?c(e,t(n)):e}},function(n,e,t){"use strict";var r,o,i,a=t(3),s=t(4),c=t(34),l=t(117),d=t(14),u=t(6),p=t(22),m=u("iterator"),g=!1;[].keys&&("next"in(i=[].keys())?(o=l(l(i)))!==Object.prototype&&(r=o):g=!0),null==r||a((function(){var n={};return r[m].call(n)!==n}))?r={}:p&&(r=c(r)),s(r[m])||d(r,m,(function(){return this})),n.exports={IteratorPrototype:r,BUGGY_SAFARI_ITERATORS:g}},function(n,e,t){var r=t(3);n.exports=!r((function(){function n(){}return n.prototype.constructor=null,Object.getPrototypeOf(new n)!==n.prototype}))},function(n,e,t){var r=t(0);n.exports=r.Promise},function(n,e,t){var r=t(6),o=t(64),i=r("iterator"),a=Array.prototype;n.exports=function(n){return void 0!==n&&(o.Array===n||a[i]===n)}},function(n,e,t){var r=t(11),o=t(7),i=t(48);n.exports=function(n,e,t){var a,s;o(n);try{if(!(a=i(n,"return"))){if("throw"===e)throw t;return t}a=r(a,n)}catch(n){s=!0,a=n}if("throw"===e)throw t;if(s)throw a;return o(a),t}},function(n,e,t){var r=t(6)("iterator"),o=!1;try{var i=0,a={next:function(){return{done:!!i++}},return:function(){o=!0}};a[r]=function(){return this},Array.from(a,(function(){throw 2}))}catch(n){}n.exports=function(n,e){if(!e&&!o)return!1;var t=!1;try{var i={};i[r]=function(){return{next:function(){return{done:t=!0}}}},n(i)}catch(n){}return t}},function(n,e,t){var r=t(0),o=t(88),i=t(83),a=r.TypeError;n.exports=function(n){if(o(n))return n;throw a(i(n)+" is not a constructor")}},function(n,e,t){var r,o,i,a,s=t(0),c=t(36),l=t(55),d=t(4),u=t(10),p=t(3),m=t(157),g=t(67),h=t(80),b=t(143),f=t(168),v=t(89),k=s.setImmediate,y=s.clearImmediate,x=s.process,w=s.Dispatch,S=s.Function,_=s.MessageChannel,E=s.String,T=0,I={};try{r=s.location}catch(n){}var j=function(n){if(u(I,n)){var e=I[n];delete I[n],e()}},A=function(n){return function(){j(n)}},C=function(n){j(n.data)},D=function(n){s.postMessage(E(n),r.protocol+"//"+r.host)};k&&y||(k=function(n){b(arguments.length,1);var e=d(n)?n:S(n),t=g(arguments,1);return I[++T]=function(){c(e,void 0,t)},o(T),T},y=function(n){delete I[n]},v?o=function(n){x.nextTick(A(n))}:w&&w.now?o=function(n){w.now(A(n))}:_&&!f?(a=(i=new _).port2,i.port1.onmessage=C,o=l(a.postMessage,a)):s.addEventListener&&d(s.postMessage)&&!s.importScripts&&r&&"file:"!==r.protocol&&!p(D)?(o=D,s.addEventListener("message",C,!1)):o="onreadystatechange"in h("script")?function(n){m.appendChild(h("script")).onreadystatechange=function(){m.removeChild(this),j(n)}}:function(n){setTimeout(A(n),0)}),n.exports={set:k,clear:y}},function(n,e,t){var r=t(27);n.exports=/(?:ipad|iphone|ipod).*applewebkit/i.test(r)},function(n,e,t){var r=t(7),o=t(8),i=t(170);n.exports=function(n,e){if(r(n),o(e)&&e.constructor===n)return e;var t=i.f(n);return(0,t.resolve)(e),t.promise}},function(n,e,t){"use strict";var r=t(37),o=function(n){var e,t;this.promise=new n((function(n,r){if(void 0!==e||void 0!==t)throw TypeError("Bad Promise constructor");e=n,t=r})),this.resolve=r(e),this.reject=r(t)};n.exports.f=function(n){return new o(n)}},function(n,e,t){var r=t(2),o=t(46),i=t(12),a=t(17),s=r("".charAt),c=r("".charCodeAt),l=r("".slice),d=function(n){return function(e,t){var r,d,u=i(a(e)),p=o(t),m=u.length;return p<0||p>=m?n?"":void 0:(r=c(u,p))<55296||r>56319||p+1===m||(d=c(u,p+1))<56320||d>57343?n?s(u,p):r:n?l(u,p,p+2):d-56320+(r-55296<<10)+65536}};n.exports={codeAt:d(!1),charAt:d(!0)}},function(n,e){n.exports={CSSRuleList:0,CSSStyleDeclaration:0,CSSValueList:0,ClientRectList:0,DOMRectList:0,DOMStringList:0,DOMTokenList:1,DataTransferItemList:0,FileList:0,HTMLAllCollection:0,HTMLCollection:0,HTMLFormElement:0,HTMLSelectElement:0,MediaList:0,MimeTypeArray:0,NamedNodeMap:0,NodeList:1,PaintRequestList:0,Plugin:0,PluginArray:0,SVGLengthList:0,SVGNumberList:0,SVGPathSegList:0,SVGPointList:0,SVGStringList:0,SVGTransformList:0,SourceBufferList:0,StyleSheetList:0,TextTrackCueList:0,TextTrackList:0,TouchList:0}},function(n,e,t){var r=t(80)("span").classList,o=r&&r.constructor&&r.constructor.prototype;n.exports=o===Object.prototype?void 0:o},function(n,e,t){var r=t(1),o=t(3),i=t(15),a=t(117),s=t(161);r({target:"Object",stat:!0,forced:o((function(){a(1)})),sham:!s},{getPrototypeOf:function(n){return a(i(n))}})},function(n,e,t){var r=t(0),o=t(144),i=r.TypeError;n.exports=function(n){if(o(n))throw i("The method doesn't accept regular expressions");return n}},function(n,e,t){var r=t(6)("match");n.exports=function(n){var e=/./;try{"/./"[n](e)}catch(t){try{return e[r]=!1,"/./"[n](e)}catch(n){}}return!1}},function(n,e,t){"use strict";var r=t(56).forEach,o=t(47)("forEach");n.exports=o?[].forEach:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}},function(n,e,t){var r=t(3);n.exports=!r((function(){return Object.isExtensible(Object.preventExtensions({}))}))},function(n,e,t){var r=t(21),o=t(20),i=t(57).f,a=t(120),s="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];n.exports.f=function(n){return s&&"Window"==r(n)?function(n){try{return i(n)}catch(n){return a(s)}}(n):i(o(n))}},function(n,e,t){var r=t(6);e.f=r},function(n,e,t){var r=t(258),o=t(10),i=t(180),a=t(13).f;n.exports=function(n){var e=r.Symbol||(r.Symbol={});o(e,n)||a(e,n,{value:i.f(n)})}},function(n,e,t){var r=t(1),o=t(259);r({target:"Array",stat:!0,forced:!t(165)((function(n){Array.from(n)}))},{from:o})},function(n,e,t){var r=t(12);n.exports=function(n,e){return void 0===n?arguments.length<2?"":e:r(n)}},function(n,e,t){t(1)({target:"Object",stat:!0,sham:!t(9)},{create:t(34)})},function(n,e,t){var r=t(1),o=t(0),i=t(16),a=t(36),s=t(2),c=t(3),l=o.Array,d=i("JSON","stringify"),u=s(/./.exec),p=s("".charAt),m=s("".charCodeAt),g=s("".replace),h=s(1..toString),b=/[\uD800-\uDFFF]/g,f=/^[\uD800-\uDBFF]$/,v=/^[\uDC00-\uDFFF]$/,k=function(n,e,t){var r=p(t,e-1),o=p(t,e+1);return u(f,n)&&!u(v,o)||u(v,n)&&!u(f,r)?"\\u"+h(m(n,0),16):n},y=c((function(){return'"\\udf06\\ud834"'!==d("\udf06\ud834")||'"\\udead"'!==d("\udead")}));d&&r({target:"JSON",stat:!0,forced:y},{stringify:function(n,e,t){for(var r=0,o=arguments.length,i=l(o);r<o;r++)i[r]=arguments[r];var s=a(d,null,i);return"string"==typeof s?g(s,b,k):s}})},function(n,e){n.exports=function(n,e){for(var t=-1,r=e.length,o=n.length;++t<r;)n[o+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var r=t(93),o=t(280),i=t(281),a=t(282),s=t(283),c=t(284);function l(n){var e=this.__data__=new r(n);this.size=e.size}l.prototype.clear=o,l.prototype.delete=i,l.prototype.get=a,l.prototype.has=s,l.prototype.set=c,n.exports=l},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var r=t(60),o=t(125);n.exports=function(n){if(!o(n))return!1;var e=r(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var r=t(301),o=t(50);n.exports=function n(e,t,i,a,s){return e===t||(null==e||null==t||!o(e)&&!o(t)?e!=e&&t!=t:r(e,t,i,a,n,s))}},function(n,e,t){var r=t(194),o=t(304),i=t(195);n.exports=function(n,e,t,a,s,c){var l=1&t,d=n.length,u=e.length;if(d!=u&&!(l&&u>d))return!1;var p=c.get(n),m=c.get(e);if(p&&m)return p==e&&m==n;var g=-1,h=!0,b=2&t?new r:void 0;for(c.set(n,e),c.set(e,n);++g<d;){var f=n[g],v=e[g];if(a)var k=l?a(v,f,g,e,n,c):a(f,v,g,n,e,c);if(void 0!==k){if(k)continue;h=!1;break}if(b){if(!o(e,(function(n,e){if(!i(b,e)&&(f===n||s(f,n,t,a,c)))return b.push(e)}))){h=!1;break}}else if(f!==v&&!s(f,v,t,a,c)){h=!1;break}}return c.delete(n),c.delete(e),h}},function(n,e,t){var r=t(126),o=t(302),i=t(303);function a(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new r;++e<t;)this.add(n[e])}a.prototype.add=a.prototype.push=o,a.prototype.has=i,n.exports=a},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var r=t(314),o=t(320),i=t(200);n.exports=function(n){return i(n)?r(n):o(n)}},function(n,e,t){(function(n){var r=t(28),o=t(316),i=e&&!e.nodeType&&e,a=i&&"object"==typeof n&&n&&!n.nodeType&&n,s=a&&a.exports===i?r.Buffer:void 0,c=(s?s.isBuffer:void 0)||o;n.exports=c}).call(this,t(147)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var r=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==r||"symbol"!=r&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var r=t(317),o=t(318),i=t(319),a=i&&i.isTypedArray,s=a?o(a):r;n.exports=s},function(n,e,t){var r=t(190),o=t(128);n.exports=function(n){return null!=n&&o(n.length)&&!r(n)}},function(n,e,t){var r=t(41)(t(28),"Set");n.exports=r},function(n,e,t){var r=t(125);n.exports=function(n){return n==n&&!r(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var r=t(205),o=t(97);n.exports=function(n,e){for(var t=0,i=(e=r(e,n)).length;null!=n&&t<i;)n=n[o(e[t++])];return t&&t==i?n:void 0}},function(n,e,t){var r=t(26),o=t(129),i=t(331),a=t(334);n.exports=function(n,e){return r(n)?n:o(n,e)?[n]:i(a(n))}},function(n,e,t){"use strict";var r=t(0),o=t(2),i=t(37),a=t(8),s=t(10),c=t(67),l=t(62),d=r.Function,u=o([].concat),p=o([].join),m={},g=function(n,e,t){if(!s(m,e)){for(var r=[],o=0;o<e;o++)r[o]="a["+o+"]";m[e]=d("C,a","return new C("+p(r,",")+")")}return m[e](n,t)};n.exports=l?d.bind:function(n){var e=i(this),t=e.prototype,r=c(arguments,1),o=function(){var t=u(r,c(arguments));return this instanceof o?g(e,t.length,t):e.apply(n,t)};return a(t)&&(o.prototype=t),o}},function(n,e,t){"use strict";var r=t(1),o=t(366).start;r({target:"String",proto:!0,forced:t(368)},{padStart:function(n){return o(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){},function(n,e,t){},function(n,e,t){t(1)({target:"Object",stat:!0},{setPrototypeOf:t(66)})},function(n,e,t){var r=t(1),o=t(16),i=t(36),a=t(206),s=t(166),c=t(7),l=t(8),d=t(34),u=t(3),p=o("Reflect","construct"),m=Object.prototype,g=[].push,h=u((function(){function n(){}return!(p((function(){}),[],n)instanceof n)})),b=!u((function(){p((function(){}))})),f=h||b;r({target:"Reflect",stat:!0,forced:f,sham:f},{construct:function(n,e){s(n),c(e);var t=arguments.length<3?n:s(arguments[2]);if(b&&!h)return p(n,e,t);if(n==t){switch(e.length){case 0:return new n;case 1:return new n(e[0]);case 2:return new n(e[0],e[1]);case 3:return new n(e[0],e[1],e[2]);case 4:return new n(e[0],e[1],e[2],e[3])}var r=[null];return i(g,r,e),new(i(a,n,r))}var o=t.prototype,u=d(l(o)?o:m),f=i(n,u,e);return l(f)?f:u}})},function(n,e,t){var r=t(1),o=t(0),i=t(58);r({global:!0},{Reflect:{}}),i(o.Reflect,"Reflect",!0)},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(267),o=t(272),i=t(343),a=t(351),s=t(360),c=t(229),l=i((function(n){var e=c(n);return s(e)&&(e=void 0),a(r(n,1,s,!0),o(e,2))}));n.exports=l},function(n,e,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var r=/["'&<>]/;n.exports=function(n){var e,t=""+n,o=r.exec(t);if(!o)return t;var i="",a=0,s=0;for(a=o.index;a<t.length;a++){switch(t.charCodeAt(a)){case 34:e="&quot;";break;case 38:e="&amp;";break;case 39:e="&#39;";break;case 60:e="&lt;";break;case 62:e="&gt;";break;default:continue}s!==a&&(i+=t.substring(s,a)),s=a+1,i+=e}return s!==a?i+t.substring(s,a):i}},function(n,e,t){"use strict";t.r(e);var r={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},o=(t(369),t(29)),i=Object(o.a)(r,(function(){var n=this.$createElement;return(this._self._c||n)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);e.default=i.exports},function(n,e,t){"use strict";t.r(e);t(32),t(5),t(33),t(44),t(30);var r={name:"CodeGroup",data:function(){return{codeTabs:[],activeCodeTabIndex:-1}},watch:{activeCodeTabIndex:function(n){this.codeTabs.forEach((function(n){n.elm.classList.remove("theme-code-block__active")})),this.codeTabs[n].elm.classList.add("theme-code-block__active")}},mounted:function(){var n=this;this.codeTabs=(this.$slots.default||[]).filter((function(n){return Boolean(n.componentOptions)})).map((function(e,t){return""===e.componentOptions.propsData.active&&(n.activeCodeTabIndex=t),{title:e.componentOptions.propsData.title,elm:e.elm}})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab:function(n){this.activeCodeTabIndex=n}}},o=(t(370),t(29)),i=Object(o.a)(r,(function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"theme-code-group"},[t("div",{staticClass:"theme-code-group__nav"},[t("ul",{staticClass:"theme-code-group__ul"},n._l(n.codeTabs,(function(e,r){return t("li",{key:e.title,staticClass:"theme-code-group__li"},[t("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":r===n.activeCodeTabIndex},on:{click:function(e){return n.changeCodeTab(r)}}},[n._v("\n            "+n._s(e.title)+"\n          ")])])})),0)]),n._v(" "),n._t("default"),n._v(" "),n.codeTabs.length<1?t("pre",{staticClass:"pre-blank"},[n._v("// Make sure to add code blocks to your code group")]):n._e()],2)}),[],!1,null,"2f5f1757",null);e.default=i.exports},function(n,e,t){"use strict";var r=t(9),o=t(0),i=t(2),a=t(101),s=t(14),c=t(10),l=t(146),d=t(35),u=t(82),p=t(155),m=t(3),g=t(57).f,h=t(39).f,b=t(13).f,f=t(365),v=t(149).trim,k=o.Number,y=k.prototype,x=o.TypeError,w=i("".slice),S=i("".charCodeAt),_=function(n){var e=p(n,"number");return"bigint"==typeof e?e:E(e)},E=function(n){var e,t,r,o,i,a,s,c,l=p(n,"number");if(u(l))throw x("Cannot convert a Symbol value to a number");if("string"==typeof l&&l.length>2)if(l=v(l),43===(e=S(l,0))||45===e){if(88===(t=S(l,2))||120===t)return NaN}else if(48===e){switch(S(l,1)){case 66:case 98:r=2,o=49;break;case 79:case 111:r=8,o=55;break;default:return+l}for(a=(i=w(l,2)).length,s=0;s<a;s++)if((c=S(i,s))<48||c>o)return NaN;return parseInt(i,r)}return+l};if(a("Number",!k(" 0o1")||!k("0b1")||k("+0x1"))){for(var T,I=function(n){var e=arguments.length<1?0:k(_(n)),t=this;return d(y,t)&&m((function(){f(t)}))?l(Object(e),t,I):e},j=r?g(k):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,isFinite,isInteger,isNaN,isSafeInteger,parseFloat,parseInt,fromString,range".split(","),A=0;j.length>A;A++)c(k,T=j[A])&&!c(I,T)&&b(I,T,h(k,T));I.prototype=y,y.constructor=I,s(o,"Number",I)}},function(n,e,t){var r=t(3),o=t(0).RegExp;n.exports=r((function(){var n=o(".","s");return!(n.dotAll&&n.exec("\n")&&"s"===n.flags)}))},function(n,e,t){"use strict";var r=t(160).IteratorPrototype,o=t(34),i=t(49),a=t(58),s=t(64),c=function(){return this};n.exports=function(n,e,t,l){var d=e+" Iterator";return n.prototype=o(r,{next:i(+!l,t)}),a(n,d,!1,!0),s[d]=c,n}},function(n,e,t){var r=t(14);n.exports=function(n,e,t){for(var o in e)r(n,o,e[o],t);return n}},function(n,e,t){"use strict";var r=t(16),o=t(13),i=t(6),a=t(9),s=i("species");n.exports=function(n){var e=r(n),t=o.f;a&&e&&!e[s]&&t(e,s,{configurable:!0,get:function(){return this}})}},function(n,e,t){var r=t(0),o=t(35),i=r.TypeError;n.exports=function(n,e){if(o(e,n))return n;throw i("Incorrect invocation")}},function(n,e,t){var r=t(1),o=t(9),i=t(159),a=t(20),s=t(39),c=t(53);r({target:"Object",stat:!0,sham:!o},{getOwnPropertyDescriptors:function(n){for(var e,t,r=a(n),o=s.f,l=i(r),d={},u=0;l.length>u;)void 0!==(t=o(r,e=l[u++]))&&c(d,e,t);return d}})},function(n,e,t){var r=t(3),o=t(0).RegExp;n.exports=r((function(){var n=o("(?<a>b)","g");return"b"!==n.exec("b").groups.a||"bc"!=="b".replace(n,"$<a>c")}))},function(n,e,t){"use strict";var r=t(1),o=t(112).includes,i=t(136);r({target:"Array",proto:!0},{includes:function(n){return o(this,n,arguments.length>1?arguments[1]:void 0)}}),i("includes")},function(n,e,t){"use strict";var r=t(1),o=t(2),i=t(175),a=t(17),s=t(12),c=t(176),l=o("".indexOf);r({target:"String",proto:!0,forced:!c("includes")},{includes:function(n){return!!~l(s(a(this)),s(i(n)),arguments.length>1?arguments[1]:void 0)}})},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){"use strict";var r=t(1),o=t(149).trim;r({target:"String",proto:!0,forced:t(363)("trim")},{trim:function(){return o(this)}})},function(n,e,t){var r=t(120),o=Math.floor,i=function(n,e){var t=n.length,c=o(t/2);return t<8?a(n,e):s(n,i(r(n,0,c),e),i(r(n,c),e),e)},a=function(n,e){for(var t,r,o=n.length,i=1;i<o;){for(r=i,t=n[i];r&&e(n[r-1],t)>0;)n[r]=n[--r];r!==i++&&(n[r]=t)}return n},s=function(n,e,t,r){for(var o=e.length,i=t.length,a=0,s=0;a<o||s<i;)n[a+s]=a<o&&s<i?r(e[a],t[s])<=0?e[a++]:t[s++]:a<o?e[a++]:t[s++];return n};n.exports=i},function(n,e,t){var r=t(0),o=t(9),i=t(105).MISSED_STICKY,a=t(21),s=t(13).f,c=t(38).get,l=RegExp.prototype,d=r.TypeError;o&&i&&s(l,"sticky",{configurable:!0,get:function(){if(this!==l){if("RegExp"===a(this))return!!c(this).sticky;throw d("Incompatible receiver, RegExp required")}}})},function(n,e,t){n.exports=t(380)},function(n,e,t){var r=t(0),o=t(11),i=t(4),a=t(8),s=r.TypeError;n.exports=function(n,e){var t,r;if("string"===e&&i(t=n.toString)&&!a(r=o(t,n)))return r;if(i(t=n.valueOf)&&!a(r=o(t,n)))return r;if("string"!==e&&i(t=n.toString)&&!a(r=o(t,n)))return r;throw s("Can't convert object to primitive value")}},function(n,e,t){var r=t(0),o=t(4),i=t(87),a=r.WeakMap;n.exports=o(a)&&/native code/.test(i(a))},function(n,e,t){var r=t(0),o=t(4),i=r.String,a=r.TypeError;n.exports=function(n){if("object"==typeof n||o(n))return n;throw a("Can't set "+i(n)+" as a prototype")}},function(n,e,t){"use strict";var r,o,i,a,s=t(1),c=t(22),l=t(0),d=t(16),u=t(11),p=t(162),m=t(14),g=t(222),h=t(66),b=t(58),f=t(223),v=t(37),k=t(4),y=t(8),x=t(224),w=t(87),S=t(238),_=t(165),E=t(119),T=t(167).set,I=t(239),j=t(169),A=t(242),C=t(170),D=t(243),z=t(244),M=t(38),P=t(101),O=t(6),R=t(245),N=t(89),L=t(51),q=O("species"),B="Promise",U=M.getterFor(B),$=M.set,G=M.getterFor(B),F=p&&p.prototype,H=p,K=F,J=l.TypeError,V=l.document,W=l.process,Q=C.f,Y=Q,X=!!(V&&V.createEvent&&l.dispatchEvent),Z=k(l.PromiseRejectionEvent),nn=!1,en=P(B,(function(){var n=w(H),e=n!==String(H);if(!e&&66===L)return!0;if(c&&!K.finally)return!0;if(L>=51&&/native code/.test(n))return!1;var t=new H((function(n){n(1)})),r=function(n){n((function(){}),(function(){}))};return(t.constructor={})[q]=r,!(nn=t.then((function(){}))instanceof r)||!e&&R&&!Z})),tn=en||!_((function(n){H.all(n).catch((function(){}))})),rn=function(n){var e;return!(!y(n)||!k(e=n.then))&&e},on=function(n,e){var t,r,o,i=e.value,a=1==e.state,s=a?n.ok:n.fail,c=n.resolve,l=n.reject,d=n.domain;try{s?(a||(2===e.rejection&&dn(e),e.rejection=1),!0===s?t=i:(d&&d.enter(),t=s(i),d&&(d.exit(),o=!0)),t===n.promise?l(J("Promise-chain cycle")):(r=rn(t))?u(r,t,c,l):c(t)):l(i)}catch(n){d&&!o&&d.exit(),l(n)}},an=function(n,e){n.notified||(n.notified=!0,I((function(){for(var t,r=n.reactions;t=r.get();)on(t,n);n.notified=!1,e&&!n.rejection&&cn(n)})))},sn=function(n,e,t){var r,o;X?((r=V.createEvent("Event")).promise=e,r.reason=t,r.initEvent(n,!1,!0),l.dispatchEvent(r)):r={promise:e,reason:t},!Z&&(o=l["on"+n])?o(r):"unhandledrejection"===n&&A("Unhandled promise rejection",t)},cn=function(n){u(T,l,(function(){var e,t=n.facade,r=n.value;if(ln(n)&&(e=D((function(){N?W.emit("unhandledRejection",r,t):sn("unhandledrejection",t,r)})),n.rejection=N||ln(n)?2:1,e.error))throw e.value}))},ln=function(n){return 1!==n.rejection&&!n.parent},dn=function(n){u(T,l,(function(){var e=n.facade;N?W.emit("rejectionHandled",e):sn("rejectionhandled",e,n.value)}))},un=function(n,e,t){return function(r){n(e,r,t)}},pn=function(n,e,t){n.done||(n.done=!0,t&&(n=t),n.value=e,n.state=2,an(n,!0))},mn=function(n,e,t){if(!n.done){n.done=!0,t&&(n=t);try{if(n.facade===e)throw J("Promise can't be resolved itself");var r=rn(e);r?I((function(){var t={done:!1};try{u(r,e,un(mn,t,n),un(pn,t,n))}catch(e){pn(t,e,n)}})):(n.value=e,n.state=1,an(n,!1))}catch(e){pn({done:!1},e,n)}}};if(en&&(K=(H=function(n){x(this,K),v(n),u(r,this);var e=U(this);try{n(un(mn,e),un(pn,e))}catch(n){pn(e,n)}}).prototype,(r=function(n){$(this,{type:B,done:!1,notified:!1,parent:!1,reactions:new z,rejection:!1,state:0,value:void 0})}).prototype=g(K,{then:function(n,e){var t=G(this),r=Q(E(this,H));return t.parent=!0,r.ok=!k(n)||n,r.fail=k(e)&&e,r.domain=N?W.domain:void 0,0==t.state?t.reactions.add(r):I((function(){on(r,t)})),r.promise},catch:function(n){return this.then(void 0,n)}}),o=function(){var n=new r,e=U(n);this.promise=n,this.resolve=un(mn,e),this.reject=un(pn,e)},C.f=Q=function(n){return n===H||n===i?new o(n):Y(n)},!c&&k(p)&&F!==Object.prototype)){a=F.then,nn||(m(F,"then",(function(n,e){var t=this;return new H((function(n,e){u(a,t,n,e)})).then(n,e)}),{unsafe:!0}),m(F,"catch",K.catch,{unsafe:!0}));try{delete F.constructor}catch(n){}h&&h(F,K)}s({global:!0,wrap:!0,forced:en},{Promise:H}),b(H,B,!1,!0),f(B),i=d(B),s({target:B,stat:!0,forced:en},{reject:function(n){var e=Q(this);return u(e.reject,void 0,n),e.promise}}),s({target:B,stat:!0,forced:c||en},{resolve:function(n){return j(c&&this===i?H:this,n)}}),s({target:B,stat:!0,forced:tn},{all:function(n){var e=this,t=Q(e),r=t.resolve,o=t.reject,i=D((function(){var t=v(e.resolve),i=[],a=0,s=1;S(n,(function(n){var c=a++,l=!1;s++,u(t,e,n).then((function(n){l||(l=!0,i[c]=n,--s||r(i))}),o)})),--s||r(i)}));return i.error&&o(i.value),t.promise},race:function(n){var e=this,t=Q(e),r=t.reject,o=D((function(){var o=v(e.resolve);S(n,(function(n){u(o,e,n).then(t.resolve,r)}))}));return o.error&&r(o.value),t.promise}})},function(n,e,t){var r=t(0),o=t(55),i=t(11),a=t(7),s=t(83),c=t(163),l=t(19),d=t(35),u=t(142),p=t(102),m=t(164),g=r.TypeError,h=function(n,e){this.stopped=n,this.result=e},b=h.prototype;n.exports=function(n,e,t){var r,f,v,k,y,x,w,S=t&&t.that,_=!(!t||!t.AS_ENTRIES),E=!(!t||!t.IS_ITERATOR),T=!(!t||!t.INTERRUPTED),I=o(e,S),j=function(n){return r&&m(r,"normal",n),new h(!0,n)},A=function(n){return _?(a(n),T?I(n[0],n[1],j):I(n[0],n[1])):T?I(n,j):I(n)};if(E)r=n;else{if(!(f=p(n)))throw g(s(n)+" is not iterable");if(c(f)){for(v=0,k=l(n);k>v;v++)if((y=A(n[v]))&&d(b,y))return y;return new h(!1)}r=u(n,f)}for(x=r.next;!(w=i(x,r)).done;){try{y=A(w.value)}catch(n){m(r,"throw",n)}if("object"==typeof y&&y&&d(b,y))return y}return new h(!1)}},function(n,e,t){var r,o,i,a,s,c,l,d,u=t(0),p=t(55),m=t(39).f,g=t(167).set,h=t(168),b=t(240),f=t(241),v=t(89),k=u.MutationObserver||u.WebKitMutationObserver,y=u.document,x=u.process,w=u.Promise,S=m(u,"queueMicrotask"),_=S&&S.value;_||(r=function(){var n,e;for(v&&(n=x.domain)&&n.exit();o;){e=o.fn,o=o.next;try{e()}catch(n){throw o?a():i=void 0,n}}i=void 0,n&&n.enter()},h||v||f||!k||!y?!b&&w&&w.resolve?((l=w.resolve(void 0)).constructor=w,d=p(l.then,l),a=function(){d(r)}):v?a=function(){x.nextTick(r)}:(g=p(g,u),a=function(){g(r)}):(s=!0,c=y.createTextNode(""),new k(r).observe(c,{characterData:!0}),a=function(){c.data=s=!s})),n.exports=_||function(n){var e={fn:n,next:void 0};i&&(i.next=e),o||(o=e,a()),i=e}},function(n,e,t){var r=t(27),o=t(0);n.exports=/ipad|iphone|ipod/i.test(r)&&void 0!==o.Pebble},function(n,e,t){var r=t(27);n.exports=/web0s(?!.*chrome)/i.test(r)},function(n,e,t){var r=t(0);n.exports=function(n,e){var t=r.console;t&&t.error&&(1==arguments.length?t.error(n):t.error(n,e))}},function(n,e){n.exports=function(n){try{return{error:!1,value:n()}}catch(n){return{error:!0,value:n}}}},function(n,e){var t=function(){this.head=null,this.tail=null};t.prototype={add:function(n){var e={item:n,next:null};this.head?this.tail.next=e:this.head=e,this.tail=e},get:function(){var n=this.head;if(n)return this.head=n.next,this.tail===n&&(this.tail=null),n.item}},n.exports=t},function(n,e){n.exports="object"==typeof window},function(n,e,t){var r=t(1),o=t(247);r({target:"Object",stat:!0,forced:Object.assign!==o},{assign:o})},function(n,e,t){"use strict";var r=t(9),o=t(2),i=t(11),a=t(3),s=t(84),c=t(116),l=t(114),d=t(15),u=t(61),p=Object.assign,m=Object.defineProperty,g=o([].concat);n.exports=!p||a((function(){if(r&&1!==p({b:1},p(m({},"a",{enumerable:!0,get:function(){m(this,"b",{value:3,enumerable:!1})}}),{b:2})).b)return!0;var n={},e={},t=Symbol();return n[t]=7,"abcdefghijklmnopqrst".split("").forEach((function(n){e[n]=n})),7!=p({},n)[t]||"abcdefghijklmnopqrst"!=s(p({},e)).join("")}))?function(n,e){for(var t=d(n),o=arguments.length,a=1,p=c.f,m=l.f;o>a;)for(var h,b=u(arguments[a++]),f=p?g(s(b),p(b)):s(b),v=f.length,k=0;v>k;)h=f[k++],r&&!i(m,b,h)||(t[h]=b[h]);return t}:p},function(n,e,t){"use strict";var r=t(1),o=t(22),i=t(162),a=t(3),s=t(16),c=t(4),l=t(119),d=t(169),u=t(14);if(r({target:"Promise",proto:!0,real:!0,forced:!!i&&a((function(){i.prototype.finally.call({then:function(){}},(function(){}))}))},{finally:function(n){var e=l(this,s("Promise")),t=c(n);return this.then(t?function(t){return d(e,n()).then((function(){return t}))}:n,t?function(t){return d(e,n()).then((function(){throw t}))}:n)}}),!o&&c(i)){var p=s("Promise").prototype.finally;i.prototype.finally!==p&&u(i.prototype,"finally",p,{unsafe:!0})}},function(n,e,t){"use strict";var r=t(118),o=t(75);n.exports=r?{}.toString:function(){return"[object "+o(this)+"]"}},function(n,e,t){var r=t(0),o=t(59),i=t(88),a=t(8),s=t(6)("species"),c=r.Array;n.exports=function(n){var e;return o(n)&&(e=n.constructor,(i(e)&&(e===c||o(e.prototype))||a(e)&&null===(e=e[s]))&&(e=void 0)),void 0===e?c:e}},function(n,e,t){"use strict";var r=t(1),o=t(252).left,i=t(47),a=t(51),s=t(89);r({target:"Array",proto:!0,forced:!i("reduce")||!s&&a>79&&a<83},{reduce:function(n){var e=arguments.length;return o(this,n,e,e>1?arguments[1]:void 0)}})},function(n,e,t){var r=t(0),o=t(37),i=t(15),a=t(61),s=t(19),c=r.TypeError,l=function(n){return function(e,t,r,l){o(t);var d=i(e),u=a(d),p=s(d),m=n?p-1:0,g=n?-1:1;if(r<2)for(;;){if(m in u){l=u[m],m+=g;break}if(m+=g,n?m<0:p<=m)throw c("Reduce of empty array with no initial value")}for(;n?m>=0:p>m;m+=g)m in u&&(l=t(l,u[m],m,d));return l}};n.exports={left:l(!1),right:l(!0)}},function(n,e,t){"use strict";var r,o=t(1),i=t(2),a=t(39).f,s=t(52),c=t(12),l=t(175),d=t(17),u=t(176),p=t(22),m=i("".startsWith),g=i("".slice),h=Math.min,b=u("startsWith");o({target:"String",proto:!0,forced:!!(p||b||(r=a(String.prototype,"startsWith"),!r||r.writable))&&!b},{startsWith:function(n){var e=c(d(this));l(n);var t=s(h(arguments.length>1?arguments[1]:void 0,e.length)),r=c(n);return m?m(e,r,t):g(e,t,t+r.length)===r}})},function(n,e,t){var r=t(1),o=t(178),i=t(3),a=t(8),s=t(255).onFreeze,c=Object.freeze;r({target:"Object",stat:!0,forced:i((function(){c(1)})),sham:!o},{freeze:function(n){return c&&a(n)?c(s(n)):n}})},function(n,e,t){var r=t(1),o=t(2),i=t(63),a=t(8),s=t(10),c=t(13).f,l=t(57),d=t(179),u=t(256),p=t(79),m=t(178),g=!1,h=p("meta"),b=0,f=function(n){c(n,h,{value:{objectID:"O"+b++,weakData:{}}})},v=n.exports={enable:function(){v.enable=function(){},g=!0;var n=l.f,e=o([].splice),t={};t[h]=1,n(t).length&&(l.f=function(t){for(var r=n(t),o=0,i=r.length;o<i;o++)if(r[o]===h){e(r,o,1);break}return r},r({target:"Object",stat:!0,forced:!0},{getOwnPropertyNames:d.f}))},fastKey:function(n,e){if(!a(n))return"symbol"==typeof n?n:("string"==typeof n?"S":"P")+n;if(!s(n,h)){if(!u(n))return"F";if(!e)return"E";f(n)}return n[h].objectID},getWeakData:function(n,e){if(!s(n,h)){if(!u(n))return!0;if(!e)return!1;f(n)}return n[h].weakData},onFreeze:function(n){return m&&g&&u(n)&&!s(n,h)&&f(n),n}};i[h]=!0},function(n,e,t){var r=t(3),o=t(8),i=t(21),a=t(257),s=Object.isExtensible,c=r((function(){s(1)}));n.exports=c||a?function(n){return!!o(n)&&((!a||"ArrayBuffer"!=i(n))&&(!s||s(n)))}:s},function(n,e,t){var r=t(3);n.exports=r((function(){if("function"==typeof ArrayBuffer){var n=new ArrayBuffer(8);Object.isExtensible(n)&&Object.defineProperty(n,"a",{value:8})}}))},function(n,e,t){var r=t(0);n.exports=r},function(n,e,t){"use strict";var r=t(0),o=t(55),i=t(11),a=t(15),s=t(260),c=t(163),l=t(88),d=t(19),u=t(53),p=t(142),m=t(102),g=r.Array;n.exports=function(n){var e=a(n),t=l(this),r=arguments.length,h=r>1?arguments[1]:void 0,b=void 0!==h;b&&(h=o(h,r>2?arguments[2]:void 0));var f,v,k,y,x,w,S=m(e),_=0;if(!S||this==g&&c(S))for(f=d(e),v=t?new this(f):g(f);f>_;_++)w=b?h(e[_],_):e[_],u(v,_,w);else for(x=(y=p(e,S)).next,v=t?new this:[];!(k=i(x,y)).done;_++)w=b?s(y,h,[k.value,_],!0):k.value,u(v,_,w);return v.length=_,v}},function(n,e,t){var r=t(7),o=t(164);n.exports=function(n,e,t,i){try{return i?e(r(t)[0],t[1]):e(t)}catch(e){o(n,"throw",e)}}},function(n,e,t){"use strict";var r=t(16),o=t(10),i=t(23),a=t(35),s=t(66),c=t(115),l=t(146),d=t(183),u=t(262),p=t(263),m=t(264),g=t(22);n.exports=function(n,e,t,h){var b=h?2:1,f=n.split("."),v=f[f.length-1],k=r.apply(null,f);if(k){var y=k.prototype;if(!g&&o(y,"cause")&&delete y.cause,!t)return k;var x=r("Error"),w=e((function(n,e){var t=d(h?e:n,void 0),r=h?new k(n):new k;return void 0!==t&&i(r,"message",t),m&&i(r,"stack",p(r.stack,2)),this&&a(y,this)&&l(r,this,w),arguments.length>b&&u(r,arguments[b]),r}));if(w.prototype=y,"Error"!==v&&(s?s(w,x):c(w,x,{name:!0})),c(w,k),!g)try{y.name!==v&&i(y,"name",v),y.constructor=w}catch(n){}return w}}},function(n,e,t){var r=t(8),o=t(23);n.exports=function(n,e){r(e)&&"cause"in e&&o(n,"cause",e.cause)}},function(n,e,t){var r=t(2)("".replace),o=String(Error("zxcasd").stack),i=/\n\s*at [^:]*:[^\n]*/,a=i.test(o);n.exports=function(n,e){if(a&&"string"==typeof n)for(;e--;)n=r(n,i,"");return n}},function(n,e,t){var r=t(3),o=t(49);n.exports=!r((function(){var n=Error("a");return!("stack"in n)||(Object.defineProperty(n,"stack",o(1,7)),7!==n.stack)}))},function(n,e,t){"use strict";var r=t(9),o=t(3),i=t(7),a=t(34),s=t(183),c=Error.prototype.toString,l=o((function(){if(r){var n=a(Object.defineProperty({},"name",{get:function(){return this===n}}));if("true"!==c.call(n))return!0}return"2: 1"!==c.call({message:1,name:2})||"Error"!==c.call({})}));n.exports=l?function(){var n=i(this),e=s(n.name,"Error"),t=s(n.message);return e?t?e+": "+t:e:t}:c},function(n,e,t){var r=t(2),o=t(15),i=Math.floor,a=r("".charAt),s=r("".replace),c=r("".slice),l=/\$([$&'`]|\d{1,2}|<[^>]*>)/g,d=/\$([$&'`]|\d{1,2})/g;n.exports=function(n,e,t,r,u,p){var m=t+n.length,g=r.length,h=d;return void 0!==u&&(u=o(u),h=l),s(p,h,(function(o,s){var l;switch(a(s,0)){case"$":return"$";case"&":return n;case"`":return c(e,0,t);case"'":return c(e,m);case"<":l=u[c(s,1,-1)];break;default:var d=+s;if(0===d)return o;if(d>g){var p=i(d/10);return 0===p?o:p<=g?void 0===r[p-1]?a(s,1):r[p-1]+a(s,1):o}l=r[d-1]}return void 0===l?"":l}))}},function(n,e,t){var r=t(186),o=t(268);n.exports=function n(e,t,i,a,s){var c=-1,l=e.length;for(i||(i=o),s||(s=[]);++c<l;){var d=e[c];t>0&&i(d)?t>1?n(d,t-1,i,a,s):r(s,d):a||(s[s.length]=d)}return s}},function(n,e,t){var r=t(69),o=t(123),i=t(26),a=r?r.isConcatSpreadable:void 0;n.exports=function(n){return i(n)||o(n)||!!(a&&n&&n[a])}},function(n,e,t){var r=t(60),o=t(50);n.exports=function(n){return o(n)&&"[object Arguments]"==r(n)}},function(n,e,t){var r=t(69),o=Object.prototype,i=o.hasOwnProperty,a=o.toString,s=r?r.toStringTag:void 0;n.exports=function(n){var e=i.call(n,s),t=n[s];try{n[s]=void 0;var r=!0}catch(n){}var o=a.call(n);return r&&(e?n[s]=t:delete n[s]),o}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var r=t(273),o=t(329),i=t(131),a=t(26),s=t(340);n.exports=function(n){return"function"==typeof n?n:null==n?i:"object"==typeof n?a(n)?o(n[0],n[1]):r(n):s(n)}},function(n,e,t){var r=t(274),o=t(328),i=t(203);n.exports=function(n){var e=o(n);return 1==e.length&&e[0][2]?i(e[0][0],e[0][1]):function(t){return t===n||r(t,n,e)}}},function(n,e,t){var r=t(188),o=t(192);n.exports=function(n,e,t,i){var a=t.length,s=a,c=!i;if(null==n)return!s;for(n=Object(n);a--;){var l=t[a];if(c&&l[2]?l[1]!==n[l[0]]:!(l[0]in n))return!1}for(;++a<s;){var d=(l=t[a])[0],u=n[d],p=l[1];if(c&&l[2]){if(void 0===u&&!(d in n))return!1}else{var m=new r;if(i)var g=i(u,p,d,n,e,m);if(!(void 0===g?o(p,u,3,i,m):g))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var r=t(94),o=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=r(e,n);return!(t<0)&&(t==e.length-1?e.pop():o.call(e,t,1),--this.size,!0)}},function(n,e,t){var r=t(94);n.exports=function(n){var e=this.__data__,t=r(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var r=t(94);n.exports=function(n){return r(this.__data__,n)>-1}},function(n,e,t){var r=t(94);n.exports=function(n,e){var t=this.__data__,o=r(t,n);return o<0?(++this.size,t.push([n,e])):t[o][1]=e,this}},function(n,e,t){var r=t(93);n.exports=function(){this.__data__=new r,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var r=t(93),o=t(124),i=t(126);n.exports=function(n,e){var t=this.__data__;if(t instanceof r){var a=t.__data__;if(!o||a.length<199)return a.push([n,e]),this.size=++t.size,this;t=this.__data__=new i(a)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var r=t(190),o=t(286),i=t(125),a=t(191),s=/^\[object .+?Constructor\]$/,c=Function.prototype,l=Object.prototype,d=c.toString,u=l.hasOwnProperty,p=RegExp("^"+d.call(u).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!i(n)||o(n))&&(r(n)?p:s).test(a(n))}},function(n,e,t){var r,o=t(287),i=(r=/[^.]+$/.exec(o&&o.keys&&o.keys.IE_PROTO||""))?"Symbol(src)_1."+r:"";n.exports=function(n){return!!i&&i in n}},function(n,e,t){var r=t(28)["__core-js_shared__"];n.exports=r},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var r=t(290),o=t(93),i=t(124);n.exports=function(){this.size=0,this.__data__={hash:new r,map:new(i||o),string:new r}}},function(n,e,t){var r=t(291),o=t(292),i=t(293),a=t(294),s=t(295);function c(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}c.prototype.clear=r,c.prototype.delete=o,c.prototype.get=i,c.prototype.has=a,c.prototype.set=s,n.exports=c},function(n,e,t){var r=t(95);n.exports=function(){this.__data__=r?r(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var r=t(95),o=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(r){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return o.call(e,n)?e[n]:void 0}},function(n,e,t){var r=t(95),o=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return r?void 0!==e[n]:o.call(e,n)}},function(n,e,t){var r=t(95);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=r&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var r=t(96);n.exports=function(n){var e=r(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var r=t(96);n.exports=function(n){return r(this,n).get(n)}},function(n,e,t){var r=t(96);n.exports=function(n){return r(this,n).has(n)}},function(n,e,t){var r=t(96);n.exports=function(n,e){var t=r(this,n),o=t.size;return t.set(n,e),this.size+=t.size==o?0:1,this}},function(n,e,t){var r=t(188),o=t(193),i=t(305),a=t(308),s=t(324),c=t(26),l=t(197),d=t(199),u="[object Object]",p=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,m,g,h){var b=c(n),f=c(e),v=b?"[object Array]":s(n),k=f?"[object Array]":s(e),y=(v="[object Arguments]"==v?u:v)==u,x=(k="[object Arguments]"==k?u:k)==u,w=v==k;if(w&&l(n)){if(!l(e))return!1;b=!0,y=!1}if(w&&!y)return h||(h=new r),b||d(n)?o(n,e,t,m,g,h):i(n,e,v,t,m,g,h);if(!(1&t)){var S=y&&p.call(n,"__wrapped__"),_=x&&p.call(e,"__wrapped__");if(S||_){var E=S?n.value():n,T=_?e.value():e;return h||(h=new r),g(E,T,t,m,h)}}return!!w&&(h||(h=new r),a(n,e,t,m,g,h))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length;++t<r;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var r=t(69),o=t(306),i=t(189),a=t(193),s=t(307),c=t(127),l=r?r.prototype:void 0,d=l?l.valueOf:void 0;n.exports=function(n,e,t,r,l,u,p){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!u(new o(n),new o(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return i(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var m=s;case"[object Set]":var g=1&r;if(m||(m=c),n.size!=e.size&&!g)return!1;var h=p.get(n);if(h)return h==e;r|=2,p.set(n,e);var b=a(m(n),m(e),r,l,u,p);return p.delete(n),b;case"[object Symbol]":if(d)return d.call(n)==d.call(e)}return!1}},function(n,e,t){var r=t(28).Uint8Array;n.exports=r},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,r){t[++e]=[r,n]})),t}},function(n,e,t){var r=t(309),o=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,i,a,s){var c=1&t,l=r(n),d=l.length;if(d!=r(e).length&&!c)return!1;for(var u=d;u--;){var p=l[u];if(!(c?p in e:o.call(e,p)))return!1}var m=s.get(n),g=s.get(e);if(m&&g)return m==e&&g==n;var h=!0;s.set(n,e),s.set(e,n);for(var b=c;++u<d;){var f=n[p=l[u]],v=e[p];if(i)var k=c?i(v,f,p,e,n,s):i(f,v,p,n,e,s);if(!(void 0===k?f===v||a(f,v,t,i,s):k)){h=!1;break}b||(b="constructor"==p)}if(h&&!b){var y=n.constructor,x=e.constructor;y==x||!("constructor"in n)||!("constructor"in e)||"function"==typeof y&&y instanceof y&&"function"==typeof x&&x instanceof x||(h=!1)}return s.delete(n),s.delete(e),h}},function(n,e,t){var r=t(310),o=t(311),i=t(196);n.exports=function(n){return r(n,i,o)}},function(n,e,t){var r=t(186),o=t(26);n.exports=function(n,e,t){var i=e(n);return o(n)?i:r(i,t(n))}},function(n,e,t){var r=t(312),o=t(313),i=Object.prototype.propertyIsEnumerable,a=Object.getOwnPropertySymbols,s=a?function(n){return null==n?[]:(n=Object(n),r(a(n),(function(e){return i.call(n,e)})))}:o;n.exports=s},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,o=0,i=[];++t<r;){var a=n[t];e(a,t,n)&&(i[o++]=a)}return i}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var r=t(315),o=t(123),i=t(26),a=t(197),s=t(198),c=t(199),l=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=i(n),d=!t&&o(n),u=!t&&!d&&a(n),p=!t&&!d&&!u&&c(n),m=t||d||u||p,g=m?r(n.length,String):[],h=g.length;for(var b in n)!e&&!l.call(n,b)||m&&("length"==b||u&&("offset"==b||"parent"==b)||p&&("buffer"==b||"byteLength"==b||"byteOffset"==b)||s(b,h))||g.push(b);return g}},function(n,e){n.exports=function(n,e){for(var t=-1,r=Array(n);++t<n;)r[t]=e(t);return r}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var r=t(60),o=t(128),i=t(50),a={};a["[object Float32Array]"]=a["[object Float64Array]"]=a["[object Int8Array]"]=a["[object Int16Array]"]=a["[object Int32Array]"]=a["[object Uint8Array]"]=a["[object Uint8ClampedArray]"]=a["[object Uint16Array]"]=a["[object Uint32Array]"]=!0,a["[object Arguments]"]=a["[object Array]"]=a["[object ArrayBuffer]"]=a["[object Boolean]"]=a["[object DataView]"]=a["[object Date]"]=a["[object Error]"]=a["[object Function]"]=a["[object Map]"]=a["[object Number]"]=a["[object Object]"]=a["[object RegExp]"]=a["[object Set]"]=a["[object String]"]=a["[object WeakMap]"]=!1,n.exports=function(n){return i(n)&&o(n.length)&&!!a[r(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var r=t(187),o=e&&!e.nodeType&&e,i=o&&"object"==typeof n&&n&&!n.nodeType&&n,a=i&&i.exports===o&&r.process,s=function(){try{var n=i&&i.require&&i.require("util").types;return n||a&&a.binding&&a.binding("util")}catch(n){}}();n.exports=s}).call(this,t(147)(n))},function(n,e,t){var r=t(321),o=t(322),i=Object.prototype.hasOwnProperty;n.exports=function(n){if(!r(n))return o(n);var e=[];for(var t in Object(n))i.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var r=t(323)(Object.keys,Object);n.exports=r},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var r=t(325),o=t(124),i=t(326),a=t(201),s=t(327),c=t(60),l=t(191),d=l(r),u=l(o),p=l(i),m=l(a),g=l(s),h=c;(r&&"[object DataView]"!=h(new r(new ArrayBuffer(1)))||o&&"[object Map]"!=h(new o)||i&&"[object Promise]"!=h(i.resolve())||a&&"[object Set]"!=h(new a)||s&&"[object WeakMap]"!=h(new s))&&(h=function(n){var e=c(n),t="[object Object]"==e?n.constructor:void 0,r=t?l(t):"";if(r)switch(r){case d:return"[object DataView]";case u:return"[object Map]";case p:return"[object Promise]";case m:return"[object Set]";case g:return"[object WeakMap]"}return e}),n.exports=h},function(n,e,t){var r=t(41)(t(28),"DataView");n.exports=r},function(n,e,t){var r=t(41)(t(28),"Promise");n.exports=r},function(n,e,t){var r=t(41)(t(28),"WeakMap");n.exports=r},function(n,e,t){var r=t(202),o=t(196);n.exports=function(n){for(var e=o(n),t=e.length;t--;){var i=e[t],a=n[i];e[t]=[i,a,r(a)]}return e}},function(n,e,t){var r=t(192),o=t(330),i=t(337),a=t(129),s=t(202),c=t(203),l=t(97);n.exports=function(n,e){return a(n)&&s(e)?c(l(n),e):function(t){var a=o(t,n);return void 0===a&&a===e?i(t,n):r(e,a,3)}}},function(n,e,t){var r=t(204);n.exports=function(n,e,t){var o=null==n?void 0:r(n,e);return void 0===o?t:o}},function(n,e,t){var r=t(332),o=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,i=/\\(\\)?/g,a=r((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(o,(function(n,t,r,o){e.push(r?o.replace(i,"$1"):t||n)})),e}));n.exports=a},function(n,e,t){var r=t(333);n.exports=function(n){var e=r(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var r=t(126);function o(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var r=arguments,o=e?e.apply(this,r):r[0],i=t.cache;if(i.has(o))return i.get(o);var a=n.apply(this,r);return t.cache=i.set(o,a)||i,a};return t.cache=new(o.Cache||r),t}o.Cache=r,n.exports=o},function(n,e,t){var r=t(335);n.exports=function(n){return null==n?"":r(n)}},function(n,e,t){var r=t(69),o=t(336),i=t(26),a=t(130),s=r?r.prototype:void 0,c=s?s.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(i(e))return o(e,n)+"";if(a(e))return c?c.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,o=Array(r);++t<r;)o[t]=e(n[t],t,n);return o}},function(n,e,t){var r=t(338),o=t(339);n.exports=function(n,e){return null!=n&&o(n,e,r)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var r=t(205),o=t(123),i=t(26),a=t(198),s=t(128),c=t(97);n.exports=function(n,e,t){for(var l=-1,d=(e=r(e,n)).length,u=!1;++l<d;){var p=c(e[l]);if(!(u=null!=n&&t(n,p)))break;n=n[p]}return u||++l!=d?u:!!(d=null==n?0:n.length)&&s(d)&&a(p,d)&&(i(n)||o(n))}},function(n,e,t){var r=t(341),o=t(342),i=t(129),a=t(97);n.exports=function(n){return i(n)?r(a(n)):o(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var r=t(204);n.exports=function(n){return function(e){return r(e,n)}}},function(n,e,t){var r=t(131),o=t(344),i=t(346);n.exports=function(n,e){return i(o(n,e,r),n+"")}},function(n,e,t){var r=t(345),o=Math.max;n.exports=function(n,e,t){return e=o(void 0===e?n.length-1:e,0),function(){for(var i=arguments,a=-1,s=o(i.length-e,0),c=Array(s);++a<s;)c[a]=i[e+a];a=-1;for(var l=Array(e+1);++a<e;)l[a]=i[a];return l[e]=t(c),r(n,this,l)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var r=t(347),o=t(350)(r);n.exports=o},function(n,e,t){var r=t(348),o=t(349),i=t(131),a=o?function(n,e){return o(n,"toString",{configurable:!0,enumerable:!1,value:r(e),writable:!0})}:i;n.exports=a},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var r=t(41),o=function(){try{var n=r(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=o},function(n,e){var t=Date.now;n.exports=function(n){var e=0,r=0;return function(){var o=t(),i=16-(o-r);if(r=o,i>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var r=t(194),o=t(352),i=t(357),a=t(195),s=t(358),c=t(127);n.exports=function(n,e,t){var l=-1,d=o,u=n.length,p=!0,m=[],g=m;if(t)p=!1,d=i;else if(u>=200){var h=e?null:s(n);if(h)return c(h);p=!1,d=a,g=new r}else g=e?[]:m;n:for(;++l<u;){var b=n[l],f=e?e(b):b;if(b=t||0!==b?b:0,p&&f==f){for(var v=g.length;v--;)if(g[v]===f)continue n;e&&g.push(f),m.push(b)}else d(g,f,t)||(g!==m&&g.push(f),m.push(b))}return m}},function(n,e,t){var r=t(353);n.exports=function(n,e){return!!(null==n?0:n.length)&&r(n,e,0)>-1}},function(n,e,t){var r=t(354),o=t(355),i=t(356);n.exports=function(n,e,t){return e==e?i(n,e,t):r(n,o,t)}},function(n,e){n.exports=function(n,e,t,r){for(var o=n.length,i=t+(r?1:-1);r?i--:++i<o;)if(e(n[i],i,n))return i;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var r=t-1,o=n.length;++r<o;)if(n[r]===e)return r;return-1}},function(n,e){n.exports=function(n,e,t){for(var r=-1,o=null==n?0:n.length;++r<o;)if(t(e,n[r]))return!0;return!1}},function(n,e,t){var r=t(201),o=t(359),i=t(127),a=r&&1/i(new r([,-0]))[1]==1/0?function(n){return new r(n)}:o;n.exports=a},function(n,e){n.exports=function(){}},function(n,e,t){var r=t(200),o=t(50);n.exports=function(n){return o(n)&&r(n)}},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(65).PROPER,o=t(3),i=t(150);n.exports=function(n){return o((function(){return!!i[n]()||"​᠎"!=="​᠎"[n]()||r&&i[n].name!==n}))}},function(n,e,t){var r=t(1),o=t(206);r({target:"Function",proto:!0,forced:Function.bind!==o},{bind:o})},function(n,e,t){var r=t(2);n.exports=r(1..valueOf)},function(n,e,t){var r=t(2),o=t(52),i=t(12),a=t(367),s=t(17),c=r(a),l=r("".slice),d=Math.ceil,u=function(n){return function(e,t,r){var a,u,p=i(s(e)),m=o(t),g=p.length,h=void 0===r?" ":i(r);return m<=g||""==h?p:((u=c(h,d((a=m-g)/h.length))).length>a&&(u=l(u,0,a)),n?p+u:u+p)}};n.exports={start:u(!1),end:u(!0)}},function(n,e,t){"use strict";var r=t(0),o=t(46),i=t(12),a=t(17),s=r.RangeError;n.exports=function(n){var e=i(a(this)),t="",r=o(n);if(r<0||r==1/0)throw s("Wrong number of repetitions");for(;r>0;(r>>>=1)&&(e+=e))1&r&&(t+=e);return t}},function(n,e,t){var r=t(27);n.exports=/Version\/10(?:\.\d+){1,2}(?: [\w./]+)?(?: Mobile\/\w+)? Safari\//.test(r)},function(n,e,t){"use strict";t(208)},function(n,e,t){"use strict";t(209)},function(n,e,t){"use strict";var r=t(1),o=t(2),i=t(37),a=t(15),s=t(19),c=t(12),l=t(3),d=t(231),u=t(47),p=t(372),m=t(373),g=t(51),h=t(374),b=[],f=o(b.sort),v=o(b.push),k=l((function(){b.sort(void 0)})),y=l((function(){b.sort(null)})),x=u("sort"),w=!l((function(){if(g)return g<70;if(!(p&&p>3)){if(m)return!0;if(h)return h<603;var n,e,t,r,o="";for(n=65;n<76;n++){switch(e=String.fromCharCode(n),n){case 66:case 69:case 70:case 72:t=3;break;case 68:case 71:t=4;break;default:t=2}for(r=0;r<47;r++)b.push({k:e+r,v:t})}for(b.sort((function(n,e){return e.v-n.v})),r=0;r<b.length;r++)e=b[r].k.charAt(0),o.charAt(o.length-1)!==e&&(o+=e);return"DGBEFHACIJK"!==o}}));r({target:"Array",proto:!0,forced:k||!y||!x||!w},{sort:function(n){void 0!==n&&i(n);var e=a(this);if(w)return void 0===n?f(e):f(e,n);var t,r,o=[],l=s(e);for(r=0;r<l;r++)r in e&&v(o,e[r]);for(d(o,function(n){return function(e,t){return void 0===t?-1:void 0===e?1:void 0!==n?+n(e,t)||0:c(e)>c(t)?1:-1}}(n)),t=o.length,r=0;r<t;)e[r]=o[r++];for(;r<l;)delete e[r++];return e}})},function(n,e,t){var r=t(27).match(/firefox\/(\d+)/i);n.exports=!!r&&+r[1]},function(n,e,t){var r=t(27);n.exports=/MSIE|Trident/.test(r)},function(n,e,t){var r=t(27).match(/AppleWebKit\/(\d+)\./);n.exports=!!r&&+r[1]},function(n,e,t){},function(n,e,t){},function(n,e,t){"use strict";var r=t(1),o=t(0),i=t(85),a=t(46),s=t(19),c=t(15),l=t(103),d=t(53),u=t(68)("splice"),p=o.TypeError,m=Math.max,g=Math.min;r({target:"Array",proto:!0,forced:!u},{splice:function(n,e){var t,r,o,u,h,b,f=c(this),v=s(f),k=i(n,v),y=arguments.length;if(0===y?t=r=0:1===y?(t=0,r=v-k):(t=y-2,r=g(m(a(e),0),v-k)),v+t-r>9007199254740991)throw p("Maximum allowed length exceeded");for(o=l(f,r),u=0;u<r;u++)(h=k+u)in f&&d(o,u,f[h]);if(o.length=r,t<r){for(u=k;u<v-r;u++)b=u+t,(h=u+r)in f?f[b]=f[h]:delete f[b];for(u=v;u>v-r+t;u--)delete f[u-1]}else if(t>r)for(u=v-r;u>k;u--)b=u+t-1,(h=u+r-1)in f?f[b]=f[h]:delete f[b];for(u=0;u<t;u++)f[u+k]=arguments[u+2];return f.length=v-r+t,o}})},function(n,e,t){"use strict";t(213)},function(n,e,t){"use strict";t(214)},function(n,e,t){"use strict";t.r(e);t(140),t(237),t(246),t(248);var r=t(90),o=(t(137),t(54),t(5),t(24),t(31),t(44),t(30),Object.freeze({}));function i(n){return null==n}function a(n){return null!=n}function s(n){return!0===n}function c(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function l(n){return null!==n&&"object"==typeof n}var d=Object.prototype.toString;function u(n){return"[object Object]"===d.call(n)}function p(n){return"[object RegExp]"===d.call(n)}function m(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function g(n){return a(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function h(n){return null==n?"":Array.isArray(n)||u(n)&&n.toString===d?JSON.stringify(n,null,2):String(n)}function b(n){var e=parseFloat(n);return isNaN(e)?n:e}function f(n,e){for(var t=Object.create(null),r=n.split(","),o=0;o<r.length;o++)t[r[o]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}f("slot,component",!0);var v=f("key,ref,slot,slot-scope,is");function k(n,e){if(n.length){var t=n.indexOf(e);if(t>-1)return n.splice(t,1)}}var y=Object.prototype.hasOwnProperty;function x(n,e){return y.call(n,e)}function w(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var S=/-(\w)/g,_=w((function(n){return n.replace(S,(function(n,e){return e?e.toUpperCase():""}))})),E=w((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),T=/\B([A-Z])/g,I=w((function(n){return n.replace(T,"-$1").toLowerCase()}));var j=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var r=arguments.length;return r?r>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function A(n,e){e=e||0;for(var t=n.length-e,r=new Array(t);t--;)r[t]=n[t+e];return r}function C(n,e){for(var t in e)n[t]=e[t];return n}function D(n){for(var e={},t=0;t<n.length;t++)n[t]&&C(e,n[t]);return e}function z(n,e,t){}var M=function(n,e,t){return!1},P=function(n){return n};function O(n,e){if(n===e)return!0;var t=l(n),r=l(e);if(!t||!r)return!t&&!r&&String(n)===String(e);try{var o=Array.isArray(n),i=Array.isArray(e);if(o&&i)return n.length===e.length&&n.every((function(n,t){return O(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(o||i)return!1;var a=Object.keys(n),s=Object.keys(e);return a.length===s.length&&a.every((function(t){return O(n[t],e[t])}))}catch(n){return!1}}function R(n,e){for(var t=0;t<n.length;t++)if(O(n[t],e))return t;return-1}function N(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}var L=["component","directive","filter"],q=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch"],B={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:M,isReservedAttr:M,isUnknownElement:M,getTagNamespace:z,parsePlatformTagName:P,mustUseProp:M,async:!0,_lifecycleHooks:q},U=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function $(n,e,t,r){Object.defineProperty(n,e,{value:t,enumerable:!!r,writable:!0,configurable:!0})}var G=new RegExp("[^"+U.source+".$_\\d]");var F,H="__proto__"in{},K="undefined"!=typeof window,J="undefined"!=typeof WXEnvironment&&!!WXEnvironment.platform,V=J&&WXEnvironment.platform.toLowerCase(),W=K&&window.navigator.userAgent.toLowerCase(),Q=W&&/msie|trident/.test(W),Y=W&&W.indexOf("msie 9.0")>0,X=W&&W.indexOf("edge/")>0,Z=(W&&W.indexOf("android"),W&&/iphone|ipad|ipod|ios/.test(W)||"ios"===V),nn=(W&&/chrome\/\d+/.test(W),W&&/phantomjs/.test(W),W&&W.match(/firefox\/(\d+)/)),en={}.watch,tn=!1;if(K)try{var rn={};Object.defineProperty(rn,"passive",{get:function(){tn=!0}}),window.addEventListener("test-passive",null,rn)}catch(n){}var on=function(){return void 0===F&&(F=!K&&!J&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),F},an=K&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function sn(n){return"function"==typeof n&&/native code/.test(n.toString())}var cn,ln="undefined"!=typeof Symbol&&sn(Symbol)&&"undefined"!=typeof Reflect&&sn(Reflect.ownKeys);cn="undefined"!=typeof Set&&sn(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var dn=z,un=0,pn=function(){this.id=un++,this.subs=[]};pn.prototype.addSub=function(n){this.subs.push(n)},pn.prototype.removeSub=function(n){k(this.subs,n)},pn.prototype.depend=function(){pn.target&&pn.target.addDep(this)},pn.prototype.notify=function(){var n=this.subs.slice();for(var e=0,t=n.length;e<t;e++)n[e].update()},pn.target=null;var mn=[];function gn(n){mn.push(n),pn.target=n}function hn(){mn.pop(),pn.target=mn[mn.length-1]}var bn=function(n,e,t,r,o,i,a,s){this.tag=n,this.data=e,this.children=t,this.text=r,this.elm=o,this.ns=void 0,this.context=i,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=a,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1},fn={child:{configurable:!0}};fn.child.get=function(){return this.componentInstance},Object.defineProperties(bn.prototype,fn);var vn=function(n){void 0===n&&(n="");var e=new bn;return e.text=n,e.isComment=!0,e};function kn(n){return new bn(void 0,void 0,void 0,String(n))}function yn(n){var e=new bn(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}var xn=Array.prototype,wn=Object.create(xn);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=xn[n];$(wn,n,(function(){for(var t=[],r=arguments.length;r--;)t[r]=arguments[r];var o,i=e.apply(this,t),a=this.__ob__;switch(n){case"push":case"unshift":o=t;break;case"splice":o=t.slice(2)}return o&&a.observeArray(o),a.dep.notify(),i}))}));var Sn=Object.getOwnPropertyNames(wn),_n=!0;function En(n){_n=n}var Tn=function(n){this.value=n,this.dep=new pn,this.vmCount=0,$(n,"__ob__",this),Array.isArray(n)?(H?function(n,e){n.__proto__=e}(n,wn):function(n,e,t){for(var r=0,o=t.length;r<o;r++){var i=t[r];$(n,i,e[i])}}(n,wn,Sn),this.observeArray(n)):this.walk(n)};function In(n,e){var t;if(l(n)&&!(n instanceof bn))return x(n,"__ob__")&&n.__ob__ instanceof Tn?t=n.__ob__:_n&&!on()&&(Array.isArray(n)||u(n))&&Object.isExtensible(n)&&!n._isVue&&(t=new Tn(n)),e&&t&&t.vmCount++,t}function jn(n,e,t,r,o){var i=new pn,a=Object.getOwnPropertyDescriptor(n,e);if(!a||!1!==a.configurable){var s=a&&a.get,c=a&&a.set;s&&!c||2!==arguments.length||(t=n[e]);var l=!o&&In(t);Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=s?s.call(n):t;return pn.target&&(i.depend(),l&&(l.dep.depend(),Array.isArray(e)&&Dn(e))),e},set:function(e){var r=s?s.call(n):t;e===r||e!=e&&r!=r||s&&!c||(c?c.call(n,e):t=e,l=!o&&In(e),i.notify())}})}}function An(n,e,t){if(Array.isArray(n)&&m(e))return n.length=Math.max(n.length,e),n.splice(e,1,t),t;if(e in n&&!(e in Object.prototype))return n[e]=t,t;var r=n.__ob__;return n._isVue||r&&r.vmCount?t:r?(jn(r.value,e,t),r.dep.notify(),t):(n[e]=t,t)}function Cn(n,e){if(Array.isArray(n)&&m(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||x(n,e)&&(delete n[e],t&&t.dep.notify())}}function Dn(n){for(var e=void 0,t=0,r=n.length;t<r;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),Array.isArray(e)&&Dn(e)}Tn.prototype.walk=function(n){for(var e=Object.keys(n),t=0;t<e.length;t++)jn(n,e[t])},Tn.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)In(n[e])};var zn=B.optionMergeStrategies;function Mn(n,e){if(!e)return n;for(var t,r,o,i=ln?Reflect.ownKeys(e):Object.keys(e),a=0;a<i.length;a++)"__ob__"!==(t=i[a])&&(r=n[t],o=e[t],x(n,t)?r!==o&&u(r)&&u(o)&&Mn(r,o):An(n,t,o));return n}function Pn(n,e,t){return t?function(){var r="function"==typeof e?e.call(t,t):e,o="function"==typeof n?n.call(t,t):n;return r?Mn(r,o):o}:e?n?function(){return Mn("function"==typeof e?e.call(this,this):e,"function"==typeof n?n.call(this,this):n)}:e:n}function On(n,e){var t=e?n?n.concat(e):Array.isArray(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function Rn(n,e,t,r){var o=Object.create(n||null);return e?C(o,e):o}zn.data=function(n,e,t){return t?Pn(n,e,t):e&&"function"!=typeof e?n:Pn(n,e)},q.forEach((function(n){zn[n]=On})),L.forEach((function(n){zn[n+"s"]=Rn})),zn.watch=function(n,e,t,r){if(n===en&&(n=void 0),e===en&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var o={};for(var i in C(o,n),e){var a=o[i],s=e[i];a&&!Array.isArray(a)&&(a=[a]),o[i]=a?a.concat(s):Array.isArray(s)?s:[s]}return o},zn.props=zn.methods=zn.inject=zn.computed=function(n,e,t,r){if(!n)return e;var o=Object.create(null);return C(o,n),e&&C(o,e),o},zn.provide=Pn;var Nn=function(n,e){return void 0===e?n:e};function Ln(n,e,t){if("function"==typeof e&&(e=e.options),function(n,e){var t=n.props;if(t){var r,o,i={};if(Array.isArray(t))for(r=t.length;r--;)"string"==typeof(o=t[r])&&(i[_(o)]={type:null});else if(u(t))for(var a in t)o=t[a],i[_(a)]=u(o)?o:{type:o};else 0;n.props=i}}(e),function(n,e){var t=n.inject;if(t){var r=n.inject={};if(Array.isArray(t))for(var o=0;o<t.length;o++)r[t[o]]={from:t[o]};else if(u(t))for(var i in t){var a=t[i];r[i]=u(a)?C({from:i},a):{from:a}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var r=e[t];"function"==typeof r&&(e[t]={bind:r,update:r})}}(e),!e._base&&(e.extends&&(n=Ln(n,e.extends,t)),e.mixins))for(var r=0,o=e.mixins.length;r<o;r++)n=Ln(n,e.mixins[r],t);var i,a={};for(i in n)s(i);for(i in e)x(n,i)||s(i);function s(r){var o=zn[r]||Nn;a[r]=o(n[r],e[r],t,r)}return a}function qn(n,e,t,r){if("string"==typeof t){var o=n[e];if(x(o,t))return o[t];var i=_(t);if(x(o,i))return o[i];var a=E(i);return x(o,a)?o[a]:o[t]||o[i]||o[a]}}function Bn(n,e,t,r){var o=e[n],i=!x(t,n),a=t[n],s=Fn(Boolean,o.type);if(s>-1)if(i&&!x(o,"default"))a=!1;else if(""===a||a===I(n)){var c=Fn(String,o.type);(c<0||s<c)&&(a=!0)}if(void 0===a){a=function(n,e,t){if(!x(e,"default"))return;var r=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return"function"==typeof r&&"Function"!==$n(e.type)?r.call(n):r}(r,o,n);var l=_n;En(!0),In(a),En(l)}return a}var Un=/^\s*function (\w+)/;function $n(n){var e=n&&n.toString().match(Un);return e?e[1]:""}function Gn(n,e){return $n(n)===$n(e)}function Fn(n,e){if(!Array.isArray(e))return Gn(e,n)?0:-1;for(var t=0,r=e.length;t<r;t++)if(Gn(e[t],n))return t;return-1}function Hn(n,e,t){gn();try{if(e)for(var r=e;r=r.$parent;){var o=r.$options.errorCaptured;if(o)for(var i=0;i<o.length;i++)try{if(!1===o[i].call(r,n,e,t))return}catch(n){Jn(n,r,"errorCaptured hook")}}Jn(n,e,t)}finally{hn()}}function Kn(n,e,t,r,o){var i;try{(i=t?n.apply(e,t):n.call(e))&&!i._isVue&&g(i)&&!i._handled&&(i.catch((function(n){return Hn(n,r,o+" (Promise/async)")})),i._handled=!0)}catch(n){Hn(n,r,o)}return i}function Jn(n,e,t){if(B.errorHandler)try{return B.errorHandler.call(null,n,e,t)}catch(e){e!==n&&Vn(e,null,"config.errorHandler")}Vn(n,e,t)}function Vn(n,e,t){if(!K&&!J||"undefined"==typeof console)throw n;console.error(n)}var Wn,Qn=!1,Yn=[],Xn=!1;function Zn(){Xn=!1;var n=Yn.slice(0);Yn.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&sn(Promise)){var ne=Promise.resolve();Wn=function(){ne.then(Zn),Z&&setTimeout(z)},Qn=!0}else if(Q||"undefined"==typeof MutationObserver||!sn(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Wn="undefined"!=typeof setImmediate&&sn(setImmediate)?function(){setImmediate(Zn)}:function(){setTimeout(Zn,0)};else{var ee=1,te=new MutationObserver(Zn),re=document.createTextNode(String(ee));te.observe(re,{characterData:!0}),Wn=function(){ee=(ee+1)%2,re.data=String(ee)},Qn=!0}function oe(n,e){var t;if(Yn.push((function(){if(n)try{n.call(e)}catch(n){Hn(n,e,"nextTick")}else t&&t(e)})),Xn||(Xn=!0,Wn()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}var ie=new cn;function ae(n){!function n(e,t){var r,o,i=Array.isArray(e);if(!i&&!l(e)||Object.isFrozen(e)||e instanceof bn)return;if(e.__ob__){var a=e.__ob__.dep.id;if(t.has(a))return;t.add(a)}if(i)for(r=e.length;r--;)n(e[r],t);else for(o=Object.keys(e),r=o.length;r--;)n(e[o[r]],t)}(n,ie),ie.clear()}var se=w((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),r="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=r?n.slice(1):n,once:t,capture:r,passive:e}}));function ce(n,e){function t(){var n=arguments,r=t.fns;if(!Array.isArray(r))return Kn(r,null,arguments,e,"v-on handler");for(var o=r.slice(),i=0;i<o.length;i++)Kn(o[i],null,n,e,"v-on handler")}return t.fns=n,t}function le(n,e,t,r,o,a){var c,l,d,u;for(c in n)l=n[c],d=e[c],u=se(c),i(l)||(i(d)?(i(l.fns)&&(l=n[c]=ce(l,a)),s(u.once)&&(l=n[c]=o(u.name,l,u.capture)),t(u.name,l,u.capture,u.passive,u.params)):l!==d&&(d.fns=l,n[c]=d));for(c in e)i(n[c])&&r((u=se(c)).name,e[c],u.capture)}function de(n,e,t){var r;n instanceof bn&&(n=n.data.hook||(n.data.hook={}));var o=n[e];function c(){t.apply(this,arguments),k(r.fns,c)}i(o)?r=ce([c]):a(o.fns)&&s(o.merged)?(r=o).fns.push(c):r=ce([o,c]),r.merged=!0,n[e]=r}function ue(n,e,t,r,o){if(a(e)){if(x(e,t))return n[t]=e[t],o||delete e[t],!0;if(x(e,r))return n[t]=e[r],o||delete e[r],!0}return!1}function pe(n){return c(n)?[kn(n)]:Array.isArray(n)?function n(e,t){var r,o,l,d,u=[];for(r=0;r<e.length;r++)i(o=e[r])||"boolean"==typeof o||(l=u.length-1,d=u[l],Array.isArray(o)?o.length>0&&(me((o=n(o,(t||"")+"_"+r))[0])&&me(d)&&(u[l]=kn(d.text+o[0].text),o.shift()),u.push.apply(u,o)):c(o)?me(d)?u[l]=kn(d.text+o):""!==o&&u.push(kn(o)):me(o)&&me(d)?u[l]=kn(d.text+o.text):(s(e._isVList)&&a(o.tag)&&i(o.key)&&a(t)&&(o.key="__vlist"+t+"_"+r+"__"),u.push(o)));return u}(n):void 0}function me(n){return a(n)&&a(n.text)&&!1===n.isComment}function ge(n,e){if(n){for(var t=Object.create(null),r=ln?Reflect.ownKeys(n):Object.keys(n),o=0;o<r.length;o++){var i=r[o];if("__ob__"!==i){for(var a=n[i].from,s=e;s;){if(s._provided&&x(s._provided,a)){t[i]=s._provided[a];break}s=s.$parent}if(!s)if("default"in n[i]){var c=n[i].default;t[i]="function"==typeof c?c.call(e):c}else 0}}return t}}function he(n,e){if(!n||!n.length)return{};for(var t={},r=0,o=n.length;r<o;r++){var i=n[r],a=i.data;if(a&&a.attrs&&a.attrs.slot&&delete a.attrs.slot,i.context!==e&&i.fnContext!==e||!a||null==a.slot)(t.default||(t.default=[])).push(i);else{var s=a.slot,c=t[s]||(t[s]=[]);"template"===i.tag?c.push.apply(c,i.children||[]):c.push(i)}}for(var l in t)t[l].every(be)&&delete t[l];return t}function be(n){return n.isComment&&!n.asyncFactory||" "===n.text}function fe(n){return n.isComment&&n.asyncFactory}function ve(n,e,t){var r,i=Object.keys(e).length>0,a=n?!!n.$stable:!i,s=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(a&&t&&t!==o&&s===t.$key&&!i&&!t.$hasNormal)return t;for(var c in r={},n)n[c]&&"$"!==c[0]&&(r[c]=ke(e,c,n[c]))}else r={};for(var l in e)l in r||(r[l]=ye(e,l));return n&&Object.isExtensible(n)&&(n._normalized=r),$(r,"$stable",a),$(r,"$key",s),$(r,"$hasNormal",i),r}function ke(n,e,t){var r=function(){var n=arguments.length?t.apply(null,arguments):t({}),e=(n=n&&"object"==typeof n&&!Array.isArray(n)?[n]:pe(n))&&n[0];return n&&(!e||1===n.length&&e.isComment&&!fe(e))?void 0:n};return t.proxy&&Object.defineProperty(n,e,{get:r,enumerable:!0,configurable:!0}),r}function ye(n,e){return function(){return n[e]}}function xe(n,e){var t,r,o,i,s;if(Array.isArray(n)||"string"==typeof n)for(t=new Array(n.length),r=0,o=n.length;r<o;r++)t[r]=e(n[r],r);else if("number"==typeof n)for(t=new Array(n),r=0;r<n;r++)t[r]=e(r+1,r);else if(l(n))if(ln&&n[Symbol.iterator]){t=[];for(var c=n[Symbol.iterator](),d=c.next();!d.done;)t.push(e(d.value,t.length)),d=c.next()}else for(i=Object.keys(n),t=new Array(i.length),r=0,o=i.length;r<o;r++)s=i[r],t[r]=e(n[s],s,r);return a(t)||(t=[]),t._isVList=!0,t}function we(n,e,t,r){var o,i=this.$scopedSlots[n];i?(t=t||{},r&&(t=C(C({},r),t)),o=i(t)||("function"==typeof e?e():e)):o=this.$slots[n]||("function"==typeof e?e():e);var a=t&&t.slot;return a?this.$createElement("template",{slot:a},o):o}function Se(n){return qn(this.$options,"filters",n)||P}function _e(n,e){return Array.isArray(n)?-1===n.indexOf(e):n!==e}function Ee(n,e,t,r,o){var i=B.keyCodes[e]||t;return o&&r&&!B.keyCodes[e]?_e(o,r):i?_e(i,n):r?I(r)!==e:void 0===n}function Te(n,e,t,r,o){if(t)if(l(t)){var i;Array.isArray(t)&&(t=D(t));var a=function(a){if("class"===a||"style"===a||v(a))i=n;else{var s=n.attrs&&n.attrs.type;i=r||B.mustUseProp(e,s,a)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var c=_(a),l=I(a);c in i||l in i||(i[a]=t[a],o&&((n.on||(n.on={}))["update:"+a]=function(n){t[a]=n}))};for(var s in t)a(s)}else;return n}function Ie(n,e){var t=this._staticTrees||(this._staticTrees=[]),r=t[n];return r&&!e||Ae(r=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,null,this),"__static__"+n,!1),r}function je(n,e,t){return Ae(n,"__once__"+e+(t?"_"+t:""),!0),n}function Ae(n,e,t){if(Array.isArray(n))for(var r=0;r<n.length;r++)n[r]&&"string"!=typeof n[r]&&Ce(n[r],e+"_"+r,t);else Ce(n,e,t)}function Ce(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function De(n,e){if(e)if(u(e)){var t=n.on=n.on?C({},n.on):{};for(var r in e){var o=t[r],i=e[r];t[r]=o?[].concat(o,i):i}}else;return n}function ze(n,e,t,r){e=e||{$stable:!t};for(var o=0;o<n.length;o++){var i=n[o];Array.isArray(i)?ze(i,e,t):i&&(i.proxy&&(i.fn.proxy=!0),e[i.key]=i.fn)}return r&&(e.$key=r),e}function Me(n,e){for(var t=0;t<e.length;t+=2){var r=e[t];"string"==typeof r&&r&&(n[e[t]]=e[t+1])}return n}function Pe(n,e){return"string"==typeof n?e+n:n}function Oe(n){n._o=je,n._n=b,n._s=h,n._l=xe,n._t=we,n._q=O,n._i=R,n._m=Ie,n._f=Se,n._k=Ee,n._b=Te,n._v=kn,n._e=vn,n._u=ze,n._g=De,n._d=Me,n._p=Pe}function Re(n,e,t,r,i){var a,c=this,l=i.options;x(r,"_uid")?(a=Object.create(r))._original=r:(a=r,r=r._original);var d=s(l._compiled),u=!d;this.data=n,this.props=e,this.children=t,this.parent=r,this.listeners=n.on||o,this.injections=ge(l.inject,r),this.slots=function(){return c.$slots||ve(n.scopedSlots,c.$slots=he(t,r)),c.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return ve(n.scopedSlots,this.slots())}}),d&&(this.$options=l,this.$slots=this.slots(),this.$scopedSlots=ve(n.scopedSlots,this.$slots)),l._scopeId?this._c=function(n,e,t,o){var i=Ge(a,n,e,t,o,u);return i&&!Array.isArray(i)&&(i.fnScopeId=l._scopeId,i.fnContext=r),i}:this._c=function(n,e,t,r){return Ge(a,n,e,t,r,u)}}function Ne(n,e,t,r,o){var i=yn(n);return i.fnContext=t,i.fnOptions=r,e.slot&&((i.data||(i.data={})).slot=e.slot),i}function Le(n,e){for(var t in e)n[_(t)]=e[t]}Oe(Re.prototype);var qe={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;qe.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},r=n.data.inlineTemplate;a(r)&&(t.render=r.render,t.staticRenderFns=r.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,Xe)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,r,i){0;var a=r.data.scopedSlots,s=n.$scopedSlots,c=!!(a&&!a.$stable||s!==o&&!s.$stable||a&&n.$scopedSlots.$key!==a.$key||!a&&n.$scopedSlots.$key),l=!!(i||n.$options._renderChildren||c);n.$options._parentVnode=r,n.$vnode=r,n._vnode&&(n._vnode.parent=r);if(n.$options._renderChildren=i,n.$attrs=r.data.attrs||o,n.$listeners=t||o,e&&n.$options.props){En(!1);for(var d=n._props,u=n.$options._propKeys||[],p=0;p<u.length;p++){var m=u[p],g=n.$options.props;d[m]=Bn(m,g,e,n)}En(!0),n.$options.propsData=e}t=t||o;var h=n.$options._parentListeners;n.$options._parentListeners=t,Ye(n,t,h),l&&(n.$slots=he(i,r.context),n.$forceUpdate());0}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,r=n.componentInstance;r._isMounted||(r._isMounted=!0,tt(r,"mounted")),n.data.keepAlive&&(t._isMounted?((e=r)._inactive=!1,ot.push(e)):et(r,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(t&&(e._directInactive=!0,nt(e)))return;if(!e._inactive){e._inactive=!0;for(var r=0;r<e.$children.length;r++)n(e.$children[r]);tt(e,"deactivated")}}(e,!0):e.$destroy())}},Be=Object.keys(qe);function Ue(n,e,t,r,c){if(!i(n)){var d=t.$options._base;if(l(n)&&(n=d.extend(n)),"function"==typeof n){var u;if(i(n.cid)&&void 0===(n=function(n,e){if(s(n.error)&&a(n.errorComp))return n.errorComp;if(a(n.resolved))return n.resolved;var t=He;t&&a(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t);if(s(n.loading)&&a(n.loadingComp))return n.loadingComp;if(t&&!a(n.owners)){var r=n.owners=[t],o=!0,c=null,d=null;t.$on("hook:destroyed",(function(){return k(r,t)}));var u=function(n){for(var e=0,t=r.length;e<t;e++)r[e].$forceUpdate();n&&(r.length=0,null!==c&&(clearTimeout(c),c=null),null!==d&&(clearTimeout(d),d=null))},p=N((function(t){n.resolved=Ke(t,e),o?r.length=0:u(!0)})),m=N((function(e){a(n.errorComp)&&(n.error=!0,u(!0))})),h=n(p,m);return l(h)&&(g(h)?i(n.resolved)&&h.then(p,m):g(h.component)&&(h.component.then(p,m),a(h.error)&&(n.errorComp=Ke(h.error,e)),a(h.loading)&&(n.loadingComp=Ke(h.loading,e),0===h.delay?n.loading=!0:c=setTimeout((function(){c=null,i(n.resolved)&&i(n.error)&&(n.loading=!0,u(!1))}),h.delay||200)),a(h.timeout)&&(d=setTimeout((function(){d=null,i(n.resolved)&&m(null)}),h.timeout)))),o=!1,n.loading?n.loadingComp:n.resolved}}(u=n,d)))return function(n,e,t,r,o){var i=vn();return i.asyncFactory=n,i.asyncMeta={data:e,context:t,children:r,tag:o},i}(u,e,t,r,c);e=e||{},_t(n),a(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",r=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var o=e.on||(e.on={}),i=o[r],s=e.model.callback;a(i)?(Array.isArray(i)?-1===i.indexOf(s):i!==s)&&(o[r]=[s].concat(i)):o[r]=s}(n.options,e);var p=function(n,e,t){var r=e.options.props;if(!i(r)){var o={},s=n.attrs,c=n.props;if(a(s)||a(c))for(var l in r){var d=I(l);ue(o,c,l,d,!0)||ue(o,s,l,d,!1)}return o}}(e,n);if(s(n.options.functional))return function(n,e,t,r,i){var s=n.options,c={},l=s.props;if(a(l))for(var d in l)c[d]=Bn(d,l,e||o);else a(t.attrs)&&Le(c,t.attrs),a(t.props)&&Le(c,t.props);var u=new Re(t,c,i,r,n),p=s.render.call(null,u._c,u);if(p instanceof bn)return Ne(p,t,u.parent,s,u);if(Array.isArray(p)){for(var m=pe(p)||[],g=new Array(m.length),h=0;h<m.length;h++)g[h]=Ne(m[h],t,u.parent,s,u);return g}}(n,p,e,t,r);var m=e.on;if(e.on=e.nativeOn,s(n.options.abstract)){var h=e.slot;e={},h&&(e.slot=h)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<Be.length;t++){var r=Be[t],o=e[r],i=qe[r];o===i||o&&o._merged||(e[r]=o?$e(i,o):i)}}(e);var b=n.options.name||c;return new bn("vue-component-"+n.cid+(b?"-"+b:""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:p,listeners:m,tag:c,children:r},u)}}}function $e(n,e){var t=function(t,r){n(t,r),e(t,r)};return t._merged=!0,t}function Ge(n,e,t,r,o,d){return(Array.isArray(t)||c(t))&&(o=r,r=t,t=void 0),s(d)&&(o=2),function(n,e,t,r,o){if(a(t)&&a(t.__ob__))return vn();a(t)&&a(t.is)&&(e=t.is);if(!e)return vn();0;Array.isArray(r)&&"function"==typeof r[0]&&((t=t||{}).scopedSlots={default:r[0]},r.length=0);2===o?r=pe(r):1===o&&(r=function(n){for(var e=0;e<n.length;e++)if(Array.isArray(n[e]))return Array.prototype.concat.apply([],n);return n}(r));var c,d;if("string"==typeof e){var u;d=n.$vnode&&n.$vnode.ns||B.getTagNamespace(e),c=B.isReservedTag(e)?new bn(B.parsePlatformTagName(e),t,r,void 0,void 0,n):t&&t.pre||!a(u=qn(n.$options,"components",e))?new bn(e,t,r,void 0,void 0,n):Ue(u,t,n,r,e)}else c=Ue(e,t,n,r);return Array.isArray(c)?c:a(c)?(a(d)&&function n(e,t,r){e.ns=t,"foreignObject"===e.tag&&(t=void 0,r=!0);if(a(e.children))for(var o=0,c=e.children.length;o<c;o++){var l=e.children[o];a(l.tag)&&(i(l.ns)||s(r)&&"svg"!==l.tag)&&n(l,t,r)}}(c,d),a(t)&&function(n){l(n.style)&&ae(n.style);l(n.class)&&ae(n.class)}(t),c):vn()}(n,e,t,r,o)}var Fe,He=null;function Ke(n,e){return(n.__esModule||ln&&"Module"===n[Symbol.toStringTag])&&(n=n.default),l(n)?e.extend(n):n}function Je(n){if(Array.isArray(n))for(var e=0;e<n.length;e++){var t=n[e];if(a(t)&&(a(t.componentOptions)||fe(t)))return t}}function Ve(n,e){Fe.$on(n,e)}function We(n,e){Fe.$off(n,e)}function Qe(n,e){var t=Fe;return function r(){var o=e.apply(null,arguments);null!==o&&t.$off(n,r)}}function Ye(n,e,t){Fe=n,le(e,t||{},Ve,We,Qe,n),Fe=void 0}var Xe=null;function Ze(n){var e=Xe;return Xe=n,function(){Xe=e}}function nt(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function et(n,e){if(e){if(n._directInactive=!1,nt(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)et(n.$children[t]);tt(n,"activated")}}function tt(n,e){gn();var t=n.$options[e],r=e+" hook";if(t)for(var o=0,i=t.length;o<i;o++)Kn(t[o],n,null,n,r);n._hasHookEvent&&n.$emit("hook:"+e),hn()}var rt=[],ot=[],it={},at=!1,st=!1,ct=0;var lt=0,dt=Date.now;if(K&&!Q){var ut=window.performance;ut&&"function"==typeof ut.now&&dt()>document.createEvent("Event").timeStamp&&(dt=function(){return ut.now()})}function pt(){var n,e;for(lt=dt(),st=!0,rt.sort((function(n,e){return n.id-e.id})),ct=0;ct<rt.length;ct++)(n=rt[ct]).before&&n.before(),e=n.id,it[e]=null,n.run();var t=ot.slice(),r=rt.slice();ct=rt.length=ot.length=0,it={},at=st=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,et(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],r=t.vm;r._watcher===t&&r._isMounted&&!r._isDestroyed&&tt(r,"updated")}}(r),an&&B.devtools&&an.emit("flush")}var mt=0,gt=function(n,e,t,r,o){this.vm=n,o&&(n._watcher=this),n._watchers.push(this),r?(this.deep=!!r.deep,this.user=!!r.user,this.lazy=!!r.lazy,this.sync=!!r.sync,this.before=r.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++mt,this.active=!0,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new cn,this.newDepIds=new cn,this.expression="","function"==typeof e?this.getter=e:(this.getter=function(n){if(!G.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=z)),this.value=this.lazy?void 0:this.get()};gt.prototype.get=function(){var n;gn(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;Hn(n,e,'getter for watcher "'+this.expression+'"')}finally{this.deep&&ae(n),hn(),this.cleanupDeps()}return n},gt.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},gt.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},gt.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():function(n){var e=n.id;if(null==it[e]){if(it[e]=!0,st){for(var t=rt.length-1;t>ct&&rt[t].id>n.id;)t--;rt.splice(t+1,0,n)}else rt.push(n);at||(at=!0,oe(pt))}}(this)},gt.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||l(n)||this.deep){var e=this.value;if(this.value=n,this.user){var t='callback for watcher "'+this.expression+'"';Kn(this.cb,this.vm,[n,e],this.vm,t)}else this.cb.call(this.vm,n,e)}}},gt.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},gt.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},gt.prototype.teardown=function(){if(this.active){this.vm._isBeingDestroyed||k(this.vm._watchers,this);for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1}};var ht={enumerable:!0,configurable:!0,get:z,set:z};function bt(n,e,t){ht.get=function(){return this[e][t]},ht.set=function(n){this[e][t]=n},Object.defineProperty(n,t,ht)}function ft(n){n._watchers=[];var e=n.$options;e.props&&function(n,e){var t=n.$options.propsData||{},r=n._props={},o=n.$options._propKeys=[];n.$parent&&En(!1);var i=function(i){o.push(i);var a=Bn(i,e,t,n);jn(r,i,a),i in n||bt(n,"_props",i)};for(var a in e)i(a);En(!0)}(n,e.props),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?z:j(e[t],n)}(n,e.methods),e.data?function(n){var e=n.$options.data;u(e=n._data="function"==typeof e?function(n,e){gn();try{return n.call(e,e)}catch(n){return Hn(n,e,"data()"),{}}finally{hn()}}(e,n):e||{})||(e={});var t=Object.keys(e),r=n.$options.props,o=(n.$options.methods,t.length);for(;o--;){var i=t[o];0,r&&x(r,i)||(a=void 0,36!==(a=(i+"").charCodeAt(0))&&95!==a&&bt(n,"_data",i))}var a;In(e,!0)}(n):In(n._data={},!0),e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),r=on();for(var o in e){var i=e[o],a="function"==typeof i?i:i.get;0,r||(t[o]=new gt(n,a||z,z,vt)),o in n||kt(n,o,i)}}(n,e.computed),e.watch&&e.watch!==en&&function(n,e){for(var t in e){var r=e[t];if(Array.isArray(r))for(var o=0;o<r.length;o++)wt(n,t,r[o]);else wt(n,t,r)}}(n,e.watch)}var vt={lazy:!0};function kt(n,e,t){var r=!on();"function"==typeof t?(ht.get=r?yt(e):xt(t),ht.set=z):(ht.get=t.get?r&&!1!==t.cache?yt(e):xt(t.get):z,ht.set=t.set||z),Object.defineProperty(n,e,ht)}function yt(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),pn.target&&e.depend(),e.value}}function xt(n){return function(){return n.call(this,this)}}function wt(n,e,t,r){return u(t)&&(r=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,r)}var St=0;function _t(n){var e=n.options;if(n.super){var t=_t(n.super);if(t!==n.superOptions){n.superOptions=t;var r=function(n){var e,t=n.options,r=n.sealedOptions;for(var o in t)t[o]!==r[o]&&(e||(e={}),e[o]=t[o]);return e}(n);r&&C(n.extendOptions,r),(e=n.options=Ln(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function Et(n){this._init(n)}function Tt(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,r=t.cid,o=n._Ctor||(n._Ctor={});if(o[r])return o[r];var i=n.name||t.options.name;var a=function(n){this._init(n)};return(a.prototype=Object.create(t.prototype)).constructor=a,a.cid=e++,a.options=Ln(t.options,n),a.super=t,a.options.props&&function(n){var e=n.options.props;for(var t in e)bt(n.prototype,"_props",t)}(a),a.options.computed&&function(n){var e=n.options.computed;for(var t in e)kt(n.prototype,t,e[t])}(a),a.extend=t.extend,a.mixin=t.mixin,a.use=t.use,L.forEach((function(n){a[n]=t[n]})),i&&(a.options.components[i]=a),a.superOptions=t.options,a.extendOptions=n,a.sealedOptions=C({},a.options),o[r]=a,a}}function It(n){return n&&(n.Ctor.options.name||n.tag)}function jt(n,e){return Array.isArray(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!p(n)&&n.test(e)}function At(n,e){var t=n.cache,r=n.keys,o=n._vnode;for(var i in t){var a=t[i];if(a){var s=a.name;s&&!e(s)&&Ct(t,i,r,o)}}}function Ct(n,e,t,r){var o=n[e];!o||r&&o.tag===r.tag||o.componentInstance.$destroy(),n[e]=null,k(t,e)}!function(n){n.prototype._init=function(n){var e=this;e._uid=St++,e._isVue=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),r=e._parentVnode;t.parent=e.parent,t._parentVnode=r;var o=r.componentOptions;t.propsData=o.propsData,t._parentListeners=o.listeners,t._renderChildren=o.children,t._componentTag=o.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=Ln(_t(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&Ye(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,r=t&&t.context;n.$slots=he(e._renderChildren,r),n.$scopedSlots=o,n._c=function(e,t,r,o){return Ge(n,e,t,r,o,!1)},n.$createElement=function(e,t,r,o){return Ge(n,e,t,r,o,!0)};var i=t&&t.data;jn(n,"$attrs",i&&i.attrs||o,null,!0),jn(n,"$listeners",e._parentListeners||o,null,!0)}(e),tt(e,"beforeCreate"),function(n){var e=ge(n.$options.inject,n);e&&(En(!1),Object.keys(e).forEach((function(t){jn(n,t,e[t])})),En(!0))}(e),ft(e),function(n){var e=n.$options.provide;e&&(n._provided="function"==typeof e?e.call(n):e)}(e),tt(e,"created"),e.$options.el&&e.$mount(e.$options.el)}}(Et),function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=An,n.prototype.$delete=Cn,n.prototype.$watch=function(n,e,t){if(u(e))return wt(this,n,e,t);(t=t||{}).user=!0;var r=new gt(this,n,e,t);if(t.immediate){var o='callback for immediate watcher "'+r.expression+'"';gn(),Kn(e,this,[r.value],this,o),hn()}return function(){r.teardown()}}}(Et),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var r=this;if(Array.isArray(n))for(var o=0,i=n.length;o<i;o++)r.$on(n[o],t);else(r._events[n]||(r._events[n]=[])).push(t),e.test(n)&&(r._hasHookEvent=!0);return r},n.prototype.$once=function(n,e){var t=this;function r(){t.$off(n,r),e.apply(t,arguments)}return r.fn=e,t.$on(n,r),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(Array.isArray(n)){for(var r=0,o=n.length;r<o;r++)t.$off(n[r],e);return t}var i,a=t._events[n];if(!a)return t;if(!e)return t._events[n]=null,t;for(var s=a.length;s--;)if((i=a[s])===e||i.fn===e){a.splice(s,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?A(t):t;for(var r=A(arguments,1),o='event handler for "'+n+'"',i=0,a=t.length;i<a;i++)Kn(t[i],e,r,e,o)}return e}}(Et),function(n){n.prototype._update=function(n,e){var t=this,r=t.$el,o=t._vnode,i=Ze(t);t._vnode=n,t.$el=o?t.__patch__(o,n):t.__patch__(t.$el,n,e,!1),i(),r&&(r.__vue__=null),t.$el&&(t.$el.__vue__=t),t.$vnode&&t.$parent&&t.$vnode===t.$parent._vnode&&(t.$parent.$el=t.$el)},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){tt(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||k(e.$children,n),n._watcher&&n._watcher.teardown();for(var t=n._watchers.length;t--;)n._watchers[t].teardown();n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),tt(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(Et),function(n){Oe(n.prototype),n.prototype.$nextTick=function(n){return oe(n,this)},n.prototype._render=function(){var n,e=this,t=e.$options,r=t.render,o=t._parentVnode;o&&(e.$scopedSlots=ve(o.data.scopedSlots,e.$slots,e.$scopedSlots)),e.$vnode=o;try{He=e,n=r.call(e._renderProxy,e.$createElement)}catch(t){Hn(t,e,"render"),n=e._vnode}finally{He=null}return Array.isArray(n)&&1===n.length&&(n=n[0]),n instanceof bn||(n=vn()),n.parent=o,n}}(Et);var Dt=[String,RegExp,Array],zt={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Dt,exclude:Dt,max:[String,Number]},methods:{cacheVNode:function(){var n=this.cache,e=this.keys,t=this.vnodeToCache,r=this.keyToCache;if(t){var o=t.tag,i=t.componentInstance,a=t.componentOptions;n[r]={name:It(a),tag:o,componentInstance:i},e.push(r),this.max&&e.length>parseInt(this.max)&&Ct(n,e[0],e,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)Ct(this.cache,n,this.keys)},mounted:function(){var n=this;this.cacheVNode(),this.$watch("include",(function(e){At(n,(function(n){return jt(e,n)}))})),this.$watch("exclude",(function(e){At(n,(function(n){return!jt(e,n)}))}))},updated:function(){this.cacheVNode()},render:function(){var n=this.$slots.default,e=Je(n),t=e&&e.componentOptions;if(t){var r=It(t),o=this.include,i=this.exclude;if(o&&(!r||!jt(o,r))||i&&r&&jt(i,r))return e;var a=this.cache,s=this.keys,c=null==e.key?t.Ctor.cid+(t.tag?"::"+t.tag:""):e.key;a[c]?(e.componentInstance=a[c].componentInstance,k(s,c),s.push(c)):(this.vnodeToCache=e,this.keyToCache=c),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return B}};Object.defineProperty(n,"config",e),n.util={warn:dn,extend:C,mergeOptions:Ln,defineReactive:jn},n.set=An,n.delete=Cn,n.nextTick=oe,n.observable=function(n){return In(n),n},n.options=Object.create(null),L.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,C(n.options.components,zt),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=A(arguments,1);return t.unshift(this),"function"==typeof n.install?n.install.apply(n,t):"function"==typeof n&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=Ln(this.options,n),this}}(n),Tt(n),function(n){L.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&u(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&"function"==typeof t&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(Et),Object.defineProperty(Et.prototype,"$isServer",{get:on}),Object.defineProperty(Et.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Et,"FunctionalRenderContext",{value:Re}),Et.version="2.6.14";var Mt=f("style,class"),Pt=f("input,textarea,option,select,progress"),Ot=f("contenteditable,draggable,spellcheck"),Rt=f("events,caret,typing,plaintext-only"),Nt=f("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),Lt="http://www.w3.org/1999/xlink",qt=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},Bt=function(n){return qt(n)?n.slice(6,n.length):""},Ut=function(n){return null==n||!1===n};function $t(n){for(var e=n.data,t=n,r=n;a(r.componentInstance);)(r=r.componentInstance._vnode)&&r.data&&(e=Gt(r.data,e));for(;a(t=t.parent);)t&&t.data&&(e=Gt(e,t.data));return function(n,e){if(a(n)||a(e))return Ft(n,Ht(e));return""}(e.staticClass,e.class)}function Gt(n,e){return{staticClass:Ft(n.staticClass,e.staticClass),class:a(n.class)?[n.class,e.class]:e.class}}function Ft(n,e){return n?e?n+" "+e:n:e||""}function Ht(n){return Array.isArray(n)?function(n){for(var e,t="",r=0,o=n.length;r<o;r++)a(e=Ht(n[r]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):l(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var Kt={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Jt=f("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Vt=f("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Wt=function(n){return Jt(n)||Vt(n)};var Qt=Object.create(null);var Yt=f("text,number,password,search,email,tel,url");var Xt=Object.freeze({createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(Kt[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),Zt={create:function(n,e){nr(e)},update:function(n,e){n.data.ref!==e.data.ref&&(nr(n,!0),nr(e))},destroy:function(n){nr(n,!0)}};function nr(n,e){var t=n.data.ref;if(a(t)){var r=n.context,o=n.componentInstance||n.elm,i=r.$refs;e?Array.isArray(i[t])?k(i[t],o):i[t]===o&&(i[t]=void 0):n.data.refInFor?Array.isArray(i[t])?i[t].indexOf(o)<0&&i[t].push(o):i[t]=[o]:i[t]=o}}var er=new bn("",{},[]),tr=["create","activate","update","remove","destroy"];function rr(n,e){return n.key===e.key&&n.asyncFactory===e.asyncFactory&&(n.tag===e.tag&&n.isComment===e.isComment&&a(n.data)===a(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,r=a(t=n.data)&&a(t=t.attrs)&&t.type,o=a(t=e.data)&&a(t=t.attrs)&&t.type;return r===o||Yt(r)&&Yt(o)}(n,e)||s(n.isAsyncPlaceholder)&&i(e.asyncFactory.error))}function or(n,e,t){var r,o,i={};for(r=e;r<=t;++r)a(o=n[r].key)&&(i[o]=r);return i}var ir={create:ar,update:ar,destroy:function(n){ar(n,er)}};function ar(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,r,o,i=n===er,a=e===er,s=cr(n.data.directives,n.context),c=cr(e.data.directives,e.context),l=[],d=[];for(t in c)r=s[t],o=c[t],r?(o.oldValue=r.value,o.oldArg=r.arg,dr(o,"update",e,n),o.def&&o.def.componentUpdated&&d.push(o)):(dr(o,"bind",e,n),o.def&&o.def.inserted&&l.push(o));if(l.length){var u=function(){for(var t=0;t<l.length;t++)dr(l[t],"inserted",e,n)};i?de(e,"insert",u):u()}d.length&&de(e,"postpatch",(function(){for(var t=0;t<d.length;t++)dr(d[t],"componentUpdated",e,n)}));if(!i)for(t in s)c[t]||dr(s[t],"unbind",n,n,a)}(n,e)}var sr=Object.create(null);function cr(n,e){var t,r,o=Object.create(null);if(!n)return o;for(t=0;t<n.length;t++)(r=n[t]).modifiers||(r.modifiers=sr),o[lr(r)]=r,r.def=qn(e.$options,"directives",r.name);return o}function lr(n){return n.rawName||n.name+"."+Object.keys(n.modifiers||{}).join(".")}function dr(n,e,t,r,o){var i=n.def&&n.def[e];if(i)try{i(t.elm,n,t,r,o)}catch(r){Hn(r,t.context,"directive "+n.name+" "+e+" hook")}}var ur=[Zt,ir];function pr(n,e){var t=e.componentOptions;if(!(a(t)&&!1===t.Ctor.options.inheritAttrs||i(n.data.attrs)&&i(e.data.attrs))){var r,o,s=e.elm,c=n.data.attrs||{},l=e.data.attrs||{};for(r in a(l.__ob__)&&(l=e.data.attrs=C({},l)),l)o=l[r],c[r]!==o&&mr(s,r,o,e.data.pre);for(r in(Q||X)&&l.value!==c.value&&mr(s,"value",l.value),c)i(l[r])&&(qt(r)?s.removeAttributeNS(Lt,Bt(r)):Ot(r)||s.removeAttribute(r))}}function mr(n,e,t,r){r||n.tagName.indexOf("-")>-1?gr(n,e,t):Nt(e)?Ut(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):Ot(e)?n.setAttribute(e,function(n,e){return Ut(e)||"false"===e?"false":"contenteditable"===n&&Rt(e)?e:"true"}(e,t)):qt(e)?Ut(t)?n.removeAttributeNS(Lt,Bt(e)):n.setAttributeNS(Lt,e,t):gr(n,e,t)}function gr(n,e,t){if(Ut(t))n.removeAttribute(e);else{if(Q&&!Y&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var r=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",r)};n.addEventListener("input",r),n.__ieph=!0}n.setAttribute(e,t)}}var hr={create:pr,update:pr};function br(n,e){var t=e.elm,r=e.data,o=n.data;if(!(i(r.staticClass)&&i(r.class)&&(i(o)||i(o.staticClass)&&i(o.class)))){var s=$t(e),c=t._transitionClasses;a(c)&&(s=Ft(s,Ht(c))),s!==t._prevClass&&(t.setAttribute("class",s),t._prevClass=s)}}var fr,vr={create:br,update:br};function kr(n,e,t){var r=fr;return function o(){var i=e.apply(null,arguments);null!==i&&wr(n,o,t,r)}}var yr=Qn&&!(nn&&Number(nn[1])<=53);function xr(n,e,t,r){if(yr){var o=lt,i=e;e=i._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=o||n.timeStamp<=0||n.target.ownerDocument!==document)return i.apply(this,arguments)}}fr.addEventListener(n,e,tn?{capture:t,passive:r}:t)}function wr(n,e,t,r){(r||fr).removeEventListener(n,e._wrapper||e,t)}function Sr(n,e){if(!i(n.data.on)||!i(e.data.on)){var t=e.data.on||{},r=n.data.on||{};fr=e.elm,function(n){if(a(n.__r)){var e=Q?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}a(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),le(t,r,xr,wr,kr,e.context),fr=void 0}}var _r,Er={create:Sr,update:Sr};function Tr(n,e){if(!i(n.data.domProps)||!i(e.data.domProps)){var t,r,o=e.elm,s=n.data.domProps||{},c=e.data.domProps||{};for(t in a(c.__ob__)&&(c=e.data.domProps=C({},c)),s)t in c||(o[t]="");for(t in c){if(r=c[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),r===s[t])continue;1===o.childNodes.length&&o.removeChild(o.childNodes[0])}if("value"===t&&"PROGRESS"!==o.tagName){o._value=r;var l=i(r)?"":String(r);Ir(o,l)&&(o.value=l)}else if("innerHTML"===t&&Vt(o.tagName)&&i(o.innerHTML)){(_r=_r||document.createElement("div")).innerHTML="<svg>"+r+"</svg>";for(var d=_r.firstChild;o.firstChild;)o.removeChild(o.firstChild);for(;d.firstChild;)o.appendChild(d.firstChild)}else if(r!==s[t])try{o[t]=r}catch(n){}}}}function Ir(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,r=n._vModifiers;if(a(r)){if(r.number)return b(t)!==b(e);if(r.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var jr={create:Tr,update:Tr},Ar=w((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var r=n.split(t);r.length>1&&(e[r[0].trim()]=r[1].trim())}})),e}));function Cr(n){var e=Dr(n.style);return n.staticStyle?C(n.staticStyle,e):e}function Dr(n){return Array.isArray(n)?D(n):"string"==typeof n?Ar(n):n}var zr,Mr=/^--/,Pr=/\s*!important$/,Or=function(n,e,t){if(Mr.test(e))n.style.setProperty(e,t);else if(Pr.test(t))n.style.setProperty(I(e),t.replace(Pr,""),"important");else{var r=Nr(e);if(Array.isArray(t))for(var o=0,i=t.length;o<i;o++)n.style[r]=t[o];else n.style[r]=t}},Rr=["Webkit","Moz","ms"],Nr=w((function(n){if(zr=zr||document.createElement("div").style,"filter"!==(n=_(n))&&n in zr)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<Rr.length;t++){var r=Rr[t]+e;if(r in zr)return r}}));function Lr(n,e){var t=e.data,r=n.data;if(!(i(t.staticStyle)&&i(t.style)&&i(r.staticStyle)&&i(r.style))){var o,s,c=e.elm,l=r.staticStyle,d=r.normalizedStyle||r.style||{},u=l||d,p=Dr(e.data.style)||{};e.data.normalizedStyle=a(p.__ob__)?C({},p):p;var m=function(n,e){var t,r={};if(e)for(var o=n;o.componentInstance;)(o=o.componentInstance._vnode)&&o.data&&(t=Cr(o.data))&&C(r,t);(t=Cr(n.data))&&C(r,t);for(var i=n;i=i.parent;)i.data&&(t=Cr(i.data))&&C(r,t);return r}(e,!0);for(s in u)i(m[s])&&Or(c,s,"");for(s in m)(o=m[s])!==u[s]&&Or(c,s,null==o?"":o)}}var qr={create:Lr,update:Lr},Br=/\s+/;function Ur(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(Br).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" "+(n.getAttribute("class")||"")+" ";t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function $r(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(Br).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" "+(n.getAttribute("class")||"")+" ",r=" "+e+" ";t.indexOf(r)>=0;)t=t.replace(r," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function Gr(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&C(e,Fr(n.name||"v")),C(e,n),e}return"string"==typeof n?Fr(n):void 0}}var Fr=w((function(n){return{enterClass:n+"-enter",enterToClass:n+"-enter-to",enterActiveClass:n+"-enter-active",leaveClass:n+"-leave",leaveToClass:n+"-leave-to",leaveActiveClass:n+"-leave-active"}})),Hr=K&&!Y,Kr="transition",Jr="transitionend",Vr="animation",Wr="animationend";Hr&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Kr="WebkitTransition",Jr="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(Vr="WebkitAnimation",Wr="webkitAnimationEnd"));var Qr=K?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function Yr(n){Qr((function(){Qr(n)}))}function Xr(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),Ur(n,e))}function Zr(n,e){n._transitionClasses&&k(n._transitionClasses,e),$r(n,e)}function no(n,e,t){var r=to(n,e),o=r.type,i=r.timeout,a=r.propCount;if(!o)return t();var s="transition"===o?Jr:Wr,c=0,l=function(){n.removeEventListener(s,d),t()},d=function(e){e.target===n&&++c>=a&&l()};setTimeout((function(){c<a&&l()}),i+1),n.addEventListener(s,d)}var eo=/\b(transform|all)(,|$)/;function to(n,e){var t,r=window.getComputedStyle(n),o=(r[Kr+"Delay"]||"").split(", "),i=(r[Kr+"Duration"]||"").split(", "),a=ro(o,i),s=(r[Vr+"Delay"]||"").split(", "),c=(r[Vr+"Duration"]||"").split(", "),l=ro(s,c),d=0,u=0;return"transition"===e?a>0&&(t="transition",d=a,u=i.length):"animation"===e?l>0&&(t="animation",d=l,u=c.length):u=(t=(d=Math.max(a,l))>0?a>l?"transition":"animation":null)?"transition"===t?i.length:c.length:0,{type:t,timeout:d,propCount:u,hasTransform:"transition"===t&&eo.test(r[Kr+"Property"])}}function ro(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return oo(e)+oo(n[t])})))}function oo(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function io(n,e){var t=n.elm;a(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var r=Gr(n.data.transition);if(!i(r)&&!a(t._enterCb)&&1===t.nodeType){for(var o=r.css,s=r.type,c=r.enterClass,d=r.enterToClass,u=r.enterActiveClass,p=r.appearClass,m=r.appearToClass,g=r.appearActiveClass,h=r.beforeEnter,f=r.enter,v=r.afterEnter,k=r.enterCancelled,y=r.beforeAppear,x=r.appear,w=r.afterAppear,S=r.appearCancelled,_=r.duration,E=Xe,T=Xe.$vnode;T&&T.parent;)E=T.context,T=T.parent;var I=!E._isMounted||!n.isRootInsert;if(!I||x||""===x){var j=I&&p?p:c,A=I&&g?g:u,C=I&&m?m:d,D=I&&y||h,z=I&&"function"==typeof x?x:f,M=I&&w||v,P=I&&S||k,O=b(l(_)?_.enter:_);0;var R=!1!==o&&!Y,L=co(z),q=t._enterCb=N((function(){R&&(Zr(t,C),Zr(t,A)),q.cancelled?(R&&Zr(t,j),P&&P(t)):M&&M(t),t._enterCb=null}));n.data.show||de(n,"insert",(function(){var e=t.parentNode,r=e&&e._pending&&e._pending[n.key];r&&r.tag===n.tag&&r.elm._leaveCb&&r.elm._leaveCb(),z&&z(t,q)})),D&&D(t),R&&(Xr(t,j),Xr(t,A),Yr((function(){Zr(t,j),q.cancelled||(Xr(t,C),L||(so(O)?setTimeout(q,O):no(t,s,q)))}))),n.data.show&&(e&&e(),z&&z(t,q)),R||L||q()}}}function ao(n,e){var t=n.elm;a(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var r=Gr(n.data.transition);if(i(r)||1!==t.nodeType)return e();if(!a(t._leaveCb)){var o=r.css,s=r.type,c=r.leaveClass,d=r.leaveToClass,u=r.leaveActiveClass,p=r.beforeLeave,m=r.leave,g=r.afterLeave,h=r.leaveCancelled,f=r.delayLeave,v=r.duration,k=!1!==o&&!Y,y=co(m),x=b(l(v)?v.leave:v);0;var w=t._leaveCb=N((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),k&&(Zr(t,d),Zr(t,u)),w.cancelled?(k&&Zr(t,c),h&&h(t)):(e(),g&&g(t)),t._leaveCb=null}));f?f(S):S()}function S(){w.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),p&&p(t),k&&(Xr(t,c),Xr(t,u),Yr((function(){Zr(t,c),w.cancelled||(Xr(t,d),y||(so(x)?setTimeout(w,x):no(t,s,w)))}))),m&&m(t,w),k||y||w())}}function so(n){return"number"==typeof n&&!isNaN(n)}function co(n){if(i(n))return!1;var e=n.fns;return a(e)?co(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function lo(n,e){!0!==e.data.show&&io(e)}var uo=function(n){var e,t,r={},o=n.modules,l=n.nodeOps;for(e=0;e<tr.length;++e)for(r[tr[e]]=[],t=0;t<o.length;++t)a(o[t][tr[e]])&&r[tr[e]].push(o[t][tr[e]]);function d(n){var e=l.parentNode(n);a(e)&&l.removeChild(e,n)}function u(n,e,t,o,i,c,d){if(a(n.elm)&&a(c)&&(n=c[d]=yn(n)),n.isRootInsert=!i,!function(n,e,t,o){var i=n.data;if(a(i)){var c=a(n.componentInstance)&&i.keepAlive;if(a(i=i.hook)&&a(i=i.init)&&i(n,!1),a(n.componentInstance))return p(n,e),m(t,n.elm,o),s(c)&&function(n,e,t,o){var i,s=n;for(;s.componentInstance;)if(s=s.componentInstance._vnode,a(i=s.data)&&a(i=i.transition)){for(i=0;i<r.activate.length;++i)r.activate[i](er,s);e.push(s);break}m(t,n.elm,o)}(n,e,t,o),!0}}(n,e,t,o)){var u=n.data,h=n.children,f=n.tag;a(f)?(n.elm=n.ns?l.createElementNS(n.ns,f):l.createElement(f,n),v(n),g(n,h,e),a(u)&&b(n,e),m(t,n.elm,o)):s(n.isComment)?(n.elm=l.createComment(n.text),m(t,n.elm,o)):(n.elm=l.createTextNode(n.text),m(t,n.elm,o))}}function p(n,e){a(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,h(n)?(b(n,e),v(n)):(nr(n),e.push(n))}function m(n,e,t){a(n)&&(a(t)?l.parentNode(t)===n&&l.insertBefore(n,e,t):l.appendChild(n,e))}function g(n,e,t){if(Array.isArray(e)){0;for(var r=0;r<e.length;++r)u(e[r],t,n.elm,null,!0,e,r)}else c(n.text)&&l.appendChild(n.elm,l.createTextNode(String(n.text)))}function h(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return a(n.tag)}function b(n,t){for(var o=0;o<r.create.length;++o)r.create[o](er,n);a(e=n.data.hook)&&(a(e.create)&&e.create(er,n),a(e.insert)&&t.push(n))}function v(n){var e;if(a(e=n.fnScopeId))l.setStyleScope(n.elm,e);else for(var t=n;t;)a(e=t.context)&&a(e=e.$options._scopeId)&&l.setStyleScope(n.elm,e),t=t.parent;a(e=Xe)&&e!==n.context&&e!==n.fnContext&&a(e=e.$options._scopeId)&&l.setStyleScope(n.elm,e)}function k(n,e,t,r,o,i){for(;r<=o;++r)u(t[r],i,n,e,!1,t,r)}function y(n){var e,t,o=n.data;if(a(o))for(a(e=o.hook)&&a(e=e.destroy)&&e(n),e=0;e<r.destroy.length;++e)r.destroy[e](n);if(a(e=n.children))for(t=0;t<n.children.length;++t)y(n.children[t])}function x(n,e,t){for(;e<=t;++e){var r=n[e];a(r)&&(a(r.tag)?(w(r),y(r)):d(r.elm))}}function w(n,e){if(a(e)||a(n.data)){var t,o=r.remove.length+1;for(a(e)?e.listeners+=o:e=function(n,e){function t(){0==--t.listeners&&d(n)}return t.listeners=e,t}(n.elm,o),a(t=n.componentInstance)&&a(t=t._vnode)&&a(t.data)&&w(t,e),t=0;t<r.remove.length;++t)r.remove[t](n,e);a(t=n.data.hook)&&a(t=t.remove)?t(n,e):e()}else d(n.elm)}function S(n,e,t,r){for(var o=t;o<r;o++){var i=e[o];if(a(i)&&rr(n,i))return o}}function _(n,e,t,o,c,d){if(n!==e){a(e.elm)&&a(o)&&(e=o[c]=yn(e));var p=e.elm=n.elm;if(s(n.isAsyncPlaceholder))a(e.asyncFactory.resolved)?I(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(s(e.isStatic)&&s(n.isStatic)&&e.key===n.key&&(s(e.isCloned)||s(e.isOnce)))e.componentInstance=n.componentInstance;else{var m,g=e.data;a(g)&&a(m=g.hook)&&a(m=m.prepatch)&&m(n,e);var b=n.children,f=e.children;if(a(g)&&h(e)){for(m=0;m<r.update.length;++m)r.update[m](n,e);a(m=g.hook)&&a(m=m.update)&&m(n,e)}i(e.text)?a(b)&&a(f)?b!==f&&function(n,e,t,r,o){var s,c,d,p=0,m=0,g=e.length-1,h=e[0],b=e[g],f=t.length-1,v=t[0],y=t[f],w=!o;for(0;p<=g&&m<=f;)i(h)?h=e[++p]:i(b)?b=e[--g]:rr(h,v)?(_(h,v,r,t,m),h=e[++p],v=t[++m]):rr(b,y)?(_(b,y,r,t,f),b=e[--g],y=t[--f]):rr(h,y)?(_(h,y,r,t,f),w&&l.insertBefore(n,h.elm,l.nextSibling(b.elm)),h=e[++p],y=t[--f]):rr(b,v)?(_(b,v,r,t,m),w&&l.insertBefore(n,b.elm,h.elm),b=e[--g],v=t[++m]):(i(s)&&(s=or(e,p,g)),i(c=a(v.key)?s[v.key]:S(v,e,p,g))?u(v,r,n,h.elm,!1,t,m):rr(d=e[c],v)?(_(d,v,r,t,m),e[c]=void 0,w&&l.insertBefore(n,d.elm,h.elm)):u(v,r,n,h.elm,!1,t,m),v=t[++m]);p>g?k(n,i(t[f+1])?null:t[f+1].elm,t,m,f,r):m>f&&x(e,p,g)}(p,b,f,t,d):a(f)?(a(n.text)&&l.setTextContent(p,""),k(p,null,f,0,f.length-1,t)):a(b)?x(b,0,b.length-1):a(n.text)&&l.setTextContent(p,""):n.text!==e.text&&l.setTextContent(p,e.text),a(g)&&a(m=g.hook)&&a(m=m.postpatch)&&m(n,e)}}}function E(n,e,t){if(s(t)&&a(n.parent))n.parent.data.pendingInsert=e;else for(var r=0;r<e.length;++r)e[r].data.hook.insert(e[r])}var T=f("attrs,class,staticClass,staticStyle,key");function I(n,e,t,r){var o,i=e.tag,c=e.data,l=e.children;if(r=r||c&&c.pre,e.elm=n,s(e.isComment)&&a(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(a(c)&&(a(o=c.hook)&&a(o=o.init)&&o(e,!0),a(o=e.componentInstance)))return p(e,t),!0;if(a(i)){if(a(l))if(n.hasChildNodes())if(a(o=c)&&a(o=o.domProps)&&a(o=o.innerHTML)){if(o!==n.innerHTML)return!1}else{for(var d=!0,u=n.firstChild,m=0;m<l.length;m++){if(!u||!I(u,l[m],t,r)){d=!1;break}u=u.nextSibling}if(!d||u)return!1}else g(e,l,t);if(a(c)){var h=!1;for(var f in c)if(!T(f)){h=!0,b(e,t);break}!h&&c.class&&ae(c.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,o){if(!i(e)){var c,d=!1,p=[];if(i(n))d=!0,u(e,p);else{var m=a(n.nodeType);if(!m&&rr(n,e))_(n,e,p,null,null,o);else{if(m){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),s(t)&&I(n,e,p))return E(e,p,!0),n;c=n,n=new bn(l.tagName(c).toLowerCase(),{},[],void 0,c)}var g=n.elm,b=l.parentNode(g);if(u(e,p,g._leaveCb?null:b,l.nextSibling(g)),a(e.parent))for(var f=e.parent,v=h(e);f;){for(var k=0;k<r.destroy.length;++k)r.destroy[k](f);if(f.elm=e.elm,v){for(var w=0;w<r.create.length;++w)r.create[w](er,f);var S=f.data.hook.insert;if(S.merged)for(var T=1;T<S.fns.length;T++)S.fns[T]()}else nr(f);f=f.parent}a(b)?x([n],0,0):a(n.tag)&&y(n)}}return E(e,p,d),e.elm}a(n)&&y(n)}}({nodeOps:Xt,modules:[hr,vr,Er,jr,qr,K?{create:lo,activate:lo,remove:function(n,e){!0!==n.data.show?ao(n,e):e()}}:{}].concat(ur)});Y&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&ko(n,"input")}));var po={inserted:function(n,e,t,r){"select"===t.tag?(r.elm&&!r.elm._vOptions?de(t,"postpatch",(function(){po.componentUpdated(n,e,t)})):mo(n,e,t.context),n._vOptions=[].map.call(n.options,bo)):("textarea"===t.tag||Yt(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",fo),n.addEventListener("compositionend",vo),n.addEventListener("change",vo),Y&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){mo(n,e,t.context);var r=n._vOptions,o=n._vOptions=[].map.call(n.options,bo);if(o.some((function(n,e){return!O(n,r[e])})))(n.multiple?e.value.some((function(n){return ho(n,o)})):e.value!==e.oldValue&&ho(e.value,o))&&ko(n,"change")}}};function mo(n,e,t){go(n,e,t),(Q||X)&&setTimeout((function(){go(n,e,t)}),0)}function go(n,e,t){var r=e.value,o=n.multiple;if(!o||Array.isArray(r)){for(var i,a,s=0,c=n.options.length;s<c;s++)if(a=n.options[s],o)i=R(r,bo(a))>-1,a.selected!==i&&(a.selected=i);else if(O(bo(a),r))return void(n.selectedIndex!==s&&(n.selectedIndex=s));o||(n.selectedIndex=-1)}}function ho(n,e){return e.every((function(e){return!O(e,n)}))}function bo(n){return"_value"in n?n._value:n.value}function fo(n){n.target.composing=!0}function vo(n){n.target.composing&&(n.target.composing=!1,ko(n.target,"input"))}function ko(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function yo(n){return!n.componentInstance||n.data&&n.data.transition?n:yo(n.componentInstance._vnode)}var xo={model:po,show:{bind:function(n,e,t){var r=e.value,o=(t=yo(t)).data&&t.data.transition,i=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;r&&o?(t.data.show=!0,io(t,(function(){n.style.display=i}))):n.style.display=r?i:"none"},update:function(n,e,t){var r=e.value;!r!=!e.oldValue&&((t=yo(t)).data&&t.data.transition?(t.data.show=!0,r?io(t,(function(){n.style.display=n.__vOriginalDisplay})):ao(t,(function(){n.style.display="none"}))):n.style.display=r?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,r,o){o||(n.style.display=n.__vOriginalDisplay)}}},wo={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function So(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?So(Je(e.children)):n}function _o(n){var e={},t=n.$options;for(var r in t.propsData)e[r]=n[r];var o=t._parentListeners;for(var i in o)e[_(i)]=o[i];return e}function Eo(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var To=function(n){return n.tag||fe(n)},Io=function(n){return"show"===n.name},jo={name:"transition",props:wo,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(To)).length){0;var r=this.mode;0;var o=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return o;var i=So(o);if(!i)return o;if(this._leaving)return Eo(n,o);var a="__transition-"+this._uid+"-";i.key=null==i.key?i.isComment?a+"comment":a+i.tag:c(i.key)?0===String(i.key).indexOf(a)?i.key:a+i.key:i.key;var s=(i.data||(i.data={})).transition=_o(this),l=this._vnode,d=So(l);if(i.data.directives&&i.data.directives.some(Io)&&(i.data.show=!0),d&&d.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(i,d)&&!fe(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var u=d.data.transition=C({},s);if("out-in"===r)return this._leaving=!0,de(u,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),Eo(n,o);if("in-out"===r){if(fe(i))return l;var p,m=function(){p()};de(s,"afterEnter",m),de(s,"enterCancelled",m),de(u,"delayLeave",(function(n){p=n}))}}return o}}},Ao=C({tag:String,moveClass:String},wo);function Co(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function Do(n){n.data.newPos=n.elm.getBoundingClientRect()}function zo(n){var e=n.data.pos,t=n.data.newPos,r=e.left-t.left,o=e.top-t.top;if(r||o){n.data.moved=!0;var i=n.elm.style;i.transform=i.WebkitTransform="translate("+r+"px,"+o+"px)",i.transitionDuration="0s"}}delete Ao.mode;var Mo={Transition:jo,TransitionGroup:{props:Ao,beforeMount:function(){var n=this,e=this._update;this._update=function(t,r){var o=Ze(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,o(),e.call(n,t,r)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),r=this.prevChildren=this.children,o=this.$slots.default||[],i=this.children=[],a=_o(this),s=0;s<o.length;s++){var c=o[s];if(c.tag)if(null!=c.key&&0!==String(c.key).indexOf("__vlist"))i.push(c),t[c.key]=c,(c.data||(c.data={})).transition=a;else;}if(r){for(var l=[],d=[],u=0;u<r.length;u++){var p=r[u];p.data.transition=a,p.data.pos=p.elm.getBoundingClientRect(),t[p.key]?l.push(p):d.push(p)}this.kept=n(e,null,l),this.removed=d}return n(e,null,i)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(Co),n.forEach(Do),n.forEach(zo),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,r=t.style;Xr(t,e),r.transform=r.WebkitTransform=r.transitionDuration="",t.addEventListener(Jr,t._moveCb=function n(r){r&&r.target!==t||r&&!/transform$/.test(r.propertyName)||(t.removeEventListener(Jr,n),t._moveCb=null,Zr(t,e))})}})))},methods:{hasMove:function(n,e){if(!Hr)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){$r(t,n)})),Ur(t,e),t.style.display="none",this.$el.appendChild(t);var r=to(t);return this.$el.removeChild(t),this._hasMove=r.hasTransform}}}};Et.config.mustUseProp=function(n,e,t){return"value"===t&&Pt(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},Et.config.isReservedTag=Wt,Et.config.isReservedAttr=Mt,Et.config.getTagNamespace=function(n){return Vt(n)?"svg":"math"===n?"math":void 0},Et.config.isUnknownElement=function(n){if(!K)return!0;if(Wt(n))return!1;if(n=n.toLowerCase(),null!=Qt[n])return Qt[n];var e=document.createElement(n);return n.indexOf("-")>-1?Qt[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:Qt[n]=/HTMLUnknownElement/.test(e.toString())},C(Et.options.directives,xo),C(Et.options.components,Mo),Et.prototype.__patch__=K?uo:z,Et.prototype.$mount=function(n,e){return function(n,e,t){var r;return n.$el=e,n.$options.render||(n.$options.render=vn),tt(n,"beforeMount"),r=function(){n._update(n._render(),t)},new gt(n,r,z,{before:function(){n._isMounted&&!n._isDestroyed&&tt(n,"beforeUpdate")}},!0),t=!1,null==n.$vnode&&(n._isMounted=!0,tt(n,"mounted")),n}(this,n=n&&K?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},K&&setTimeout((function(){B.devtools&&an&&an.emit("init",Et)}),0);var Po=Et;
/*!
  * vue-router v3.5.3
  * (c) 2021 Evan You
  * @license MIT
  */function Oo(n,e){for(var t in e)n[t]=e[t];return n}var Ro=/[!'()*]/g,No=function(n){return"%"+n.charCodeAt(0).toString(16)},Lo=/%2C/g,qo=function(n){return encodeURIComponent(n).replace(Ro,No).replace(Lo,",")};function Bo(n){try{return decodeURIComponent(n)}catch(n){0}return n}var Uo=function(n){return null==n||"object"==typeof n?n:String(n)};function $o(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),r=Bo(t.shift()),o=t.length>0?Bo(t.join("=")):null;void 0===e[r]?e[r]=o:Array.isArray(e[r])?e[r].push(o):e[r]=[e[r],o]})),e):e}function Go(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return qo(e);if(Array.isArray(t)){var r=[];return t.forEach((function(n){void 0!==n&&(null===n?r.push(qo(e)):r.push(qo(e)+"="+qo(n)))})),r.join("&")}return qo(e)+"="+qo(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var Fo=/\/?$/;function Ho(n,e,t,r){var o=r&&r.options.stringifyQuery,i=e.query||{};try{i=Ko(i)}catch(n){}var a={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:i,params:e.params||{},fullPath:Wo(e,o),matched:n?Vo(n):[]};return t&&(a.redirectedFrom=Wo(t,o)),Object.freeze(a)}function Ko(n){if(Array.isArray(n))return n.map(Ko);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=Ko(n[t]);return e}return n}var Jo=Ho(null,{path:"/"});function Vo(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function Wo(n,e){var t=n.path,r=n.query;void 0===r&&(r={});var o=n.hash;return void 0===o&&(o=""),(t||"/")+(e||Go)(r)+o}function Qo(n,e,t){return e===Jo?n===e:!!e&&(n.path&&e.path?n.path.replace(Fo,"")===e.path.replace(Fo,"")&&(t||n.hash===e.hash&&Yo(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&Yo(n.query,e.query)&&Yo(n.params,e.params))))}function Yo(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),r=Object.keys(e).sort();return t.length===r.length&&t.every((function(t,o){var i=n[t];if(r[o]!==t)return!1;var a=e[t];return null==i||null==a?i===a:"object"==typeof i&&"object"==typeof a?Yo(i,a):String(i)===String(a)}))}function Xo(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var r in t.instances){var o=t.instances[r],i=t.enteredCbs[r];if(o&&i){delete t.enteredCbs[r];for(var a=0;a<i.length;a++)o._isBeingDestroyed||i[a](o)}}}}var Zo={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,r=e.children,o=e.parent,i=e.data;i.routerView=!0;for(var a=o.$createElement,s=t.name,c=o.$route,l=o._routerViewCache||(o._routerViewCache={}),d=0,u=!1;o&&o._routerRoot!==o;){var p=o.$vnode?o.$vnode.data:{};p.routerView&&d++,p.keepAlive&&o._directInactive&&o._inactive&&(u=!0),o=o.$parent}if(i.routerViewDepth=d,u){var m=l[s],g=m&&m.component;return g?(m.configProps&&ni(g,i,m.route,m.configProps),a(g,i,r)):a()}var h=c.matched[d],b=h&&h.components[s];if(!h||!b)return l[s]=null,a();l[s]={component:b},i.registerRouteInstance=function(n,e){var t=h.instances[s];(e&&t!==n||!e&&t===n)&&(h.instances[s]=e)},(i.hook||(i.hook={})).prepatch=function(n,e){h.instances[s]=e.componentInstance},i.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==h.instances[s]&&(h.instances[s]=n.componentInstance),Xo(c)};var f=h.props&&h.props[s];return f&&(Oo(l[s],{route:c,configProps:f}),ni(b,i,c,f)),a(b,i,r)}};function ni(n,e,t,r){var o=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,r);if(o){o=e.props=Oo({},o);var i=e.attrs=e.attrs||{};for(var a in o)n.props&&a in n.props||(i[a]=o[a],delete o[a])}}function ei(n,e,t){var r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;var o=e.split("/");t&&o[o.length-1]||o.pop();for(var i=n.replace(/^\//,"").split("/"),a=0;a<i.length;a++){var s=i[a];".."===s?o.pop():"."!==s&&o.push(s)}return""!==o[0]&&o.unshift(""),o.join("/")}function ti(n){return n.replace(/\/+/g,"/")}var ri=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},oi=vi,ii=di,ai=function(n,e){return pi(di(n,e),e)},si=pi,ci=fi,li=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function di(n,e){for(var t,r=[],o=0,i=0,a="",s=e&&e.delimiter||"/";null!=(t=li.exec(n));){var c=t[0],l=t[1],d=t.index;if(a+=n.slice(i,d),i=d+c.length,l)a+=l[1];else{var u=n[i],p=t[2],m=t[3],g=t[4],h=t[5],b=t[6],f=t[7];a&&(r.push(a),a="");var v=null!=p&&null!=u&&u!==p,k="+"===b||"*"===b,y="?"===b||"*"===b,x=t[2]||s,w=g||h;r.push({name:m||o++,prefix:p||"",delimiter:x,optional:y,repeat:k,partial:v,asterisk:!!f,pattern:w?gi(w):f?".*":"[^"+mi(x)+"]+?"})}}return i<n.length&&(a+=n.substr(i)),a&&r.push(a),r}function ui(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function pi(n,e){for(var t=new Array(n.length),r=0;r<n.length;r++)"object"==typeof n[r]&&(t[r]=new RegExp("^(?:"+n[r].pattern+")$",bi(e)));return function(e,r){for(var o="",i=e||{},a=(r||{}).pretty?ui:encodeURIComponent,s=0;s<n.length;s++){var c=n[s];if("string"!=typeof c){var l,d=i[c.name];if(null==d){if(c.optional){c.partial&&(o+=c.prefix);continue}throw new TypeError('Expected "'+c.name+'" to be defined')}if(ri(d)){if(!c.repeat)throw new TypeError('Expected "'+c.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(c.optional)continue;throw new TypeError('Expected "'+c.name+'" to not be empty')}for(var u=0;u<d.length;u++){if(l=a(d[u]),!t[s].test(l))throw new TypeError('Expected all "'+c.name+'" to match "'+c.pattern+'", but received `'+JSON.stringify(l)+"`");o+=(0===u?c.prefix:c.delimiter)+l}}else{if(l=c.asterisk?encodeURI(d).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):a(d),!t[s].test(l))throw new TypeError('Expected "'+c.name+'" to match "'+c.pattern+'", but received "'+l+'"');o+=c.prefix+l}}else o+=c}return o}}function mi(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function gi(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function hi(n,e){return n.keys=e,n}function bi(n){return n&&n.sensitive?"":"i"}function fi(n,e,t){ri(e)||(t=e||t,e=[]);for(var r=(t=t||{}).strict,o=!1!==t.end,i="",a=0;a<n.length;a++){var s=n[a];if("string"==typeof s)i+=mi(s);else{var c=mi(s.prefix),l="(?:"+s.pattern+")";e.push(s),s.repeat&&(l+="(?:"+c+l+")*"),i+=l=s.optional?s.partial?c+"("+l+")?":"(?:"+c+"("+l+"))?":c+"("+l+")"}}var d=mi(t.delimiter||"/"),u=i.slice(-d.length)===d;return r||(i=(u?i.slice(0,-d.length):i)+"(?:"+d+"(?=$))?"),i+=o?"$":r&&u?"":"(?="+d+"|$)",hi(new RegExp("^"+i,bi(t)),e)}function vi(n,e,t){return ri(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var r=0;r<t.length;r++)e.push({name:r,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return hi(n,e)}(n,e):ri(n)?function(n,e,t){for(var r=[],o=0;o<n.length;o++)r.push(vi(n[o],e,t).source);return hi(new RegExp("(?:"+r.join("|")+")",bi(t)),e)}(n,e,t):function(n,e,t){return fi(di(n,t),e,t)}(n,e,t)}oi.parse=ii,oi.compile=ai,oi.tokensToFunction=si,oi.tokensToRegExp=ci;var ki=Object.create(null);function yi(n,e,t){e=e||{};try{var r=ki[n]||(ki[n]=oi.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),r(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function xi(n,e,t,r){var o="string"==typeof n?{path:n}:n;if(o._normalized)return o;if(o.name){var i=(o=Oo({},n)).params;return i&&"object"==typeof i&&(o.params=Oo({},i)),o}if(!o.path&&o.params&&e){(o=Oo({},o))._normalized=!0;var a=Oo(Oo({},e.params),o.params);if(e.name)o.name=e.name,o.params=a;else if(e.matched.length){var s=e.matched[e.matched.length-1].path;o.path=yi(s,a,e.path)}else 0;return o}var c=function(n){var e="",t="",r=n.indexOf("#");r>=0&&(e=n.slice(r),n=n.slice(0,r));var o=n.indexOf("?");return o>=0&&(t=n.slice(o+1),n=n.slice(0,o)),{path:n,query:t,hash:e}}(o.path||""),l=e&&e.path||"/",d=c.path?ei(c.path,l,t||o.append):l,u=function(n,e,t){void 0===e&&(e={});var r,o=t||$o;try{r=o(n||"")}catch(n){r={}}for(var i in e){var a=e[i];r[i]=Array.isArray(a)?a.map(Uo):Uo(a)}return r}(c.query,o.query,r&&r.options.parseQuery),p=o.hash||c.hash;return p&&"#"!==p.charAt(0)&&(p="#"+p),{_normalized:!0,path:d,query:u,hash:p}}var wi,Si=function(){},_i={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,r=this.$route,o=t.resolve(this.to,r,this.append),i=o.location,a=o.route,s=o.href,c={},l=t.options.linkActiveClass,d=t.options.linkExactActiveClass,u=null==l?"router-link-active":l,p=null==d?"router-link-exact-active":d,m=null==this.activeClass?u:this.activeClass,g=null==this.exactActiveClass?p:this.exactActiveClass,h=a.redirectedFrom?Ho(null,xi(a.redirectedFrom),null,t):a;c[g]=Qo(r,h,this.exactPath),c[m]=this.exact||this.exactPath?c[g]:function(n,e){return 0===n.path.replace(Fo,"/").indexOf(e.path.replace(Fo,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(r,h);var b=c[g]?this.ariaCurrentValue:null,f=function(n){Ei(n)&&(e.replace?t.replace(i,Si):t.push(i,Si))},v={click:Ei};Array.isArray(this.event)?this.event.forEach((function(n){v[n]=f})):v[this.event]=f;var k={class:c},y=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:s,route:a,navigate:f,isActive:c[m],isExactActive:c[g]});if(y){if(1===y.length)return y[0];if(y.length>1||!y.length)return 0===y.length?n():n("span",{},y)}if("a"===this.tag)k.on=v,k.attrs={href:s,"aria-current":b};else{var x=function n(e){var t;if(e)for(var r=0;r<e.length;r++){if("a"===(t=e[r]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(x){x.isStatic=!1;var w=x.data=Oo({},x.data);for(var S in w.on=w.on||{},w.on){var _=w.on[S];S in v&&(w.on[S]=Array.isArray(_)?_:[_])}for(var E in v)E in w.on?w.on[E].push(v[E]):w.on[E]=f;var T=x.data.attrs=Oo({},x.data.attrs);T.href=s,T["aria-current"]=b}else k.on=v}return n(this.tag,k,this.$slots.default)}};function Ei(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var Ti="undefined"!=typeof window;function Ii(n,e,t,r,o){var i=e||[],a=t||Object.create(null),s=r||Object.create(null);n.forEach((function(n){!function n(e,t,r,o,i,a){var s=o.path,c=o.name;0;var l=o.pathToRegexpOptions||{},d=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return ti(e.path+"/"+n)}(s,i,l.strict);"boolean"==typeof o.caseSensitive&&(l.sensitive=o.caseSensitive);var u={path:d,regex:ji(d,l),components:o.components||{default:o.component},alias:o.alias?"string"==typeof o.alias?[o.alias]:o.alias:[],instances:{},enteredCbs:{},name:c,parent:i,matchAs:a,redirect:o.redirect,beforeEnter:o.beforeEnter,meta:o.meta||{},props:null==o.props?{}:o.components?o.props:{default:o.props}};o.children&&o.children.forEach((function(o){var i=a?ti(a+"/"+o.path):void 0;n(e,t,r,o,u,i)}));t[u.path]||(e.push(u.path),t[u.path]=u);if(void 0!==o.alias)for(var p=Array.isArray(o.alias)?o.alias:[o.alias],m=0;m<p.length;++m){0;var g={path:p[m],children:o.children};n(e,t,r,g,i,u.path||"/")}c&&(r[c]||(r[c]=u))}(i,a,s,n,o)}));for(var c=0,l=i.length;c<l;c++)"*"===i[c]&&(i.push(i.splice(c,1)[0]),l--,c--);return{pathList:i,pathMap:a,nameMap:s}}function ji(n,e){return oi(n,[],e)}function Ai(n,e){var t=Ii(n),r=t.pathList,o=t.pathMap,i=t.nameMap;function a(n,t,a){var s=xi(n,t,!1,e),l=s.name;if(l){var d=i[l];if(!d)return c(null,s);var u=d.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof s.params&&(s.params={}),t&&"object"==typeof t.params)for(var p in t.params)!(p in s.params)&&u.indexOf(p)>-1&&(s.params[p]=t.params[p]);return s.path=yi(d.path,s.params),c(d,s,a)}if(s.path){s.params={};for(var m=0;m<r.length;m++){var g=r[m],h=o[g];if(Ci(h.regex,s.path,s.params))return c(h,s,a)}}return c(null,s)}function s(n,t){var r=n.redirect,o="function"==typeof r?r(Ho(n,t,null,e)):r;if("string"==typeof o&&(o={path:o}),!o||"object"!=typeof o)return c(null,t);var s=o,l=s.name,d=s.path,u=t.query,p=t.hash,m=t.params;if(u=s.hasOwnProperty("query")?s.query:u,p=s.hasOwnProperty("hash")?s.hash:p,m=s.hasOwnProperty("params")?s.params:m,l){i[l];return a({_normalized:!0,name:l,query:u,hash:p,params:m},void 0,t)}if(d){var g=function(n,e){return ei(n,e.parent?e.parent.path:"/",!0)}(d,n);return a({_normalized:!0,path:yi(g,m),query:u,hash:p},void 0,t)}return c(null,t)}function c(n,t,r){return n&&n.redirect?s(n,r||t):n&&n.matchAs?function(n,e,t){var r=a({_normalized:!0,path:yi(t,e.params)});if(r){var o=r.matched,i=o[o.length-1];return e.params=r.params,c(i,e)}return c(null,e)}(0,t,n.matchAs):Ho(n,t,r,e)}return{match:a,addRoute:function(n,e){var t="object"!=typeof n?i[n]:void 0;Ii([e||n],r,o,i,t),t&&t.alias.length&&Ii(t.alias.map((function(n){return{path:n,children:[e]}})),r,o,i,t)},getRoutes:function(){return r.map((function(n){return o[n]}))},addRoutes:function(n){Ii(n,r,o,i)}}}function Ci(n,e,t){var r=e.match(n);if(!r)return!1;if(!t)return!0;for(var o=1,i=r.length;o<i;++o){var a=n.keys[o-1];a&&(t[a.name||"pathMatch"]="string"==typeof r[o]?Bo(r[o]):r[o])}return!0}var Di=Ti&&window.performance&&window.performance.now?window.performance:Date;function zi(){return Di.now().toFixed(3)}var Mi=zi();function Pi(){return Mi}function Oi(n){return Mi=n}var Ri=Object.create(null);function Ni(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=Oo({},window.history.state);return t.key=Pi(),window.history.replaceState(t,"",e),window.addEventListener("popstate",Bi),function(){window.removeEventListener("popstate",Bi)}}function Li(n,e,t,r){if(n.app){var o=n.options.scrollBehavior;o&&n.app.$nextTick((function(){var i=function(){var n=Pi();if(n)return Ri[n]}(),a=o.call(n,e,t,r?i:null);a&&("function"==typeof a.then?a.then((function(n){Hi(n,i)})).catch((function(n){0})):Hi(a,i))}))}}function qi(){var n=Pi();n&&(Ri[n]={x:window.pageXOffset,y:window.pageYOffset})}function Bi(n){qi(),n.state&&n.state.key&&Oi(n.state.key)}function Ui(n){return Gi(n.x)||Gi(n.y)}function $i(n){return{x:Gi(n.x)?n.x:window.pageXOffset,y:Gi(n.y)?n.y:window.pageYOffset}}function Gi(n){return"number"==typeof n}var Fi=/^#\d/;function Hi(n,e){var t,r="object"==typeof n;if(r&&"string"==typeof n.selector){var o=Fi.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(o){var i=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),r=n.getBoundingClientRect();return{x:r.left-t.left-e.x,y:r.top-t.top-e.y}}(o,i={x:Gi((t=i).x)?t.x:0,y:Gi(t.y)?t.y:0})}else Ui(n)&&(e=$i(n))}else r&&Ui(n)&&(e=$i(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var Ki,Ji=Ti&&((-1===(Ki=window.navigator.userAgent).indexOf("Android 2.")&&-1===Ki.indexOf("Android 4.0")||-1===Ki.indexOf("Mobile Safari")||-1!==Ki.indexOf("Chrome")||-1!==Ki.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function Vi(n,e){qi();var t=window.history;try{if(e){var r=Oo({},t.state);r.key=Pi(),t.replaceState(r,"",n)}else t.pushState({key:Oi(zi())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function Wi(n){Vi(n,!0)}function Qi(n,e,t){var r=function(o){o>=n.length?t():n[o]?e(n[o],(function(){r(o+1)})):r(o+1)};r(0)}var Yi={redirected:2,aborted:4,cancelled:8,duplicated:16};function Xi(n,e){return na(n,e,Yi.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return ea.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function Zi(n,e){return na(n,e,Yi.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function na(n,e,t,r){var o=new Error(r);return o._isRouter=!0,o.from=n,o.to=e,o.type=t,o}var ea=["params","query","hash"];function ta(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function ra(n,e){return ta(n)&&n._isRouter&&(null==e||n.type===e)}function oa(n){return function(e,t,r){var o=!1,i=0,a=null;ia(n,(function(n,e,t,s){if("function"==typeof n&&void 0===n.cid){o=!0,i++;var c,l=ca((function(e){var o;((o=e).__esModule||sa&&"Module"===o[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:wi.extend(e),t.components[s]=e,--i<=0&&r()})),d=ca((function(n){var e="Failed to resolve async component "+s+": "+n;a||(a=ta(n)?n:new Error(e),r(a))}));try{c=n(l,d)}catch(n){d(n)}if(c)if("function"==typeof c.then)c.then(l,d);else{var u=c.component;u&&"function"==typeof u.then&&u.then(l,d)}}})),o||r()}}function ia(n,e){return aa(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function aa(n){return Array.prototype.concat.apply([],n)}var sa="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function ca(n){var e=!1;return function(){for(var t=[],r=arguments.length;r--;)t[r]=arguments[r];if(!e)return e=!0,n.apply(this,t)}}var la=function(n,e){this.router=n,this.base=function(n){if(!n)if(Ti){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=Jo,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function da(n,e,t,r){var o=ia(n,(function(n,r,o,i){var a=function(n,e){"function"!=typeof n&&(n=wi.extend(n));return n.options[e]}(n,e);if(a)return Array.isArray(a)?a.map((function(n){return t(n,r,o,i)})):t(a,r,o,i)}));return aa(r?o.reverse():o)}function ua(n,e){if(e)return function(){return n.apply(e,arguments)}}la.prototype.listen=function(n){this.cb=n},la.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},la.prototype.onError=function(n){this.errorCbs.push(n)},la.prototype.transitionTo=function(n,e,t){var r,o=this;try{r=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var i=this.current;this.confirmTransition(r,(function(){o.updateRoute(r),e&&e(r),o.ensureURL(),o.router.afterHooks.forEach((function(n){n&&n(r,i)})),o.ready||(o.ready=!0,o.readyCbs.forEach((function(n){n(r)})))}),(function(n){t&&t(n),n&&!o.ready&&(ra(n,Yi.redirected)&&i===Jo||(o.ready=!0,o.readyErrorCbs.forEach((function(e){e(n)}))))}))},la.prototype.confirmTransition=function(n,e,t){var r=this,o=this.current;this.pending=n;var i,a,s=function(n){!ra(n)&&ta(n)&&(r.errorCbs.length?r.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},c=n.matched.length-1,l=o.matched.length-1;if(Qo(n,o)&&c===l&&n.matched[c]===o.matched[l])return this.ensureURL(),n.hash&&Li(this.router,o,n,!1),s(((a=na(i=o,n,Yi.duplicated,'Avoided redundant navigation to current location: "'+i.fullPath+'".')).name="NavigationDuplicated",a));var d=function(n,e){var t,r=Math.max(n.length,e.length);for(t=0;t<r&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),u=d.updated,p=d.deactivated,m=d.activated,g=[].concat(function(n){return da(n,"beforeRouteLeave",ua,!0)}(p),this.router.beforeHooks,function(n){return da(n,"beforeRouteUpdate",ua)}(u),m.map((function(n){return n.beforeEnter})),oa(m)),h=function(e,t){if(r.pending!==n)return s(Zi(o,n));try{e(n,o,(function(e){!1===e?(r.ensureURL(!0),s(function(n,e){return na(n,e,Yi.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(o,n))):ta(e)?(r.ensureURL(!0),s(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(s(Xi(o,n)),"object"==typeof e&&e.replace?r.replace(e):r.push(e)):t(e)}))}catch(n){s(n)}};Qi(g,h,(function(){Qi(function(n){return da(n,"beforeRouteEnter",(function(n,e,t,r){return function(n,e,t){return function(r,o,i){return n(r,o,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),i(n)}))}}(n,t,r)}))}(m).concat(r.router.resolveHooks),h,(function(){if(r.pending!==n)return s(Zi(o,n));r.pending=null,e(n),r.router.app&&r.router.app.$nextTick((function(){Xo(n)}))}))}))},la.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},la.prototype.setupListeners=function(){},la.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=Jo,this.pending=null};var pa=function(n){function e(e,t){n.call(this,e,t),this._startLocation=ma(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,r=Ji&&t;r&&this.listeners.push(Ni());var o=function(){var t=n.current,o=ma(n.base);n.current===Jo&&o===n._startLocation||n.transitionTo(o,(function(n){r&&Li(e,n,t,!0)}))};window.addEventListener("popstate",o),this.listeners.push((function(){window.removeEventListener("popstate",o)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var r=this,o=this.current;this.transitionTo(n,(function(n){Vi(ti(r.base+n.fullPath)),Li(r.router,n,o,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,o=this.current;this.transitionTo(n,(function(n){Wi(ti(r.base+n.fullPath)),Li(r.router,n,o,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(ma(this.base)!==this.current.fullPath){var e=ti(this.base+this.current.fullPath);n?Vi(e):Wi(e)}},e.prototype.getCurrentLocation=function(){return ma(this.base)},e}(la);function ma(n){var e=window.location.pathname,t=e.toLowerCase(),r=n.toLowerCase();return!n||t!==r&&0!==t.indexOf(ti(r+"/"))||(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var ga=function(n){function e(e,t,r){n.call(this,e,t),r&&function(n){var e=ma(n);if(!/^\/#/.test(e))return window.location.replace(ti(n+"/#"+e)),!0}(this.base)||ha()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=Ji&&e;t&&this.listeners.push(Ni());var r=function(){var e=n.current;ha()&&n.transitionTo(ba(),(function(r){t&&Li(n.router,r,e,!0),Ji||ka(r.fullPath)}))},o=Ji?"popstate":"hashchange";window.addEventListener(o,r),this.listeners.push((function(){window.removeEventListener(o,r)}))}},e.prototype.push=function(n,e,t){var r=this,o=this.current;this.transitionTo(n,(function(n){va(n.fullPath),Li(r.router,n,o,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,o=this.current;this.transitionTo(n,(function(n){ka(n.fullPath),Li(r.router,n,o,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;ba()!==e&&(n?va(e):ka(e))},e.prototype.getCurrentLocation=function(){return ba()},e}(la);function ha(){var n=ba();return"/"===n.charAt(0)||(ka("/"+n),!1)}function ba(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function fa(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function va(n){Ji?Vi(fa(n)):window.location.hash=n}function ka(n){Ji?Wi(fa(n)):window.location.replace(fa(n))}var ya=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index+1).concat(n),r.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var r=this.stack[t];this.confirmTransition(r,(function(){var n=e.current;e.index=t,e.updateRoute(r),e.router.afterHooks.forEach((function(e){e&&e(r,n)}))}),(function(n){ra(n,Yi.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}(la),xa=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Ai(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!Ji&&!1!==n.fallback,this.fallback&&(e="hash"),Ti||(e="abstract"),this.mode=e,e){case"history":this.history=new pa(this,n.base);break;case"hash":this.history=new ga(this,n.base,this.fallback);break;case"abstract":this.history=new ya(this,n.base);break;default:0}},wa={currentRoute:{configurable:!0}};function Sa(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}xa.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},wa.currentRoute.get=function(){return this.history&&this.history.current},xa.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof pa||t instanceof ga){var r=function(n){t.setupListeners(),function(n){var r=t.current,o=e.options.scrollBehavior;Ji&&o&&"fullPath"in n&&Li(e,n,r,!1)}(n)};t.transitionTo(t.getCurrentLocation(),r,r)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},xa.prototype.beforeEach=function(n){return Sa(this.beforeHooks,n)},xa.prototype.beforeResolve=function(n){return Sa(this.resolveHooks,n)},xa.prototype.afterEach=function(n){return Sa(this.afterHooks,n)},xa.prototype.onReady=function(n,e){this.history.onReady(n,e)},xa.prototype.onError=function(n){this.history.onError(n)},xa.prototype.push=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.push(n,e,t)}));this.history.push(n,e,t)},xa.prototype.replace=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.replace(n,e,t)}));this.history.replace(n,e,t)},xa.prototype.go=function(n){this.history.go(n)},xa.prototype.back=function(){this.go(-1)},xa.prototype.forward=function(){this.go(1)},xa.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},xa.prototype.resolve=function(n,e,t){var r=xi(n,e=e||this.history.current,t,this),o=this.match(r,e),i=o.redirectedFrom||o.fullPath;return{location:r,route:o,href:function(n,e,t){var r="hash"===t?"#"+e:e;return n?ti(n+"/"+r):r}(this.history.base,i,this.mode),normalizedTo:r,resolved:o}},xa.prototype.getRoutes=function(){return this.matcher.getRoutes()},xa.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==Jo&&this.history.transitionTo(this.history.getCurrentLocation())},xa.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==Jo&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(xa.prototype,wa),xa.install=function n(e){if(!n.installed||wi!==e){n.installed=!0,wi=e;var t=function(n){return void 0!==n},r=function(n,e){var r=n.$options._parentVnode;t(r)&&t(r=r.data)&&t(r=r.registerRouteInstance)&&r(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,r(this,this)},destroyed:function(){r(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",Zo),e.component("RouterLink",_i);var o=e.config.optionMergeStrategies;o.beforeRouteEnter=o.beforeRouteLeave=o.beforeRouteUpdate=o.created}},xa.version="3.5.3",xa.isNavigationFailure=ra,xa.NavigationFailureType=Yi,xa.START_LOCATION=Jo,Ti&&window.Vue&&window.Vue.use(xa);var _a=xa;t(225),t(174),t(251),t(100),t(253),t(32),t(33),t(254);function Ea(n){n.locales&&Object.keys(n.locales).forEach((function(e){n.locales[e].path=e})),Object.freeze(n)}t(76),t(91),t(121);function Ta(n){return(Ta="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n})(n)}var Ia=t(70),ja=(t(184),t(18),t(45),t(227),t(228),t(40),t(25),{NotFound:function(){return Promise.all([t.e(0),t.e(5)]).then(t.bind(null,476))},Layout:function(){return Promise.all([t.e(0),t.e(2)]).then(t.bind(null,475))}}),Aa={"v-dc140a80":function(){return t.e(6).then(t.bind(null,477))},"v-c6d7e3c0":function(){return t.e(7).then(t.bind(null,478))},"v-812c033c":function(){return t.e(8).then(t.bind(null,479))},"v-30b385c2":function(){return t.e(9).then(t.bind(null,480))},"v-5a2cd282":function(){return t.e(10).then(t.bind(null,481))},"v-2f93a0e6":function(){return t.e(11).then(t.bind(null,482))},"v-95827e74":function(){return t.e(12).then(t.bind(null,483))},"v-00e5aa38":function(){return t.e(13).then(t.bind(null,484))},"v-ea2cdb88":function(){return t.e(14).then(t.bind(null,485))},"v-63a62e60":function(){return t.e(15).then(t.bind(null,486))},"v-bded170c":function(){return t.e(16).then(t.bind(null,487))},"v-0e25e56a":function(){return t.e(17).then(t.bind(null,488))},"v-68dd6adb":function(){return t.e(18).then(t.bind(null,489))},"v-1893221c":function(){return t.e(19).then(t.bind(null,490))},"v-1094a947":function(){return t.e(20).then(t.bind(null,491))},"v-9690772c":function(){return t.e(21).then(t.bind(null,492))},"v-2ded6bf9":function(){return t.e(22).then(t.bind(null,493))},"v-1f7d690b":function(){return t.e(23).then(t.bind(null,494))},"v-56be3166":function(){return t.e(24).then(t.bind(null,495))},"v-02353645":function(){return t.e(25).then(t.bind(null,496))},"v-7bc439b8":function(){return t.e(26).then(t.bind(null,497))},"v-8a53495c":function(){return t.e(27).then(t.bind(null,498))},"v-3834644e":function(){return t.e(28).then(t.bind(null,499))},"v-a54afc5a":function(){return t.e(29).then(t.bind(null,500))},"v-0a8f6788":function(){return t.e(30).then(t.bind(null,501))},"v-cfad191a":function(){return t.e(31).then(t.bind(null,502))},"v-2a54cea8":function(){return t.e(32).then(t.bind(null,503))},"v-0a4aaf0e":function(){return t.e(33).then(t.bind(null,504))},"v-93569b68":function(){return t.e(34).then(t.bind(null,505))},"v-eba23aa0":function(){return t.e(35).then(t.bind(null,506))},"v-2d9e6cf8":function(){return t.e(36).then(t.bind(null,507))},"v-7d6fc128":function(){return t.e(37).then(t.bind(null,508))},"v-98da2ee0":function(){return t.e(38).then(t.bind(null,509))},"v-9bb044a0":function(){return t.e(39).then(t.bind(null,510))},"v-12e16df6":function(){return t.e(40).then(t.bind(null,511))},"v-6a35a7dc":function(){return t.e(41).then(t.bind(null,512))},"v-4c567690":function(){return t.e(42).then(t.bind(null,513))},"v-3fe1ff73":function(){return t.e(43).then(t.bind(null,514))},"v-20b52a0c":function(){return t.e(44).then(t.bind(null,515))},"v-1a563de3":function(){return t.e(45).then(t.bind(null,516))},"v-1ce7bb32":function(){return t.e(46).then(t.bind(null,517))},"v-095cc5ff":function(){return t.e(47).then(t.bind(null,518))},"v-19944e88":function(){return t.e(48).then(t.bind(null,519))},"v-35935e9c":function(){return t.e(49).then(t.bind(null,520))},"v-0a2f2246":function(){return t.e(50).then(t.bind(null,521))},"v-150d8184":function(){return t.e(54).then(t.bind(null,522))},"v-e5b55ae4":function(){return t.e(55).then(t.bind(null,523))},"v-4fa0bc5f":function(){return t.e(56).then(t.bind(null,524))},"v-536252ab":function(){return t.e(57).then(t.bind(null,525))},"v-6694efc8":function(){return t.e(58).then(t.bind(null,526))},"v-2835dba9":function(){return t.e(59).then(t.bind(null,527))},"v-37c9db00":function(){return t.e(60).then(t.bind(null,528))},"v-30105e6e":function(){return t.e(61).then(t.bind(null,529))},"v-a983545e":function(){return t.e(62).then(t.bind(null,530))},"v-5adc669c":function(){return t.e(63).then(t.bind(null,531))},"v-0670b552":function(){return t.e(64).then(t.bind(null,532))},"v-685bb760":function(){return t.e(65).then(t.bind(null,533))},"v-7dad8b02":function(){return t.e(66).then(t.bind(null,534))},"v-77b753e2":function(){return t.e(67).then(t.bind(null,535))},"v-220226bd":function(){return t.e(68).then(t.bind(null,536))},"v-d809a770":function(){return t.e(69).then(t.bind(null,537))},"v-e3e7063c":function(){return t.e(70).then(t.bind(null,538))},"v-704eb51c":function(){return t.e(71).then(t.bind(null,539))},"v-48944d5a":function(){return t.e(72).then(t.bind(null,540))},"v-695dfcb6":function(){return t.e(73).then(t.bind(null,541))},"v-7ddb1434":function(){return t.e(74).then(t.bind(null,542))},"v-f26a2222":function(){return t.e(75).then(t.bind(null,543))},"v-5f044039":function(){return t.e(76).then(t.bind(null,544))},"v-310b44d6":function(){return t.e(77).then(t.bind(null,545))},"v-0163bee7":function(){return t.e(78).then(t.bind(null,546))},"v-a4144134":function(){return t.e(79).then(t.bind(null,547))},"v-818a32e0":function(){return t.e(80).then(t.bind(null,548))},"v-8ba9dd7a":function(){return t.e(81).then(t.bind(null,549))},"v-8cb52cfc":function(){return t.e(82).then(t.bind(null,550))},"v-48788de2":function(){return t.e(83).then(t.bind(null,551))},"v-4c9ef69c":function(){return t.e(84).then(t.bind(null,552))},"v-c80a5a9c":function(){return t.e(85).then(t.bind(null,553))},"v-e4187760":function(){return t.e(86).then(t.bind(null,554))},"v-29934bff":function(){return t.e(87).then(t.bind(null,555))},"v-6b185955":function(){return t.e(88).then(t.bind(null,556))},"v-9431b692":function(){return t.e(89).then(t.bind(null,557))},"v-79bebf43":function(){return t.e(90).then(t.bind(null,558))},"v-bf7747fa":function(){return t.e(91).then(t.bind(null,559))},"v-2a4a0bb7":function(){return t.e(92).then(t.bind(null,560))},"v-7f6889c6":function(){return t.e(93).then(t.bind(null,561))},"v-459c694a":function(){return t.e(94).then(t.bind(null,562))},"v-7a72d387":function(){return t.e(95).then(t.bind(null,563))},"v-e6c9547a":function(){return t.e(96).then(t.bind(null,564))},"v-1128e807":function(){return t.e(97).then(t.bind(null,565))},"v-a77e2b92":function(){return t.e(98).then(t.bind(null,566))},"v-40000776":function(){return t.e(51).then(t.bind(null,567))},"v-2ac12114":function(){return t.e(99).then(t.bind(null,568))},"v-d416917a":function(){return t.e(100).then(t.bind(null,569))},"v-3785d681":function(){return t.e(101).then(t.bind(null,570))},"v-117cc90e":function(){return t.e(102).then(t.bind(null,571))},"v-84e3ebde":function(){return t.e(103).then(t.bind(null,572))},"v-427cb080":function(){return t.e(104).then(t.bind(null,573))},"v-03ae78c4":function(){return t.e(105).then(t.bind(null,574))},"v-931f934a":function(){return t.e(106).then(t.bind(null,575))},"v-4b4c35dc":function(){return t.e(107).then(t.bind(null,576))},"v-0a92eeec":function(){return t.e(109).then(t.bind(null,577))},"v-c205d0b2":function(){return t.e(108).then(t.bind(null,578))},"v-50673fcc":function(){return t.e(110).then(t.bind(null,579))},"v-6680c492":function(){return t.e(111).then(t.bind(null,580))},"v-12cc11ea":function(){return t.e(113).then(t.bind(null,581))},"v-8f47802a":function(){return t.e(112).then(t.bind(null,582))},"v-dc3c5474":function(){return t.e(114).then(t.bind(null,583))},"v-08740c32":function(){return t.e(115).then(t.bind(null,584))},"v-3704fefa":function(){return t.e(116).then(t.bind(null,585))},"v-fc4d58d8":function(){return t.e(118).then(t.bind(null,586))},"v-1dd07656":function(){return t.e(119).then(t.bind(null,587))},"v-13de5e4a":function(){return t.e(120).then(t.bind(null,588))},"v-c2d87842":function(){return t.e(121).then(t.bind(null,589))},"v-c1a66828":function(){return t.e(122).then(t.bind(null,590))},"v-ed96cd98":function(){return t.e(123).then(t.bind(null,591))},"v-19430f44":function(){return t.e(124).then(t.bind(null,592))},"v-5b9e5448":function(){return t.e(125).then(t.bind(null,593))},"v-3058349d":function(){return t.e(126).then(t.bind(null,594))},"v-741ac0fb":function(){return t.e(127).then(t.bind(null,595))},"v-0dc6bf26":function(){return t.e(128).then(t.bind(null,596))},"v-17b7bcd2":function(){return t.e(129).then(t.bind(null,597))},"v-397e4290":function(){return t.e(130).then(t.bind(null,598))},"v-4b0f9931":function(){return t.e(131).then(t.bind(null,599))},"v-2046e738":function(){return t.e(132).then(t.bind(null,600))},"v-05980e1b":function(){return t.e(133).then(t.bind(null,601))},"v-4637e644":function(){return t.e(134).then(t.bind(null,602))},"v-5b6a58d6":function(){return t.e(135).then(t.bind(null,603))},"v-a1271344":function(){return t.e(136).then(t.bind(null,604))},"v-79b29984":function(){return t.e(137).then(t.bind(null,605))},"v-782329dc":function(){return t.e(138).then(t.bind(null,606))},"v-36dcd140":function(){return t.e(139).then(t.bind(null,607))},"v-02185a76":function(){return t.e(140).then(t.bind(null,608))},"v-38a7db08":function(){return t.e(141).then(t.bind(null,609))},"v-1bf741b0":function(){return t.e(142).then(t.bind(null,610))},"v-74089f43":function(){return t.e(143).then(t.bind(null,611))},"v-16c13813":function(){return t.e(144).then(t.bind(null,612))},"v-65ae1b58":function(){return t.e(145).then(t.bind(null,613))},"v-92e32acc":function(){return t.e(146).then(t.bind(null,614))},"v-12b8a377":function(){return t.e(147).then(t.bind(null,615))},"v-dd62df30":function(){return t.e(148).then(t.bind(null,616))},"v-1247ca23":function(){return t.e(149).then(t.bind(null,617))},"v-39ac2c18":function(){return t.e(150).then(t.bind(null,618))},"v-80330662":function(){return t.e(151).then(t.bind(null,619))},"v-73993688":function(){return t.e(152).then(t.bind(null,620))},"v-570536b6":function(){return t.e(153).then(t.bind(null,621))},"v-045a323e":function(){return t.e(154).then(t.bind(null,622))},"v-ae2ed992":function(){return t.e(155).then(t.bind(null,623))},"v-059385f2":function(){return t.e(156).then(t.bind(null,624))},"v-687cd7b8":function(){return t.e(157).then(t.bind(null,625))},"v-f8655a0e":function(){return t.e(158).then(t.bind(null,626))},"v-093f317f":function(){return t.e(159).then(t.bind(null,627))},"v-9766780a":function(){return t.e(160).then(t.bind(null,628))},"v-59a33950":function(){return t.e(161).then(t.bind(null,629))},"v-87ec7d3a":function(){return t.e(162).then(t.bind(null,630))},"v-046481ec":function(){return t.e(163).then(t.bind(null,631))},"v-098439d0":function(){return t.e(164).then(t.bind(null,632))},"v-8eabad82":function(){return t.e(165).then(t.bind(null,633))},"v-262a1fde":function(){return t.e(166).then(t.bind(null,634))},"v-0c749932":function(){return t.e(167).then(t.bind(null,635))},"v-3c3f9ba1":function(){return t.e(168).then(t.bind(null,636))},"v-ae86ef32":function(){return t.e(169).then(t.bind(null,637))},"v-28665664":function(){return t.e(117).then(t.bind(null,638))},"v-4c82f50b":function(){return t.e(170).then(t.bind(null,639))},"v-6272dfb6":function(){return t.e(172).then(t.bind(null,640))},"v-1dfe3ec2":function(){return t.e(171).then(t.bind(null,641))},"v-a0974e96":function(){return t.e(173).then(t.bind(null,642))},"v-321c5958":function(){return t.e(174).then(t.bind(null,643))},"v-efa3c938":function(){return t.e(175).then(t.bind(null,644))},"v-255f6b29":function(){return t.e(176).then(t.bind(null,645))},"v-28726c01":function(){return t.e(177).then(t.bind(null,646))},"v-4a1b2e40":function(){return t.e(178).then(t.bind(null,647))},"v-f8a50a56":function(){return t.e(179).then(t.bind(null,648))},"v-4b869476":function(){return t.e(180).then(t.bind(null,649))},"v-20e14c4b":function(){return t.e(181).then(t.bind(null,650))},"v-7e4ade2c":function(){return t.e(182).then(t.bind(null,651))},"v-61b52e07":function(){return t.e(183).then(t.bind(null,652))},"v-88106b62":function(){return t.e(184).then(t.bind(null,653))},"v-18890533":function(){return t.e(185).then(t.bind(null,654))},"v-74c698d0":function(){return t.e(186).then(t.bind(null,655))},"v-78a70910":function(){return t.e(187).then(t.bind(null,656))},"v-3222f444":function(){return t.e(188).then(t.bind(null,657))},"v-3c995a47":function(){return t.e(189).then(t.bind(null,658))},"v-18111e3e":function(){return t.e(190).then(t.bind(null,659))},"v-2d7230e8":function(){return t.e(191).then(t.bind(null,660))},"v-2fa8ca6b":function(){return t.e(192).then(t.bind(null,661))},"v-ca52ce64":function(){return t.e(193).then(t.bind(null,662))},"v-620b25ca":function(){return t.e(194).then(t.bind(null,663))},"v-d70c8d9c":function(){return t.e(195).then(t.bind(null,664))},"v-8dc9cdca":function(){return t.e(196).then(t.bind(null,665))},"v-994030a6":function(){return t.e(197).then(t.bind(null,666))},"v-c4e4c304":function(){return t.e(198).then(t.bind(null,667))},"v-c4de77d8":function(){return t.e(199).then(t.bind(null,668))},"v-8851eab0":function(){return t.e(200).then(t.bind(null,669))},"v-f80986b8":function(){return t.e(201).then(t.bind(null,670))},"v-7afa71b5":function(){return t.e(202).then(t.bind(null,671))},"v-bee7b4c4":function(){return t.e(203).then(t.bind(null,672))},"v-7bad6842":function(){return t.e(204).then(t.bind(null,673))},"v-d8f92f9e":function(){return t.e(205).then(t.bind(null,674))},"v-f6cd3ec8":function(){return t.e(206).then(t.bind(null,675))},"v-d7bef6c8":function(){return t.e(207).then(t.bind(null,676))},"v-4988c303":function(){return t.e(208).then(t.bind(null,677))},"v-7073a58e":function(){return t.e(209).then(t.bind(null,678))},"v-60dca1e3":function(){return t.e(210).then(t.bind(null,679))},"v-69282982":function(){return t.e(211).then(t.bind(null,680))},"v-fae7c1bc":function(){return t.e(212).then(t.bind(null,681))},"v-7c133ad0":function(){return t.e(213).then(t.bind(null,682))},"v-0f2a3d88":function(){return t.e(214).then(t.bind(null,683))},"v-78c2fe6d":function(){return t.e(215).then(t.bind(null,684))},"v-7183b4d5":function(){return t.e(216).then(t.bind(null,685))},"v-8f10297e":function(){return t.e(217).then(t.bind(null,686))},"v-233d0506":function(){return t.e(218).then(t.bind(null,687))},"v-64d0cdf1":function(){return t.e(219).then(t.bind(null,688))},"v-6dbf4a31":function(){return t.e(220).then(t.bind(null,689))},"v-f97f4056":function(){return t.e(221).then(t.bind(null,690))},"v-03f86a9e":function(){return t.e(222).then(t.bind(null,691))},"v-31abbd86":function(){return t.e(223).then(t.bind(null,692))},"v-77f35f9e":function(){return t.e(224).then(t.bind(null,693))},"v-6de74afa":function(){return t.e(226).then(t.bind(null,694))},"v-4c1cfbd2":function(){return t.e(225).then(t.bind(null,695))},"v-a146c5d8":function(){return t.e(227).then(t.bind(null,696))},"v-9d6dab12":function(){return t.e(228).then(t.bind(null,697))},"v-1572d344":function(){return t.e(229).then(t.bind(null,698))},"v-613443b0":function(){return t.e(230).then(t.bind(null,699))},"v-2da1a437":function(){return t.e(231).then(t.bind(null,700))},"v-a0a5c11c":function(){return t.e(232).then(t.bind(null,701))},"v-229229ee":function(){return t.e(233).then(t.bind(null,702))},"v-0792908f":function(){return t.e(234).then(t.bind(null,703))},"v-0ba2fba8":function(){return t.e(235).then(t.bind(null,704))},"v-89e63956":function(){return t.e(236).then(t.bind(null,705))},"v-080f0c7c":function(){return t.e(237).then(t.bind(null,706))},"v-5840b4cb":function(){return t.e(238).then(t.bind(null,707))},"v-4de06ab6":function(){return t.e(239).then(t.bind(null,708))},"v-eb9fcf56":function(){return t.e(240).then(t.bind(null,709))},"v-183d693e":function(){return t.e(241).then(t.bind(null,710))},"v-09f5c492":function(){return t.e(242).then(t.bind(null,711))},"v-0a72df0a":function(){return t.e(243).then(t.bind(null,712))},"v-363a61d8":function(){return Promise.all([t.e(0),t.e(3)]).then(t.bind(null,713))},"v-250b49a5":function(){return t.e(53).then(t.bind(null,714))},"v-2617300a":function(){return t.e(52).then(t.bind(null,715))}};function Ca(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var Da=/-(\w)/g,za=Ca((function(n){return n.replace(Da,(function(n,e){return e?e.toUpperCase():""}))})),Ma=/\B([A-Z])/g,Pa=Ca((function(n){return n.replace(Ma,"-$1").toLowerCase()})),Oa=Ca((function(n){return n.charAt(0).toUpperCase()+n.slice(1)}));function Ra(n,e){if(e)return n(e)?n(e):e.includes("-")?n(Oa(za(e))):n(Oa(e))||n(Pa(e))}var Na=Object.assign({},ja,Aa),La=function(n){return Na[n]},qa=function(n){return Aa[n]},Ba=function(n){return ja[n]},Ua=function(n){return Po.component(n)};function $a(n){return Ra(qa,n)}function Ga(n){return Ra(Ba,n)}function Fa(n){return Ra(La,n)}function Ha(n){return Ra(Ua,n)}function Ka(){for(var n=arguments.length,e=new Array(n),t=0;t<n;t++)e[t]=arguments[t];return Promise.all(e.filter((function(n){return n})).map(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(e){var t;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(Ha(e)||!Fa(e)){n.next=5;break}return n.next=3,Fa(e)();case 3:t=n.sent,Po.component(e,t.default);case 5:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}()))}function Ja(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}var Va=t(132),Wa=(t(185),t(108),t(72),t(215)),Qa=t.n(Wa),Ya=t(216),Xa=t.n(Ya),Za={created:function(){if(this.siteMeta=this.$site.headTags.filter((function(n){return"meta"===Object(Va.a)(n,1)[0]})).map((function(n){var e=Object(Va.a)(n,2);e[0];return e[1]})),this.$ssrContext){var n=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(e=n)?e.map((function(n){var e="<meta";return Object.keys(n).forEach((function(t){e+=" ".concat(t,'="').concat(Xa()(n[t]),'"')})),e+">"})).join("\n    "):"",this.$ssrContext.canonicalLink=es(this.$canonicalUrl)}var e},mounted:function(){this.currentMetaTags=Object(Ia.a)(document.querySelectorAll("meta")),this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta:function(){document.title=this.$title,document.documentElement.lang=this.$lang;var n=this.getMergedMetaTags();this.currentMetaTags=ts(n,this.currentMetaTags)},getMergedMetaTags:function(){var n=this.$page.frontmatter.meta||[];return Qa()([{name:"description",content:this.$description}],n,this.siteMeta,rs)},updateCanonicalLink:function(){ns(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",es(this.$canonicalUrl))}},watch:{$page:function(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy:function(){ts(null,this.currentMetaTags),ns()}};function ns(){var n=document.querySelector("link[rel='canonical']");n&&n.remove()}function es(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"";return n?'<link href="'.concat(n,'" rel="canonical" />'):""}function ts(n,e){if(e&&Object(Ia.a)(e).filter((function(n){return n.parentNode===document.head})).forEach((function(n){return document.head.removeChild(n)})),n)return n.map((function(n){var e=document.createElement("meta");return Object.keys(n).forEach((function(t){e.setAttribute(t,n[t])})),document.head.appendChild(e),e}))}function rs(n){for(var e=0,t=["name","property","itemprop"];e<t.length;e++){var r=t[e];if(n.hasOwnProperty(r))return n[r]+r}return JSON.stringify(n)}t(134);var os=t(148),is={mounted:function(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(os)()((function(){this.setActiveHash()}),300),setActiveHash:function(){for(var n=this,e=[].slice.call(document.querySelectorAll(".sidebar-link")),t=[].slice.call(document.querySelectorAll(".header-anchor")).filter((function(n){return e.some((function(e){return e.hash===n.hash}))})),r=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),o=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),i=window.innerHeight+r,a=0;a<t.length;a++){var s=t[a],c=t[a+1],l=0===a&&0===r||r>=s.parentElement.offsetTop+10&&(!c||r<c.parentElement.offsetTop-10),d=decodeURIComponent(this.$route.hash);if(l&&d!==decodeURIComponent(s.hash)){var u=s;if(i===o)for(var p=a+1;p<t.length;p++)if(d===decodeURIComponent(t[p].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(u.hash),(function(){n.$nextTick((function(){n.$vuepress.$set("disableScrollBehavior",!1)}))}))}}}},beforeDestroy:function(){window.removeEventListener("scroll",this.onScroll)}},as=(t(104),t(99)),ss=t.n(as),cs={mounted:function(){var n=this;ss.a.configure({showSpinner:!1}),this.$router.beforeEach((function(n,e,t){n.path===e.path||Po.component(n.name)||ss.a.start(),t()})),this.$router.afterEach((function(){ss.a.done(),n.isSidebarOpen=!1}))}};t(74),t(42),t(77),t(361);function ls(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}t(138);function ds(n,e){for(var t=0;t<e.length;t++){var r=e[t];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(n,r.key,r)}}function us(n,e,t){return e&&ds(n.prototype,e),t&&ds(n,t),Object.defineProperty(n,"prototype",{writable:!1}),n}t(362);var ps=function(){function n(){ls(this,n);this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}return us(n,[{key:"show",value:function(n){var e=this,t=n.text,r=void 0===t?"":t,o=n.duration,i=void 0===o?3e3:o,a=document.createElement("div");a.className="message move-in",a.innerHTML='\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">'.concat(r,"</div>\n    "),this.containerEl.appendChild(a),i>0&&setTimeout((function(){e.close(a)}),i)}},{key:"close",value:function(n){n.className=n.className.replace("move-in",""),n.className+="move-out",n.addEventListener("animationend",(function(){n.remove()}))}}]),n}(),ms={mounted:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy:function(){var n=this;setTimeout((function(){(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach((function(e){document.querySelectorAll(e).forEach(n.generateCopyButton)}))}),1e3)},generateCopyButton:function(n){var e=this;if(!n.classList.contains("codecopy-enabled")){var t=document.createElement("i");t.className="code-copy",t.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',t.title="Copy to clipboard",t.addEventListener("click",(function(){e.copyToClipboard(n.innerText)})),n.appendChild(t),n.classList.add("codecopy-enabled")}},copyToClipboard:function(n){var e=document.createElement("textarea");e.value=n,e.setAttribute("readonly",""),e.style.position="absolute",e.style.left="-9999px",document.body.appendChild(e);var t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);e.select(),document.execCommand("copy"),(new ps).show({text:"复制成功",duration:1e3}),document.body.removeChild(e),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}};t(230),t(73),t(133),t(135),t(364);!function(n,e){void 0===e&&(e={});var t=e.insertAt;if(n&&"undefined"!=typeof document){var r=document.head||document.getElementsByTagName("head")[0],o=document.createElement("style");o.type="text/css","top"===t&&r.firstChild?r.insertBefore(o,r.firstChild):r.appendChild(o),o.styleSheet?o.styleSheet.cssText=n:o.appendChild(document.createTextNode(n))}}("@media (max-width: 1000px) {\n  .vuepress-plugin-demo-block__h_code {\n    display: none;\n  }\n  .vuepress-plugin-demo-block__app {\n    margin-left: auto !important;\n    margin-right: auto !important;\n  }\n}\n.vuepress-plugin-demo-block__wrapper {\n  margin-top: 10px;\n  border: 1px solid #ebebeb;\n  border-radius: 4px;\n  transition: all 0.2s;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display {\n  height: 400px;\n  display: flex;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__app {\n  width: 300px;\n  border: 1px solid #ebebeb;\n  box-shadow: 1px 1px 3px #ebebeb;\n  margin-right: 5px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code {\n  flex: 1;\n  overflow: auto;\n  height: 100%;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code > pre {\n  overflow: visible;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  max-height: 400px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper div {\n  box-sizing: border-box;\n}\n.vuepress-plugin-demo-block__wrapper:hover {\n  box-shadow: 0 0 11px rgba(33, 33, 33, 0.2);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code {\n  overflow: hidden;\n  height: 0;\n  padding: 0 !important;\n  background-color: #282c34;\n  border-radius: 0 !important;\n  transition: height 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code pre {\n  margin: 0 !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  padding: 20px;\n  border-bottom: 1px solid #ebebeb;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer {\n  position: relative;\n  text-align: center;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__codepen {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__expand::before {\n  border-top: none;\n  border-right: 6px solid transparent;\n  border-bottom: 6px solid #ccc;\n  border-left: 6px solid transparent;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__codepen,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand span,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand::before {\n  border-top-color: #3eaf7c !important;\n  border-bottom-color: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover svg {\n  fill: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand-text {\n  transition: all 0.5s;\n  opacity: 0;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:nth-last-child(2) {\n  right: 50px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:last-child {\n  right: 10px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button {\n  border-color: transparent;\n  background-color: transparent;\n  font-size: 14px;\n  color: #3eaf7c;\n  cursor: pointer;\n  outline: none;\n  margin: 0;\n  width: 46px;\n  position: relative;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::before {\n  content: attr(data-tip);\n  white-space: nowrap;\n  position: absolute;\n  top: -30px;\n  left: 50%;\n  color: #eee;\n  line-height: 1;\n  z-index: 1000;\n  border-radius: 4px;\n  padding: 6px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  background-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::after {\n  content: '' !important;\n  display: block;\n  position: absolute;\n  left: 50%;\n  top: -5px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  border: 5px solid transparent;\n  border-top-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button svg {\n  width: 34px;\n  height: 20px;\n  fill: #ccc;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__codepen {\n  position: absolute;\n  top: 10px;\n  transition: all 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand {\n  position: relative;\n  width: 100px;\n  height: 40px;\n  margin: 0;\n  color: #3eaf7c;\n  font-size: 14px;\n  background-color: transparent;\n  border-color: transparent;\n  outline: none;\n  transition: all 0.5s;\n  cursor: pointer;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand::before {\n  content: \"\";\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  width: 0;\n  height: 0;\n  border-top: 6px solid #ccc;\n  border-right: 6px solid transparent;\n  border-left: 6px solid transparent;\n  -webkit-transform: translate(-50%, -50%);\n          transform: translate(-50%, -50%);\n}\n");var gs={jsLib:[],cssLib:[],jsfiddle:!0,codepen:!0,codepenLayout:"left",codepenJsProcessor:"babel",codepenEditors:"101",horizontal:!1,vue:"https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js",react:"https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js",reactDOM:"https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"},hs={},bs=function(n){return'<div id="app">\n'.concat(n,"\n</div>")},fs=function(n){return window.$VUEPRESS_DEMO_BLOCK&&void 0!==window.$VUEPRESS_DEMO_BLOCK[n]?window.$VUEPRESS_DEMO_BLOCK[n]:gs[n]},vs=function n(e,t,r){var o=document.createElement(e);return t&&Object.keys(t).forEach((function(n){if(n.indexOf("data"))o[n]=t[n];else{var e=n.replace("data","");o.dataset[e]=t[n]}})),r&&r.forEach((function(e){var t=e.tag,r=e.attrs,i=e.children;o.appendChild(n(t,r,i))})),o},ks=function(n,e,t){var r,o=(r=n.querySelectorAll(".".concat(e)),Array.prototype.slice.call(r));return 1!==o.length||t?o:o[0]},ys=function(n,e){var t,r,o=n.match(/<style>([\s\S]+)<\/style>/),i=n.match(/<template>([\s\S]+)<\/template>/),a=n.match(/<script>([\s\S]+)<\/script>/),s={css:o&&o[1].replace(/^\n|\n$/g,""),html:i&&i[1].replace(/^\n|\n$/g,""),js:a&&a[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};s.htmlTpl=bs(s.html),s.jsTpl=(t=s.js,r=t.replace(/export\s+default\s*?\{\n*/,"").replace(/\n*\}\s*$/,"").trim(),"new Vue({\n  el: '#app',\n  ".concat(r,"\n})")),s.script=function(n,e){var t=n.split(/export\s+default/),r="(function() {".concat(t[0]," ; return ").concat(t[1],"})()"),o=window.Babel?window.Babel.transform(r,{presets:["es2015"]}).code:r,i=[eval][0](o);return i.template=e,i}(s.js,s.html);var c=fs("vue");return s.jsLib.unshift(c),s},xs=function(n,e){var t,r=n.match(/<style>([\s\S]+)<\/style>/),o=n.match(/<html>([\s\S]+)<\/html>/),i=n.match(/<script>([\s\S]+)<\/script>/),a={css:r&&r[1].replace(/^\n|\n$/g,""),html:o&&o[1].replace(/^\n|\n$/g,""),js:i&&i[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};return a.htmlTpl=a.html,a.jsTpl=a.js,a.script=(t=a.js,window.Babel?window.Babel.transform(t,{presets:["es2015"]}).code:t),a},ws=function(n){return n=n.replace("export default ","").replace(/App\.__style__(\s*)=(\s*)`([\s\S]*)?`/,""),n+='ReactDOM.render(React.createElement(App), document.getElementById("app"))'};function Ss(){var n=ks(document,"vuepress-plugin-demo-block__wrapper",!0);n.length?n.forEach((function(n){if("true"!==n.dataset.created){n.style.display="block";var e=ks(n,"vuepress-plugin-demo-block__code"),t=ks(n,"vuepress-plugin-demo-block__display"),r=ks(n,"vuepress-plugin-demo-block__footer"),o=ks(t,"vuepress-plugin-demo-block__app"),i=decodeURIComponent(n.dataset.code),a=decodeURIComponent(n.dataset.config),s=decodeURIComponent(n.dataset.type);a=a?JSON.parse(a):{};var c=e.querySelector("div").clientHeight,l="react"===s?function(n,e){var t=(0,window.Babel.transform)(n,{presets:["es2015","react"]}).code,r="(function(exports){var module={};module.exports=exports;".concat(t,";return module.exports.__esModule?module.exports.default:module.exports;})({})"),o=new Function("return ".concat(r))(),i={js:o,css:o.__style__||"",jsLib:e.jsLib||[],cssLib:e.cssLib||[],jsTpl:ws(n),htmlTpl:bs("")},a=fs("react"),s=fs("reactDOM");return i.jsLib.unshift(a,s),i}(i,a):"vanilla"===s?xs(i,a):ys(i,a),d=vs("button",{className:"".concat("vuepress-plugin-demo-block__expand")});if(r.appendChild(d),d.addEventListener("click",_s.bind(null,d,c,e,r)),fs("jsfiddle")&&r.appendChild(function(n){var e=n.css,t=n.htmlTpl,r=n.jsTpl,o=n.jsLib,i=n.cssLib,a=o.concat(i).concat(fs("cssLib")).concat(fs("jsLib")).join(",");return vs("form",{className:"vuepress-plugin-demo-block__jsfiddle",target:"_blank",action:"https://jsfiddle.net/api/post/library/pure/",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"css",value:e}},{tag:"input",attrs:{type:"hidden",name:"html",value:t}},{tag:"input",attrs:{type:"hidden",name:"js",value:r}},{tag:"input",attrs:{type:"hidden",name:"panel_js",value:3}},{tag:"input",attrs:{type:"hidden",name:"wrap",value:1}},{tag:"input",attrs:{type:"hidden",name:"resources",value:a}},{tag:"button",attrs:{type:"submit",className:"vuepress-plugin-demo-block__button",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088289967" class="icon" style="" viewBox="0 0 1170 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1952" xmlns:xlink="http://www.w3.org/1999/xlink" width="228.515625" height="200"><defs><style type="text/css"></style></defs><path d="M1028.571429 441.142857q63.428571 26.285714 102.571428 83.142857T1170.285714 650.857143q0 93.714286-67.428571 160.285714T940 877.714286q-2.285714 0-6.571429-0.285715t-6-0.285714H232q-97.142857-5.714286-164.571429-71.714286T0 645.142857q0-62.857143 31.428571-116t84-84q-6.857143-22.285714-6.857142-46.857143 0-65.714286 46.857142-112t113.714286-46.285714q54.285714 0 98.285714 33.142857 42.857143-88 127.142858-141.714286t186.571428-53.714285q94.857143 0 174.857143 46T982.571429 248.571429t46.571428 172q0 3.428571-0.285714 10.285714t-0.285714 10.285714zM267.428571 593.142857q0 69.714286 48 110.285714t118.857143 40.571429q78.285714 0 137.142857-56.571429-9.142857-11.428571-27.142857-32.285714T519.428571 626.285714q-38.285714 37.142857-82.285714 37.142857-31.428571 0-53.428571-19.142857T361.714286 594.285714q0-30.285714 22-49.714285t52.285714-19.428572q25.142857 0 48.285714 12t41.714286 31.428572 37.142857 42.857142 39.428572 46.857143 44 42.857143 55.428571 31.428572 69.428571 12q69.142857 0 116.857143-40.857143T936 594.857143q0-69.142857-48-109.714286t-118.285714-40.571428q-81.714286 0-137.714286 55.428571l53.142857 61.714286q37.714286-36.571429 81.142857-36.571429 29.714286 0 52.571429 18.857143t22.857143 48q0 32.571429-21.142857 52.285714t-53.714286 19.714286q-24.571429 0-47.142857-12t-41.142857-31.428571-37.428572-42.857143-39.714286-46.857143-44.285714-42.857143-55.142857-31.428571T434.285714 444.571429q-69.714286 0-118.285714 40.285714T267.428571 593.142857z" p-id="1953"></path></svg>',datatip:"JSFiddle"}}])}(l)),fs("codepen")&&r.appendChild(function(n){var e=n.css,t=n.htmlTpl,r=n.jsTpl,o=n.jsLib,i=n.cssLib,a=JSON.stringify({css:e,html:t,js:r,js_external:o.concat(fs("jsLib")).join(";"),css_external:i.concat(fs("cssLib")).join(";"),layout:fs("codepenLayout"),js_pre_processor:fs("codepenJsProcessor"),editors:fs("codepenEditors")});return vs("form",{className:"vuepress-plugin-demo-block__codepen",target:"_blank",action:"https://codepen.io/pen/define",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"data",value:a}},{tag:"button",attrs:{type:"submit",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088271207" class="icon" style="" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1737" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><defs><style type="text/css"></style></defs><path d="M123.428571 668l344.571429 229.714286v-205.142857L277.142857 565.142857z m-35.428571-82.285714l110.285714-73.714286-110.285714-73.714286v147.428572z m468 312l344.571429-229.714286-153.714286-102.857143-190.857143 127.428572v205.142857z m-44-281.714286l155.428571-104-155.428571-104-155.428571 104zM277.142857 458.857143l190.857143-127.428572V126.285714L123.428571 356z m548.571429 53.142857l110.285714 73.714286V438.285714z m-78.857143-53.142857l153.714286-102.857143-344.571429-229.714286v205.142857z m277.142857-102.857143v312q0 23.428571-19.428571 36.571429l-468 312q-12 7.428571-24.571429 7.428571t-24.571429-7.428571L19.428571 704.571429q-19.428571-13.142857-19.428571-36.571429V356q0-23.428571 19.428571-36.571429L487.428571 7.428571q12-7.428571 24.571429-7.428571t24.571429 7.428571l468 312q19.428571 13.142857 19.428571 36.571429z" p-id="1738"></path></svg>',className:"vuepress-plugin-demo-block__button",datatip:"Codepen"}}])}(l)),void 0!==a.horizontal?a.horizontal:fs("horizontal")){n.classList.add("vuepress-plugin-demo-block__horizontal");var u=e.firstChild.cloneNode(!0);u.classList.add("vuepress-plugin-demo-block__h_code"),t.appendChild(u)}if(l.css&&function(n){if(!hs[n]){var e=vs("style",{innerHTML:n});document.body.appendChild(e),hs[n]=!0}}(l.css),"react"===s)ReactDOM.render(React.createElement(l.js),o);else if("vue"===s){var p=(new(Vue.extend(l.script))).$mount();o.appendChild(p.$el)}else"vanilla"===s&&(o.innerHTML=l.html,new Function("return (function(){".concat(l.script,"})()"))());n.dataset.created="true"}})):setTimeout((function(n){Ss()}),300)}function _s(n,e,t,r){var o="1"!==n.dataset.isExpand;t.style.height=o?"".concat(e,"px"):0,o?r.classList.add("vuepress-plugin-demo-block__show-link"):r.classList.remove("vuepress-plugin-demo-block__show-link"),n.dataset.isExpand=o?"1":"0"}var Es={mounted:function(){window.$VUEPRESS_DEMO_BLOCK={jsfiddle:!1,codepen:!0,horizontal:!1},Ss()},updated:function(){Ss()}},Ts=(t(219),"auto"),Is="zoom-in",js="zoom-out",As="grab",Cs="move";function Ds(n,e,t){var r=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],o={passive:!1};r?n.addEventListener(e,t,o):n.removeEventListener(e,t,o)}function zs(n,e){if(n){var t=new Image;t.onload=function(){e&&e(t)},t.src=n}}function Ms(n){return n.dataset.original?n.dataset.original:"A"===n.parentNode.tagName?n.parentNode.getAttribute("href"):null}function Ps(n,e,t){!function(n){var e=Os,t=Rs;if(n.transition){var r=n.transition;delete n.transition,n[e]=r}if(n.transform){var o=n.transform;delete n.transform,n[t]=o}}(e);var r=n.style,o={};for(var i in e)t&&(o[i]=r[i]||""),r[i]=e[i];return o}var Os="transition",Rs="transform",Ns="transform",Ls="transitionend";var qs=function(){},Bs={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:qs,onClose:qs,onGrab:qs,onMove:qs,onRelease:qs,onBeforeOpen:qs,onBeforeClose:qs,onBeforeGrab:qs,onBeforeRelease:qs,onImageLoading:qs,onImageLoaded:qs},Us={init:function(n){var e,t;e=this,t=n,Object.getOwnPropertyNames(Object.getPrototypeOf(e)).forEach((function(n){e[n]=e[n].bind(t)}))},click:function(n){if(n.preventDefault(),Gs(n))return window.open(this.target.srcOriginal||n.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(n.currentTarget)},scroll:function(){var n=document.documentElement||document.body.parentNode||document.body,e=window.pageXOffset||n.scrollLeft,t=window.pageYOffset||n.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:e,y:t});var r=this.lastScrollPosition.x-e,o=this.lastScrollPosition.y-t,i=this.options.scrollThreshold;(Math.abs(o)>=i||Math.abs(r)>=i)&&(this.lastScrollPosition=null,this.close())},keydown:function(n){(function(n){return"Escape"===(n.key||n.code)||27===n.keyCode})(n)&&(this.released?this.close():this.release(this.close))},mousedown:function(n){if($s(n)&&!Gs(n)){n.preventDefault();var e=n.clientX,t=n.clientY;this.pressTimer=setTimeout(function(){this.grab(e,t)}.bind(this),200)}},mousemove:function(n){this.released||this.move(n.clientX,n.clientY)},mouseup:function(n){$s(n)&&!Gs(n)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(n){n.preventDefault();var e=n.touches[0],t=e.clientX,r=e.clientY;this.pressTimer=setTimeout(function(){this.grab(t,r)}.bind(this),200)},touchmove:function(n){if(!this.released){var e=n.touches[0],t=e.clientX,r=e.clientY;this.move(t,r)}},touchend:function(n){(function(n){n.targetTouches.length})(n)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function $s(n){return 0===n.button}function Gs(n){return n.metaKey||n.ctrlKey}var Fs={init:function(n){this.el=document.createElement("div"),this.instance=n,this.parent=document.body,Ps(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(n.options),Ds(this.el,"click",n.handler.clickOverlay.bind(n))},updateStyle:function(n){Ps(this.el,{zIndex:n.zIndex,backgroundColor:n.bgColor,transition:"opacity\n        "+n.transitionDuration+"s\n        "+n.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},Hs="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},Ks=function(){function n(n,e){for(var t=0;t<e.length;t++){var r=e[t];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(n,r.key,r)}}return function(e,t,r){return t&&n(e.prototype,t),r&&n(e,r),e}}(),Js=Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&(n[r]=t[r])}return n},Vs={init:function(n,e){this.el=n,this.instance=e,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=Ms(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var n=this.instance.options,e=n.zIndex,t=n.enableGrab,r=n.transitionDuration,o=n.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:e+1,cursor:t?As:js,transition:Ns+"\n        "+r+"s\n        "+o,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=Ps(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,Ps(this.el,{transform:"none"})},grab:function(n,e,t){var r=Ws(),o=r.x-n,i=r.y-e;Ps(this.el,{cursor:Cs,transform:"translate3d(\n        "+(this.translate.x+o)+"px, "+(this.translate.y+i)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(n,e,t){var r=Ws(),o=r.x-n,i=r.y-e;Ps(this.el,{transition:Ns,transform:"translate3d(\n        "+(this.translate.x+o)+"px, "+(this.translate.y+i)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){Ps(this.el,this.styleClose)},restoreOpenStyle:function(){Ps(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var n=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var e=this.el.cloneNode(!1);e.setAttribute("src",this.srcOriginal),e.style.position="fixed",e.style.visibility="hidden",n.appendChild(e),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),n.removeChild(e)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var n=Ws(),e=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:n.x-e,y:n.y-t}},calculateScale:function(){var n=this.el.dataset,e=n.zoomingHeight,t=n.zoomingWidth,r=this.instance.options,o=r.customSize,i=r.scaleBase;if(!o&&e&&t)return{x:t/this.rect.width,y:e/this.rect.height};if(o&&"object"===(void 0===o?"undefined":Hs(o)))return{x:o.width/this.rect.width,y:o.height/this.rect.height};var a=this.rect.width/2,s=this.rect.height/2,c=Ws(),l={x:c.x-a,y:c.y-s},d=l.x/a,u=l.y/s,p=i+Math.min(d,u);if(o&&"string"==typeof o){var m=t||this.el.naturalWidth,g=e||this.el.naturalHeight,h=parseFloat(o)*m/(100*this.rect.width),b=parseFloat(o)*g/(100*this.rect.height);if(p>h||p>b)return{x:h,y:b}}return{x:p,y:p}}};function Ws(){var n=document.documentElement;return{x:Math.min(n.clientWidth,window.innerWidth)/2,y:Math.min(n.clientHeight,window.innerHeight)/2}}function Qs(n,e,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(r){Ds(n,r,e[r],t)}))}var Ys=function(){function n(e){!function(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}(this,n),this.target=Object.create(Vs),this.overlay=Object.create(Fs),this.handler=Object.create(Us),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=Js({},Bs,e),this.overlay.init(this),this.handler.init(this)}return Ks(n,[{key:"listen",value:function(n){if("string"==typeof n)for(var e=document.querySelectorAll(n),t=e.length;t--;)this.listen(e[t]);else"IMG"===n.tagName&&(n.style.cursor=Is,Ds(n,"click",this.handler.click),this.options.preloadImage&&zs(Ms(n)));return this}},{key:"config",value:function(n){return n?(Js(this.options,n),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(n){var e=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var r="string"==typeof n?document.querySelector(n):n;if("IMG"===r.tagName){if(this.options.onBeforeOpen(r),this.target.init(r,this),!this.options.preloadImage){var o=this.target.srcOriginal;null!=o&&(this.options.onImageLoading(r),zs(o,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),Ds(document,"scroll",this.handler.scroll),Ds(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&Ds(window,"resize",this.handler.resizeWindow);var i=function n(){Ds(r,Ls,n,!1),e.lock=!1,e.target.upgradeSource(),e.options.enableGrab&&Qs(document,e.handler,!0),t(r)};return Ds(r,Ls,i),this}}}},{key:"close",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=Ts,this.overlay.fadeOut(),this.target.zoomOut(),Ds(document,"scroll",this.handler.scroll,!1),Ds(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&Ds(window,"resize",this.handler.resizeWindow,!1);var r=function r(){Ds(t,Ls,r,!1),n.shown=!1,n.lock=!1,n.target.downgradeSource(),n.options.enableGrab&&Qs(document,n.handler,!1),n.target.restoreCloseStyle(),n.overlay.remove(),e(t)};return Ds(t,Ls,r),this}}},{key:"grab",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var o=this.target.el;this.options.onBeforeGrab(o),this.released=!1,this.target.grab(n,e,t);var i=function n(){Ds(o,Ls,n,!1),r(o)};return Ds(o,Ls,i),this}}},{key:"move",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=Cs,this.target.move(n,e,t);var o=this.target.el,i=function n(){Ds(o,Ls,n,!1),r(o)};return Ds(o,Ls,i),this}}},{key:"release",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=Ts,this.target.restoreOpenStyle();var r=function r(){Ds(t,Ls,r,!1),n.lock=!1,n.released=!0,e(t)};return Ds(t,Ls,r),this}}}]),n}(),Xs=".theme-vdoing-content img:not(.no-zoom)",Zs=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),nc=Number("500"),ec=function(){function n(){ls(this,n),this.instance=new Ys(Zs)}return us(n,[{key:"update",value:function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:Xs;"undefined"!=typeof window&&this.instance.listen(n)}},{key:"updateDelay",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:Xs,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:nc;setTimeout((function(){return n.update(e)}),t)}}]),n}(),tc=[Za,is,cs,ms,Es,{watch:{"$page.path":function(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted:function(){this.$vuepress.zooming=new ec,this.$vuepress.zooming.updateDelay()}}],rc={name:"GlobalLayout",computed:{layout:function(){var n=this.getLayout();return Ja("layout",n),Po.component(n)}},methods:{getLayout:function(){if(this.$page.path){var n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},oc=t(29),ic=Object(oc.a)(rc,(function(){var n=this.$createElement;return(this._self._c||n)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(n,e,t){var r;switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),(r=n[e]).push.apply(r,Object(Ia.a)(t));break;default:throw new Error("Unknown option name.")}}(ic,"mixins",tc);var ac=[{name:"v-dc140a80",path:"/pages/250f21/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-dc140a80").then(t)}},{path:"/pages/250f21/index.html",redirect:"/pages/250f21/"},{path:"/01.指南/01.基础/01.基础设施即服务.html",redirect:"/pages/250f21/"},{name:"v-c6d7e3c0",path:"/pages/f04654/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-c6d7e3c0").then(t)}},{path:"/pages/f04654/index.html",redirect:"/pages/f04654/"},{path:"/01.指南/01.基础/02.平台即服务.html",redirect:"/pages/f04654/"},{name:"v-812c033c",path:"/pages/58bcba/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-812c033c").then(t)}},{path:"/pages/58bcba/index.html",redirect:"/pages/58bcba/"},{path:"/01.指南/01.基础/03.微服务解决复杂问题.html",redirect:"/pages/58bcba/"},{name:"v-30b385c2",path:"/pages/b05793/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-30b385c2").then(t)}},{path:"/pages/b05793/index.html",redirect:"/pages/b05793/"},{path:"/01.指南/01.基础/04.Spring Cloud.html",redirect:"/pages/b05793/"},{name:"v-5a2cd282",path:"/pages/1946f3/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-5a2cd282").then(t)}},{path:"/pages/1946f3/index.html",redirect:"/pages/1946f3/"},{path:"/01.指南/01.基础/05.Spring Cloud Alibaba.html",redirect:"/pages/1946f3/"},{name:"v-2f93a0e6",path:"/pages/aeaf26/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2f93a0e6").then(t)}},{path:"/pages/aeaf26/index.html",redirect:"/pages/aeaf26/"},{path:"/01.指南/02.高级/01.Docker.html",redirect:"/pages/aeaf26/"},{name:"v-95827e74",path:"/pages/059418/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-95827e74").then(t)}},{path:"/pages/059418/index.html",redirect:"/pages/059418/"},{path:"/01.指南/02.高级/02.Kubernetes.html",redirect:"/pages/059418/"},{name:"v-00e5aa38",path:"/pages/fbef9a/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-00e5aa38").then(t)}},{path:"/pages/fbef9a/index.html",redirect:"/pages/fbef9a/"},{path:"/01.指南/03.专题/01.Spring Security OAuth2.html",redirect:"/pages/fbef9a/"},{name:"v-ea2cdb88",path:"/pages/72ccd6/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-ea2cdb88").then(t)}},{path:"/pages/72ccd6/index.html",redirect:"/pages/72ccd6/"},{path:"/01.指南/03.专题/02.企业级 DevOps 实践.html",redirect:"/pages/72ccd6/"},{name:"v-63a62e60",path:"/pages/202ee1/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-63a62e60").then(t)}},{path:"/pages/202ee1/index.html",redirect:"/pages/202ee1/"},{path:"/02.Linux/01.Linux/01.Linux 简介.html",redirect:"/pages/202ee1/"},{name:"v-bded170c",path:"/pages/715f2d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-bded170c").then(t)}},{path:"/pages/715f2d/index.html",redirect:"/pages/715f2d/"},{path:"/02.Linux/01.Linux/02.Linux 与 Windows 比较.html",redirect:"/pages/715f2d/"},{name:"v-0e25e56a",path:"/pages/56abb4/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0e25e56a").then(t)}},{path:"/pages/56abb4/index.html",redirect:"/pages/56abb4/"},{path:"/02.Linux/01.Linux/03.Linux 远程控制管理.html",redirect:"/pages/56abb4/"},{name:"v-68dd6adb",path:"/pages/1e9034/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-68dd6adb").then(t)}},{path:"/pages/1e9034/index.html",redirect:"/pages/1e9034/"},{path:"/02.Linux/01.Linux/04.Linux 的目录结构.html",redirect:"/pages/1e9034/"},{name:"v-1893221c",path:"/pages/8ad474/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-1893221c").then(t)}},{path:"/pages/8ad474/index.html",redirect:"/pages/8ad474/"},{path:"/02.Linux/01.Linux/05.Linux 操作文件目录.html",redirect:"/pages/8ad474/"},{name:"v-1094a947",path:"/pages/6f8d47/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-1094a947").then(t)}},{path:"/pages/6f8d47/index.html",redirect:"/pages/6f8d47/"},{path:"/02.Linux/01.Linux/06.Linux 系统管理命令.html",redirect:"/pages/6f8d47/"},{name:"v-9690772c",path:"/pages/ef1cfd/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-9690772c").then(t)}},{path:"/pages/ef1cfd/index.html",redirect:"/pages/ef1cfd/"},{path:"/02.Linux/01.Linux/07.Linux 开关机命令.html",redirect:"/pages/ef1cfd/"},{name:"v-2ded6bf9",path:"/pages/657e76/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2ded6bf9").then(t)}},{path:"/pages/657e76/index.html",redirect:"/pages/657e76/"},{path:"/02.Linux/01.Linux/08.Linux 压缩命令.html",redirect:"/pages/657e76/"},{name:"v-1f7d690b",path:"/pages/287d9f/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-1f7d690b").then(t)}},{path:"/pages/287d9f/index.html",redirect:"/pages/287d9f/"},{path:"/02.Linux/01.Linux/09.Linux 编辑器.html",redirect:"/pages/287d9f/"},{name:"v-56be3166",path:"/pages/0d3526/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-56be3166").then(t)}},{path:"/pages/0d3526/index.html",redirect:"/pages/0d3526/"},{path:"/02.Linux/01.Linux/10.Linux 软件包管理.html",redirect:"/pages/0d3526/"},{name:"v-02353645",path:"/pages/037bdb/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-02353645").then(t)}},{path:"/pages/037bdb/index.html",redirect:"/pages/037bdb/"},{path:"/02.Linux/01.Linux/11.Linux 用户和组管理.html",redirect:"/pages/037bdb/"},{name:"v-7bc439b8",path:"/pages/c29423/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7bc439b8").then(t)}},{path:"/pages/c29423/index.html",redirect:"/pages/c29423/"},{path:"/02.Linux/01.Linux/12.Linux 文件权限管理.html",redirect:"/pages/c29423/"},{name:"v-8a53495c",path:"/pages/6afa9f/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-8a53495c").then(t)}},{path:"/pages/6afa9f/index.html",redirect:"/pages/6afa9f/"},{path:"/02.Linux/01.Linux/13.Linux LVM 磁盘扩容.html",redirect:"/pages/6afa9f/"},{name:"v-3834644e",path:"/pages/8eb38e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-3834644e").then(t)}},{path:"/pages/8eb38e/index.html",redirect:"/pages/8eb38e/"},{path:"/03.CentOS/01.入门/01.什么是 CentOS.html",redirect:"/pages/8eb38e/"},{name:"v-a54afc5a",path:"/pages/4d8bf7/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-a54afc5a").then(t)}},{path:"/pages/4d8bf7/index.html",redirect:"/pages/4d8bf7/"},{path:"/03.CentOS/01.入门/02.CentOS 7 中使用阿里云的yum源.html",redirect:"/pages/4d8bf7/"},{name:"v-0a8f6788",path:"/pages/801448/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0a8f6788").then(t)}},{path:"/pages/801448/index.html",redirect:"/pages/801448/"},{path:"/03.CentOS/01.入门/03.CentOS 7 设置 SSH 通过密钥登录.html",redirect:"/pages/801448/"},{name:"v-cfad191a",path:"/pages/72cb23/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-cfad191a").then(t)}},{path:"/pages/72cb23/index.html",redirect:"/pages/72cb23/"},{path:"/03.CentOS/01.入门/04.CentOS 7 时间与网络时间同步.html",redirect:"/pages/72cb23/"},{name:"v-2a54cea8",path:"/pages/b65cee/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2a54cea8").then(t)}},{path:"/pages/b65cee/index.html",redirect:"/pages/b65cee/"},{path:"/03.CentOS/01.入门/05.CentOS 7 安装Jdk并配置环境变量.html",redirect:"/pages/b65cee/"},{name:"v-0a4aaf0e",path:"/pages/8a00d6/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0a4aaf0e").then(t)}},{path:"/pages/8a00d6/index.html",redirect:"/pages/8a00d6/"},{path:"/03.CentOS/01.入门/06.CentOS 7 配置静态ip和动态ip.html",redirect:"/pages/8a00d6/"},{name:"v-93569b68",path:"/pages/2bd107/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-93569b68").then(t)}},{path:"/pages/2bd107/index.html",redirect:"/pages/2bd107/"},{path:"/03.CentOS/01.入门/07.CentOS 7 安装 Nginx.html",redirect:"/pages/2bd107/"},{name:"v-eba23aa0",path:"/pages/b0c71d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-eba23aa0").then(t)}},{path:"/pages/b0c71d/index.html",redirect:"/pages/b0c71d/"},{path:"/03.CentOS/01.入门/08.CentOS 7 安装 Tomcat.html",redirect:"/pages/b0c71d/"},{name:"v-2d9e6cf8",path:"/pages/3748a2/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2d9e6cf8").then(t)}},{path:"/pages/3748a2/index.html",redirect:"/pages/3748a2/"},{path:"/03.CentOS/01.入门/09.CentOS 7 安装 MySQL.html",redirect:"/pages/3748a2/"},{name:"v-7d6fc128",path:"/pages/866b2e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7d6fc128").then(t)}},{path:"/pages/866b2e/index.html",redirect:"/pages/866b2e/"},{path:"/03.CentOS/01.入门/10.CentOS 7 安装 MongoDB.html",redirect:"/pages/866b2e/"},{name:"v-98da2ee0",path:"/pages/39ed97/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-98da2ee0").then(t)}},{path:"/pages/39ed97/index.html",redirect:"/pages/39ed97/"},{path:"/03.CentOS/01.入门/11.CentOS 7 安装 NodeJS.html",redirect:"/pages/39ed97/"},{name:"v-9bb044a0",path:"/pages/5cddbb/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-9bb044a0").then(t)}},{path:"/pages/5cddbb/index.html",redirect:"/pages/5cddbb/"},{path:"/03.CentOS/01.入门/12.CentOS 7 安装 Gitlab.html",redirect:"/pages/5cddbb/"},{name:"v-12e16df6",path:"/pages/1dfc9f/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-12e16df6").then(t)}},{path:"/pages/1dfc9f/index.html",redirect:"/pages/1dfc9f/"},{path:"/03.CentOS/01.入门/13.CentOS 7 安装 SonarQube.html",redirect:"/pages/1dfc9f/"},{name:"v-6a35a7dc",path:"/pages/893d19/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-6a35a7dc").then(t)}},{path:"/pages/893d19/index.html",redirect:"/pages/893d19/"},{path:"/03.CentOS/01.入门/14.CentOS 7 安装 Jenkins.html",redirect:"/pages/893d19/"},{name:"v-4c567690",path:"/pages/54c917/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4c567690").then(t)}},{path:"/pages/54c917/index.html",redirect:"/pages/54c917/"},{path:"/03.CentOS/01.入门/15.CentOS 7 安装 Redis.html",redirect:"/pages/54c917/"},{name:"v-3fe1ff73",path:"/pages/8c6aad/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-3fe1ff73").then(t)}},{path:"/pages/8c6aad/index.html",redirect:"/pages/8c6aad/"},{path:"/03.CentOS/01.入门/16.CentOS 7 安装 TigerVNC Server.html",redirect:"/pages/8c6aad/"},{name:"v-20b52a0c",path:"/pages/50595e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-20b52a0c").then(t)}},{path:"/pages/50595e/index.html",redirect:"/pages/50595e/"},{path:"/03.CentOS/01.入门/17.CentOS 7 安装 NFS.html",redirect:"/pages/50595e/"},{name:"v-1a563de3",path:"/pages/619aa3/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-1a563de3").then(t)}},{path:"/pages/619aa3/index.html",redirect:"/pages/619aa3/"},{path:"/03.CentOS/02.高级/01.CentOS7 LVM操作实战.html",redirect:"/pages/619aa3/"},{name:"v-1ce7bb32",path:"/pages/1b51c8/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-1ce7bb32").then(t)}},{path:"/pages/1b51c8/index.html",redirect:"/pages/1b51c8/"},{path:"/03.CentOS/03.实战/01.部署后台portal-web项目.html",redirect:"/pages/1b51c8/"},{name:"v-095cc5ff",path:"/pages/7ba23c/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-095cc5ff").then(t)}},{path:"/pages/7ba23c/index.html",redirect:"/pages/7ba23c/"},{path:"/03.CentOS/03.实战/02.部署前端ITP项目.html",redirect:"/pages/7ba23c/"},{name:"v-19944e88",path:"/pages/49420c/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-19944e88").then(t)}},{path:"/pages/49420c/index.html",redirect:"/pages/49420c/"},{path:"/04.Ubuntu/01.Ubuntu/01.什么是 Ubuntu.html",redirect:"/pages/49420c/"},{name:"v-35935e9c",path:"/pages/075d45/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-35935e9c").then(t)}},{path:"/pages/075d45/index.html",redirect:"/pages/075d45/"},{path:"/04.Ubuntu/02.Ubuntu 使用/01.Ubuntu 安装教程.html",redirect:"/pages/075d45/"},{name:"v-0a2f2246",path:"/pages/5d8a7e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0a2f2246").then(t)}},{path:"/pages/5d8a7e/index.html",redirect:"/pages/5d8a7e/"},{path:"/04.Ubuntu/02.Ubuntu 使用/02.Ubuntu 更换国内源.html",redirect:"/pages/5d8a7e/"},{name:"v-150d8184",path:"/pages/d6e0ac/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-150d8184").then(t)}},{path:"/pages/d6e0ac/index.html",redirect:"/pages/d6e0ac/"},{path:"/05.MacOS/01.macOS/01.macOS开发环境安装.html",redirect:"/pages/d6e0ac/"},{name:"v-e5b55ae4",path:"/pages/3b1a68/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-e5b55ae4").then(t)}},{path:"/pages/3b1a68/index.html",redirect:"/pages/3b1a68/"},{path:"/05.MacOS/01.macOS/02.macOS终端利器iTerm2.html",redirect:"/pages/3b1a68/"},{name:"v-4fa0bc5f",path:"/pages/078f4d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4fa0bc5f").then(t)}},{path:"/pages/078f4d/index.html",redirect:"/pages/078f4d/"},{path:"/05.MacOS/01.macOS/03.macOS自带终端美化.html",redirect:"/pages/078f4d/"},{name:"v-536252ab",path:"/pages/9ee77d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-536252ab").then(t)}},{path:"/pages/9ee77d/index.html",redirect:"/pages/9ee77d/"},{path:"/05.MacOS/01.macOS/04.macOS常用软件.html",redirect:"/pages/9ee77d/"},{name:"v-6694efc8",path:"/pages/22c758/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-6694efc8").then(t)}},{path:"/pages/22c758/index.html",redirect:"/pages/22c758/"},{path:"/06.GitLab/01.GitLab/01.什么是 Git.html",redirect:"/pages/22c758/"},{name:"v-2835dba9",path:"/pages/75dfd9/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2835dba9").then(t)}},{path:"/pages/75dfd9/index.html",redirect:"/pages/75dfd9/"},{path:"/06.GitLab/01.GitLab/02.安装 Git.html",redirect:"/pages/75dfd9/"},{name:"v-37c9db00",path:"/pages/8383bb/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-37c9db00").then(t)}},{path:"/pages/8383bb/index.html",redirect:"/pages/8383bb/"},{path:"/06.GitLab/01.GitLab/03.Git 的一般工作流程.html",redirect:"/pages/8383bb/"},{name:"v-30105e6e",path:"/pages/20e9b1/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-30105e6e").then(t)}},{path:"/pages/20e9b1/index.html",redirect:"/pages/20e9b1/"},{path:"/06.GitLab/01.GitLab/04.Git 的基本操作.html",redirect:"/pages/20e9b1/"},{name:"v-a983545e",path:"/pages/7eca40/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-a983545e").then(t)}},{path:"/pages/7eca40/index.html",redirect:"/pages/7eca40/"},{path:"/06.GitLab/01.GitLab/05.TortoiseGit 简化 Git 操作.html",redirect:"/pages/7eca40/"},{name:"v-5adc669c",path:"/pages/89c106/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-5adc669c").then(t)}},{path:"/pages/89c106/index.html",redirect:"/pages/89c106/"},{path:"/06.GitLab/01.GitLab/06.什么是 GitLab.html",redirect:"/pages/89c106/"},{name:"v-0670b552",path:"/pages/ffc177/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0670b552").then(t)}},{path:"/pages/ffc177/index.html",redirect:"/pages/ffc177/"},{path:"/06.GitLab/01.GitLab/07.基于 Docker 安装 GitLab.html",redirect:"/pages/ffc177/"},{name:"v-685bb760",path:"/pages/655544/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-685bb760").then(t)}},{path:"/pages/655544/index.html",redirect:"/pages/655544/"},{path:"/06.GitLab/01.GitLab/08.GitLab 的基本设置.html",redirect:"/pages/655544/"},{name:"v-7dad8b02",path:"/pages/0ba162/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7dad8b02").then(t)}},{path:"/pages/0ba162/index.html",redirect:"/pages/0ba162/"},{path:"/06.GitLab/01.GitLab/09.GitLab 的账户管理.html",redirect:"/pages/0ba162/"},{name:"v-77b753e2",path:"/pages/aa27ff/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-77b753e2").then(t)}},{path:"/pages/aa27ff/index.html",redirect:"/pages/aa27ff/"},{path:"/06.GitLab/01.GitLab/10.GitLab 创建第一个项目.html",redirect:"/pages/aa27ff/"},{name:"v-220226bd",path:"/pages/2b230d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-220226bd").then(t)}},{path:"/pages/2b230d/index.html",redirect:"/pages/2b230d/"},{path:"/06.GitLab/01.GitLab/11.GitLab 使用 SSH 免密登录.html",redirect:"/pages/2b230d/"},{name:"v-d809a770",path:"/pages/ceff17/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-d809a770").then(t)}},{path:"/pages/ceff17/index.html",redirect:"/pages/ceff17/"},{path:"/07.Nexus/01.Nexus/01.什么是 Nexus.html",redirect:"/pages/ceff17/"},{name:"v-e3e7063c",path:"/pages/dc57a4/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-e3e7063c").then(t)}},{path:"/pages/dc57a4/index.html",redirect:"/pages/dc57a4/"},{path:"/07.Nexus/01.Nexus/02.基于 Docker 安装 Nexus.html",redirect:"/pages/dc57a4/"},{name:"v-704eb51c",path:"/pages/159c7d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-704eb51c").then(t)}},{path:"/pages/159c7d/index.html",redirect:"/pages/159c7d/"},{path:"/07.Nexus/01.Nexus/03.Nexus配置阿里云代理仓库.html",redirect:"/pages/159c7d/"},{name:"v-48944d5a",path:"/pages/ee14ce/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-48944d5a").then(t)}},{path:"/pages/ee14ce/index.html",redirect:"/pages/ee14ce/"},{path:"/07.Nexus/01.Nexus/04.Maven 仓库介绍.html",redirect:"/pages/ee14ce/"},{name:"v-695dfcb6",path:"/pages/433dea/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-695dfcb6").then(t)}},{path:"/pages/433dea/index.html",redirect:"/pages/433dea/"},{path:"/07.Nexus/01.Nexus/05.在项目中使用 Maven 私服.html",redirect:"/pages/433dea/"},{name:"v-7ddb1434",path:"/pages/d1f496/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7ddb1434").then(t)}},{path:"/pages/d1f496/index.html",redirect:"/pages/d1f496/"},{path:"/07.Nexus/01.Nexus/06.Maven pom文件标签大全详解.html",redirect:"/pages/d1f496/"},{name:"v-f26a2222",path:"/pages/f28aa7/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-f26a2222").then(t)}},{path:"/pages/f28aa7/index.html",redirect:"/pages/f28aa7/"},{path:"/07.Nexus/01.Nexus/07.将项目推送到 Maven 中央仓库实践.html",redirect:"/pages/f28aa7/"},{name:"v-5f044039",path:"/pages/654ea2/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-5f044039").then(t)}},{path:"/pages/654ea2/index.html",redirect:"/pages/654ea2/"},{path:"/08.Registry/01.Registry/01.安装 Docker Registry 私服.html",redirect:"/pages/654ea2/"},{name:"v-310b44d6",path:"/pages/87fa6f/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-310b44d6").then(t)}},{path:"/pages/87fa6f/index.html",redirect:"/pages/87fa6f/"},{path:"/08.Registry/01.Registry/02.配置 Docker Registry 客户端.html",redirect:"/pages/87fa6f/"},{name:"v-0163bee7",path:"/pages/22b02b/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0163bee7").then(t)}},{path:"/pages/22b02b/index.html",redirect:"/pages/22b02b/"},{path:"/08.Registry/01.Registry/03.部署 Harbor 企业级Registry服务器.html",redirect:"/pages/22b02b/"},{name:"v-a4144134",path:"/pages/7ad507/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-a4144134").then(t)}},{path:"/pages/7ad507/index.html",redirect:"/pages/7ad507/"},{path:"/08.Registry/01.Registry/04.部署 Docker 可视化管理工具Portainer.html",redirect:"/pages/7ad507/"},{name:"v-818a32e0",path:"/pages/5b81e8/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-818a32e0").then(t)}},{path:"/pages/5b81e8/index.html",redirect:"/pages/5b81e8/"},{path:"/09.Redis/01.Redis/01.Redis 简介.html",redirect:"/pages/5b81e8/"},{name:"v-8ba9dd7a",path:"/pages/3f32a2/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-8ba9dd7a").then(t)}},{path:"/pages/3f32a2/index.html",redirect:"/pages/3f32a2/"},{path:"/09.Redis/01.Redis/02.基于源码编译安装 Redis.html",redirect:"/pages/3f32a2/"},{name:"v-8cb52cfc",path:"/pages/7a3c10/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-8cb52cfc").then(t)}},{path:"/pages/7a3c10/index.html",redirect:"/pages/7a3c10/"},{path:"/09.Redis/01.Redis/03.基于 Docker 安装 Redis.html",redirect:"/pages/7a3c10/"},{name:"v-48788de2",path:"/pages/8c9288/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-48788de2").then(t)}},{path:"/pages/8c9288/index.html",redirect:"/pages/8c9288/"},{path:"/10.Nginx/01.Nginx/01.什么是 Nginx.html",redirect:"/pages/8c9288/"},{name:"v-4c9ef69c",path:"/pages/df904c/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4c9ef69c").then(t)}},{path:"/pages/df904c/index.html",redirect:"/pages/df904c/"},{path:"/10.Nginx/01.Nginx/02.Nginx 安装SSL证书.html",redirect:"/pages/df904c/"},{name:"v-c80a5a9c",path:"/pages/73e9ae/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-c80a5a9c").then(t)}},{path:"/pages/73e9ae/index.html",redirect:"/pages/73e9ae/"},{path:"/10.Nginx/01.Nginx/03.Nginx 开启gzip压缩.html",redirect:"/pages/73e9ae/"},{name:"v-e4187760",path:"/pages/9d4443/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-e4187760").then(t)}},{path:"/pages/9d4443/index.html",redirect:"/pages/9d4443/"},{path:"/10.Nginx/01.Nginx/04.Nginx location正则写法.html",redirect:"/pages/9d4443/"},{name:"v-29934bff",path:"/pages/035a1b/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-29934bff").then(t)}},{path:"/pages/035a1b/index.html",redirect:"/pages/035a1b/"},{path:"/11.MongoDB/01.MongoDB/01.什么是MongoDB.html",redirect:"/pages/035a1b/"},{name:"v-6b185955",path:"/pages/25c76e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-6b185955").then(t)}},{path:"/pages/25c76e/index.html",redirect:"/pages/25c76e/"},{path:"/11.MongoDB/01.MongoDB/02.CentOS7 离线安装MongoDB.html",redirect:"/pages/25c76e/"},{name:"v-9431b692",path:"/pages/76dc6a/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-9431b692").then(t)}},{path:"/pages/76dc6a/index.html",redirect:"/pages/76dc6a/"},{path:"/12.MinIO/01.Minio服务器/01.Minio快速入门指南.html",redirect:"/pages/76dc6a/"},{name:"v-79bebf43",path:"/pages/0cd85f/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-79bebf43").then(t)}},{path:"/pages/0cd85f/index.html",redirect:"/pages/0cd85f/"},{path:"/12.MinIO/01.Minio服务器/02.Minio Docker快速入门.html",redirect:"/pages/0cd85f/"},{name:"v-bf7747fa",path:"/pages/6bd2f8/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-bf7747fa").then(t)}},{path:"/pages/6bd2f8/index.html",redirect:"/pages/6bd2f8/"},{path:"/12.MinIO/01.Minio服务器/03.Minio纠删码快速入门.html",redirect:"/pages/6bd2f8/"},{name:"v-2a4a0bb7",path:"/pages/8153b1/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2a4a0bb7").then(t)}},{path:"/pages/8153b1/index.html",redirect:"/pages/8153b1/"},{path:"/12.MinIO/01.Minio服务器/04.分布式Minio快速入门.html",redirect:"/pages/8153b1/"},{name:"v-7f6889c6",path:"/pages/98cf59/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7f6889c6").then(t)}},{path:"/pages/98cf59/index.html",redirect:"/pages/98cf59/"},{path:"/12.MinIO/01.Minio服务器/05.如何使用 TLS 保护对 MinIO 服务器的访问.html",redirect:"/pages/98cf59/"},{name:"v-459c694a",path:"/pages/50e89b/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-459c694a").then(t)}},{path:"/pages/50e89b/index.html",redirect:"/pages/50e89b/"},{path:"/12.MinIO/01.Minio服务器/06.Minio存储桶通知指南.html",redirect:"/pages/50e89b/"},{name:"v-7a72d387",path:"/pages/f9e3a2/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7a72d387").then(t)}},{path:"/pages/f9e3a2/index.html",redirect:"/pages/f9e3a2/"},{path:"/12.MinIO/01.Minio服务器/07.Minio服务限制租户.html",redirect:"/pages/f9e3a2/"},{name:"v-e6c9547a",path:"/pages/625c17/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-e6c9547a").then(t)}},{path:"/pages/625c17/index.html",redirect:"/pages/625c17/"},{path:"/12.MinIO/01.Minio服务器/08.Minio Server配置指南.html",redirect:"/pages/625c17/"},{name:"v-1128e807",path:"/pages/9b5f74/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-1128e807").then(t)}},{path:"/pages/9b5f74/index.html",redirect:"/pages/9b5f74/"},{path:"/12.MinIO/01.Minio服务器/09.Minio多租户（Multi-tenant）部署指南.html",redirect:"/pages/9b5f74/"},{name:"v-a77e2b92",path:"/pages/17c975/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-a77e2b92").then(t)}},{path:"/pages/17c975/index.html",redirect:"/pages/17c975/"},{path:"/12.MinIO/02.Minio客户端快速入门指南/01.Minio客户端快速入门指南.html",redirect:"/pages/17c975/"},{name:"v-40000776",path:"/pages/65555a/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-40000776").then(t)}},{path:"/pages/65555a/index.html",redirect:"/pages/65555a/"},{path:"/04.Ubuntu/02.Ubuntu 使用/03.Ubuntu 开发环境变量配置.html",redirect:"/pages/65555a/"},{name:"v-2ac12114",path:"/pages/538f88/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2ac12114").then(t)}},{path:"/pages/538f88/index.html",redirect:"/pages/538f88/"},{path:"/12.MinIO/02.Minio客户端快速入门指南/02.Minio Client完全指南.html",redirect:"/pages/538f88/"},{name:"v-d416917a",path:"/pages/53a165/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-d416917a").then(t)}},{path:"/pages/53a165/index.html",redirect:"/pages/53a165/"},{path:"/12.MinIO/03.Minio SDKs/01.Java Client快速入门指南.html",redirect:"/pages/53a165/"},{name:"v-3785d681",path:"/pages/b8b311/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-3785d681").then(t)}},{path:"/pages/b8b311/index.html",redirect:"/pages/b8b311/"},{path:"/12.MinIO/03.Minio SDKs/02.Java Client API参考文档.html",redirect:"/pages/b8b311/"},{name:"v-117cc90e",path:"/pages/4a395a/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-117cc90e").then(t)}},{path:"/pages/4a395a/index.html",redirect:"/pages/4a395a/"},{path:"/12.MinIO/04.实战秘籍/01.使用Certbot生成Let's Encrypt证书.html",redirect:"/pages/4a395a/"},{name:"v-84e3ebde",path:"/pages/7acf5a/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-84e3ebde").then(t)}},{path:"/pages/7acf5a/index.html",redirect:"/pages/7acf5a/"},{path:"/12.MinIO/04.实战秘籍/02.使用 MinIO 服务器设置 Nginx 代理.html",redirect:"/pages/7acf5a/"},{name:"v-427cb080",path:"/pages/542516/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-427cb080").then(t)}},{path:"/pages/542516/index.html",redirect:"/pages/542516/"},{path:"/12.MinIO/05.Minio部署/01.Minio部署快速入门.html",redirect:"/pages/542516/"},{name:"v-03ae78c4",path:"/pages/b1ea49/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-03ae78c4").then(t)}},{path:"/pages/b1ea49/index.html",redirect:"/pages/b1ea49/"},{path:"/12.MinIO/05.Minio部署/02.使用Docker Swarm部署Minio.html",redirect:"/pages/b1ea49/"},{name:"v-931f934a",path:"/pages/01eafc/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-931f934a").then(t)}},{path:"/pages/01eafc/index.html",redirect:"/pages/01eafc/"},{path:"/12.MinIO/05.Minio部署/03.使用Kubernetes部署Minio.html",redirect:"/pages/01eafc/"},{name:"v-4b4c35dc",path:"/pages/46c251/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4b4c35dc").then(t)}},{path:"/pages/46c251/index.html",redirect:"/pages/46c251/"},{path:"/12.MinIO/05.Minio部署/04.在 Docker Compose 上部署 MinIO.html",redirect:"/pages/46c251/"},{name:"v-0a92eeec",path:"/pages/1a684e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0a92eeec").then(t)}},{path:"/pages/1a684e/index.html",redirect:"/pages/1a684e/"},{path:"/13.RabbitMQ/01.快速入门/02.RabbitMQ 快速入门.html",redirect:"/pages/1a684e/"},{name:"v-c205d0b2",path:"/pages/d710e9/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-c205d0b2").then(t)}},{path:"/pages/d710e9/index.html",redirect:"/pages/d710e9/"},{path:"/13.RabbitMQ/01.快速入门/01.RabbitMQ 简介.html",redirect:"/pages/d710e9/"},{name:"v-50673fcc",path:"/pages/05a61a/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-50673fcc").then(t)}},{path:"/pages/05a61a/index.html",redirect:"/pages/05a61a/"},{path:"/13.RabbitMQ/01.快速入门/03.RabbitMQ 消息模型.html",redirect:"/pages/05a61a/"},{name:"v-6680c492",path:"/pages/36085c/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-6680c492").then(t)}},{path:"/pages/36085c/index.html",redirect:"/pages/36085c/"},{path:"/13.RabbitMQ/01.快速入门/04.Spring整合RabbitMQ.html",redirect:"/pages/36085c/"},{name:"v-12cc11ea",path:"/pages/593778/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-12cc11ea").then(t)}},{path:"/pages/593778/index.html",redirect:"/pages/593778/"},{path:"/14.微服务/01.微服务简介/01.构建单体应用模型.html",redirect:"/pages/593778/"},{name:"v-8f47802a",path:"/pages/c5c502/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-8f47802a").then(t)}},{path:"/pages/c5c502/index.html",redirect:"/pages/c5c502/"},{path:"/13.RabbitMQ/02.高级用法/01.TTL队列_死信队列.html",redirect:"/pages/c5c502/"},{name:"v-dc3c5474",path:"/pages/72d00f/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-dc3c5474").then(t)}},{path:"/pages/72d00f/index.html",redirect:"/pages/72d00f/"},{path:"/14.微服务/01.微服务简介/02.走向单体地狱.html",redirect:"/pages/72d00f/"},{name:"v-08740c32",path:"/pages/0b2d6e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-08740c32").then(t)}},{path:"/pages/0b2d6e/index.html",redirect:"/pages/0b2d6e/"},{path:"/14.微服务/01.微服务简介/03.微服务-解决复杂问题.html",redirect:"/pages/0b2d6e/"},{name:"v-3704fefa",path:"/pages/47dac2/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-3704fefa").then(t)}},{path:"/pages/47dac2/index.html",redirect:"/pages/47dac2/"},{path:"/14.微服务/01.微服务简介/04.微服务的优点.html",redirect:"/pages/47dac2/"},{name:"v-fc4d58d8",path:"/pages/aa8849/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-fc4d58d8").then(t)}},{path:"/pages/aa8849/index.html",redirect:"/pages/aa8849/"},{path:"/15.Spring Cloud Gateway/01.Spring Cloud Gateway/01.Spring Cloud 之 Gateway.html",redirect:"/pages/aa8849/"},{name:"v-1dd07656",path:"/pages/f0610e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-1dd07656").then(t)}},{path:"/pages/f0610e/index.html",redirect:"/pages/f0610e/"},{path:"/15.Spring Cloud Gateway/01.Spring Cloud Gateway/02.Spring Cloud Gateway 配置大全.html",redirect:"/pages/f0610e/"},{name:"v-13de5e4a",path:"/pages/f7b8bf/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-13de5e4a").then(t)}},{path:"/pages/f7b8bf/index.html",redirect:"/pages/f7b8bf/"},{path:"/15.Spring Cloud Gateway/01.Spring Cloud Gateway/03.Spring Cloud Gateway 中文官网文档.html",redirect:"/pages/f7b8bf/"},{name:"v-c2d87842",path:"/pages/b62716/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-c2d87842").then(t)}},{path:"/pages/b62716/index.html",redirect:"/pages/b62716/"},{path:"/16.Spring Cloud Alibaba/01.Spring Cloud Alibaba/01.简介.html",redirect:"/pages/b62716/"},{name:"v-c1a66828",path:"/pages/e48516/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-c1a66828").then(t)}},{path:"/pages/e48516/index.html",redirect:"/pages/e48516/"},{path:"/16.Spring Cloud Alibaba/01.Spring Cloud Alibaba/02.服务注册与发现.html",redirect:"/pages/e48516/"},{name:"v-ed96cd98",path:"/pages/01bdf6/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-ed96cd98").then(t)}},{path:"/pages/01bdf6/index.html",redirect:"/pages/01bdf6/"},{path:"/16.Spring Cloud Alibaba/01.Spring Cloud Alibaba/03.Nacos discovery.html",redirect:"/pages/01bdf6/"},{name:"v-19430f44",path:"/pages/3a1e7d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-19430f44").then(t)}},{path:"/pages/3a1e7d/index.html",redirect:"/pages/3a1e7d/"},{path:"/16.Spring Cloud Alibaba/01.Spring Cloud Alibaba/04.Nacos config.html",redirect:"/pages/3a1e7d/"},{name:"v-5b9e5448",path:"/pages/555c55/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-5b9e5448").then(t)}},{path:"/pages/555c55/index.html",redirect:"/pages/555c55/"},{path:"/17.Docker/01.Docker/01.什么是 Docker.html",redirect:"/pages/555c55/"},{name:"v-3058349d",path:"/pages/d1d9db/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-3058349d").then(t)}},{path:"/pages/d1d9db/index.html",redirect:"/pages/d1d9db/"},{path:"/17.Docker/01.Docker/02.为什么要使用 Docker.html",redirect:"/pages/d1d9db/"},{name:"v-741ac0fb",path:"/pages/ad1da1/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-741ac0fb").then(t)}},{path:"/pages/ad1da1/index.html",redirect:"/pages/ad1da1/"},{path:"/17.Docker/02.Docker 基本概念/01.Docker 基本概念.html",redirect:"/pages/ad1da1/"},{name:"v-0dc6bf26",path:"/pages/77a885/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0dc6bf26").then(t)}},{path:"/pages/77a885/index.html",redirect:"/pages/77a885/"},{path:"/17.Docker/02.Docker 基本概念/02.Docker 引擎.html",redirect:"/pages/77a885/"},{name:"v-17b7bcd2",path:"/pages/f566e4/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-17b7bcd2").then(t)}},{path:"/pages/f566e4/index.html",redirect:"/pages/f566e4/"},{path:"/17.Docker/02.Docker 基本概念/03.Docker 系统架构.html",redirect:"/pages/f566e4/"},{name:"v-397e4290",path:"/pages/546146/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-397e4290").then(t)}},{path:"/pages/546146/index.html",redirect:"/pages/546146/"},{path:"/17.Docker/02.Docker 基本概念/04.Docker 镜像.html",redirect:"/pages/546146/"},{name:"v-4b0f9931",path:"/pages/c9e8a8/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4b0f9931").then(t)}},{path:"/pages/c9e8a8/index.html",redirect:"/pages/c9e8a8/"},{path:"/17.Docker/02.Docker 基本概念/05.Docker 容器.html",redirect:"/pages/c9e8a8/"},{name:"v-2046e738",path:"/pages/f00cd0/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2046e738").then(t)}},{path:"/pages/f00cd0/index.html",redirect:"/pages/f00cd0/"},{path:"/17.Docker/02.Docker 基本概念/06.Docker 仓库.html",redirect:"/pages/f00cd0/"},{name:"v-05980e1b",path:"/pages/11ef44/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-05980e1b").then(t)}},{path:"/pages/11ef44/index.html",redirect:"/pages/11ef44/"},{path:"/17.Docker/03.安装 Docker/01.安装 Docker.html",redirect:"/pages/11ef44/"},{name:"v-4637e644",path:"/pages/748872/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4637e644").then(t)}},{path:"/pages/748872/index.html",redirect:"/pages/748872/"},{path:"/17.Docker/03.安装 Docker/02.Ubuntu 安装 Docker.html",redirect:"/pages/748872/"},{name:"v-5b6a58d6",path:"/pages/102284/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-5b6a58d6").then(t)}},{path:"/pages/102284/index.html",redirect:"/pages/102284/"},{path:"/17.Docker/03.安装 Docker/03.CentOS 安装 Docker.html",redirect:"/pages/102284/"},{name:"v-a1271344",path:"/pages/847488/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-a1271344").then(t)}},{path:"/pages/847488/index.html",redirect:"/pages/847488/"},{path:"/17.Docker/03.安装 Docker/04.macOS 安装 Docker.html",redirect:"/pages/847488/"},{name:"v-79b29984",path:"/pages/c4f755/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-79b29984").then(t)}},{path:"/pages/c4f755/index.html",redirect:"/pages/c4f755/"},{path:"/17.Docker/03.安装 Docker/05.Windows 安装 Docker.html",redirect:"/pages/c4f755/"},{name:"v-782329dc",path:"/pages/87062e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-782329dc").then(t)}},{path:"/pages/87062e/index.html",redirect:"/pages/87062e/"},{path:"/17.Docker/03.安装 Docker/06.Docker 镜像加速器.html",redirect:"/pages/87062e/"},{name:"v-36dcd140",path:"/pages/13e1d1/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-36dcd140").then(t)}},{path:"/pages/13e1d1/index.html",redirect:"/pages/13e1d1/"},{path:"/17.Docker/04.Docker 镜像/01.使用 Docker 镜像.html",redirect:"/pages/13e1d1/"},{name:"v-02185a76",path:"/pages/840a1a/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-02185a76").then(t)}},{path:"/pages/840a1a/index.html",redirect:"/pages/840a1a/"},{path:"/17.Docker/04.Docker 镜像/02.Docker 获取镜像.html",redirect:"/pages/840a1a/"},{name:"v-38a7db08",path:"/pages/be83bd/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-38a7db08").then(t)}},{path:"/pages/be83bd/index.html",redirect:"/pages/be83bd/"},{path:"/17.Docker/04.Docker 镜像/03.Docker 列出镜像.html",redirect:"/pages/be83bd/"},{name:"v-1bf741b0",path:"/pages/058894/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-1bf741b0").then(t)}},{path:"/pages/058894/index.html",redirect:"/pages/058894/"},{path:"/17.Docker/04.Docker 镜像/04.Docker 删除本地镜像.html",redirect:"/pages/058894/"},{name:"v-74089f43",path:"/pages/79f5f4/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-74089f43").then(t)}},{path:"/pages/79f5f4/index.html",redirect:"/pages/79f5f4/"},{path:"/17.Docker/04.Docker 镜像/05.Docker 镜像导入导出.html",redirect:"/pages/79f5f4/"},{name:"v-16c13813",path:"/pages/134f38/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-16c13813").then(t)}},{path:"/pages/134f38/index.html",redirect:"/pages/134f38/"},{path:"/17.Docker/04.Docker 镜像/06.利用 commit 理解镜像构成.html",redirect:"/pages/134f38/"},{name:"v-65ae1b58",path:"/pages/ffea77/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-65ae1b58").then(t)}},{path:"/pages/ffea77/index.html",redirect:"/pages/ffea77/"},{path:"/17.Docker/04.Docker 镜像/07.使用 Dockerfile 定制镜像.html",redirect:"/pages/ffea77/"},{name:"v-92e32acc",path:"/pages/6d2add/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-92e32acc").then(t)}},{path:"/pages/6d2add/index.html",redirect:"/pages/6d2add/"},{path:"/17.Docker/04.Docker 镜像/08.Dockerfile 指令详解.html",redirect:"/pages/6d2add/"},{name:"v-12b8a377",path:"/pages/f99491/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-12b8a377").then(t)}},{path:"/pages/f99491/index.html",redirect:"/pages/f99491/"},{path:"/17.Docker/04.Docker 镜像/09.Dockerfile 多阶段构建.html",redirect:"/pages/f99491/"},{name:"v-dd62df30",path:"/pages/3937d4/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-dd62df30").then(t)}},{path:"/pages/3937d4/index.html",redirect:"/pages/3937d4/"},{path:"/17.Docker/04.Docker 镜像/10.镜像的实现原理.html",redirect:"/pages/3937d4/"},{name:"v-1247ca23",path:"/pages/7e034a/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-1247ca23").then(t)}},{path:"/pages/7e034a/index.html",redirect:"/pages/7e034a/"},{path:"/17.Docker/05.Docker 容器/01.操作 Docker 容器.html",redirect:"/pages/7e034a/"},{name:"v-39ac2c18",path:"/pages/f5a508/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-39ac2c18").then(t)}},{path:"/pages/f5a508/index.html",redirect:"/pages/f5a508/"},{path:"/17.Docker/05.Docker 容器/02.Docker 启动容器.html",redirect:"/pages/f5a508/"},{name:"v-80330662",path:"/pages/4c0f60/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-80330662").then(t)}},{path:"/pages/4c0f60/index.html",redirect:"/pages/4c0f60/"},{path:"/17.Docker/05.Docker 容器/03.Docker 守护态运行.html",redirect:"/pages/4c0f60/"},{name:"v-73993688",path:"/pages/a777e4/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-73993688").then(t)}},{path:"/pages/a777e4/index.html",redirect:"/pages/a777e4/"},{path:"/17.Docker/05.Docker 容器/04.Docker 终止容器.html",redirect:"/pages/a777e4/"},{name:"v-570536b6",path:"/pages/9efa8d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-570536b6").then(t)}},{path:"/pages/9efa8d/index.html",redirect:"/pages/9efa8d/"},{path:"/17.Docker/05.Docker 容器/05.Docker 进入容器.html",redirect:"/pages/9efa8d/"},{name:"v-045a323e",path:"/pages/d3a79f/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-045a323e").then(t)}},{path:"/pages/d3a79f/index.html",redirect:"/pages/d3a79f/"},{path:"/17.Docker/05.Docker 容器/06.Docker 导出和导入容器.html",redirect:"/pages/d3a79f/"},{name:"v-ae2ed992",path:"/pages/8b2f36/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-ae2ed992").then(t)}},{path:"/pages/8b2f36/index.html",redirect:"/pages/8b2f36/"},{path:"/17.Docker/05.Docker 容器/07.Docker 删除容器.html",redirect:"/pages/8b2f36/"},{name:"v-059385f2",path:"/pages/c0886b/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-059385f2").then(t)}},{path:"/pages/c0886b/index.html",redirect:"/pages/c0886b/"},{path:"/17.Docker/06.访问 Docker 仓库/01.访问 Docker 仓库.html",redirect:"/pages/c0886b/"},{name:"v-687cd7b8",path:"/pages/ebce48/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-687cd7b8").then(t)}},{path:"/pages/ebce48/index.html",redirect:"/pages/ebce48/"},{path:"/17.Docker/06.访问 Docker 仓库/02.Docker Hub.html",redirect:"/pages/ebce48/"},{name:"v-f8655a0e",path:"/pages/92e266/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-f8655a0e").then(t)}},{path:"/pages/92e266/index.html",redirect:"/pages/92e266/"},{path:"/17.Docker/06.访问 Docker 仓库/03.Docker 私有仓库.html",redirect:"/pages/92e266/"},{name:"v-093f317f",path:"/pages/141d21/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-093f317f").then(t)}},{path:"/pages/141d21/index.html",redirect:"/pages/141d21/"},{path:"/17.Docker/06.访问 Docker 仓库/04.Docker 私有仓库高级配置.html",redirect:"/pages/141d21/"},{name:"v-9766780a",path:"/pages/bcfae7/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-9766780a").then(t)}},{path:"/pages/bcfae7/index.html",redirect:"/pages/bcfae7/"},{path:"/17.Docker/07.Docker 数据管理/01.Docker 数据管理.html",redirect:"/pages/bcfae7/"},{name:"v-59a33950",path:"/pages/5f0882/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-59a33950").then(t)}},{path:"/pages/5f0882/index.html",redirect:"/pages/5f0882/"},{path:"/17.Docker/07.Docker 数据管理/02.Docker 数据卷.html",redirect:"/pages/5f0882/"},{name:"v-87ec7d3a",path:"/pages/565ee4/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-87ec7d3a").then(t)}},{path:"/pages/565ee4/index.html",redirect:"/pages/565ee4/"},{path:"/17.Docker/07.Docker 数据管理/03.监听主机目录.html",redirect:"/pages/565ee4/"},{name:"v-046481ec",path:"/pages/e17a65/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-046481ec").then(t)}},{path:"/pages/e17a65/index.html",redirect:"/pages/e17a65/"},{path:"/17.Docker/08.Docker 三剑客 Compose/01.什么是 Docker Compose.html",redirect:"/pages/e17a65/"},{name:"v-098439d0",path:"/pages/f7fbae/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-098439d0").then(t)}},{path:"/pages/f7fbae/index.html",redirect:"/pages/f7fbae/"},{path:"/17.Docker/08.Docker 三剑客 Compose/02.Docker Compose 安装与卸载.html",redirect:"/pages/f7fbae/"},{name:"v-8eabad82",path:"/pages/a48959/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-8eabad82").then(t)}},{path:"/pages/a48959/index.html",redirect:"/pages/a48959/"},{path:"/17.Docker/08.Docker 三剑客 Compose/03.Docker Compose 入门.html",redirect:"/pages/a48959/"},{name:"v-262a1fde",path:"/pages/cddd90/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-262a1fde").then(t)}},{path:"/pages/cddd90/index.html",redirect:"/pages/cddd90/"},{path:"/17.Docker/08.Docker 三剑客 Compose/04.Docker Compose 命令说明.html",redirect:"/pages/cddd90/"},{name:"v-0c749932",path:"/pages/5e9e65/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0c749932").then(t)}},{path:"/pages/5e9e65/index.html",redirect:"/pages/5e9e65/"},{path:"/17.Docker/08.Docker 三剑客 Compose/05.Docker Compose 模板文件.html",redirect:"/pages/5e9e65/"},{name:"v-3c3f9ba1",path:"/pages/7285c6/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-3c3f9ba1").then(t)}},{path:"/pages/7285c6/index.html",redirect:"/pages/7285c6/"},{path:"/17.Docker/08.Docker 三剑客 Compose/06.Docker Compose 实战 WordPress.html",redirect:"/pages/7285c6/"},{name:"v-ae86ef32",path:"/pages/8a9ceb/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-ae86ef32").then(t)}},{path:"/pages/8a9ceb/index.html",redirect:"/pages/8a9ceb/"},{path:"/17.Docker/08.Docker 三剑客 Compose/07.Docker Compose 实战 Tomcat.html",redirect:"/pages/8a9ceb/"},{name:"v-28665664",path:"/pages/fb92c3/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-28665664").then(t)}},{path:"/pages/fb92c3/index.html",redirect:"/pages/fb92c3/"},{path:"/14.微服务/01.微服务简介/05.微服务的缺点.html",redirect:"/pages/fb92c3/"},{name:"v-4c82f50b",path:"/pages/853cbd/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4c82f50b").then(t)}},{path:"/pages/853cbd/index.html",redirect:"/pages/853cbd/"},{path:"/17.Docker/08.Docker 三剑客 Compose/08.Docker Compose 实战 MySQL.html",redirect:"/pages/853cbd/"},{name:"v-6272dfb6",path:"/pages/bd41de/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-6272dfb6").then(t)}},{path:"/pages/bd41de/index.html",redirect:"/pages/bd41de/"},{path:"/17.Docker/08.Docker 三剑客 Compose/10.YAML 配置文件语言.html",redirect:"/pages/bd41de/"},{name:"v-1dfe3ec2",path:"/pages/f508e3/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-1dfe3ec2").then(t)}},{path:"/pages/f508e3/index.html",redirect:"/pages/f508e3/"},{path:"/17.Docker/08.Docker 三剑客 Compose/09.Docker Compose 常用命令.html",redirect:"/pages/f508e3/"},{name:"v-a0974e96",path:"/pages/0c8168/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-a0974e96").then(t)}},{path:"/pages/0c8168/index.html",redirect:"/pages/0c8168/"},{path:"/17.Docker/08.Docker 三剑客 Compose/11.附：为什么说 JSON 不适合做配置文件？.html",redirect:"/pages/0c8168/"},{name:"v-321c5958",path:"/pages/be0094/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-321c5958").then(t)}},{path:"/pages/be0094/index.html",redirect:"/pages/be0094/"},{path:"/17.Docker/09.常见问题/01.docker磁盘空间不足解决办法.html",redirect:"/pages/be0094/"},{name:"v-efa3c938",path:"/pages/419ea5/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-efa3c938").then(t)}},{path:"/pages/419ea5/index.html",redirect:"/pages/419ea5/"},{path:"/18.Kubernetes/01.Kubernetes 入门/01.什么是 Kubernetes.html",redirect:"/pages/419ea5/"},{name:"v-255f6b29",path:"/pages/b2e7fa/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-255f6b29").then(t)}},{path:"/pages/b2e7fa/index.html",redirect:"/pages/b2e7fa/"},{path:"/18.Kubernetes/01.Kubernetes 入门/02.Kubernetes 安装前的准备.html",redirect:"/pages/b2e7fa/"},{name:"v-28726c01",path:"/pages/babf51/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-28726c01").then(t)}},{path:"/pages/babf51/index.html",redirect:"/pages/babf51/"},{path:"/18.Kubernetes/01.Kubernetes 入门/03.安装 kubeadm.html",redirect:"/pages/babf51/"},{name:"v-4a1b2e40",path:"/pages/14ea2d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4a1b2e40").then(t)}},{path:"/pages/14ea2d/index.html",redirect:"/pages/14ea2d/"},{path:"/18.Kubernetes/01.Kubernetes 入门/05.配置 kubeadm.html",redirect:"/pages/14ea2d/"},{name:"v-f8a50a56",path:"/pages/9b17d0/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-f8a50a56").then(t)}},{path:"/pages/9b17d0/index.html",redirect:"/pages/9b17d0/"},{path:"/18.Kubernetes/01.Kubernetes 入门/06.使用 kubeadm 部署 Master 节点.html",redirect:"/pages/9b17d0/"},{name:"v-4b869476",path:"/pages/9477b8/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4b869476").then(t)}},{path:"/pages/9477b8/index.html",redirect:"/pages/9477b8/"},{path:"/18.Kubernetes/01.Kubernetes 入门/07.安装网络插件.html",redirect:"/pages/9477b8/"},{name:"v-20e14c4b",path:"/pages/f6617c/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-20e14c4b").then(t)}},{path:"/pages/f6617c/index.html",redirect:"/pages/f6617c/"},{path:"/18.Kubernetes/01.Kubernetes 入门/08.使用 kubeadm 配置 Worker 节点.html",redirect:"/pages/f6617c/"},{name:"v-7e4ade2c",path:"/pages/f4b696/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7e4ade2c").then(t)}},{path:"/pages/f4b696/index.html",redirect:"/pages/f4b696/"},{path:"/18.Kubernetes/01.Kubernetes 入门/09.概念总结.html",redirect:"/pages/f4b696/"},{name:"v-61b52e07",path:"/pages/e68987/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-61b52e07").then(t)}},{path:"/pages/e68987/index.html",redirect:"/pages/e68987/"},{path:"/18.Kubernetes/02.Kubernetes 实践/01.Kubernetes Dashboard.html",redirect:"/pages/e68987/"},{name:"v-88106b62",path:"/pages/1b012a/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-88106b62").then(t)}},{path:"/pages/1b012a/index.html",redirect:"/pages/1b012a/"},{path:"/18.Kubernetes/02.Kubernetes 实践/02.NFS 高可用方案.html",redirect:"/pages/1b012a/"},{name:"v-18890533",path:"/pages/8a1473/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-18890533").then(t)}},{path:"/pages/8a1473/index.html",redirect:"/pages/8a1473/"},{path:"/18.Kubernetes/02.Kubernetes 实践/03.离线部署 Kubernetess + KubeSphere.html",redirect:"/pages/8a1473/"},{name:"v-74c698d0",path:"/pages/1b4b4e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-74c698d0").then(t)}},{path:"/pages/1b4b4e/index.html",redirect:"/pages/1b4b4e/"},{path:"/19.Spring Security OAuth2/01.Spring Security OAuth2/01.介绍.html",redirect:"/pages/1b4b4e/"},{name:"v-78a70910",path:"/pages/17b300/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-78a70910").then(t)}},{path:"/pages/17b300/index.html",redirect:"/pages/17b300/"},{path:"/19.Spring Security OAuth2/01.Spring Security OAuth2/02.为什么需要 OAuth2.html",redirect:"/pages/17b300/"},{name:"v-3222f444",path:"/pages/5cb943/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-3222f444").then(t)}},{path:"/pages/5cb943/index.html",redirect:"/pages/5cb943/"},{path:"/19.Spring Security OAuth2/01.Spring Security OAuth2/03.开放平台.html",redirect:"/pages/5cb943/"},{name:"v-3c995a47",path:"/pages/37f648/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-3c995a47").then(t)}},{path:"/pages/37f648/index.html",redirect:"/pages/37f648/"},{path:"/19.Spring Security OAuth2/01.Spring Security OAuth2/04.令牌的访问与刷新.html",redirect:"/pages/37f648/"},{name:"v-18111e3e",path:"/pages/fbe541/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-18111e3e").then(t)}},{path:"/pages/fbe541/index.html",redirect:"/pages/fbe541/"},{path:"/19.Spring Security OAuth2/01.Spring Security OAuth2/05.客户端授权模式.html",redirect:"/pages/fbe541/"},{name:"v-2d7230e8",path:"/pages/00c356/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2d7230e8").then(t)}},{path:"/pages/00c356/index.html",redirect:"/pages/00c356/"},{path:"/19.Spring Security OAuth2/02.创建案例工程/01.创建案例工程项目.html",redirect:"/pages/00c356/"},{name:"v-2fa8ca6b",path:"/pages/c3686c/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2fa8ca6b").then(t)}},{path:"/pages/c3686c/index.html",redirect:"/pages/c3686c/"},{path:"/19.Spring Security OAuth2/02.创建案例工程/02.创建统一的依赖管理模块.html",redirect:"/pages/c3686c/"},{name:"v-ca52ce64",path:"/pages/5eb137/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-ca52ce64").then(t)}},{path:"/pages/5eb137/index.html",redirect:"/pages/5eb137/"},{path:"/19.Spring Security OAuth2/03.创建认证服务器/01.授权服务器配置.html",redirect:"/pages/5eb137/"},{name:"v-620b25ca",path:"/pages/2279cf/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-620b25ca").then(t)}},{path:"/pages/2279cf/index.html",redirect:"/pages/2279cf/"},{path:"/19.Spring Security OAuth2/03.创建认证服务器/02.基于内存存储令牌.html",redirect:"/pages/2279cf/"},{name:"v-d70c8d9c",path:"/pages/9b9a7b/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-d70c8d9c").then(t)}},{path:"/pages/9b9a7b/index.html",redirect:"/pages/9b9a7b/"},{path:"/19.Spring Security OAuth2/03.创建认证服务器/03.基于 JDBC 存储令牌.html",redirect:"/pages/9b9a7b/"},{name:"v-8dc9cdca",path:"/pages/cee981/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-8dc9cdca").then(t)}},{path:"/pages/cee981/index.html",redirect:"/pages/cee981/"},{path:"/19.Spring Security OAuth2/03.创建认证服务器/04.RBAC 基于角色的权限控制.html",redirect:"/pages/cee981/"},{name:"v-994030a6",path:"/pages/4bff79/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-994030a6").then(t)}},{path:"/pages/4bff79/index.html",redirect:"/pages/4bff79/"},{path:"/19.Spring Security OAuth2/03.创建认证服务器/05.基于 RBAC 的自定义认证.html",redirect:"/pages/4bff79/"},{name:"v-c4e4c304",path:"/pages/8dc90c/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-c4e4c304").then(t)}},{path:"/pages/8dc90c/index.html",redirect:"/pages/8dc90c/"},{path:"/19.Spring Security OAuth2/03.创建认证服务器/06.JWT令牌.html",redirect:"/pages/8dc90c/"},{name:"v-c4de77d8",path:"/pages/3414a7/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-c4de77d8").then(t)}},{path:"/pages/3414a7/index.html",redirect:"/pages/3414a7/"},{path:"/19.Spring Security OAuth2/04.创建资源服务器/01.资源服务器配置.html",redirect:"/pages/3414a7/"},{name:"v-8851eab0",path:"/pages/d62ff2/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-8851eab0").then(t)}},{path:"/pages/d62ff2/index.html",redirect:"/pages/d62ff2/"},{path:"/19.Spring Security OAuth2/05.分布式系统认证方案/01.什么是分布式系统.html",redirect:"/pages/d62ff2/"},{name:"v-f80986b8",path:"/pages/16b31e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-f80986b8").then(t)}},{path:"/pages/16b31e/index.html",redirect:"/pages/16b31e/"},{path:"/19.Spring Security OAuth2/05.分布式系统认证方案/02.分布式认证需求.html",redirect:"/pages/16b31e/"},{name:"v-7afa71b5",path:"/pages/d6bec0/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7afa71b5").then(t)}},{path:"/pages/d6bec0/index.html",redirect:"/pages/d6bec0/"},{path:"/19.Spring Security OAuth2/05.分布式系统认证方案/03.分布式认证方案.html",redirect:"/pages/d6bec0/"},{name:"v-bee7b4c4",path:"/pages/7b210e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-bee7b4c4").then(t)}},{path:"/pages/7b210e/index.html",redirect:"/pages/7b210e/"},{path:"/19.Spring Security OAuth2/05.分布式系统认证方案/04.分布式认证实战.html",redirect:"/pages/7b210e/"},{name:"v-7bad6842",path:"/pages/11f8a5/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7bad6842").then(t)}},{path:"/pages/11f8a5/index.html",redirect:"/pages/11f8a5/"},{path:"/20.DevOps/01.入门/01.软件工程介绍.html",redirect:"/pages/11f8a5/"},{name:"v-d8f92f9e",path:"/pages/5b4493/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-d8f92f9e").then(t)}},{path:"/pages/5b4493/index.html",redirect:"/pages/5b4493/"},{path:"/20.DevOps/01.入门/02.持续继承流程说明.html",redirect:"/pages/5b4493/"},{name:"v-f6cd3ec8",path:"/pages/eaeddc/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-f6cd3ec8").then(t)}},{path:"/pages/eaeddc/index.html",redirect:"/pages/eaeddc/"},{path:"/20.DevOps/01.入门/03.Gitlab代码托管服务器安装.html",redirect:"/pages/eaeddc/"},{name:"v-d7bef6c8",path:"/pages/605fde/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-d7bef6c8").then(t)}},{path:"/pages/605fde/index.html",redirect:"/pages/605fde/"},{path:"/20.DevOps/01.入门/04.Jenkins自动化服务器安装.html",redirect:"/pages/605fde/"},{name:"v-4988c303",path:"/pages/6519b5/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4988c303").then(t)}},{path:"/pages/6519b5/index.html",redirect:"/pages/6519b5/"},{path:"/20.DevOps/01.入门/05.Jenkins插件管理.html",redirect:"/pages/6519b5/"},{name:"v-7073a58e",path:"/pages/61d667/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7073a58e").then(t)}},{path:"/pages/61d667/index.html",redirect:"/pages/61d667/"},{path:"/20.DevOps/01.入门/06.Jenkins用户权限管理.html",redirect:"/pages/61d667/"},{name:"v-60dca1e3",path:"/pages/a1fe86/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-60dca1e3").then(t)}},{path:"/pages/a1fe86/index.html",redirect:"/pages/a1fe86/"},{path:"/20.DevOps/01.入门/07.Jenkins凭证管理.html",redirect:"/pages/a1fe86/"},{name:"v-69282982",path:"/pages/c15beb/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-69282982").then(t)}},{path:"/pages/c15beb/index.html",redirect:"/pages/c15beb/"},{path:"/20.DevOps/01.入门/08.Maven安装和配置.html",redirect:"/pages/c15beb/"},{name:"v-fae7c1bc",path:"/pages/9d384b/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-fae7c1bc").then(t)}},{path:"/pages/9d384b/index.html",redirect:"/pages/9d384b/"},{path:"/20.DevOps/01.入门/09.Tomcat安装和配置.html",redirect:"/pages/9d384b/"},{name:"v-7c133ad0",path:"/pages/5448da/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7c133ad0").then(t)}},{path:"/pages/5448da/index.html",redirect:"/pages/5448da/"},{path:"/20.DevOps/02.实战/01.Jenkins构建Maven项目.html",redirect:"/pages/5448da/"},{name:"v-0f2a3d88",path:"/pages/bf228f/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0f2a3d88").then(t)}},{path:"/pages/bf228f/index.html",redirect:"/pages/bf228f/"},{path:"/20.DevOps/02.实战/02.Jenkins常用构建触发器.html",redirect:"/pages/bf228f/"},{name:"v-78c2fe6d",path:"/pages/991f2e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-78c2fe6d").then(t)}},{path:"/pages/991f2e/index.html",redirect:"/pages/991f2e/"},{path:"/20.DevOps/02.实战/03.Jenkins参数化构建.html",redirect:"/pages/991f2e/"},{name:"v-7183b4d5",path:"/pages/bd7581/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-7183b4d5").then(t)}},{path:"/pages/bd7581/index.html",redirect:"/pages/bd7581/"},{path:"/20.DevOps/02.实战/04.Jenkins配置邮箱服务器.html",redirect:"/pages/bd7581/"},{name:"v-8f10297e",path:"/pages/e2748f/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-8f10297e").then(t)}},{path:"/pages/e2748f/index.html",redirect:"/pages/e2748f/"},{path:"/20.DevOps/02.实战/05.SonarQube自动代码审查工具.html",redirect:"/pages/e2748f/"},{name:"v-233d0506",path:"/pages/848aec/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-233d0506").then(t)}},{path:"/pages/848aec/index.html",redirect:"/pages/848aec/"},{path:"/20.DevOps/02.实战/06.Jenkins整合SonarQube实现代码审查.html",redirect:"/pages/848aec/"},{name:"v-64d0cdf1",path:"/pages/8e8067/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-64d0cdf1").then(t)}},{path:"/pages/8e8067/index.html",redirect:"/pages/8e8067/"},{path:"/20.DevOps/02.实战/07.微服务持续集成(上).html",redirect:"/pages/8e8067/"},{name:"v-6dbf4a31",path:"/pages/b7044d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-6dbf4a31").then(t)}},{path:"/pages/b7044d/index.html",redirect:"/pages/b7044d/"},{path:"/20.DevOps/02.实战/08.微服务持续集成(下).html",redirect:"/pages/b7044d/"},{name:"v-f97f4056",path:"/pages/d49c54/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-f97f4056").then(t)}},{path:"/pages/d49c54/index.html",redirect:"/pages/d49c54/"},{path:"/21.Simplify/01.Simplify Docs/01.Lombok使用指南.html",redirect:"/pages/d49c54/"},{name:"v-03f86a9e",path:"/pages/ff62ec/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-03f86a9e").then(t)}},{path:"/pages/ff62ec/index.html",redirect:"/pages/ff62ec/"},{path:"/21.Simplify/01.Simplify Docs/02.Spring Boot整合Swagger.html",redirect:"/pages/ff62ec/"},{name:"v-31abbd86",path:"/pages/92c82f/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-31abbd86").then(t)}},{path:"/pages/92c82f/index.html",redirect:"/pages/92c82f/"},{path:"/21.Simplify/01.Simplify Docs/03.axios中文文档.html",redirect:"/pages/92c82f/"},{name:"v-77f35f9e",path:"/pages/305234/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-77f35f9e").then(t)}},{path:"/pages/305234/index.html",redirect:"/pages/305234/"},{path:"/21.Simplify/01.Simplify Docs/04.vue-axios.html",redirect:"/pages/305234/"},{name:"v-6de74afa",path:"/pages/46a509/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-6de74afa").then(t)}},{path:"/pages/46a509/index.html",redirect:"/pages/46a509/"},{path:"/21.Simplify/01.Simplify Docs/06.自己制作一个java11的docker镜像.html",redirect:"/pages/46a509/"},{name:"v-4c1cfbd2",path:"/pages/59f664/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4c1cfbd2").then(t)}},{path:"/pages/59f664/index.html",redirect:"/pages/59f664/"},{path:"/21.Simplify/01.Simplify Docs/05.优化Docker中的Spring Boot应用.html",redirect:"/pages/59f664/"},{name:"v-a146c5d8",path:"/pages/97d41d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-a146c5d8").then(t)}},{path:"/pages/97d41d/index.html",redirect:"/pages/97d41d/"},{path:"/21.Simplify/01.Simplify Docs/07.Spring Boot 新特性 Layered Jar.html",redirect:"/pages/97d41d/"},{name:"v-9d6dab12",path:"/pages/a60efa/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-9d6dab12").then(t)}},{path:"/pages/a60efa/index.html",redirect:"/pages/a60efa/"},{path:"/21.Simplify/01.Simplify Docs/08.Docker客户端使用TLS保护连接远程Docker服务.html",redirect:"/pages/a60efa/"},{name:"v-1572d344",path:"/pages/779e01/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-1572d344").then(t)}},{path:"/pages/779e01/index.html",redirect:"/pages/779e01/"},{path:"/21.Simplify/01.Simplify Docs/09.docker-maven-plugin.html",redirect:"/pages/779e01/"},{name:"v-613443b0",path:"/pages/9dfd5c/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-613443b0").then(t)}},{path:"/pages/9dfd5c/index.html",redirect:"/pages/9dfd5c/"},{path:"/21.Simplify/01.Simplify Docs/10.Inner注解使用说明.html",redirect:"/pages/9dfd5c/"},{name:"v-2da1a437",path:"/pages/5f71ee/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2da1a437").then(t)}},{path:"/pages/5f71ee/index.html",redirect:"/pages/5f71ee/"},{path:"/21.Simplify/01.Simplify Docs/11.Java8 Stream.html",redirect:"/pages/5f71ee/"},{name:"v-a0a5c11c",path:"/pages/e51b3b/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-a0a5c11c").then(t)}},{path:"/pages/e51b3b/index.html",redirect:"/pages/e51b3b/"},{path:"/22.Kubernetes/01.Kubernetes 入门/01.什么是 Kubernetes.html",redirect:"/pages/e51b3b/"},{name:"v-229229ee",path:"/pages/43a41d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-229229ee").then(t)}},{path:"/pages/43a41d/index.html",redirect:"/pages/43a41d/"},{path:"/22.Kubernetes/01.Kubernetes 入门/02.Kubernetes 安装前的准备.html",redirect:"/pages/43a41d/"},{name:"v-0792908f",path:"/pages/248b3e/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0792908f").then(t)}},{path:"/pages/248b3e/index.html",redirect:"/pages/248b3e/"},{path:"/22.Kubernetes/01.Kubernetes 入门/03.安装 kubeadm.html",redirect:"/pages/248b3e/"},{name:"v-0ba2fba8",path:"/pages/f792df/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0ba2fba8").then(t)}},{path:"/pages/f792df/index.html",redirect:"/pages/f792df/"},{path:"/22.Kubernetes/01.Kubernetes 入门/04.配置 kubeadm.html",redirect:"/pages/f792df/"},{name:"v-89e63956",path:"/pages/91b30f/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-89e63956").then(t)}},{path:"/pages/91b30f/index.html",redirect:"/pages/91b30f/"},{path:"/22.Kubernetes/01.Kubernetes 入门/05.使用 kubeadm 部署 Master 节点.html",redirect:"/pages/91b30f/"},{name:"v-080f0c7c",path:"/pages/1d4070/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-080f0c7c").then(t)}},{path:"/pages/1d4070/index.html",redirect:"/pages/1d4070/"},{path:"/22.Kubernetes/01.Kubernetes 入门/06.安装网络插件.html",redirect:"/pages/1d4070/"},{name:"v-5840b4cb",path:"/pages/b23692/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-5840b4cb").then(t)}},{path:"/pages/b23692/index.html",redirect:"/pages/b23692/"},{path:"/22.Kubernetes/01.Kubernetes 入门/07.使用 kubeadm 配置 Worker 节点.html",redirect:"/pages/b23692/"},{name:"v-4de06ab6",path:"/pages/cfa4e2/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-4de06ab6").then(t)}},{path:"/pages/cfa4e2/index.html",redirect:"/pages/cfa4e2/"},{path:"/22.Kubernetes/01.Kubernetes 入门/08.概念总结.html",redirect:"/pages/cfa4e2/"},{name:"v-eb9fcf56",path:"/pages/11af34/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-eb9fcf56").then(t)}},{path:"/pages/11af34/index.html",redirect:"/pages/11af34/"},{path:"/22.Kubernetes/02.Kubernetes 实践/01.Kubernetes Dashboard.html",redirect:"/pages/11af34/"},{name:"v-183d693e",path:"/pages/f61a3d/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-183d693e").then(t)}},{path:"/pages/f61a3d/index.html",redirect:"/pages/f61a3d/"},{path:"/22.Kubernetes/03.内部平台对接/01.整合云平台相关配置（内部）.html",redirect:"/pages/f61a3d/"},{name:"v-09f5c492",path:"/pages/1b12ed/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-09f5c492").then(t)}},{path:"/pages/1b12ed/index.html",redirect:"/pages/1b12ed/"},{path:"/99.支持/01.支持.html",redirect:"/pages/1b12ed/"},{name:"v-0a72df0a",path:"/blog/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-0a72df0a").then(t)}},{path:"/blog/index.html",redirect:"/blog/"},{path:"/@pages/archivesPage.html",redirect:"/blog/"},{name:"v-363a61d8",path:"/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-363a61d8").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-250b49a5",path:"/pages/7b6be8/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-250b49a5").then(t)}},{path:"/pages/7b6be8/index.html",redirect:"/pages/7b6be8/"},{path:"/04.Ubuntu/02.Ubuntu 使用/05.常见问题与解决方案.html",redirect:"/pages/7b6be8/"},{name:"v-2617300a",path:"/pages/63ed35/",component:ic,beforeEnter:function(n,e,t){Ka("Layout","v-2617300a").then(t)}},{path:"/pages/63ed35/index.html",redirect:"/pages/63ed35/"},{path:"/04.Ubuntu/02.Ubuntu 使用/04.Ubuntu 配置静态ip和动态ip.html",redirect:"/pages/63ed35/"},{path:"*",component:ic}],sc={title:"Java全栈",description:"🚀有道无术，术尚可求，有术无道，止于术",base:"/",headTags:[["link",{rel:"icon",href:"/img/favicon.ico"}],["meta",{name:"keywords",content:"vuepress,theme,blog,vdoing"}],["meta",{name:"theme-color",content:"#11a8cd"}]],pages:[{title:"基础设施即服务",frontmatter:{title:"基础设施即服务",date:"2022-02-09T09:30:38.000Z",permalink:"/pages/250f21/"},regularPath:"/01.%E6%8C%87%E5%8D%97/01.%E5%9F%BA%E7%A1%80/01.%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E5%8D%B3%E6%9C%8D%E5%8A%A1.html",relativePath:"01.指南/01.基础/01.基础设施即服务.md",key:"v-dc140a80",path:"/pages/250f21/",headers:[{level:2,title:"Linux",slug:"linux",normalizedTitle:"linux",charIndex:14},{level:2,title:"CentOS",slug:"centos",normalizedTitle:"centos",charIndex:200},{level:2,title:"Ubuntu",slug:"ubuntu",normalizedTitle:"ubuntu",charIndex:425},{level:2,title:"macOS",slug:"macos",normalizedTitle:"macos",charIndex:673}],headersStr:"Linux CentOS Ubuntu macOS",content:'# 基础设施即服务\n\n\n# Linux\n\nLinux 是一种自由和开放源码的类 UNIX 操作系统，使用 Linux 内核。目前存在着许多不同的 Linux 发行版，可安装在各种各样的电脑硬件设备，从手机、平板电脑、路由器和影音游戏控制台，到桌上型电脑，大型电脑和超级电脑。 Linux 是一个领先的操作系统，世界上运算最快的 10 台超级电脑运行的都是 Linux 操作系统。\n\n开始学习\n\n\n# CentOS\n\nCentOS（Community Enterprise Operating System，中文意思是社区企业操作系统）是Linux发行版之一，它是来自于Red Hat Enterprise Linux依照开放源代码规定释出的源代码所编译而成。由于出自同样的源代码，因此有些要求高度稳定性的服务器以CentOS替代商业版的Red Hat Enterprise Linux使用。两者的不同，在于CentOS完全开源。\n\n开始学习\n\n\n# Ubuntu\n\nUbuntu是一个以桌面应用为主的Linux操作系统，其名称来自非洲南部祖鲁语或豪萨语的“ubuntu"一词，意思是“人性”“我的存在是因为大家的存在"，是非洲传统的一种价值观。Ubuntu基于Debian发行版和Gnome桌面环境，而从11.04版起，Ubuntu发行版放弃了Gnome桌面环境，改为Unity。从前人们认为Linux难以安装、难以使用，在Ubuntu出现后这些都成为了历史。Ubuntu也拥有庞大的社区力量，用户可以方便地从社区获得帮助。\n\n开始学习\n\n\n# macOS\n\n开始学习',normalizedContent:'# 基础设施即服务\n\n\n# linux\n\nlinux 是一种自由和开放源码的类 unix 操作系统，使用 linux 内核。目前存在着许多不同的 linux 发行版，可安装在各种各样的电脑硬件设备，从手机、平板电脑、路由器和影音游戏控制台，到桌上型电脑，大型电脑和超级电脑。 linux 是一个领先的操作系统，世界上运算最快的 10 台超级电脑运行的都是 linux 操作系统。\n\n开始学习\n\n\n# centos\n\ncentos（community enterprise operating system，中文意思是社区企业操作系统）是linux发行版之一，它是来自于red hat enterprise linux依照开放源代码规定释出的源代码所编译而成。由于出自同样的源代码，因此有些要求高度稳定性的服务器以centos替代商业版的red hat enterprise linux使用。两者的不同，在于centos完全开源。\n\n开始学习\n\n\n# ubuntu\n\nubuntu是一个以桌面应用为主的linux操作系统，其名称来自非洲南部祖鲁语或豪萨语的“ubuntu"一词，意思是“人性”“我的存在是因为大家的存在"，是非洲传统的一种价值观。ubuntu基于debian发行版和gnome桌面环境，而从11.04版起，ubuntu发行版放弃了gnome桌面环境，改为unity。从前人们认为linux难以安装、难以使用，在ubuntu出现后这些都成为了历史。ubuntu也拥有庞大的社区力量，用户可以方便地从社区获得帮助。\n\n开始学习\n\n\n# macos\n\n开始学习',charsets:{cjk:!0},lastUpdated:"2022/02/10, 09:38:09",lastUpdatedTimestamp:1644457089e3},{title:"平台即服务",frontmatter:{title:"平台即服务",date:"2022-02-09T09:30:38.000Z",permalink:"/pages/f04654/"},regularPath:"/01.%E6%8C%87%E5%8D%97/01.%E5%9F%BA%E7%A1%80/02.%E5%B9%B3%E5%8F%B0%E5%8D%B3%E6%9C%8D%E5%8A%A1.html",relativePath:"01.指南/01.基础/02.平台即服务.md",key:"v-c6d7e3c0",path:"/pages/f04654/",headers:[{level:2,title:"GitLab",slug:"gitlab",normalizedTitle:"gitlab",charIndex:12},{level:2,title:"Nexus",slug:"nexus",normalizedTitle:"nexus",charIndex:251},{level:2,title:"Registry",slug:"registry",normalizedTitle:"registry",charIndex:309},{level:2,title:"Redis",slug:"redis",normalizedTitle:"redis",charIndex:471},{level:2,title:"Nginx",slug:"nginx",normalizedTitle:"nginx",charIndex:736},{level:2,title:"MongoDB",slug:"mongodb",normalizedTitle:"mongodb",charIndex:1122},{level:2,title:"MinIO",slug:"minio",normalizedTitle:"minio",charIndex:1263},{level:2,title:"RabbitMQ",slug:"rabbitmq",normalizedTitle:"rabbitmq",charIndex:1473}],headersStr:"GitLab Nexus Registry Redis Nginx MongoDB MinIO RabbitMQ",content:"# 平台即服务\n\n\n# GitLab\n\nGitLab 是利用 Ruby on Rails 一个开源的版本管理系统，实现一个自托管的 Git 项目仓库，可通过 Web 界面进行访问公开的或者私人项目。它拥有与 Github 类似的功能，能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，它非常易于浏览提交过的版本并提供一个文件历史库。团队成员可以利用内置的简单聊天程序 (Wall) 进行交流。它还提供一个代码片段收集功能可以轻松实现代码复用，便于日后有需要的时候进行查找。\n\n开始学习\n\n\n# Nexus\n\nNexus 是一个强大的仓库管理器，极大地简化了内部仓库的维护和外部仓库的访问。\n\n开始学习\n\n\n# Registry\n\n官方的 Docker Hub 是一个用于管理公共镜像的地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候我们的服务器无法访问互联网，或者你不希望将自己的镜像放到公网当中，那么你就需要 Docker Registry，它可以用来存储和管理自己的镜像。\n\n开始学习\n\n\n# Redis\n\nRedis（Remote Dictionary Server ) 是一个由Salvatore Sanfilippo写的key-value存储系统。\n\nRedis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。\n\n它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。\n\n开始学习\n\n\n# Nginx\n\nNginx (engine x) 是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP/POP3/SMTP服务。Nginx是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Rambler.ru站点（俄文：Рамблер）开发的，第一个公开版本0.1.0发布于2004年10月4日。\n\n其将源代码以类BSD许可证的形式发布，因它的稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名。2011年6月1日，nginx 1.0.4发布。\n\nNginx是一款轻量级的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器，在BSD-like 协议下发行。其特点是占有内存少，并发能力强，事实上nginx的并发能力在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东、新浪、网易、腾讯、淘宝等。\n\n开始学习\n\n\n# MongoDB\n\nMongoDB 是一个基于分布式文件存储的数据库。由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。\n\nMongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。\n\n开始学习\n\n\n# MinIO\n\nMinIO 是一个基于Apache License v2.0开源协议的对象存储服务。它兼容亚马逊S3云存储服务接口，非 常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而 一个对象文件可以是任意大小，从几kb到最大5T不等。\n\nMinIO是一个非常轻量的服务,可以很简单的和其他应用的结合，类似 NodeJS, Redis 或者 MySQL。\n\n开始学习\n\n\n# RabbitMQ\n\nRabbitMQ是由erlang语言开发，基于AMQP（Advanced Message Queue 高级消息队列协议）协议实现的消息队列，它是一种应用程序之间的通信方法，消息队列在分布式系统开发中应用非常广泛。RabbitMQ官方地址：http://www.rabbitmq.com\n\n开始学习",normalizedContent:"# 平台即服务\n\n\n# gitlab\n\ngitlab 是利用 ruby on rails 一个开源的版本管理系统，实现一个自托管的 git 项目仓库，可通过 web 界面进行访问公开的或者私人项目。它拥有与 github 类似的功能，能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，它非常易于浏览提交过的版本并提供一个文件历史库。团队成员可以利用内置的简单聊天程序 (wall) 进行交流。它还提供一个代码片段收集功能可以轻松实现代码复用，便于日后有需要的时候进行查找。\n\n开始学习\n\n\n# nexus\n\nnexus 是一个强大的仓库管理器，极大地简化了内部仓库的维护和外部仓库的访问。\n\n开始学习\n\n\n# registry\n\n官方的 docker hub 是一个用于管理公共镜像的地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候我们的服务器无法访问互联网，或者你不希望将自己的镜像放到公网当中，那么你就需要 docker registry，它可以用来存储和管理自己的镜像。\n\n开始学习\n\n\n# redis\n\nredis（remote dictionary server ) 是一个由salvatore sanfilippo写的key-value存储系统。\n\nredis是一个开源的使用ansi c语言编写、遵守bsd协议、支持网络、可基于内存亦可持久化的日志型、key-value数据库，并提供多种语言的api。\n\n它通常被称为数据结构服务器，因为值（value）可以是 字符串(string), 哈希(hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。\n\n开始学习\n\n\n# nginx\n\nnginx (engine x) 是一个高性能的http和反向代理web服务器，同时也提供了imap/pop3/smtp服务。nginx是由伊戈尔·赛索耶夫为俄罗斯访问量第二的rambler.ru站点（俄文：рамблер）开发的，第一个公开版本0.1.0发布于2004年10月4日。\n\n其将源代码以类bsd许可证的形式发布，因它的稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名。2011年6月1日，nginx 1.0.4发布。\n\nnginx是一款轻量级的web 服务器/反向代理服务器及电子邮件（imap/pop3）代理服务器，在bsd-like 协议下发行。其特点是占有内存少，并发能力强，事实上nginx的并发能力在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东、新浪、网易、腾讯、淘宝等。\n\n开始学习\n\n\n# mongodb\n\nmongodb 是一个基于分布式文件存储的数据库。由 c++ 语言编写。旨在为 web 应用提供可扩展的高性能数据存储解决方案。\n\nmongodb 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。\n\n开始学习\n\n\n# minio\n\nminio 是一个基于apache license v2.0开源协议的对象存储服务。它兼容亚马逊s3云存储服务接口，非 常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而 一个对象文件可以是任意大小，从几kb到最大5t不等。\n\nminio是一个非常轻量的服务,可以很简单的和其他应用的结合，类似 nodejs, redis 或者 mysql。\n\n开始学习\n\n\n# rabbitmq\n\nrabbitmq是由erlang语言开发，基于amqp（advanced message queue 高级消息队列协议）协议实现的消息队列，它是一种应用程序之间的通信方法，消息队列在分布式系统开发中应用非常广泛。rabbitmq官方地址：http://www.rabbitmq.com\n\n开始学习",charsets:{cyrillic:!0,cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"微服务解决复杂问题",frontmatter:{title:"微服务解决复杂问题",date:"2022-02-09T09:30:38.000Z",permalink:"/pages/58bcba/"},regularPath:"/01.%E6%8C%87%E5%8D%97/01.%E5%9F%BA%E7%A1%80/03.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%A7%A3%E5%86%B3%E5%A4%8D%E6%9D%82%E9%97%AE%E9%A2%98.html",relativePath:"01.指南/01.基础/03.微服务解决复杂问题.md",key:"v-812c033c",path:"/pages/58bcba/",headers:[{level:2,title:"微服务简介",slug:"微服务简介",normalizedTitle:"微服务简介",charIndex:201}],headersStr:"微服务简介",content:"# 微服务解决复杂问题\n\nJava 微服务架构，互联网开发真正需要的技术，也是我们高薪就业的保证，经过本轮学习，我们会掌握从底层 Linux 的安装到最终 DevOps 的所需技能。包括但不限于 Spring Boot、Spring Cloud、Dubbo、Zookeeper、Redis、ELK、RabbitMQ、Ubuntu、Docker、Kubernetes、Jenkins 等全栈技能\n\n\n# 微服务简介\n\n近年来，微服务在应用开发和部署方面取得了显著的进步。将应用开发或者重构成微服务以分离服务，通过 API 以明确的方式来相互“对话” 。例如，每个微服务都是自包含（self-contained），各自维护自己的数据存储（这非常有意义），可以独立更新其他服务。\n\n开始学习",normalizedContent:"# 微服务解决复杂问题\n\njava 微服务架构，互联网开发真正需要的技术，也是我们高薪就业的保证，经过本轮学习，我们会掌握从底层 linux 的安装到最终 devops 的所需技能。包括但不限于 spring boot、spring cloud、dubbo、zookeeper、redis、elk、rabbitmq、ubuntu、docker、kubernetes、jenkins 等全栈技能\n\n\n# 微服务简介\n\n近年来，微服务在应用开发和部署方面取得了显著的进步。将应用开发或者重构成微服务以分离服务，通过 api 以明确的方式来相互“对话” 。例如，每个微服务都是自包含（self-contained），各自维护自己的数据存储（这非常有意义），可以独立更新其他服务。\n\n开始学习",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Spring Cloud",frontmatter:{title:"Spring Cloud",date:"2022-02-09T09:30:38.000Z",permalink:"/pages/b05793/"},regularPath:"/01.%E6%8C%87%E5%8D%97/01.%E5%9F%BA%E7%A1%80/04.Spring%20Cloud.html",relativePath:"01.指南/01.基础/04.Spring Cloud.md",key:"v-30b385c2",path:"/pages/b05793/",headers:[{level:2,title:"Spring Cloud Gateway",slug:"spring-cloud-gateway",normalizedTitle:"spring cloud gateway",charIndex:149}],headersStr:"Spring Cloud Gateway",content:"# Spring Cloud\n\nSpring Cloud 为开发者提供了在分布式系统（如配置管理、服务发现、断路器、智能路由、微代理、控制总线、一次性 Token、全局锁、决策竞选、分布式会话和集群状态）操作的开发工具。使用 Spring Cloud 开发者可以快速实现上述这些模式。\n\n\n\n\n# Spring Cloud Gateway\n\nGateway建立在Spring Framework 5，Project Reactor 和Spring Boot 2 上，使用非阻塞API。支持Websocket，因为它与Spring紧密集成，所以它是一个更好的开发者体验。\n\n开始学习",normalizedContent:"# spring cloud\n\nspring cloud 为开发者提供了在分布式系统（如配置管理、服务发现、断路器、智能路由、微代理、控制总线、一次性 token、全局锁、决策竞选、分布式会话和集群状态）操作的开发工具。使用 spring cloud 开发者可以快速实现上述这些模式。\n\n\n\n\n# spring cloud gateway\n\ngateway建立在spring framework 5，project reactor 和spring boot 2 上，使用非阻塞api。支持websocket，因为它与spring紧密集成，所以它是一个更好的开发者体验。\n\n开始学习",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Spring Cloud Alibaba",frontmatter:{title:"Spring Cloud Alibaba",date:"2022-02-09T09:30:39.000Z",permalink:"/pages/1946f3/"},regularPath:"/01.%E6%8C%87%E5%8D%97/01.%E5%9F%BA%E7%A1%80/05.Spring%20Cloud%20Alibaba.html",relativePath:"01.指南/01.基础/05.Spring Cloud Alibaba.md",key:"v-5a2cd282",path:"/pages/1946f3/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:85}],headersStr:"概述",content:"# Spring Cloud Alibaba\n\n目前市场上主流的 第三套微服务架构解决方案：Spring Boot + Spring Cloud Alibaba\n\n\n# 概述\n\nSpring Cloud Alibaba 致力于提供微服务开发的一站式解决方案。此项目包含开发分布式应用微服务的必需组件，方便开发者通过 Spring Cloud 编程模型轻松使用这些组件来开发分布式应用服务。\n\n依托 Spring Cloud Alibaba，您只需要添加一些注解和少量配置，就可以将 Spring Cloud 应用接入阿里分布式应用解决方案，通过阿里中间件来迅速搭建分布式应用系统。\n\n开始学习",normalizedContent:"# spring cloud alibaba\n\n目前市场上主流的 第三套微服务架构解决方案：spring boot + spring cloud alibaba\n\n\n# 概述\n\nspring cloud alibaba 致力于提供微服务开发的一站式解决方案。此项目包含开发分布式应用微服务的必需组件，方便开发者通过 spring cloud 编程模型轻松使用这些组件来开发分布式应用服务。\n\n依托 spring cloud alibaba，您只需要添加一些注解和少量配置，就可以将 spring cloud 应用接入阿里分布式应用解决方案，通过阿里中间件来迅速搭建分布式应用系统。\n\n开始学习",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:47:02",lastUpdatedTimestamp:1644378422e3},{title:"Docker",frontmatter:{title:"Docker",date:"2022-02-09T09:34:28.000Z",permalink:"/pages/aeaf26/"},regularPath:"/01.%E6%8C%87%E5%8D%97/02.%E9%AB%98%E7%BA%A7/01.Docker.html",relativePath:"01.指南/02.高级/01.Docker.md",key:"v-2f93a0e6",path:"/pages/aeaf26/",headers:[{level:2,title:"Build once，Run anywhere",slug:"build-once-run-anywhere",normalizedTitle:"build once，run anywhere",charIndex:13}],headersStr:"Build once，Run anywhere",content:"# Docker\n\n\n# Build once，Run anywhere\n\nDocker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 AUFS 类的 Union FS 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd。\n\n开始学习",normalizedContent:"# docker\n\n\n# build once，run anywhere\n\ndocker 使用 google 公司推出的 go 语言 进行开发实现，基于 linux 内核的 cgroup，namespace，以及 aufs 类的 union fs 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 lxc，从 0.7 版本以后开始去除 lxc，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runc 和 containerd。\n\n开始学习",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"Kubernetes",frontmatter:{title:"Kubernetes",date:"2022-02-09T09:34:28.000Z",permalink:"/pages/059418/"},regularPath:"/01.%E6%8C%87%E5%8D%97/02.%E9%AB%98%E7%BA%A7/02.Kubernetes.html",relativePath:"01.指南/02.高级/02.Kubernetes.md",key:"v-95827e74",path:"/pages/059418/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:17}],headersStr:"概述",content:"# Kubernetes\n\n\n# 概述\n\nKubernetes 是 Google 2014 年创建管理的，是 Google 10 多年大规模容器管理技术 Borg 的开源版本。是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。其目标是促进完善组件和工具的生态系统，以减轻应用程序在公有云或私有云中运行的负担。\n\n开始学习",normalizedContent:"# kubernetes\n\n\n# 概述\n\nkubernetes 是 google 2014 年创建管理的，是 google 10 多年大规模容器管理技术 borg 的开源版本。是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。其目标是促进完善组件和工具的生态系统，以减轻应用程序在公有云或私有云中运行的负担。\n\n开始学习",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Spring Security OAuth2",frontmatter:{title:"Spring Security OAuth2",date:"2022-02-09T09:35:05.000Z",permalink:"/pages/fbef9a/"},regularPath:"/01.%E6%8C%87%E5%8D%97/03.%E4%B8%93%E9%A2%98/01.Spring%20Security%20OAuth2.html",relativePath:"01.指南/03.专题/01.Spring Security OAuth2.md",key:"v-00e5aa38",path:"/pages/fbef9a/",headers:[{level:2,title:"什么是Spring Security OAuth2",slug:"什么是spring-security-oauth2",normalizedTitle:"什么是spring security oauth2",charIndex:29},{level:2,title:"什么是分布式系统",slug:"什么是分布式系统",normalizedTitle:"什么是分布式系统",charIndex:197}],headersStr:"什么是Spring Security OAuth2 什么是分布式系统",content:"# Spring Security OAuth2\n\n\n# 什么是Spring Security OAuth2\n\nSpring Security OAuth2建立在Spring Security的基础之上，实现了OAuth2的规范，官方原文链接：http://projects.spring.io/spring-security-oauth/docs/oauth2.html\n\n开始学习\n\n\n# 什么是分布式系统\n\n随着软件环境和需求的变化，软件的架构由单体结构演变为分布式架构，具有分布式架构的系统叫分布式系统，分布式系统的运行通常依赖网络，它将单体结构的系统分为若干服务，服务之间通过网络交互来完成用户的业务处理，当前流行的微服务架构就是分布式系统架构。\n\n开始学习",normalizedContent:"# spring security oauth2\n\n\n# 什么是spring security oauth2\n\nspring security oauth2建立在spring security的基础之上，实现了oauth2的规范，官方原文链接：http://projects.spring.io/spring-security-oauth/docs/oauth2.html\n\n开始学习\n\n\n# 什么是分布式系统\n\n随着软件环境和需求的变化，软件的架构由单体结构演变为分布式架构，具有分布式架构的系统叫分布式系统，分布式系统的运行通常依赖网络，它将单体结构的系统分为若干服务，服务之间通过网络交互来完成用户的业务处理，当前流行的微服务架构就是分布式系统架构。\n\n开始学习",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"企业级 DevOps CI/CD实践",frontmatter:{title:"企业级 DevOps CI/CD实践",date:"2022-02-09T09:35:05.000Z",permalink:"/pages/72ccd6/"},regularPath:"/01.%E6%8C%87%E5%8D%97/03.%E4%B8%93%E9%A2%98/02.%E4%BC%81%E4%B8%9A%E7%BA%A7%20DevOps%20%E5%AE%9E%E8%B7%B5.html",relativePath:"01.指南/03.专题/02.企业级 DevOps 实践.md",key:"v-ea2cdb88",path:"/pages/72ccd6/",headers:[{level:2,title:"什么是DevOps",slug:"什么是devops",normalizedTitle:"什么是devops",charIndex:25},{level:2,title:"CI",slug:"ci",normalizedTitle:"ci",charIndex:13},{level:2,title:"CD",slug:"cd",normalizedTitle:"cd",charIndex:16},{level:3,title:"持续交付",slug:"持续交付",normalizedTitle:"持续交付",charIndex:711},{level:3,title:"持续部署",slug:"持续部署",normalizedTitle:"持续部署",charIndex:735},{level:2,title:"CI、CD、DevOps关系",slug:"ci、cd、devops关系",normalizedTitle:"ci、cd、devops关系",charIndex:1328},{level:2,title:"如何落地实现DevOps理念",slug:"如何落地实现devops理念",normalizedTitle:"如何落地实现devops理念",charIndex:1435}],headersStr:"什么是DevOps CI CD 持续交付 持续部署 CI、CD、DevOps关系 如何落地实现DevOps理念",content:"# 企业级 DevOps CI/CD实践\n\n\n# 什么是DevOps\n\nDevOps是Development和Operations的组合，是一种方法论，是一组过程、方法与系统的统称，用于促进应用开发、应用运维和质量保障（QA）部门之间的沟通、协作与整合。以期打破传统开发和运营之间的壁垒和鸿沟。\n\n\n\nDevOps是一种重视“软件开发人员（Dev）”和“IT运维技术人员（Ops）”之间沟通合作的文化、运动或惯例。通过自动化“软件交付”和“架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。具体来说，就是在软件交付和部署过程中提高沟通与协作的效率，旨在更快、更可靠的的发布更高质量的产品。\n\n也就是说DevOps是一组过程和方法的统称，并不指代某一特定的软件工具或软件工具组合。各种工具软件或软件组合可以实现DevOps的概念方法。其本质是一整套的方法论，而不是指某种或某些工具集合，与软件开发中设计到的OOP、AOP、IOC（或DI）等类似，是一种理论或过程或方法的抽象或代称。\n\n\n# CI\n\nCI的英文名称是Continuous Integration，中文翻译为：持续集成。\n\n\n\nCI中，开发人员将会频繁地向主干提交代码，这些新提交的代码在最终合并到主干前，需要经过编译和自动化测试流进行验证。\n\n持续集成（CI）是在源代码变更后自动检测、拉取、构建和（在大多数情况下）进行单元测试的过程。持续集成的目标是快速确保开发人员新提交的变更是好的，并且适合在代码库中进一步使用。CI的流程执行和理论实践让我们可以确定新代码和原有代码能否正确地集成在一起。\n\n\n# CD\n\nCD可对应多个英文名称，持续交付Continuous Delivery和持续部署Continuous Deployment ，一下分别介绍。\n\n查了一些资料，关于持续交互和持续部署的概念比较混乱，以下的概念总结按大部分的资料总结而来。\n\n\n# 持续交付\n\n完成 CI 中构建及单元测试和集成测试的自动化流程后，持续交付可自动将已验证的代码发布到存储库。为了实现高效的持续交付流程，务必要确保 CI 已内置于开发管道。持续交付的目标是拥有一个可随时部署到生产环境的代码库。\n\n\n\n在持续交付中，每个阶段（从代码更改的合并，到生产就绪型构建版本的交付）都涉及测试自动化和代码发布自动化。在流程结束时，运维团队可以快速、轻松地将应用部署到生产环境中或发布给最终使用的用户。\n\n\n# 持续部署\n\n对于一个成熟的CI/CD管道（Pipeline）来说，最后的阶段是持续部署。作为持续交付——自动将生产就绪型构建版本发布到代码存储库——的延伸，持续部署可以自动将应用发布到生产环境。\n\n\n\n持续部署意味着所有的变更都会被自动部署到生产环境中。持续交付意味着所有的变更都可以被部署到生产环境中，但是出于业务考虑，可以选择不部署。如果要实施持续部署，必须先实施持续交付。\n\n持续交付并不是指软件每一个改动都要尽快部署到产品环境中，它指的是任何的代码修改都可以在任何时候实施部署。\n\n持续交付表示的是一种能力，而持续部署表示的则一种方式。持续部署是持续交付的最高阶段\n\n\n# CI、CD、DevOps关系\n\n概念性的内容，每个人的理解都有所不同。就好比CGI 这个词，即可以理解成CGI这种协议，也可以理解成实现了CGI协议的软件工具，都没有问题，咬文嚼字过犹不及。留下一图：\n\n\n\n\n# 如何落地实现DevOps理念\n\nDevOps兴起于2009年,近年来由于云计算、互联网的发展，促进了DevOps的基础设施及工具链的发展,涌现了-大批优秀的工具，这些工具包括开发,测试，运维的各个领域，例如: GitHub、Git/SVN、Docker、Jenkins、HudSon、Ant/Maven/Gradle、QUnit、JMeter等， 看下图:\n\n\n\n开始学习",normalizedContent:"# 企业级 devops ci/cd实践\n\n\n# 什么是devops\n\ndevops是development和operations的组合，是一种方法论，是一组过程、方法与系统的统称，用于促进应用开发、应用运维和质量保障（qa）部门之间的沟通、协作与整合。以期打破传统开发和运营之间的壁垒和鸿沟。\n\n\n\ndevops是一种重视“软件开发人员（dev）”和“it运维技术人员（ops）”之间沟通合作的文化、运动或惯例。通过自动化“软件交付”和“架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。具体来说，就是在软件交付和部署过程中提高沟通与协作的效率，旨在更快、更可靠的的发布更高质量的产品。\n\n也就是说devops是一组过程和方法的统称，并不指代某一特定的软件工具或软件工具组合。各种工具软件或软件组合可以实现devops的概念方法。其本质是一整套的方法论，而不是指某种或某些工具集合，与软件开发中设计到的oop、aop、ioc（或di）等类似，是一种理论或过程或方法的抽象或代称。\n\n\n# ci\n\nci的英文名称是continuous integration，中文翻译为：持续集成。\n\n\n\nci中，开发人员将会频繁地向主干提交代码，这些新提交的代码在最终合并到主干前，需要经过编译和自动化测试流进行验证。\n\n持续集成（ci）是在源代码变更后自动检测、拉取、构建和（在大多数情况下）进行单元测试的过程。持续集成的目标是快速确保开发人员新提交的变更是好的，并且适合在代码库中进一步使用。ci的流程执行和理论实践让我们可以确定新代码和原有代码能否正确地集成在一起。\n\n\n# cd\n\ncd可对应多个英文名称，持续交付continuous delivery和持续部署continuous deployment ，一下分别介绍。\n\n查了一些资料，关于持续交互和持续部署的概念比较混乱，以下的概念总结按大部分的资料总结而来。\n\n\n# 持续交付\n\n完成 ci 中构建及单元测试和集成测试的自动化流程后，持续交付可自动将已验证的代码发布到存储库。为了实现高效的持续交付流程，务必要确保 ci 已内置于开发管道。持续交付的目标是拥有一个可随时部署到生产环境的代码库。\n\n\n\n在持续交付中，每个阶段（从代码更改的合并，到生产就绪型构建版本的交付）都涉及测试自动化和代码发布自动化。在流程结束时，运维团队可以快速、轻松地将应用部署到生产环境中或发布给最终使用的用户。\n\n\n# 持续部署\n\n对于一个成熟的ci/cd管道（pipeline）来说，最后的阶段是持续部署。作为持续交付——自动将生产就绪型构建版本发布到代码存储库——的延伸，持续部署可以自动将应用发布到生产环境。\n\n\n\n持续部署意味着所有的变更都会被自动部署到生产环境中。持续交付意味着所有的变更都可以被部署到生产环境中，但是出于业务考虑，可以选择不部署。如果要实施持续部署，必须先实施持续交付。\n\n持续交付并不是指软件每一个改动都要尽快部署到产品环境中，它指的是任何的代码修改都可以在任何时候实施部署。\n\n持续交付表示的是一种能力，而持续部署表示的则一种方式。持续部署是持续交付的最高阶段\n\n\n# ci、cd、devops关系\n\n概念性的内容，每个人的理解都有所不同。就好比cgi 这个词，即可以理解成cgi这种协议，也可以理解成实现了cgi协议的软件工具，都没有问题，咬文嚼字过犹不及。留下一图：\n\n\n\n\n# 如何落地实现devops理念\n\ndevops兴起于2009年,近年来由于云计算、互联网的发展，促进了devops的基础设施及工具链的发展,涌现了-大批优秀的工具，这些工具包括开发,测试，运维的各个领域，例如: github、git/svn、docker、jenkins、hudson、ant/maven/gradle、qunit、jmeter等， 看下图:\n\n\n\n开始学习",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Linux 简介",frontmatter:{title:"Linux 简介",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/202ee1/"},regularPath:"/02.Linux/01.Linux/01.Linux%20%E7%AE%80%E4%BB%8B.html",relativePath:"02.Linux/01.Linux/01.Linux 简介.md",key:"v-63a62e60",path:"/pages/202ee1/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:15}],headersStr:"概述",content:"# Linux 简介\n\n\n# 概述\n\nLinux 是一种自由和开放源码的类 UNIX 操作系统，使用 Linux 内核。目前存在着许多不同的 Linux 发行版，可安装在各种各样的电脑硬件设备，从手机、平板电脑、路由器和影音游戏控制台，到桌上型电脑，大型电脑和超级电脑。 Linux 是一个领先的操作系统，世界上运算最快的 10 台超级电脑运行的都是 Linux 操作系统。\n\nLinux 操作系统也是自由软件和开放源代码发展中最著名的例子。只要遵循 GNU 通用公共许可证,任何人和机构都可以自由地使用 Linux 的所有底层源代码，也可以自由地修改和再发布。严格来讲，Linux 这个词本身只表示 Linux 内核，但在实际上人们已经习惯了用 Linux 来形容整个基于 Linux 内核，并且使用 GNU 工程各种工具和数据库的操作系统 (也被称为 GNU/ Linux)。通常情况下，Linux 被打包成供桌上型电脑和服务器使用的 Linux 发行版本。一些流行的主流 Linux 发行版本，包括 Debian (及其衍生版本 Ubuntu)，Fedora 和 OpenSUSE 等。Kernel + Softwares + Tools 就是 Linux Distribution\n\n目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debian、Fedora、SuSE、OpenSUSE、TurboLinux、BluePoint、RedFlag、Xterm、SlackWare等。",normalizedContent:"# linux 简介\n\n\n# 概述\n\nlinux 是一种自由和开放源码的类 unix 操作系统，使用 linux 内核。目前存在着许多不同的 linux 发行版，可安装在各种各样的电脑硬件设备，从手机、平板电脑、路由器和影音游戏控制台，到桌上型电脑，大型电脑和超级电脑。 linux 是一个领先的操作系统，世界上运算最快的 10 台超级电脑运行的都是 linux 操作系统。\n\nlinux 操作系统也是自由软件和开放源代码发展中最著名的例子。只要遵循 gnu 通用公共许可证,任何人和机构都可以自由地使用 linux 的所有底层源代码，也可以自由地修改和再发布。严格来讲，linux 这个词本身只表示 linux 内核，但在实际上人们已经习惯了用 linux 来形容整个基于 linux 内核，并且使用 gnu 工程各种工具和数据库的操作系统 (也被称为 gnu/ linux)。通常情况下，linux 被打包成供桌上型电脑和服务器使用的 linux 发行版本。一些流行的主流 linux 发行版本，包括 debian (及其衍生版本 ubuntu)，fedora 和 opensuse 等。kernel + softwares + tools 就是 linux distribution\n\n目前市面上较知名的发行版有：ubuntu、redhat、centos、debian、fedora、suse、opensuse、turbolinux、bluepoint、redflag、xterm、slackware等。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"Linux 与 Windows 比较",frontmatter:{title:"Linux 与 Windows 比较",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/715f2d/"},regularPath:"/02.Linux/01.Linux/02.Linux%20%E4%B8%8E%20Windows%20%E6%AF%94%E8%BE%83.html",relativePath:"02.Linux/01.Linux/02.Linux 与 Windows 比较.md",key:"v-bded170c",path:"/pages/715f2d/",headersStr:null,content:"# Linux 与 Windows 比较\n\n目前国内 Linux 更多的是应用于服务器上，而桌面操作系统更多使用的是 Windows。主要区别如下\n\n比较     WINDOWS                                                                                                     LINUX\n界面     界面统一，外壳程序固定所有 Windows 程序菜单几乎一致，快捷键也几乎相同                                                                     图形界面风格依发布版不同而不同，可能互不兼容。GNU/Linux 的终端机是从 UNIX\n                                                                                                                   传承下来，基本命令和操作方法也几乎一致。\n驱动程序   驱动程序丰富，版本更新频繁。默认安装程序里面一般包含有该版本发布时流行的硬件驱动程序，之后所出的新硬件驱动依赖于硬件厂商提供。对于一些老硬件，如果没有了原配的驱动有时很难支持。另外，有时硬件厂商未提供所需版本的   由志愿者开发，由Linux核心开发小组发布，很多硬件厂商基于版权考虑并未提供驱动程序，尽管多数无需手动安装，但是涉及安装则相对复杂，使得新用户面对驱动程序问题（是否存在和安装方法）会一筹莫展。但是在开源开发模式下，许多老硬件尽管在\n       Windows 下的驱动，也会比较头痛。                                                                                        Windows 下很难支持的也容易找到驱动。HP、Intel、AMD\n                                                                                                                   等硬件厂商逐步不同程度支持开源驱动，问题正在得到缓解。\n使用     使用比较简单，容易入门。图形化界面对没有计算机背景知识的用户使用十分有利。                                                                       图形界面使用简单，容易入门。文字界面，需要学习才能掌握。\n学习     系统构造复杂、变化频繁，且知识、技能淘汰快，深入学习困难。                                                                               系统构造简单、稳定，且知识、技能传承性好，深入学习相对容易。\n软件     每一种特定功能可能都需要商业软件的支持，需要购买相应的授权。                                                                              大部分软件都可以自由获取，同样功能的软件选择较少。",normalizedContent:"# linux 与 windows 比较\n\n目前国内 linux 更多的是应用于服务器上，而桌面操作系统更多使用的是 windows。主要区别如下\n\n比较     windows                                                                                                     linux\n界面     界面统一，外壳程序固定所有 windows 程序菜单几乎一致，快捷键也几乎相同                                                                     图形界面风格依发布版不同而不同，可能互不兼容。gnu/linux 的终端机是从 unix\n                                                                                                                   传承下来，基本命令和操作方法也几乎一致。\n驱动程序   驱动程序丰富，版本更新频繁。默认安装程序里面一般包含有该版本发布时流行的硬件驱动程序，之后所出的新硬件驱动依赖于硬件厂商提供。对于一些老硬件，如果没有了原配的驱动有时很难支持。另外，有时硬件厂商未提供所需版本的   由志愿者开发，由linux核心开发小组发布，很多硬件厂商基于版权考虑并未提供驱动程序，尽管多数无需手动安装，但是涉及安装则相对复杂，使得新用户面对驱动程序问题（是否存在和安装方法）会一筹莫展。但是在开源开发模式下，许多老硬件尽管在\n       windows 下的驱动，也会比较头痛。                                                                                        windows 下很难支持的也容易找到驱动。hp、intel、amd\n                                                                                                                   等硬件厂商逐步不同程度支持开源驱动，问题正在得到缓解。\n使用     使用比较简单，容易入门。图形化界面对没有计算机背景知识的用户使用十分有利。                                                                       图形界面使用简单，容易入门。文字界面，需要学习才能掌握。\n学习     系统构造复杂、变化频繁，且知识、技能淘汰快，深入学习困难。                                                                               系统构造简单、稳定，且知识、技能传承性好，深入学习相对容易。\n软件     每一种特定功能可能都需要商业软件的支持，需要购买相应的授权。                                                                              大部分软件都可以自由获取，同样功能的软件选择较少。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"Linux 远程控制管理",frontmatter:{title:"Linux 远程控制管理",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/56abb4/"},regularPath:"/02.Linux/01.Linux/03.Linux%20%E8%BF%9C%E7%A8%8B%E6%8E%A7%E5%88%B6%E7%AE%A1%E7%90%86.html",relativePath:"02.Linux/01.Linux/03.Linux 远程控制管理.md",key:"v-0e25e56a",path:"/pages/56abb4/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:19},{level:2,title:"OpenSSH",slug:"openssh",normalizedTitle:"openssh",charIndex:159},{level:3,title:"检查软件是否安装",slug:"检查软件是否安装",normalizedTitle:"检查软件是否安装",charIndex:371},{level:3,title:"安装服务端",slug:"安装服务端",normalizedTitle:"安装服务端",charIndex:433},{level:3,title:"安装客户端",slug:"安装客户端",normalizedTitle:"安装客户端",charIndex:476},{level:2,title:"SecureCRT",slug:"securecrt",normalizedTitle:"securecrt",charIndex:578},{level:3,title:"特点",slug:"特点",normalizedTitle:"特点",charIndex:783},{level:3,title:"快捷键",slug:"快捷键",normalizedTitle:"快捷键",charIndex:1417},{level:3,title:"项目中应用",slug:"项目中应用",normalizedTitle:"项目中应用",charIndex:2038},{level:3,title:"软件效果图",slug:"软件效果图",normalizedTitle:"软件效果图",charIndex:2132}],headersStr:"概述 OpenSSH 检查软件是否安装 安装服务端 安装客户端 SecureCRT 特点 快捷键 项目中应用 软件效果图",content:"# Linux 远程控制管理\n\n\n# 概述\n\n传统的网络服务程序，FTP、POP、telnet 本质上都是不安全的，因为它们在网络上通过明文传送口令和数据，这些数据非常容易被截获。SSH 叫做 Secure Shell。通过 SSH，可以把传输数据进行加密，预防攻击，传输的数据进行了压缩，可以加快传输速度。\n\n\n# OpenSSH\n\nSSH 是芬兰一家公司开发。但是受到版权和加密算法限制，现在很多人都使用 OpenSSH。OpenSSH 是 SSH 的替代软件，免费。\n\nOpenSSH 由客户端和服务端组成。\n\n * 基于口令的安全验证：知道服务器的帐号密码即可远程登录，口令和数据在传输过程中都会被加密。\n * 基于密钥的安全验证：此时需要在创建一对密钥，把公有密钥放到远程服务器上自己的宿主目录中，而私有密钥则由自己保存。\n\n\n# 检查软件是否安装\n\napt-cache policy openssh-client openssh-server\n\n\n\n# 安装服务端\n\napt-get install openssh-server\n\n\n\n# 安装客户端\n\napt-get install openssh-client\n\n\nOpenSSH 服务器的主要配置文件为 /etc/ssh/sshd_config，几乎所有的配置信息都在此文件中。\n\n\n# SecureCRT\n\nSecureCRT是一款支持SSH（SSH1和SSH2）的终端仿真程序，简单地说是Windows下登录UNIX或Linux服务器主机的软件。\n\nSecureCRT支持SSH，同时支持Telnet和rlogin协议。SecureCRT是一款用于连接运行包括Windows、UNIX和VMS的理想工具。通过使用内含的VCP命令行程序可以进行加密文件的传输。有流行CRTTelnet客户机的所有特点,包括:自动注册、对不同主机保持不同的特性、打印功能、颜色设置、可变屏幕尺寸、用户定义的键位图和优良的VT100,VT102,VT220和ANSI竞争.能从命令行中运行或从浏览器中运行.其它特点包括文本手稿、易于使用的工具条、用户的键位图编辑器、可定制的ANSI颜色等.SecureCRT的SSH协议支持DES,3DES和RC4密码和密码与RSA鉴别。\n\n\n# 特点\n\n广泛的终端仿真：\n\nVT100，VT102，VT220，ANSI，SCO ANSI，Xterm，Wyse 50/60和 Linux console 仿真（带有 ANSI 颜色）。\n\n优秀的会话管理特性：\n\n新的带标签的用户界面和 Activator 托盘工具，最小化桌面的杂乱。\n\n会话设置可以保存在命名的会话中。\n\n协议支持:\n\n支持 SSH1，SSH2，Telnet，RLogin，Serial，和 TAPI 协议。\n\nSecure Shell：\n\nSecure Shell 加密登录和会话数据，包括以下支持：\n\n- 端口转发使 TCP/IP 数据更安全\n\n- 口令，公钥，键盘交互和 Kerberos 验证\n\n- AES，Twofish，Blowfish，3DES，RC4，和 DES 加密\n\n- X11 转发\n\n文件传输工具：\n\nVCP 和 VSFTP 命令行公用程序让使用 SFTP 的文件传输更安全。\n\n脚本支持：\n\n支持 VBScript 和 JScript 脚本语言。\n\n\n# 快捷键\n\n命令                     说明\nAlt + Enter            全屏\nAlt + B                打开新的连接\nAlt + 1/2/3/4/5.../9   切换到第1/2/3/4/5.../9个标签\nCtrl + A               光标移至行首\nCtrl + B               光标前移1个字符\nCtrl + D               删除光标后1个字符\nCtrl + E               光标移至行末\nCtrl + F               标后移1个字符\nCtrl + H               删除光标前的1个字符\nCtrl + J               回车\nCtrl + k               删除当前光标至行末的字符\nCtrl + P               显示前一条命令\nCtrl + M               回车\nCtrl + N               下一条命令\nCtrl + T               交换光标前最后两个字符（思科路由器可用）\nCtrl + U               清除当前行和缓存的输入\nCtrl + V               输入ctrl字符\nCtrl + W               删除当前光标至行首的字符\n\n\n# 项目中应用\n\n在SecureCRT中配置本地端口转发，涉及到本机、跳板机、目标服务器，因为本机与目标服务器不能直接ping通，所以需要配置端口转发，将本机的请求转发到目标服务器。\n\n\n# 软件效果图\n\n",normalizedContent:"# linux 远程控制管理\n\n\n# 概述\n\n传统的网络服务程序，ftp、pop、telnet 本质上都是不安全的，因为它们在网络上通过明文传送口令和数据，这些数据非常容易被截获。ssh 叫做 secure shell。通过 ssh，可以把传输数据进行加密，预防攻击，传输的数据进行了压缩，可以加快传输速度。\n\n\n# openssh\n\nssh 是芬兰一家公司开发。但是受到版权和加密算法限制，现在很多人都使用 openssh。openssh 是 ssh 的替代软件，免费。\n\nopenssh 由客户端和服务端组成。\n\n * 基于口令的安全验证：知道服务器的帐号密码即可远程登录，口令和数据在传输过程中都会被加密。\n * 基于密钥的安全验证：此时需要在创建一对密钥，把公有密钥放到远程服务器上自己的宿主目录中，而私有密钥则由自己保存。\n\n\n# 检查软件是否安装\n\napt-cache policy openssh-client openssh-server\n\n\n\n# 安装服务端\n\napt-get install openssh-server\n\n\n\n# 安装客户端\n\napt-get install openssh-client\n\n\nopenssh 服务器的主要配置文件为 /etc/ssh/sshd_config，几乎所有的配置信息都在此文件中。\n\n\n# securecrt\n\nsecurecrt是一款支持ssh（ssh1和ssh2）的终端仿真程序，简单地说是windows下登录unix或linux服务器主机的软件。\n\nsecurecrt支持ssh，同时支持telnet和rlogin协议。securecrt是一款用于连接运行包括windows、unix和vms的理想工具。通过使用内含的vcp命令行程序可以进行加密文件的传输。有流行crttelnet客户机的所有特点,包括:自动注册、对不同主机保持不同的特性、打印功能、颜色设置、可变屏幕尺寸、用户定义的键位图和优良的vt100,vt102,vt220和ansi竞争.能从命令行中运行或从浏览器中运行.其它特点包括文本手稿、易于使用的工具条、用户的键位图编辑器、可定制的ansi颜色等.securecrt的ssh协议支持des,3des和rc4密码和密码与rsa鉴别。\n\n\n# 特点\n\n广泛的终端仿真：\n\nvt100，vt102，vt220，ansi，sco ansi，xterm，wyse 50/60和 linux console 仿真（带有 ansi 颜色）。\n\n优秀的会话管理特性：\n\n新的带标签的用户界面和 activator 托盘工具，最小化桌面的杂乱。\n\n会话设置可以保存在命名的会话中。\n\n协议支持:\n\n支持 ssh1，ssh2，telnet，rlogin，serial，和 tapi 协议。\n\nsecure shell：\n\nsecure shell 加密登录和会话数据，包括以下支持：\n\n- 端口转发使 tcp/ip 数据更安全\n\n- 口令，公钥，键盘交互和 kerberos 验证\n\n- aes，twofish，blowfish，3des，rc4，和 des 加密\n\n- x11 转发\n\n文件传输工具：\n\nvcp 和 vsftp 命令行公用程序让使用 sftp 的文件传输更安全。\n\n脚本支持：\n\n支持 vbscript 和 jscript 脚本语言。\n\n\n# 快捷键\n\n命令                     说明\nalt + enter            全屏\nalt + b                打开新的连接\nalt + 1/2/3/4/5.../9   切换到第1/2/3/4/5.../9个标签\nctrl + a               光标移至行首\nctrl + b               光标前移1个字符\nctrl + d               删除光标后1个字符\nctrl + e               光标移至行末\nctrl + f               标后移1个字符\nctrl + h               删除光标前的1个字符\nctrl + j               回车\nctrl + k               删除当前光标至行末的字符\nctrl + p               显示前一条命令\nctrl + m               回车\nctrl + n               下一条命令\nctrl + t               交换光标前最后两个字符（思科路由器可用）\nctrl + u               清除当前行和缓存的输入\nctrl + v               输入ctrl字符\nctrl + w               删除当前光标至行首的字符\n\n\n# 项目中应用\n\n在securecrt中配置本地端口转发，涉及到本机、跳板机、目标服务器，因为本机与目标服务器不能直接ping通，所以需要配置端口转发，将本机的请求转发到目标服务器。\n\n\n# 软件效果图\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Linux 的目录结构",frontmatter:{title:"Linux 的目录结构",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/1e9034/"},regularPath:"/02.Linux/01.Linux/04.Linux%20%E7%9A%84%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84.html",relativePath:"02.Linux/01.Linux/04.Linux 的目录结构.md",key:"v-68dd6adb",path:"/pages/1e9034/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:18}],headersStr:"概述",content:"# Linux 的目录结构\n\n\n# 概述\n\n\n\n目录     说明\nbin    存放二进制可执行文件(ls,cat,mkdir等)\nboot   存放用于系统引导时使用的各种文件\ndev    用于存放设备文件\netc    存放系统配置文件\nhome   存放所有用户文件的根目录\nlib    存放跟文件系统中的程序运行所需要的共享库及内核模块\nmnt    系统管理员安装临时文件系统的安装点\nopt    额外安装的可选应用程序包所放置的位置\nproc   虚拟文件系统，存放当前内存的映射\nroot   超级用户目录\nsbin   存放二进制可执行文件，只有root才能访问\ntmp    用于存放各种临时文件\nusr    用于存放系统应用程序，比较重要的目录/usr/local 本地管理员软件安装目录\nvar    用于存放运行时需要改变数据的文件",normalizedContent:"# linux 的目录结构\n\n\n# 概述\n\n\n\n目录     说明\nbin    存放二进制可执行文件(ls,cat,mkdir等)\nboot   存放用于系统引导时使用的各种文件\ndev    用于存放设备文件\netc    存放系统配置文件\nhome   存放所有用户文件的根目录\nlib    存放跟文件系统中的程序运行所需要的共享库及内核模块\nmnt    系统管理员安装临时文件系统的安装点\nopt    额外安装的可选应用程序包所放置的位置\nproc   虚拟文件系统，存放当前内存的映射\nroot   超级用户目录\nsbin   存放二进制可执行文件，只有root才能访问\ntmp    用于存放各种临时文件\nusr    用于存放系统应用程序，比较重要的目录/usr/local 本地管理员软件安装目录\nvar    用于存放运行时需要改变数据的文件",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Linux 操作文件目录",frontmatter:{title:"Linux 操作文件目录",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/8ad474/"},regularPath:"/02.Linux/01.Linux/05.Linux%20%E6%93%8D%E4%BD%9C%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95.html",relativePath:"02.Linux/01.Linux/05.Linux 操作文件目录.md",key:"v-1893221c",path:"/pages/8ad474/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:19}],headersStr:"概述",content:"# Linux 操作文件目录\n\n\n# 概述\n\n命令      说明                  语法                                                参数      参数说明\nls      显示文件和目录列表           ls [-alrtAFR] [name...]                                   \n                                                                              -l      列出文件的详细信息\n                                                                              -a      列出当前目录所有文件，包含隐藏文件\nmkdir   创建目录                mkdir [-p] dirName                                        \n                                                                              -p      父目录不存在情况下先生成父目录\ncd      切换目录                cd [dirName]                                              \ntouch   生成一个空文件                                                                       \necho    生成一个带内容文件           echo abcd > 1.txt，echo 1234 >> 1.txt                      \ncat     显示文本文件内容            cat [-AbeEnstTuv] [--help] [--version] fileName           \ncp      复制文件或目录             cp [options] source dest                                  \nrm      删除文件                rm [options] name...                                      \n                                                                              -f      强制删除文件或目录\n                                                                              -r      同时删除该目录下的所有文件\nmv      移动文件或目录             mv [options] source dest                                  \nfind    在文件系统中查找指定的文件                                                                 \n                                                                              -name   文件名\ngrep    在指定的文本文件中查找指定的字符串                                                             \ntree    用于以树状图列出目录的内容                                                                 \npwd     显示当前工作目录                                                                      \nln      建立软链接                                                                         \nmore    分页显示文本文件内容                                                                    \nhead    显示文件开头内容                                                                      \ntail    显示文件结尾内容                                                                      \n                                                                              -f      跟踪输出",normalizedContent:"# linux 操作文件目录\n\n\n# 概述\n\n命令      说明                  语法                                                参数      参数说明\nls      显示文件和目录列表           ls [-alrtafr] [name...]                                   \n                                                                              -l      列出文件的详细信息\n                                                                              -a      列出当前目录所有文件，包含隐藏文件\nmkdir   创建目录                mkdir [-p] dirname                                        \n                                                                              -p      父目录不存在情况下先生成父目录\ncd      切换目录                cd [dirname]                                              \ntouch   生成一个空文件                                                                       \necho    生成一个带内容文件           echo abcd > 1.txt，echo 1234 >> 1.txt                      \ncat     显示文本文件内容            cat [-abeensttuv] [--help] [--version] filename           \ncp      复制文件或目录             cp [options] source dest                                  \nrm      删除文件                rm [options] name...                                      \n                                                                              -f      强制删除文件或目录\n                                                                              -r      同时删除该目录下的所有文件\nmv      移动文件或目录             mv [options] source dest                                  \nfind    在文件系统中查找指定的文件                                                                 \n                                                                              -name   文件名\ngrep    在指定的文本文件中查找指定的字符串                                                             \ntree    用于以树状图列出目录的内容                                                                 \npwd     显示当前工作目录                                                                      \nln      建立软链接                                                                         \nmore    分页显示文本文件内容                                                                    \nhead    显示文件开头内容                                                                      \ntail    显示文件结尾内容                                                                      \n                                                                              -f      跟踪输出",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"Linux 系统管理命令",frontmatter:{title:"Linux 系统管理命令",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/6f8d47/"},regularPath:"/02.Linux/01.Linux/06.Linux%20%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4.html",relativePath:"02.Linux/01.Linux/06.Linux 系统管理命令.md",key:"v-1094a947",path:"/pages/6f8d47/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:19}],headersStr:"概述",content:"# Linux 系统管理命令\n\n\n# 概述\n\n命令         说明\nstat       显示指定文件的相关信息,比ls命令显示内容更多\nwho        显示在线登录用户\nhostname   显示主机名称\nuname      显示系统信息\ntop        显示当前系统中耗费资源最多的进程\nps         显示瞬间的进程状态\ndu         显示指定的文件（目录）已使用的磁盘空间的总量\ndf         显示文件系统磁盘空间的使用情况\nfree       显示当前内存和交换空间的使用情况\nifconfig   显示网络接口信息\nping       测试网络的连通性\nnetstat    显示网络状态信息\nclear      清屏\nkill       杀死一个进程",normalizedContent:"# linux 系统管理命令\n\n\n# 概述\n\n命令         说明\nstat       显示指定文件的相关信息,比ls命令显示内容更多\nwho        显示在线登录用户\nhostname   显示主机名称\nuname      显示系统信息\ntop        显示当前系统中耗费资源最多的进程\nps         显示瞬间的进程状态\ndu         显示指定的文件（目录）已使用的磁盘空间的总量\ndf         显示文件系统磁盘空间的使用情况\nfree       显示当前内存和交换空间的使用情况\nifconfig   显示网络接口信息\nping       测试网络的连通性\nnetstat    显示网络状态信息\nclear      清屏\nkill       杀死一个进程",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"Linux 开关机命令",frontmatter:{title:"Linux 开关机命令",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/ef1cfd/"},regularPath:"/02.Linux/01.Linux/07.Linux%20%E5%BC%80%E5%85%B3%E6%9C%BA%E5%91%BD%E4%BB%A4.html",relativePath:"02.Linux/01.Linux/07.Linux 开关机命令.md",key:"v-9690772c",path:"/pages/ef1cfd/",headers:[{level:2,title:"重启",slug:"重启",normalizedTitle:"重启",charIndex:505},{level:2,title:"关机",slug:"关机",normalizedTitle:"关机",charIndex:9}],headersStr:"重启 关机",content:"# Linux 开关机命令\n\nshutdown 命令可以用来进行关机程序，并且在关机以前传送讯息给所有使用者正在执行的程序，shutdown 也可以用来重开机。\n\n命令         语法                                                参数           参数说明\nshutdown   shutdown [-t seconds] [-rkhncfF] time [message]                \n                                                             -t seconds   设定在几秒钟之后进行关机程序\n                                                             -k           并不会真的关机，只是将警告讯息传送给所有只用者\n                                                             -r           关机后重新开机（重启）\n                                                             -h           关机后停机\n                                                             -n           不采用正常程序来关机，用强迫的方式杀掉所有执行中的程序后自行关机\n                                                             -c           取消目前已经进行中的关机动作\n                                                             -f           关机时，不做 fcsk 动作(检查 Linux 档系统)\n                                                             -F           关机时，强迫进行 fsck 动作\n                                                             time         设定关机的时间\n                                                             message      传送给所有使用者的警告讯息\n\n\n# 重启\n\n * reboot\n * shutdown -r now\n\n\n# 关机\n\n * shutdown -h now",normalizedContent:"# linux 开关机命令\n\nshutdown 命令可以用来进行关机程序，并且在关机以前传送讯息给所有使用者正在执行的程序，shutdown 也可以用来重开机。\n\n命令         语法                                                参数           参数说明\nshutdown   shutdown [-t seconds] [-rkhncff] time [message]                \n                                                             -t seconds   设定在几秒钟之后进行关机程序\n                                                             -k           并不会真的关机，只是将警告讯息传送给所有只用者\n                                                             -r           关机后重新开机（重启）\n                                                             -h           关机后停机\n                                                             -n           不采用正常程序来关机，用强迫的方式杀掉所有执行中的程序后自行关机\n                                                             -c           取消目前已经进行中的关机动作\n                                                             -f           关机时，不做 fcsk 动作(检查 linux 档系统)\n                                                             -f           关机时，强迫进行 fsck 动作\n                                                             time         设定关机的时间\n                                                             message      传送给所有使用者的警告讯息\n\n\n# 重启\n\n * reboot\n * shutdown -r now\n\n\n# 关机\n\n * shutdown -h now",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"Linux 压缩命令",frontmatter:{title:"Linux 压缩命令",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/657e76/"},regularPath:"/02.Linux/01.Linux/08.Linux%20%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4.html",relativePath:"02.Linux/01.Linux/08.Linux 压缩命令.md",key:"v-2ded6bf9",path:"/pages/657e76/",headers:[{level:2,title:"tar",slug:"tar",normalizedTitle:"tar",charIndex:17},{level:2,title:"gzip",slug:"gzip",normalizedTitle:"gzip",charIndex:282},{level:2,title:"bzip2",slug:"bzip2",normalizedTitle:"bzip2",charIndex:340}],headersStr:"tar gzip bzip2",content:"# Linux 压缩命令\n\n\n# tar\n\n命令    语法                              参数    参数说明\ntar   tar [-cxzjvf] 压缩打包文档的名称 欲打包目录         \n                                      -c    建立一个归档文件的参数指令\n                                      -x    解开一个归档文件的参数指令\n                                      -z    是否需要用 gzip 压缩\n                                      -j    是否需要用 bzip2 压缩\n                                      -v    压缩的过程中显示文件\n                                      -f    使用档名，在 f 之后要立即接档名\n                                      -tf   查看归档文件里面的文件\n\n例子：\n\n * 压缩文件夹：tar -zcvf test.tar.gz test\\\n * 解压文件夹：tar -zxvf test.tar.gz\n\n\n# gzip\n\n命令     语法                      参数     参数说明\ngzip   gzip [选项] 压缩（解压缩）的文件名          \n                               -d     解压缩\n                               -l     对每个压缩文件，显示压缩文件的大小，未压缩文件的大小，压缩比，未压缩文件的名字\n                               -v     对每一个压缩和解压的文件，显示文件名和压缩比\n                               -num   用指定的数字num调整压缩的速度，-1或--fast表示最快压缩方法（低压缩比），-9或--best表示最慢压缩方法（高压缩比）。系统缺省值为6\n\n说明：压缩文件后缀为 gz\n\n\n# bzip2\n\n命令      语法             参数     参数说明\nbzip2   bzip2 [-cdz]          \n                       -d     解压缩\n                       -z     压缩参数\n                       -num   用指定的数字num调整压缩的速度，-1或--fast表示最快压缩方法（低压缩比），-9或--best表示最慢压缩方法（高压缩比）。系统缺省值为6\n\n说明：压缩文件后缀为 bz2",normalizedContent:"# linux 压缩命令\n\n\n# tar\n\n命令    语法                              参数    参数说明\ntar   tar [-cxzjvf] 压缩打包文档的名称 欲打包目录         \n                                      -c    建立一个归档文件的参数指令\n                                      -x    解开一个归档文件的参数指令\n                                      -z    是否需要用 gzip 压缩\n                                      -j    是否需要用 bzip2 压缩\n                                      -v    压缩的过程中显示文件\n                                      -f    使用档名，在 f 之后要立即接档名\n                                      -tf   查看归档文件里面的文件\n\n例子：\n\n * 压缩文件夹：tar -zcvf test.tar.gz test\\\n * 解压文件夹：tar -zxvf test.tar.gz\n\n\n# gzip\n\n命令     语法                      参数     参数说明\ngzip   gzip [选项] 压缩（解压缩）的文件名          \n                               -d     解压缩\n                               -l     对每个压缩文件，显示压缩文件的大小，未压缩文件的大小，压缩比，未压缩文件的名字\n                               -v     对每一个压缩和解压的文件，显示文件名和压缩比\n                               -num   用指定的数字num调整压缩的速度，-1或--fast表示最快压缩方法（低压缩比），-9或--best表示最慢压缩方法（高压缩比）。系统缺省值为6\n\n说明：压缩文件后缀为 gz\n\n\n# bzip2\n\n命令      语法             参数     参数说明\nbzip2   bzip2 [-cdz]          \n                       -d     解压缩\n                       -z     压缩参数\n                       -num   用指定的数字num调整压缩的速度，-1或--fast表示最快压缩方法（低压缩比），-9或--best表示最慢压缩方法（高压缩比）。系统缺省值为6\n\n说明：压缩文件后缀为 bz2",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"Linux 编辑器",frontmatter:{title:"Linux 编辑器",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/287d9f/"},regularPath:"/02.Linux/01.Linux/09.Linux%20%E7%BC%96%E8%BE%91%E5%99%A8.html",relativePath:"02.Linux/01.Linux/09.Linux 编辑器.md",key:"v-1f7d690b",path:"/pages/287d9f/",headers:[{level:2,title:"vim",slug:"vim",normalizedTitle:"vim",charIndex:16},{level:3,title:"运行模式",slug:"运行模式",normalizedTitle:"运行模式",charIndex:24},{level:3,title:"命令",slug:"命令",normalizedTitle:"命令",charIndex:39},{level:2,title:"nano",slug:"nano",normalizedTitle:"nano",charIndex:225},{level:3,title:"命令",slug:"命令-2",normalizedTitle:"命令",charIndex:39}],headersStr:"vim 运行模式 命令 nano 命令",content:"# Linux 编辑器\n\n\n# vim\n\n\n# 运行模式\n\n编辑模式：等待编辑命令输入\n\n插入模式：编辑模式下，输入 i 进入插入模式，插入文本信息\n\n命令模式：在编辑模式下，输入 : 进行命令模式\n\n\n# 命令\n\n:q 直接退出vi\n\n:wq 保存后退出vi ，并可以新建文件\n\n:q! 强制退出\n\n:w file 将当前内容保存成某个文件\n\n:set number 在编辑文件显示行号\n\n:set nonumber 在编辑文件不显示行号\n\n\n# nano\n\nnano 是一个字符终端的文本编辑器，有点像 DOS 下的 editor 程序。它比 vi/vim 要简单得多，比较适合 Linux 初学者使用。某些 Linux 发行版的默认编辑器就是 nano。\n\n\n# 命令\n\n * 保存：ctrl + o\n * 搜索：ctrl + w\n * 上一页：ctrl + y\n * 下一页：ctrl + v\n * 退出：ctrl + x",normalizedContent:"# linux 编辑器\n\n\n# vim\n\n\n# 运行模式\n\n编辑模式：等待编辑命令输入\n\n插入模式：编辑模式下，输入 i 进入插入模式，插入文本信息\n\n命令模式：在编辑模式下，输入 : 进行命令模式\n\n\n# 命令\n\n:q 直接退出vi\n\n:wq 保存后退出vi ，并可以新建文件\n\n:q! 强制退出\n\n:w file 将当前内容保存成某个文件\n\n:set number 在编辑文件显示行号\n\n:set nonumber 在编辑文件不显示行号\n\n\n# nano\n\nnano 是一个字符终端的文本编辑器，有点像 dos 下的 editor 程序。它比 vi/vim 要简单得多，比较适合 linux 初学者使用。某些 linux 发行版的默认编辑器就是 nano。\n\n\n# 命令\n\n * 保存：ctrl + o\n * 搜索：ctrl + w\n * 上一页：ctrl + y\n * 下一页：ctrl + v\n * 退出：ctrl + x",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"Linux 软件包管理",frontmatter:{title:"Linux 软件包管理",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/0d3526/"},regularPath:"/02.Linux/01.Linux/10.Linux%20%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%AE%A1%E7%90%86.html",relativePath:"02.Linux/01.Linux/10.Linux 软件包管理.md",key:"v-56be3166",path:"/pages/0d3526/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:18},{level:2,title:"修改数据源",slug:"修改数据源",normalizedTitle:"修改数据源",charIndex:184},{level:3,title:"查看系统版本",slug:"查看系统版本",normalizedTitle:"查看系统版本",charIndex:241},{level:3,title:"编辑数据源",slug:"编辑数据源",normalizedTitle:"编辑数据源",charIndex:451},{level:3,title:"更新数据源",slug:"更新数据源",normalizedTitle:"更新数据源",charIndex:1404},{level:2,title:"常用 APT 命令",slug:"常用-apt-命令",normalizedTitle:"常用 apt 命令",charIndex:1431},{level:3,title:"安装软件包",slug:"安装软件包",normalizedTitle:"安装软件包",charIndex:1445},{level:3,title:"删除软件包",slug:"删除软件包",normalizedTitle:"删除软件包",charIndex:1485},{level:3,title:"更新软件包列表",slug:"更新软件包列表",normalizedTitle:"更新软件包列表",charIndex:1524},{level:3,title:"升级有可用更新的系统（慎用）",slug:"升级有可用更新的系统-慎用",normalizedTitle:"升级有可用更新的系统（慎用）",charIndex:1553},{level:2,title:"其它 APT 命令",slug:"其它-apt-命令",normalizedTitle:"其它 apt 命令",charIndex:1590},{level:3,title:"搜索",slug:"搜索",normalizedTitle:"搜索",charIndex:1604},{level:3,title:"获取包信息",slug:"获取包信息",normalizedTitle:"获取包信息",charIndex:1638},{level:3,title:"删除包及配置文件",slug:"删除包及配置文件",normalizedTitle:"删除包及配置文件",charIndex:1673},{level:3,title:"了解使用依赖",slug:"了解使用依赖",normalizedTitle:"了解使用依赖",charIndex:1719},{level:3,title:"查看被哪些包依赖",slug:"查看被哪些包依赖",normalizedTitle:"查看被哪些包依赖",charIndex:1758},{level:3,title:"安装相关的编译环境",slug:"安装相关的编译环境",normalizedTitle:"安装相关的编译环境",charIndex:1800},{level:3,title:"下载源代码",slug:"下载源代码",normalizedTitle:"下载源代码",charIndex:1842},{level:3,title:"清理无用的包",slug:"清理无用的包",normalizedTitle:"清理无用的包",charIndex:1877},{level:3,title:"检查是否有损坏的依赖",slug:"检查是否有损坏的依赖",normalizedTitle:"检查是否有损坏的依赖",charIndex:1926}],headersStr:"概述 修改数据源 查看系统版本 编辑数据源 更新数据源 常用 APT 命令 安装软件包 删除软件包 更新软件包列表 升级有可用更新的系统（慎用） 其它 APT 命令 搜索 获取包信息 删除包及配置文件 了解使用依赖 查看被哪些包依赖 安装相关的编译环境 下载源代码 清理无用的包 检查是否有损坏的依赖",content:"# Linux 软件包管理\n\n\n# 概述\n\nAPT(Advanced Packaging Tool) 是 Debian/Ubuntu 类 Linux 系统中的软件包管理程序, 使用它可以找到想要的软件包, 而且安装、卸载、更新都很简便；也可以用来对 Ubuntu 进行升级; APT 的源文件为 /etc/apt/ 目录下的 sources.list 文件。\n\n\n# 修改数据源\n\n由于国内的网络环境问题，我们需要将 Ubuntu 的数据源修改为国内数据源，操作步骤如下：\n\n\n# 查看系统版本\n\nlsb_release -a\n\n\n输出结果为\n\nNo LSB modules are available.\nDistributor ID:\tUbuntu\nDescription:\tUbuntu 18.04.4 LTS\nRelease:\t18.04\nCodename:\tbionic\n\n\n注意： Codename 为 bionic，该名称为我们 Ubuntu 系统的名称，修改数据源需要用到该名称\n\n\n# 编辑数据源\n\nvi /etc/apt/sources.list\n\n\n删除全部内容并修改为\n\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n\n\n\n# 更新数据源\n\napt-get update\n\n\n\n# 常用 APT 命令\n\n\n# 安装软件包\n\napt-get install packagename\n\n\n\n# 删除软件包\n\napt-get remove packagename\n\n\n\n# 更新软件包列表\n\napt-get update\n\n\n\n# 升级有可用更新的系统（慎用）\n\napt-get upgrade\n\n\n\n# 其它 APT 命令\n\n\n# 搜索\n\napt-cache search package\n\n\n\n# 获取包信息\n\napt-cache show package\n\n\n\n# 删除包及配置文件\n\napt-get remove package --purge\n\n\n\n# 了解使用依赖\n\napt-cache depends package\n\n\n\n# 查看被哪些包依赖\n\napt-cache rdepends package\n\n\n\n# 安装相关的编译环境\n\napt-get build-dep package\n\n\n\n# 下载源代码\n\napt-get source package\n\n\n\n# 清理无用的包\n\napt-get clean && apt-get autoclean1\n\n\n\n# 检查是否有损坏的依赖\n\napt-get check\n",normalizedContent:"# linux 软件包管理\n\n\n# 概述\n\napt(advanced packaging tool) 是 debian/ubuntu 类 linux 系统中的软件包管理程序, 使用它可以找到想要的软件包, 而且安装、卸载、更新都很简便；也可以用来对 ubuntu 进行升级; apt 的源文件为 /etc/apt/ 目录下的 sources.list 文件。\n\n\n# 修改数据源\n\n由于国内的网络环境问题，我们需要将 ubuntu 的数据源修改为国内数据源，操作步骤如下：\n\n\n# 查看系统版本\n\nlsb_release -a\n\n\n输出结果为\n\nno lsb modules are available.\ndistributor id:\tubuntu\ndescription:\tubuntu 18.04.4 lts\nrelease:\t18.04\ncodename:\tbionic\n\n\n注意： codename 为 bionic，该名称为我们 ubuntu 系统的名称，修改数据源需要用到该名称\n\n\n# 编辑数据源\n\nvi /etc/apt/sources.list\n\n\n删除全部内容并修改为\n\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n\n\n\n# 更新数据源\n\napt-get update\n\n\n\n# 常用 apt 命令\n\n\n# 安装软件包\n\napt-get install packagename\n\n\n\n# 删除软件包\n\napt-get remove packagename\n\n\n\n# 更新软件包列表\n\napt-get update\n\n\n\n# 升级有可用更新的系统（慎用）\n\napt-get upgrade\n\n\n\n# 其它 apt 命令\n\n\n# 搜索\n\napt-cache search package\n\n\n\n# 获取包信息\n\napt-cache show package\n\n\n\n# 删除包及配置文件\n\napt-get remove package --purge\n\n\n\n# 了解使用依赖\n\napt-cache depends package\n\n\n\n# 查看被哪些包依赖\n\napt-cache rdepends package\n\n\n\n# 安装相关的编译环境\n\napt-get build-dep package\n\n\n\n# 下载源代码\n\napt-get source package\n\n\n\n# 清理无用的包\n\napt-get clean && apt-get autoclean1\n\n\n\n# 检查是否有损坏的依赖\n\napt-get check\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"Linux 用户和组管理",frontmatter:{title:"Linux 用户和组管理",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/037bdb/"},regularPath:"/02.Linux/01.Linux/11.Linux%20%E7%94%A8%E6%88%B7%E5%92%8C%E7%BB%84%E7%AE%A1%E7%90%86.html",relativePath:"02.Linux/01.Linux/11.Linux 用户和组管理.md",key:"v-02353645",path:"/pages/037bdb/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:19},{level:2,title:"使用 Root 用户",slug:"使用-root-用户",normalizedTitle:"使用 root 用户",charIndex:112},{level:3,title:"设置 Root 账户密码",slug:"设置-root-账户密码",normalizedTitle:"设置 root 账户密码",charIndex:230},{level:3,title:"切换到 Root",slug:"切换到-root",normalizedTitle:"切换到 root",charIndex:266},{level:3,title:"设置允许远程登录 root",slug:"设置允许远程登录-root",normalizedTitle:"设置允许远程登录 root",charIndex:284},{level:2,title:"用户账户说明",slug:"用户账户说明",normalizedTitle:"用户账户说明",charIndex:519},{level:3,title:"普通用户",slug:"普通用户",normalizedTitle:"普通用户",charIndex:530},{level:3,title:"超级管理员",slug:"超级管理员",normalizedTitle:"超级管理员",charIndex:142},{level:3,title:"安装时创建的系统用户",slug:"安装时创建的系统用户",normalizedTitle:"安装时创建的系统用户",charIndex:678},{level:2,title:"组账户说明",slug:"组账户说明",normalizedTitle:"组账户说明",charIndex:789},{level:3,title:"私有组",slug:"私有组",normalizedTitle:"私有组",charIndex:799},{level:3,title:"标准组",slug:"标准组",normalizedTitle:"标准组",charIndex:860},{level:2,title:"账户系统文件说明",slug:"账户系统文件说明",normalizedTitle:"账户系统文件说明",charIndex:922},{level:3,title:"/etc/passwd",slug:"etc-passwd",normalizedTitle:"/etc/passwd",charIndex:935},{level:3,title:"/etc/shadow",slug:"etc-shadow",normalizedTitle:"/etc/shadow",charIndex:1156},{level:3,title:"/etc/group",slug:"etc-group",normalizedTitle:"/etc/group",charIndex:1281},{level:3,title:"/etc/gshadow",slug:"etc-gshadow",normalizedTitle:"/etc/gshadow",charIndex:2199},{level:2,title:"账户管理常用命令",slug:"账户管理常用命令",normalizedTitle:"账户管理常用命令",charIndex:2301},{level:3,title:"增加用户",slug:"增加用户",normalizedTitle:"增加用户",charIndex:2314},{level:3,title:"修改用户",slug:"修改用户",normalizedTitle:"修改用户",charIndex:2461},{level:3,title:"删除用户",slug:"删除用户",normalizedTitle:"删除用户",charIndex:2723},{level:3,title:"组账户维护",slug:"组账户维护",normalizedTitle:"组账户维护",charIndex:2836},{level:3,title:"口令维护",slug:"口令维护",normalizedTitle:"口令维护",charIndex:2951},{level:3,title:"用户和组状态",slug:"用户和组状态",normalizedTitle:"用户和组状态",charIndex:3166}],headersStr:"概述 使用 Root 用户 设置 Root 账户密码 切换到 Root 设置允许远程登录 root 用户账户说明 普通用户 超级管理员 安装时创建的系统用户 组账户说明 私有组 标准组 账户系统文件说明 /etc/passwd /etc/shadow /etc/group /etc/gshadow 账户管理常用命令 增加用户 修改用户 删除用户 组账户维护 口令维护 用户和组状态",content:"# Linux 用户和组管理\n\n\n# 概述\n\nLinux 操作系统是一个多用户操作系统，它允许多用户同时登录到系统上并使用资源。系统会根据账户来区分每个用户的文件，进程，任务和工作环境，使得每个用户工作都不受干扰。\n\n\n# 使用 Root 用户\n\n在实际生产操作中，我们基本上都是使用超级管理员账户操作 Linux 系统，也就是 Root 用户，Linux 系统默认是关闭 Root 账户的，我们需要为 Root 用户设置一个初始密码以方便我们使用。\n\n\n# 设置 Root 账户密码\n\nsudo passwd root\n\n\n\n# 切换到 Root\n\nsu\n\n\n\n# 设置允许远程登录 root\n\nvi /etc/ssh/sshd_config\n\n# Authentication:\nLoginGraceTime 120\n#PermitRootLogin without-password     //注释此行\nPermitRootLogin yes                             //加入此行\nStrictModes yes\n\n重启服务\nsystemctl restart sshd.service\n\n\n\n# 用户账户说明\n\n\n# 普通用户\n\n普通用户在系统上的任务是进行普通操作\n\n\n# 超级管理员\n\n管理员在系统上的任务是对普通用户和整个系统进行管理。对系统具有绝对的控制权，能够对系统进行一切操作。用 root 表示，root 用户在系统中拥有最高权限，默认下 Ubuntu 用户的 root 用户是不能登录的。\n\n\n# 安装时创建的系统用户\n\n此用户创建时被添加到 admin 组中，在 Ubuntu 中，admin 组中的用户默认是可以使用 sudo 命令来执行只有管理员才能执行的命令的。如果不使用 sudo 就是一个普通用户。\n\n\n# 组账户说明\n\n\n# 私有组\n\n当创建一个用户时没有指定属于哪个组，Linux 就会建立一个与用户同名的私有组，此私有组只含有该用户。\n\n\n# 标准组\n\n当创建一个用户时可以选定一个标准组，如果一个用户同时属于多个组时，登录后所属的组为主组，其他的为附加组。\n\n\n# 账户系统文件说明\n\n\n# /etc/passwd\n\n每一行代表一个账号，众多账号是系统正常运行所必须的，例如 bin，nobody 每行定义一个用户账户，此文件对所有用户可读。每行账户包含如下信息：\n\nroot:x:0:0:root:/root:/bin/bash\n\n\n * 用户名： 就是账号，用来对应 UID，root UID 是 0。\n * 口令： 密码，早期 UNIX 系统密码存在此字段，由于此文件所有用户都可以读取，密码容易泄露，后来这个字段数据就存放到 /etc/shadow 中，这里只能看到 X。\n * 用户标示号（UID）： 系统内唯一，root 用户的 UID 为 0，普通用户从 1000 开始，1-999 是系统的标准账户，500-65536 是可登陆账号。\n * 组标示号（GID）： 与 /etc/group 相关用来规定组名和 GID 相对应。\n * 注释： 注释账号\n * 宿主目录（主文件夹）： 用户登录系统后所进入的目录 root 在 /root/itcast\n * 命令解释器（shell）： 指定该用户使用的 shell ，默认的是 /bin/bash\n\n\n# /etc/shadow\n\n为了增加系统的安全性，用户口令通常用 shadow passwords 保护。只有 root 可读。每行包含如下信息：\n\nroot:$6$Reu571.V$Ci/kd.OTzaSGU.TagZ5KjYx2MLzQv2IkZ24E1.yeTT3Pp4o/yniTjus/rRaJ92Z18MVy6suf1W5uxxurqssel.:17465:0:99999:7:::\n\n\n * 账号名称： 需要和 /etc/passwd 一致。\n * 密码： 经过加密，虽然加密，但不表示不会被破解，该文件默认权限如下：\n   * -rw------- 1 root root 1560 Oct 26 17:20 passwd-\n   * 只有root能都读写\n * 最近修改密码日期： 从1970-1-1起，到用户最后一次更改口令的天数\n * 密码最小时间间隔： 从1970-1-1起，到用户可以更改口令的天数\n * 密码最大时间间隔： 从1970-1-1起，必须更改的口令天数\n * 密码到期警告时间： 在口令过期之前几天通知\n * 密码到期后账号宽限时间\n * 密码到期禁用账户时间： 在用户口令过期后到禁用账户的天数\n * 保留\n\n\n# /etc/group\n\n用户组的配置文件\n\nroot:x:0:\n\n\n * 用户组名称\n * 用户组密码： 给用户组管理员使用，通常不用\n * GID： 用户组的ID\n * 此用户支持的账号名称： 一个账号可以加入多个用户组，例如想要 itcast 加入 root 这个用户组，将该账号填入该字段即可 root:x:0:root, icast 将用户进行分组是 Linux 对用户进行管理及控制访问权限的一种手段。一个中可以有多个用户，一个用户可以同时属于多个组。该文件对所有用户可读。\n\n\n# /etc/gshadow\n\n该文件用户定义用户组口令，组管理员等信息只有root用户可读。\n\nroot:\\*::\n\n\n * 用户组名\n * 密码列\n * 用户组管理员的账号\n * 用户组所属账号\n\n\n# 账户管理常用命令\n\n\n# 增加用户\n\nuseradd 用户名\nuseradd -u (UID号)\nuseradd -p (口令)\nuseradd -g (分组)\nuseradd -s (SHELL)\nuseradd -d (用户目录)\n\n\n如：useradd centos\n\n增加用户名为 centos 的账户\n\n\n# 修改用户\n\nusermod -u (新UID)\nusermod -d (用户目录)\nusermod -g (组名)\nusermod -s (SHELL)\nusermod -p (新口令)\nusermod -l (新登录名)\nusermod -L (锁定用户账号密码)\nusermod -U (解锁用户账号)\n\n\n如：usermod -u 1024 -g group2 -G root centos\n\n将 centos 用户 uid 修改为 1024，默认组改为系统中已经存在的 group2，并且加入到系统管理员组\n\n\n# 删除用户\n\nuserdel 用户名 (删除用户账号)\nuserdel -r 删除账号时同时删除目录\n\n\n如：userdel -r centos\n\n删除用户名为 centos 的账户并同时删除 centos 的用户目录\n\n\n# 组账户维护\n\ngroupadd 组账户名 (创建新组)\ngroupadd -g 指定组GID\ngroupmod -g 更改组的GID\ngroupmod -n 更改组账户名\ngroupdel 组账户名 (删除指定组账户)\n\n\n\n# 口令维护\n\npasswd 用户账户名 (设置用户口令)\npasswd -l 用户账户名 (锁定用户账户)\npasswd -u 用户账户名 (解锁用户账户)\npasswd -d 用户账户名 (删除账户口令)\ngpasswd -a 用户账户名 组账户名 (将指定用户添加到指定组)\ngpasswd -d 用户账户名 组账户名 (将用户从指定组中删除)\ngpasswd -A 用户账户名 组账户名 (将用户指定为组的管理员)\n\n\n\n# 用户和组状态\n\nsu 用户名(切换用户账户)\nid 用户名(显示用户的UID，GID)\nwhoami (显示当前用户名称)\ngroups (显示用户所属组)\n",normalizedContent:"# linux 用户和组管理\n\n\n# 概述\n\nlinux 操作系统是一个多用户操作系统，它允许多用户同时登录到系统上并使用资源。系统会根据账户来区分每个用户的文件，进程，任务和工作环境，使得每个用户工作都不受干扰。\n\n\n# 使用 root 用户\n\n在实际生产操作中，我们基本上都是使用超级管理员账户操作 linux 系统，也就是 root 用户，linux 系统默认是关闭 root 账户的，我们需要为 root 用户设置一个初始密码以方便我们使用。\n\n\n# 设置 root 账户密码\n\nsudo passwd root\n\n\n\n# 切换到 root\n\nsu\n\n\n\n# 设置允许远程登录 root\n\nvi /etc/ssh/sshd_config\n\n# authentication:\nlogingracetime 120\n#permitrootlogin without-password     //注释此行\npermitrootlogin yes                             //加入此行\nstrictmodes yes\n\n重启服务\nsystemctl restart sshd.service\n\n\n\n# 用户账户说明\n\n\n# 普通用户\n\n普通用户在系统上的任务是进行普通操作\n\n\n# 超级管理员\n\n管理员在系统上的任务是对普通用户和整个系统进行管理。对系统具有绝对的控制权，能够对系统进行一切操作。用 root 表示，root 用户在系统中拥有最高权限，默认下 ubuntu 用户的 root 用户是不能登录的。\n\n\n# 安装时创建的系统用户\n\n此用户创建时被添加到 admin 组中，在 ubuntu 中，admin 组中的用户默认是可以使用 sudo 命令来执行只有管理员才能执行的命令的。如果不使用 sudo 就是一个普通用户。\n\n\n# 组账户说明\n\n\n# 私有组\n\n当创建一个用户时没有指定属于哪个组，linux 就会建立一个与用户同名的私有组，此私有组只含有该用户。\n\n\n# 标准组\n\n当创建一个用户时可以选定一个标准组，如果一个用户同时属于多个组时，登录后所属的组为主组，其他的为附加组。\n\n\n# 账户系统文件说明\n\n\n# /etc/passwd\n\n每一行代表一个账号，众多账号是系统正常运行所必须的，例如 bin，nobody 每行定义一个用户账户，此文件对所有用户可读。每行账户包含如下信息：\n\nroot:x:0:0:root:/root:/bin/bash\n\n\n * 用户名： 就是账号，用来对应 uid，root uid 是 0。\n * 口令： 密码，早期 unix 系统密码存在此字段，由于此文件所有用户都可以读取，密码容易泄露，后来这个字段数据就存放到 /etc/shadow 中，这里只能看到 x。\n * 用户标示号（uid）： 系统内唯一，root 用户的 uid 为 0，普通用户从 1000 开始，1-999 是系统的标准账户，500-65536 是可登陆账号。\n * 组标示号（gid）： 与 /etc/group 相关用来规定组名和 gid 相对应。\n * 注释： 注释账号\n * 宿主目录（主文件夹）： 用户登录系统后所进入的目录 root 在 /root/itcast\n * 命令解释器（shell）： 指定该用户使用的 shell ，默认的是 /bin/bash\n\n\n# /etc/shadow\n\n为了增加系统的安全性，用户口令通常用 shadow passwords 保护。只有 root 可读。每行包含如下信息：\n\nroot:$6$reu571.v$ci/kd.otzasgu.tagz5kjyx2mlzqv2ikz24e1.yett3pp4o/ynitjus/rraj92z18mvy6suf1w5uxxurqssel.:17465:0:99999:7:::\n\n\n * 账号名称： 需要和 /etc/passwd 一致。\n * 密码： 经过加密，虽然加密，但不表示不会被破解，该文件默认权限如下：\n   * -rw------- 1 root root 1560 oct 26 17:20 passwd-\n   * 只有root能都读写\n * 最近修改密码日期： 从1970-1-1起，到用户最后一次更改口令的天数\n * 密码最小时间间隔： 从1970-1-1起，到用户可以更改口令的天数\n * 密码最大时间间隔： 从1970-1-1起，必须更改的口令天数\n * 密码到期警告时间： 在口令过期之前几天通知\n * 密码到期后账号宽限时间\n * 密码到期禁用账户时间： 在用户口令过期后到禁用账户的天数\n * 保留\n\n\n# /etc/group\n\n用户组的配置文件\n\nroot:x:0:\n\n\n * 用户组名称\n * 用户组密码： 给用户组管理员使用，通常不用\n * gid： 用户组的id\n * 此用户支持的账号名称： 一个账号可以加入多个用户组，例如想要 itcast 加入 root 这个用户组，将该账号填入该字段即可 root:x:0:root, icast 将用户进行分组是 linux 对用户进行管理及控制访问权限的一种手段。一个中可以有多个用户，一个用户可以同时属于多个组。该文件对所有用户可读。\n\n\n# /etc/gshadow\n\n该文件用户定义用户组口令，组管理员等信息只有root用户可读。\n\nroot:\\*::\n\n\n * 用户组名\n * 密码列\n * 用户组管理员的账号\n * 用户组所属账号\n\n\n# 账户管理常用命令\n\n\n# 增加用户\n\nuseradd 用户名\nuseradd -u (uid号)\nuseradd -p (口令)\nuseradd -g (分组)\nuseradd -s (shell)\nuseradd -d (用户目录)\n\n\n如：useradd centos\n\n增加用户名为 centos 的账户\n\n\n# 修改用户\n\nusermod -u (新uid)\nusermod -d (用户目录)\nusermod -g (组名)\nusermod -s (shell)\nusermod -p (新口令)\nusermod -l (新登录名)\nusermod -l (锁定用户账号密码)\nusermod -u (解锁用户账号)\n\n\n如：usermod -u 1024 -g group2 -g root centos\n\n将 centos 用户 uid 修改为 1024，默认组改为系统中已经存在的 group2，并且加入到系统管理员组\n\n\n# 删除用户\n\nuserdel 用户名 (删除用户账号)\nuserdel -r 删除账号时同时删除目录\n\n\n如：userdel -r centos\n\n删除用户名为 centos 的账户并同时删除 centos 的用户目录\n\n\n# 组账户维护\n\ngroupadd 组账户名 (创建新组)\ngroupadd -g 指定组gid\ngroupmod -g 更改组的gid\ngroupmod -n 更改组账户名\ngroupdel 组账户名 (删除指定组账户)\n\n\n\n# 口令维护\n\npasswd 用户账户名 (设置用户口令)\npasswd -l 用户账户名 (锁定用户账户)\npasswd -u 用户账户名 (解锁用户账户)\npasswd -d 用户账户名 (删除账户口令)\ngpasswd -a 用户账户名 组账户名 (将指定用户添加到指定组)\ngpasswd -d 用户账户名 组账户名 (将用户从指定组中删除)\ngpasswd -a 用户账户名 组账户名 (将用户指定为组的管理员)\n\n\n\n# 用户和组状态\n\nsu 用户名(切换用户账户)\nid 用户名(显示用户的uid，gid)\nwhoami (显示当前用户名称)\ngroups (显示用户所属组)\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"Linux 文件权限管理",frontmatter:{title:"Linux 文件权限管理",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/c29423/"},regularPath:"/02.Linux/01.Linux/12.Linux%20%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86.html",relativePath:"02.Linux/01.Linux/12.Linux 文件权限管理.md",key:"v-7bc439b8",path:"/pages/c29423/",headers:[{level:2,title:"查看文件和目录的权限",slug:"查看文件和目录的权限",normalizedTitle:"查看文件和目录的权限",charIndex:19},{level:3,title:"文档类型",slug:"文档类型",normalizedTitle:"文档类型",charIndex:391},{level:3,title:"连接数",slug:"连接数",normalizedTitle:"连接数",charIndex:404},{level:3,title:"文档所属用户和所属组",slug:"文档所属用户和所属组",normalizedTitle:"文档所属用户和所属组",charIndex:708},{level:3,title:"文档大小",slug:"文档大小",normalizedTitle:"文档大小",charIndex:427},{level:2,title:"更改操作权限",slug:"更改操作权限",normalizedTitle:"更改操作权限",charIndex:774},{level:3,title:"chown",slug:"chown",normalizedTitle:"chown",charIndex:785},{level:3,title:"chmod",slug:"chmod",normalizedTitle:"chmod",charIndex:937},{level:2,title:"数字设定法",slug:"数字设定法",normalizedTitle:"数字设定法",charIndex:1440}],headersStr:"查看文件和目录的权限 文档类型 连接数 文档所属用户和所属组 文档大小 更改操作权限 chown chmod 数字设定法",content:"# Linux 文件权限管理\n\n\n# 查看文件和目录的权限\n\nls –al使用 ls 不带参数只显示文件名称，通过ls –al` 可以显示文件或者目录的权限信息。\n\nls -l 文件名 显示信息包括：文件类型 (d 目录，- 普通文件，l 链接文件)，文件权限，文件的用户，文件的所属组，文件的大小，文件的创建时间，文件的名称\n\n-rw-r--r-- 1 ivoov ivoov 675 Oct 26 17:20 .profile\n\n\n * -：普通文件\n * rw-：说明用户 ivoov 有读写权限，没有运行权限\n * r--：表示用户组 ivoov 只有读权限，没有写和运行的权限\n * r--：其他用户只有读权限，没有写权限和运行的权限\n\n-RW-R--R--   1     IVOOV    IVOOV   675    OCT 26 17:20   .PROFILE\n文档类型及权限      连接数   文档所属用户   文档所属组   文档大小   文档最后被修改日期      文档名称\n\n-      RW-             R--                R--\n文档类型   文档所有者权限（user）   文档所属用户组权限（group）   其他用户权限（other）\n\n\n# 文档类型\n\n * d 表示目录\n * l 表示软连接\n * – 表示文件\n * c 表示串行端口字符设备文件\n * b 表示可供存储的块设备文件\n * 余下的字符 3 个字符为一组。r 只读，w 可写，x 可执行，- 表示无此权限\n\n\n# 连接数\n\n指有多少个文件指向同一个索引节点。\n\n\n# 文档所属用户和所属组\n\n就是文档属于哪个用户和用户组。文件所属用户和组是可以更改的\n\n\n# 文档大小\n\n默认是 bytes\n\n\n# 更改操作权限\n\n\n# chown\n\n是 change owner 的意思，主要作用就是改变文件或者目录所有者，所有者包含用户和用户组\n\nchown [-R] 用户名称 文件或者目录\nchown [-R] 用户名称 用户组名称 文件或目录\n\n\n-R：进行递归式的权限更改，将目录下的所有文件、子目录更新为指定用户组权限\n\n\n# chmod\n\n改变访问权限\n\nchmod [who] [+ | - | =] [mode] 文件名\n\n\n# who\n\n表示操作对象可以是以下字母的一个或者组合\n\n * u：用户 user\n * g：用户组 group\n * o：表示其他用户\n * a：表示所有用户是系统默认的\n\n# 操作符号\n\n * +：表示添加某个权限\n * -：表示取消某个权限\n * =：赋予给定的权限，取消文档以前的所有权限\n\n# mode\n\n表示可执行的权限，可以是 r、w、x\n\n# 文件名\n\n文件名可以使空格分开的文件列表\n\n# 示例\n\nivoov@localhost:~$ ls -al test.txt \n-rw-rw-r-- 1 ivoov ivoov 6 Nov  2 21:47 test.txt\nivoov@localhost:~$ chmod u=rwx,g+r,o+r test.txt \nivoov@localhost:~$ ls -al test.txt \n-rwxrw-r-- 1 ivoov ivoov 6 Nov  2 21:47 test.txt\nivoov@localhost:~$\n\n\n\n# 数字设定法\n\n数字设定法中数字表示的含义\n\n * 0 表示没有任何权限\n * 1 表示有可执行权限 = x\n * 2 表示有可写权限 = w\n * 4 表示有可读权限 = r\n\n也可以用数字来表示权限如 chmod 755 file_name\n\nR W X   R – X   R - X\n4 2 1   4 - 1   4 - 1\nuser    group   others\n\n若要 rwx 属性则 4+2+1=7\n\n若要 rw- 属性则 4+2=6\n\n若要 r-x 属性则 4+1=5\n\nivoov@localhost:~$ chmod 777 test.txt \nivoov@localhost:~$ ls -al test.txt \n-rwxrwxrwx 1 ivoov ivoov 6 Nov  2 21:47 test.txt\n\nivoov@localhost:~$ chmod 770 test.txt \nivoov@localhost:~$ ls -al test.txt \n-rwxrwx--- 1 ivoov ivoov 6 Nov  2 21:47 test.txt\n",normalizedContent:"# linux 文件权限管理\n\n\n# 查看文件和目录的权限\n\nls –al使用 ls 不带参数只显示文件名称，通过ls –al` 可以显示文件或者目录的权限信息。\n\nls -l 文件名 显示信息包括：文件类型 (d 目录，- 普通文件，l 链接文件)，文件权限，文件的用户，文件的所属组，文件的大小，文件的创建时间，文件的名称\n\n-rw-r--r-- 1 ivoov ivoov 675 oct 26 17:20 .profile\n\n\n * -：普通文件\n * rw-：说明用户 ivoov 有读写权限，没有运行权限\n * r--：表示用户组 ivoov 只有读权限，没有写和运行的权限\n * r--：其他用户只有读权限，没有写权限和运行的权限\n\n-rw-r--r--   1     ivoov    ivoov   675    oct 26 17:20   .profile\n文档类型及权限      连接数   文档所属用户   文档所属组   文档大小   文档最后被修改日期      文档名称\n\n-      rw-             r--                r--\n文档类型   文档所有者权限（user）   文档所属用户组权限（group）   其他用户权限（other）\n\n\n# 文档类型\n\n * d 表示目录\n * l 表示软连接\n * – 表示文件\n * c 表示串行端口字符设备文件\n * b 表示可供存储的块设备文件\n * 余下的字符 3 个字符为一组。r 只读，w 可写，x 可执行，- 表示无此权限\n\n\n# 连接数\n\n指有多少个文件指向同一个索引节点。\n\n\n# 文档所属用户和所属组\n\n就是文档属于哪个用户和用户组。文件所属用户和组是可以更改的\n\n\n# 文档大小\n\n默认是 bytes\n\n\n# 更改操作权限\n\n\n# chown\n\n是 change owner 的意思，主要作用就是改变文件或者目录所有者，所有者包含用户和用户组\n\nchown [-r] 用户名称 文件或者目录\nchown [-r] 用户名称 用户组名称 文件或目录\n\n\n-r：进行递归式的权限更改，将目录下的所有文件、子目录更新为指定用户组权限\n\n\n# chmod\n\n改变访问权限\n\nchmod [who] [+ | - | =] [mode] 文件名\n\n\n# who\n\n表示操作对象可以是以下字母的一个或者组合\n\n * u：用户 user\n * g：用户组 group\n * o：表示其他用户\n * a：表示所有用户是系统默认的\n\n# 操作符号\n\n * +：表示添加某个权限\n * -：表示取消某个权限\n * =：赋予给定的权限，取消文档以前的所有权限\n\n# mode\n\n表示可执行的权限，可以是 r、w、x\n\n# 文件名\n\n文件名可以使空格分开的文件列表\n\n# 示例\n\nivoov@localhost:~$ ls -al test.txt \n-rw-rw-r-- 1 ivoov ivoov 6 nov  2 21:47 test.txt\nivoov@localhost:~$ chmod u=rwx,g+r,o+r test.txt \nivoov@localhost:~$ ls -al test.txt \n-rwxrw-r-- 1 ivoov ivoov 6 nov  2 21:47 test.txt\nivoov@localhost:~$\n\n\n\n# 数字设定法\n\n数字设定法中数字表示的含义\n\n * 0 表示没有任何权限\n * 1 表示有可执行权限 = x\n * 2 表示有可写权限 = w\n * 4 表示有可读权限 = r\n\n也可以用数字来表示权限如 chmod 755 file_name\n\nr w x   r – x   r - x\n4 2 1   4 - 1   4 - 1\nuser    group   others\n\n若要 rwx 属性则 4+2+1=7\n\n若要 rw- 属性则 4+2=6\n\n若要 r-x 属性则 4+1=5\n\nivoov@localhost:~$ chmod 777 test.txt \nivoov@localhost:~$ ls -al test.txt \n-rwxrwxrwx 1 ivoov ivoov 6 nov  2 21:47 test.txt\n\nivoov@localhost:~$ chmod 770 test.txt \nivoov@localhost:~$ ls -al test.txt \n-rwxrwx--- 1 ivoov ivoov 6 nov  2 21:47 test.txt\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"Linux LVM 磁盘扩容",frontmatter:{title:"Linux LVM 磁盘扩容",date:"2022-02-09T09:59:32.000Z",permalink:"/pages/6afa9f/"},regularPath:"/02.Linux/01.Linux/13.Linux%20LVM%20%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9.html",relativePath:"02.Linux/01.Linux/13.Linux LVM 磁盘扩容.md",key:"v-8a53495c",path:"/pages/6afa9f/",headers:[{level:2,title:"LVM 的基本概念",slug:"lvm-的基本概念",normalizedTitle:"lvm 的基本概念",charIndex:21},{level:3,title:"物理卷 Physical volume (PV)",slug:"物理卷-physical-volume-pv",normalizedTitle:"物理卷 physical volume (pv)",charIndex:35},{level:3,title:"卷组 Volume group (VG)",slug:"卷组-volume-group-vg",normalizedTitle:"卷组 volume group (vg)",charIndex:169},{level:3,title:"逻辑卷 Logical volume (LV)",slug:"逻辑卷-logical-volume-lv",normalizedTitle:"逻辑卷 logical volume (lv)",charIndex:212},{level:3,title:"物理区域 Physical extent (PE)",slug:"物理区域-physical-extent-pe",normalizedTitle:"物理区域 physical extent (pe)",charIndex:273},{level:2,title:"磁盘操作相关命令",slug:"磁盘操作相关命令",normalizedTitle:"磁盘操作相关命令",charIndex:330},{level:3,title:"df -h（查看挂载点）",slug:"df-h-查看挂载点",normalizedTitle:"df -h（查看挂载点）",charIndex:343},{level:3,title:"lvdisplay（显示当前的 logical volume）",slug:"lvdisplay-显示当前的-logical-volume",normalizedTitle:"lvdisplay（显示当前的 logical volume）",charIndex:362},{level:3,title:"vgdisplay（显示当前的 volume group）",slug:"vgdisplay-显示当前的-volume-group",normalizedTitle:"vgdisplay（显示当前的 volume group）",charIndex:471},{level:3,title:"pvdisplay（显示当前的 physical volume）",slug:"pvdisplay-显示当前的-physical-volume",normalizedTitle:"pvdisplay（显示当前的 physical volume）",charIndex:559},{level:2,title:"开始 LVM 扩容",slug:"开始-lvm-扩容",normalizedTitle:"开始 lvm 扩容",charIndex:598},{level:3,title:"查看 fdisk",slug:"查看-fdisk",normalizedTitle:"查看 fdisk",charIndex:612},{level:3,title:"查看所有连接到电脑上的储存设备",slug:"查看所有连接到电脑上的储存设备",normalizedTitle:"查看所有连接到电脑上的储存设备",charIndex:841},{level:3,title:"创建 sdb 分区",slug:"创建-sdb-分区",normalizedTitle:"创建 sdb 分区",charIndex:924},{level:3,title:"格式化磁盘",slug:"格式化磁盘",normalizedTitle:"格式化磁盘",charIndex:1056},{level:3,title:"创建 PV",slug:"创建-pv",normalizedTitle:"创建 pv",charIndex:1095},{level:3,title:"查看卷组",slug:"查看卷组",normalizedTitle:"查看卷组",charIndex:1126},{level:3,title:"扩容 VG",slug:"扩容-vg",normalizedTitle:"扩容 vg",charIndex:1146},{level:3,title:"扩容 LV",slug:"扩容-lv",normalizedTitle:"扩容 lv",charIndex:1201},{level:3,title:"刷新分区",slug:"刷新分区",normalizedTitle:"刷新分区",charIndex:1314},{level:3,title:"删除 unknown device",slug:"删除-unknown-device",normalizedTitle:"删除 unknown device",charIndex:1355}],headersStr:"LVM 的基本概念 物理卷 Physical volume (PV) 卷组 Volume group (VG) 逻辑卷 Logical volume (LV) 物理区域 Physical extent (PE) 磁盘操作相关命令 df -h（查看挂载点） lvdisplay（显示当前的 logical volume） vgdisplay（显示当前的 volume group） pvdisplay（显示当前的 physical volume） 开始 LVM 扩容 查看 fdisk 查看所有连接到电脑上的储存设备 创建 sdb 分区 格式化磁盘 创建 PV 查看卷组 扩容 VG 扩容 LV 刷新分区 删除 unknown device",content:"# Linux LVM 磁盘扩容\n\n\n# LVM 的基本概念\n\n\n# 物理卷 Physical volume (PV)\n\n可以在上面建立卷组的媒介，可以是硬盘分区，也可以是硬盘本身或者回环文件（loopback file）。物理卷包括一个特殊的 header，其余部分被切割为一块块物理区域（physical extents）。\n\n\n# 卷组 Volume group (VG)\n\n将一组物理卷收集为一个管理单元。\n\n\n# 逻辑卷 Logical volume (LV)\n\n虚拟分区，由物理区域（physical extents）组成。\n\n\n# 物理区域 Physical extent (PE)\n\n硬盘可供指派给逻辑卷的最小单位（通常为 4MB）。\n\n\n# 磁盘操作相关命令\n\n\n# df -h（查看挂载点）\n\n\n\n\n# lvdisplay（显示当前的 logical volume）\n\n\n\n备注： 注意这里目前有两个，一个是文件系统所在的 volume，另一个是 swap 分区使用的 volume，当然，我们需要扩容的是第一个\n\n\n# vgdisplay（显示当前的 volume group）\n\n\n\n备注： 注意 VG SIZE，这里应该是你当前的可用空间大小，待扩容完毕，这里显示的应该是最终的大小\n\n\n# pvdisplay（显示当前的 physical volume）\n\n\n\n\n# 开始 LVM 扩容\n\n\n# 查看 fdisk\n\nfdisk -l\n\n\n\n\n因为这台机器默认开启了 LVM，所以目前有一个 extended 分区和一个 LVM 分区，并且他们是完全重叠的。这是因为，LVM 分区作为一个虚拟的分区，完全占用了这个 extended 分区，原理图见下：\n\n\n\n因此，现在需要做的就是将 extended partition (sda2) 扩展到最大，然后创建一个新的 LVM logical partition (sda6)，用它来填满 sda2\n\n\n# 查看所有连接到电脑上的储存设备\n\nfdisk -l |grep '/dev'\n\n\n# 1 块磁盘效果图\n\n\n\n# 2 块磁盘效果图（新增磁盘，尚未挂载）\n\n\n\n\n# 创建 sdb 分区\n\nfdisk /dev/sdb\nn\t# 新建分区\nl\t# 选择逻辑分区，如果没有，则首先创建扩展分区（p），然后再添加逻辑分区（硬盘：最多四个分区 P-P-P-P 或 P-P-P-E）\n\n\n\n\n回车\n回车\n回车\nw\t# 写入磁盘分区\n\n\n\n# 格式化磁盘\n\n\n\nmkfs -t ext4 /dev/sdb1\n\n\n\n\n\n# 创建 PV\n\npvcreate /dev/sdb1\n\n\n\n# 查看卷组\n\npvscan\n\n\n\n\n\n# 扩容 VG\n\nvgdisplay\n\n\n\n\nvgextend ubuntu-vg /dev/sdb1\n\n\n\n# 扩容 LV\n\n\n\n\n\n# 增加指定大小\nlvextend -L +30G /dev/ubuntu-vg/root\n# 按百分比扩容\nlvextend -l +100%FREE /dev/ubuntu-vg/root\n\n\n\n# 刷新分区\n\nresize2fs /dev/ubuntu-vg/root\n\n\n\n# 删除 unknown device\n\npvscan\nvgreduce --removemissing ubuntu-vg\n\n\n注意：不要卸载扩容的磁盘，可能出现丢失数据或是系统无法启动",normalizedContent:"# linux lvm 磁盘扩容\n\n\n# lvm 的基本概念\n\n\n# 物理卷 physical volume (pv)\n\n可以在上面建立卷组的媒介，可以是硬盘分区，也可以是硬盘本身或者回环文件（loopback file）。物理卷包括一个特殊的 header，其余部分被切割为一块块物理区域（physical extents）。\n\n\n# 卷组 volume group (vg)\n\n将一组物理卷收集为一个管理单元。\n\n\n# 逻辑卷 logical volume (lv)\n\n虚拟分区，由物理区域（physical extents）组成。\n\n\n# 物理区域 physical extent (pe)\n\n硬盘可供指派给逻辑卷的最小单位（通常为 4mb）。\n\n\n# 磁盘操作相关命令\n\n\n# df -h（查看挂载点）\n\n\n\n\n# lvdisplay（显示当前的 logical volume）\n\n\n\n备注： 注意这里目前有两个，一个是文件系统所在的 volume，另一个是 swap 分区使用的 volume，当然，我们需要扩容的是第一个\n\n\n# vgdisplay（显示当前的 volume group）\n\n\n\n备注： 注意 vg size，这里应该是你当前的可用空间大小，待扩容完毕，这里显示的应该是最终的大小\n\n\n# pvdisplay（显示当前的 physical volume）\n\n\n\n\n# 开始 lvm 扩容\n\n\n# 查看 fdisk\n\nfdisk -l\n\n\n\n\n因为这台机器默认开启了 lvm，所以目前有一个 extended 分区和一个 lvm 分区，并且他们是完全重叠的。这是因为，lvm 分区作为一个虚拟的分区，完全占用了这个 extended 分区，原理图见下：\n\n\n\n因此，现在需要做的就是将 extended partition (sda2) 扩展到最大，然后创建一个新的 lvm logical partition (sda6)，用它来填满 sda2\n\n\n# 查看所有连接到电脑上的储存设备\n\nfdisk -l |grep '/dev'\n\n\n# 1 块磁盘效果图\n\n\n\n# 2 块磁盘效果图（新增磁盘，尚未挂载）\n\n\n\n\n# 创建 sdb 分区\n\nfdisk /dev/sdb\nn\t# 新建分区\nl\t# 选择逻辑分区，如果没有，则首先创建扩展分区（p），然后再添加逻辑分区（硬盘：最多四个分区 p-p-p-p 或 p-p-p-e）\n\n\n\n\n回车\n回车\n回车\nw\t# 写入磁盘分区\n\n\n\n# 格式化磁盘\n\n\n\nmkfs -t ext4 /dev/sdb1\n\n\n\n\n\n# 创建 pv\n\npvcreate /dev/sdb1\n\n\n\n# 查看卷组\n\npvscan\n\n\n\n\n\n# 扩容 vg\n\nvgdisplay\n\n\n\n\nvgextend ubuntu-vg /dev/sdb1\n\n\n\n# 扩容 lv\n\n\n\n\n\n# 增加指定大小\nlvextend -l +30g /dev/ubuntu-vg/root\n# 按百分比扩容\nlvextend -l +100%free /dev/ubuntu-vg/root\n\n\n\n# 刷新分区\n\nresize2fs /dev/ubuntu-vg/root\n\n\n\n# 删除 unknown device\n\npvscan\nvgreduce --removemissing ubuntu-vg\n\n\n注意：不要卸载扩容的磁盘，可能出现丢失数据或是系统无法启动",charsets:{cjk:!0},lastUpdated:"2022/02/09, 10:06:13",lastUpdatedTimestamp:1644372373e3},{title:"什么是 CentOS",frontmatter:{title:"什么是 CentOS",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/8eb38e/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/01.%E4%BB%80%E4%B9%88%E6%98%AF%20CentOS.html",relativePath:"03.CentOS/01.入门/01.什么是 CentOS.md",key:"v-3834644e",path:"/pages/8eb38e/",headers:[{level:2,title:"介绍",slug:"介绍",normalizedTitle:"介绍",charIndex:17},{level:2,title:"最新版本",slug:"最新版本",normalizedTitle:"最新版本",charIndex:618},{level:2,title:"特点",slug:"特点",normalizedTitle:"特点",charIndex:830}],headersStr:"介绍 最新版本 特点",content:'# 什么是 CentOS\n\n\n# 介绍\n\nCentOS 是一个基于Red Hat Linux 提供的可自由使用源代码的企业级Linux发行版本。每个版本的 CentOS都会获得十年的支持（通过安全更新方式）。新版本的 CentOS 大约每两年发行一次，而每个版本的 CentOS 会定期（大概每六个月）更新一次，以便支持新的硬件。这样，建立一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。CentOS是Community Enterprise Operating System的缩写。\n\nCentOS 是RHEL（Red Hat Enterprise Linux）源代码再编译的产物，而且在RHEL的基础上修正了不少已知的 Bug ，相对于其他 Linux 发行版，其稳定性值得信赖。\n\nCentOS在2014初，宣布加入Red Hat。\n\nCentOS 加入红帽后不变的是：\n\n 1. CentOS 继续不收费\n\n 2. 保持赞助内容驱动的网络中心不变\n\n 3. Bug、Issue 和紧急事件处理策略不变\n\n 4. Red Hat Enterprise Linux 和 CentOS 防火墙也依然存在\n\n变化的是：\n\n 1. 我们是为红帽工作，不是为 RHEL\n\n 2. 红帽提供构建系统和初始内容分发资源的赞助\n\n 3. 一些开发的资源包括源码的获取将更加容易\n\n 4. 避免了原来和红帽上一些法律的问题\n\n\n# 最新版本\n\n最新版本为 CentOS 8.0.1905。\n\n上游RHEL 7主要改进：\n\n内核更新至3.10.0；支持Linux容器（Docker），Open VMware Tools及3D图像能即装即用，Open JDK7作为缺省JDK，ext4及XFS的LVM快照，转用systemd、firewalld及GRUB2，XFS作为缺省文件系统，内核空间内的iSCSI及FCoE，支持PTPv2，支持40G网卡等。\n\n\n# 特点\n\n1．可以把CentOS理解为Red Hat AS系列！它完全就是对Red Hat AS进行改进后发布的！各种操作、使用和RED HAT没有区别！\n\n2．CentOS完全免费，不存在RED HAT AS4需要序列号的问题。\n\n3．CentOS独有的yum命令支持在线升级，可以即时更新系统，不像RED HAT那样需要花钱购买支持服务！\n\n4．CentOS修正了许多RHEL的BUG！\n\n5．CentOS版本说明：CentOS3.1 等同于 RED HAT AS3 Update1 CentOS3.4 等同于 RED HAT AS3 Update4 CentOS4.0 等同于 RED HAT AS4\n\n与 RHEL的关系\n\nRHEL 在发行的时候，有两种方式。一种是二进制的发行方式，另外一种是源代码的发行方式。无论是哪一种发行方式，你都可以免费获得（例如从网上下载），并再次发布。但如果你使用了他们的在线升级（包括补丁）或咨询服务，就必须要付费。RHEL 一直都提供源代码的发行方式，CentOS 就是将 RHEL 发行的源代码重新编译一次，形成一个可使用的二进制版本。由于 LINUX 的源代码是 GNU，所以从获得 RHEL 的源代码到编译成新的二进制，都是合法。只是 red hat 是商标，所以必须在新的发行版里将red hat 的商标去掉。red hat对这种发行版的态度是："我们其实并不反对这种发行版，真正向我们付费的用户，他们重视的并不是系统本身，而是我们所提供的商业服务。" 所以，CentOS 可以得到 RHEL 的所有功能，甚至是更好的软件。但 CentOS 并不向用户提供商业支持，当然也不负上任何商业责任。如果你要将你的 RHEL 转到 CentOS 上，因为你不希望为 RHEL 升级而付费。当然，你必须有丰富 linux 使用经验，因此 RHEL 的商业技术支持对你来说并不重要。比如说，尽管没有RHEL的商业支持，2019年也有不少企业选择使用CentOS，比如著名会议管理系统MUNPANEL。但如果你是单纯的业务型企业，那么还是建议你选购 RHEL 软件并购买相应服务。这样可以节省你的 IT 管理费用，并可得到专业服务。一句话，选用 CentOS 还是 RHEL，取决于你所在公司是否拥有相应的技术力量。',normalizedContent:'# 什么是 centos\n\n\n# 介绍\n\ncentos 是一个基于red hat linux 提供的可自由使用源代码的企业级linux发行版本。每个版本的 centos都会获得十年的支持（通过安全更新方式）。新版本的 centos 大约每两年发行一次，而每个版本的 centos 会定期（大概每六个月）更新一次，以便支持新的硬件。这样，建立一个安全、低维护、稳定、高预测性、高重复性的 linux 环境。centos是community enterprise operating system的缩写。\n\ncentos 是rhel（red hat enterprise linux）源代码再编译的产物，而且在rhel的基础上修正了不少已知的 bug ，相对于其他 linux 发行版，其稳定性值得信赖。\n\ncentos在2014初，宣布加入red hat。\n\ncentos 加入红帽后不变的是：\n\n 1. centos 继续不收费\n\n 2. 保持赞助内容驱动的网络中心不变\n\n 3. bug、issue 和紧急事件处理策略不变\n\n 4. red hat enterprise linux 和 centos 防火墙也依然存在\n\n变化的是：\n\n 1. 我们是为红帽工作，不是为 rhel\n\n 2. 红帽提供构建系统和初始内容分发资源的赞助\n\n 3. 一些开发的资源包括源码的获取将更加容易\n\n 4. 避免了原来和红帽上一些法律的问题\n\n\n# 最新版本\n\n最新版本为 centos 8.0.1905。\n\n上游rhel 7主要改进：\n\n内核更新至3.10.0；支持linux容器（docker），open vmware tools及3d图像能即装即用，open jdk7作为缺省jdk，ext4及xfs的lvm快照，转用systemd、firewalld及grub2，xfs作为缺省文件系统，内核空间内的iscsi及fcoe，支持ptpv2，支持40g网卡等。\n\n\n# 特点\n\n1．可以把centos理解为red hat as系列！它完全就是对red hat as进行改进后发布的！各种操作、使用和red hat没有区别！\n\n2．centos完全免费，不存在red hat as4需要序列号的问题。\n\n3．centos独有的yum命令支持在线升级，可以即时更新系统，不像red hat那样需要花钱购买支持服务！\n\n4．centos修正了许多rhel的bug！\n\n5．centos版本说明：centos3.1 等同于 red hat as3 update1 centos3.4 等同于 red hat as3 update4 centos4.0 等同于 red hat as4\n\n与 rhel的关系\n\nrhel 在发行的时候，有两种方式。一种是二进制的发行方式，另外一种是源代码的发行方式。无论是哪一种发行方式，你都可以免费获得（例如从网上下载），并再次发布。但如果你使用了他们的在线升级（包括补丁）或咨询服务，就必须要付费。rhel 一直都提供源代码的发行方式，centos 就是将 rhel 发行的源代码重新编译一次，形成一个可使用的二进制版本。由于 linux 的源代码是 gnu，所以从获得 rhel 的源代码到编译成新的二进制，都是合法。只是 red hat 是商标，所以必须在新的发行版里将red hat 的商标去掉。red hat对这种发行版的态度是："我们其实并不反对这种发行版，真正向我们付费的用户，他们重视的并不是系统本身，而是我们所提供的商业服务。" 所以，centos 可以得到 rhel 的所有功能，甚至是更好的软件。但 centos 并不向用户提供商业支持，当然也不负上任何商业责任。如果你要将你的 rhel 转到 centos 上，因为你不希望为 rhel 升级而付费。当然，你必须有丰富 linux 使用经验，因此 rhel 的商业技术支持对你来说并不重要。比如说，尽管没有rhel的商业支持，2019年也有不少企业选择使用centos，比如著名会议管理系统munpanel。但如果你是单纯的业务型企业，那么还是建议你选购 rhel 软件并购买相应服务。这样可以节省你的 it 管理费用，并可得到专业服务。一句话，选用 centos 还是 rhel，取决于你所在公司是否拥有相应的技术力量。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 中使用阿里云的yum源",frontmatter:{title:"CentOS 7 中使用阿里云的yum源",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/4d8bf7/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/02.CentOS%207%20%E4%B8%AD%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91%E7%9A%84yum%E6%BA%90.html",relativePath:"03.CentOS/01.入门/02.CentOS 7 中使用阿里云的yum源.md",key:"v-a54afc5a",path:"/pages/4d8bf7/",headers:[{level:2,title:"备份原来的yum源",slug:"备份原来的yum源",normalizedTitle:"备份原来的yum源",charIndex:27},{level:2,title:"下载阿里云的CentOS-Base.repo 到/etc/yum.repos.d/",slug:"下载阿里云的centos-base-repo-到-etc-yum-repos-d",normalizedTitle:"下载阿里云的centos-base.repo 到/etc/yum.repos.d/",charIndex:138},{level:2,title:"清理缓存",slug:"清理缓存",normalizedTitle:"清理缓存",charIndex:374},{level:2,title:"生成新的缓存",slug:"生成新的缓存",normalizedTitle:"生成新的缓存",charIndex:404}],headersStr:"备份原来的yum源 下载阿里云的CentOS-Base.repo 到/etc/yum.repos.d/ 清理缓存 生成新的缓存",content:"# CentOS 7 中使用阿里云的yum源\n\n\n# 备份原来的yum源\n\nyum install wget\nmv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup\n\n\n\n# 下载阿里云的CentOS-Base.repo 到/etc/yum.repos.d/\n\n安装wget\n\nwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n\n\n或者\n\ncurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n\n\n\n# 清理缓存\n\nsudo yum clean all\n\n\n\n# 生成新的缓存\n\nsudo yum makecache\n",normalizedContent:"# centos 7 中使用阿里云的yum源\n\n\n# 备份原来的yum源\n\nyum install wget\nmv /etc/yum.repos.d/centos-base.repo /etc/yum.repos.d/centos-base.repo.backup\n\n\n\n# 下载阿里云的centos-base.repo 到/etc/yum.repos.d/\n\n安装wget\n\nwget -o /etc/yum.repos.d/centos-base.repo http://mirrors.aliyun.com/repo/centos-7.repo\n\n\n或者\n\ncurl -o /etc/yum.repos.d/centos-base.repo http://mirrors.aliyun.com/repo/centos-7.repo\n\n\n\n# 清理缓存\n\nsudo yum clean all\n\n\n\n# 生成新的缓存\n\nsudo yum makecache\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 设置 SSH 通过密钥登录",frontmatter:{title:"CentOS 7 设置 SSH 通过密钥登录",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/801448/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/03.CentOS%207%20%E8%AE%BE%E7%BD%AE%20SSH%20%E9%80%9A%E8%BF%87%E5%AF%86%E9%92%A5%E7%99%BB%E5%BD%95.html",relativePath:"03.CentOS/01.入门/03.CentOS 7 设置 SSH 通过密钥登录.md",key:"v-0a8f6788",path:"/pages/801448/",headers:[{level:2,title:"制作密钥对",slug:"制作密钥对",normalizedTitle:"制作密钥对",charIndex:358},{level:2,title:"在服务器上安装公钥",slug:"在服务器上安装公钥",normalizedTitle:"在服务器上安装公钥",charIndex:1429},{level:2,title:"将私钥下载到客户端，然后转换为 PuTTY 能使用的格式",slug:"将私钥下载到客户端-然后转换为-putty-能使用的格式",normalizedTitle:"将私钥下载到客户端，然后转换为 putty 能使用的格式",charIndex:1783}],headersStr:"制作密钥对 在服务器上安装公钥 将私钥下载到客户端，然后转换为 PuTTY 能使用的格式",content:"# CentOS 7 设置 SSH 通过密钥登录\n\n我们一般使用 PuTTY 等 SSH 客户端来远程管理 Linux 服务器。但是，一般的密码方式登录，容易有密码被暴力破解的问题。所以，一般我们会将 SSH 的端口设置为默认的 22 以外的端口，或者禁用 root 账户登录。其实，有一个更好的办法来保证安全，而且让你可以放心地用 root 账户从远程登录——那就是通过密钥方式登录。\n\n密钥形式登录的原理是：利用密钥生成器制作一对密钥——一只公钥和一只私钥。将公钥添加到服务器的某个账户上，然后在客户端利用私钥即可完成认证并登录。这样一来，没有私钥，任何人都无法通过 SSH 暴力破解你的密码来远程登录到系统。此外，如果将公钥复制到其他账户甚至主机，利用私钥也可以登录。\n\n下面来讲解如何在 Linux 服务器上制作密钥对，将公钥添加给账户，设置 SSH，最后通过客户端登录。\n\n\n# 制作密钥对\n\n首先在服务器上制作密钥对。首先用密码登录到你打算使用密钥登录的账户，然后执行以下命令：\n\nssh-keygen\n# 或\nssh-keygen -t rsa -b 4096 -C \"<info@ieooc.com>\"\n\n\n密钥锁码在使用私钥时必须输入，这样就可以保护私钥不被盗用。当然，也可以留空，实现无密码登录。\n\n现在，在 root 用户的家目录中生成了一个 .ssh 的隐藏目录，内含两个密钥文件。id_rsa 为私钥，id_rsa.pub 为公钥。\n\n[root@localhost ~]# ssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/root/.ssh/id_rsa): <== 按 Enter\nCreated directory '/root/.ssh'.\nEnter passphrase (empty for no passphrase): <== 输入密钥锁码，或直接按 Enter 留空\nEnter same passphrase again: <== 再输入一遍密钥锁码\nYour identification has been saved in /root/.ssh/id_rsa. <== 私钥\nYour public key has been saved in /root/.ssh/id_rsa.pub. <== 公钥\nThe key fingerprint is:\nSHA256:ARUt2gVZu6SqS7mn14XRLXwM1XHz1sEnAz6uVqkcfdI root@localhost.localdomain\nThe key's randomart image is:\n+---[RSA 2048]----+\n|   ..+*..ooo+.|\n|    .o +o +o*|\n|    o.=o+o o=|\n|   . o+++++ . |\n|    Sooo* E |\n|   . ....= o  |\n|  o .. .=   |\n|  . oo ..    |\n|  ==      |\n+----[SHA256]-----+\n\n\n将公钥拷贝到远程服务器\n\n[root@localhost ~]# ssh−copy−id username@remotehost\n\n\n\n# 在服务器上安装公钥\n\n 1. 键入以下命令，在服务器上安装公钥：\n\ncd .ssh\ncat id_rsa.pub >> authorized_keys\n\n\n 2. 如此便完成了公钥的安装。为了确保连接成功，请保证以下文件权限正确：\n\nchmod 600 authorized_keys\nchmod 700 ~/.ssh\n\n\n 3. 设置 SSH，打开密钥登录功能\n\n编辑 /etc/ssh/sshd_config 文件，进行如下设置：\n\nPubkeyAuthentication yes\n\n当你完成全部设置，并以密钥方式登录成功后，再禁用密码登录：\n\nPasswordAuthentication no\n\n最后，重启 SSH 服务：\n\nsystemctl restart sshd.service\n\n\n\n# 将私钥下载到客户端，然后转换为 PuTTY 能使用的格式\n\n使用 WinSCP、SFTP 等工具将私钥文件 id_rsa 下载到客户端机器上。然后打开 PuTTYGen，单击 Actions 中的 Load 按钮，载入你刚才下载到的私钥文件。如果你刚才设置了密钥锁码，这时则需要输入。\n\n载入成功后，PuTTYGen 会显示密钥相关的信息。在 Key comment 中键入对密钥的说明信息，然后单击 Save private key 按钮即可将私钥文件存放为 PuTTY 能使用的格式。\n\n今后，当你使用 PuTTY 登录时，可以在左侧的 Connection -> SSH -> Auth 中的 Private key file for authentication: 处选择你的私钥文件，然后即可登录了，过程中只需输入密钥锁码即可。",normalizedContent:"# centos 7 设置 ssh 通过密钥登录\n\n我们一般使用 putty 等 ssh 客户端来远程管理 linux 服务器。但是，一般的密码方式登录，容易有密码被暴力破解的问题。所以，一般我们会将 ssh 的端口设置为默认的 22 以外的端口，或者禁用 root 账户登录。其实，有一个更好的办法来保证安全，而且让你可以放心地用 root 账户从远程登录——那就是通过密钥方式登录。\n\n密钥形式登录的原理是：利用密钥生成器制作一对密钥——一只公钥和一只私钥。将公钥添加到服务器的某个账户上，然后在客户端利用私钥即可完成认证并登录。这样一来，没有私钥，任何人都无法通过 ssh 暴力破解你的密码来远程登录到系统。此外，如果将公钥复制到其他账户甚至主机，利用私钥也可以登录。\n\n下面来讲解如何在 linux 服务器上制作密钥对，将公钥添加给账户，设置 ssh，最后通过客户端登录。\n\n\n# 制作密钥对\n\n首先在服务器上制作密钥对。首先用密码登录到你打算使用密钥登录的账户，然后执行以下命令：\n\nssh-keygen\n# 或\nssh-keygen -t rsa -b 4096 -c \"<info@ieooc.com>\"\n\n\n密钥锁码在使用私钥时必须输入，这样就可以保护私钥不被盗用。当然，也可以留空，实现无密码登录。\n\n现在，在 root 用户的家目录中生成了一个 .ssh 的隐藏目录，内含两个密钥文件。id_rsa 为私钥，id_rsa.pub 为公钥。\n\n[root@localhost ~]# ssh-keygen\ngenerating public/private rsa key pair.\nenter file in which to save the key (/root/.ssh/id_rsa): <== 按 enter\ncreated directory '/root/.ssh'.\nenter passphrase (empty for no passphrase): <== 输入密钥锁码，或直接按 enter 留空\nenter same passphrase again: <== 再输入一遍密钥锁码\nyour identification has been saved in /root/.ssh/id_rsa. <== 私钥\nyour public key has been saved in /root/.ssh/id_rsa.pub. <== 公钥\nthe key fingerprint is:\nsha256:arut2gvzu6sqs7mn14xrlxwm1xhz1senaz6uvqkcfdi root@localhost.localdomain\nthe key's randomart image is:\n+---[rsa 2048]----+\n|   ..+*..ooo+.|\n|    .o +o +o*|\n|    o.=o+o o=|\n|   . o+++++ . |\n|    sooo* e |\n|   . ....= o  |\n|  o .. .=   |\n|  . oo ..    |\n|  ==      |\n+----[sha256]-----+\n\n\n将公钥拷贝到远程服务器\n\n[root@localhost ~]# ssh−copy−id username@remotehost\n\n\n\n# 在服务器上安装公钥\n\n 1. 键入以下命令，在服务器上安装公钥：\n\ncd .ssh\ncat id_rsa.pub >> authorized_keys\n\n\n 2. 如此便完成了公钥的安装。为了确保连接成功，请保证以下文件权限正确：\n\nchmod 600 authorized_keys\nchmod 700 ~/.ssh\n\n\n 3. 设置 ssh，打开密钥登录功能\n\n编辑 /etc/ssh/sshd_config 文件，进行如下设置：\n\npubkeyauthentication yes\n\n当你完成全部设置，并以密钥方式登录成功后，再禁用密码登录：\n\npasswordauthentication no\n\n最后，重启 ssh 服务：\n\nsystemctl restart sshd.service\n\n\n\n# 将私钥下载到客户端，然后转换为 putty 能使用的格式\n\n使用 winscp、sftp 等工具将私钥文件 id_rsa 下载到客户端机器上。然后打开 puttygen，单击 actions 中的 load 按钮，载入你刚才下载到的私钥文件。如果你刚才设置了密钥锁码，这时则需要输入。\n\n载入成功后，puttygen 会显示密钥相关的信息。在 key comment 中键入对密钥的说明信息，然后单击 save private key 按钮即可将私钥文件存放为 putty 能使用的格式。\n\n今后，当你使用 putty 登录时，可以在左侧的 connection -> ssh -> auth 中的 private key file for authentication: 处选择你的私钥文件，然后即可登录了，过程中只需输入密钥锁码即可。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 时间与网络时间同步",frontmatter:{title:"CentOS 7 时间与网络时间同步",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/72cb23/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/04.CentOS%207%20%E6%97%B6%E9%97%B4%E4%B8%8E%E7%BD%91%E7%BB%9C%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5.html",relativePath:"03.CentOS/01.入门/04.CentOS 7 时间与网络时间同步.md",key:"v-cfad191a",path:"/pages/72cb23/",headersStr:null,content:"# CentOS 7 时间与网络时间同步\n\n1、安装ntpdate工具\n\nyum -y install ntp ntpdate\n\n\n2、设置系统时间与网络时间同步\n\nntpdate 0.asia.pool.ntp.org\n\n\n这里主要就是通过时间服务器对系统时间进行同步，所以0.asia.pool.ntp.org并不是固定的，大家可以选择time.nist.gov、time.nuri.net、0.asia.pool.ntp.org、1.asia.pool.ntp.org、2.asia.pool.ntp.org、3.asia.pool.ntp.org中任意一个，只要保证可用就OK。3、将系统时间写入硬件时间\n\nhwclock --systohc\n",normalizedContent:"# centos 7 时间与网络时间同步\n\n1、安装ntpdate工具\n\nyum -y install ntp ntpdate\n\n\n2、设置系统时间与网络时间同步\n\nntpdate 0.asia.pool.ntp.org\n\n\n这里主要就是通过时间服务器对系统时间进行同步，所以0.asia.pool.ntp.org并不是固定的，大家可以选择time.nist.gov、time.nuri.net、0.asia.pool.ntp.org、1.asia.pool.ntp.org、2.asia.pool.ntp.org、3.asia.pool.ntp.org中任意一个，只要保证可用就ok。3、将系统时间写入硬件时间\n\nhwclock --systohc\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 安装Jdk并配置环境变量",frontmatter:{title:"CentOS 7 安装Jdk并配置环境变量",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/b65cee/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/05.CentOS%207%20%E5%AE%89%E8%A3%85Jdk%E5%B9%B6%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F.html",relativePath:"03.CentOS/01.入门/05.CentOS 7 安装Jdk并配置环境变量.md",key:"v-2a54cea8",path:"/pages/b65cee/",headers:[{level:2,title:"环境",slug:"环境",normalizedTitle:"环境",charIndex:19},{level:2,title:"下载jdk-8u251-linux-x64.tar.gz",slug:"下载jdk-8u251-linux-x64-tar-gz",normalizedTitle:"下载jdk-8u251-linux-x64.tar.gz",charIndex:73},{level:2,title:"检查和删除系统预装jdk",slug:"检查和删除系统预装jdk",normalizedTitle:"检查和删除系统预装jdk",charIndex:117},{level:2,title:"上传jdk-8u251-linux-x64.tar.gz到/usr/local/src",slug:"上传jdk-8u251-linux-x64-tar-gz到-usr-local-src",normalizedTitle:"上传jdk-8u251-linux-x64.tar.gz到/usr/local/src",charIndex:309},{level:2,title:"配置环境变量",slug:"配置环境变量",normalizedTitle:"配置环境变量",charIndex:17},{level:2,title:"检查jdk是否生效",slug:"检查jdk是否生效",normalizedTitle:"检查jdk是否生效",charIndex:736}],headersStr:"环境 下载jdk-8u251-linux-x64.tar.gz 检查和删除系统预装jdk 上传jdk-8u251-linux-x64.tar.gz到/usr/local/src 配置环境变量 检查jdk是否生效",content:'# CentOS 7 安装Jdk并配置环境变量\n\n\n# 环境\n\nCentOS Linux release 7.8.2003 (Core)\n\n\n# 下载jdk-8u251-linux-x64.tar.gz\n\n链接地址：官方地址\n\n\n# 检查和删除系统预装jdk\n\n如果操作系统不是最小安装，会默认安装openjdk\n\nrpm -qa | grep java\n\n\n删除系统预装jdk，可以一条命令直接删除\n\nrpm -e --nodeps `rpm -qa | grep java`\n\n\n输入java -version测试，提示bash: java: command not found表面openjdk已经删除\n\n\n# 上传jdk-8u251-linux-x64.tar.gz到/usr/local/src\n\n新建/usr/local/java\n\nmkdir -p /usr/local/java\n\n\n把jdk解压到java目录下\n\ntar -zxvf /usr/local/src/jdk-8u251-linux-x64.tar.gz -C /usr/local/java\n\n\n\n# 配置环境变量\n\nvim /etc/profile\n\n\n在文件末尾插入如下内容\n\nexport JAVA_HOME=/usr/local/java/jdk1.8.0_251\nexport JRE_HOME=${JAVA_HOME}/jre\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\nexport PATH=${JAVA_HOME}/bin:$PATH\n\n\n执行profile\n\nsource /etc/profile\n\n\n\n# 检查jdk是否生效\n\n[root@localhost src]# java -version \njava version "1.8.0_251"\nJava(TM) SE Runtime Environment (build 1.8.0_251-b08)\nJava HotSpot(TM) 64-Bit Server VM (build 25.251-b08, mixed mode)\n\n\n到此，安装配置成功',normalizedContent:'# centos 7 安装jdk并配置环境变量\n\n\n# 环境\n\ncentos linux release 7.8.2003 (core)\n\n\n# 下载jdk-8u251-linux-x64.tar.gz\n\n链接地址：官方地址\n\n\n# 检查和删除系统预装jdk\n\n如果操作系统不是最小安装，会默认安装openjdk\n\nrpm -qa | grep java\n\n\n删除系统预装jdk，可以一条命令直接删除\n\nrpm -e --nodeps `rpm -qa | grep java`\n\n\n输入java -version测试，提示bash: java: command not found表面openjdk已经删除\n\n\n# 上传jdk-8u251-linux-x64.tar.gz到/usr/local/src\n\n新建/usr/local/java\n\nmkdir -p /usr/local/java\n\n\n把jdk解压到java目录下\n\ntar -zxvf /usr/local/src/jdk-8u251-linux-x64.tar.gz -c /usr/local/java\n\n\n\n# 配置环境变量\n\nvim /etc/profile\n\n\n在文件末尾插入如下内容\n\nexport java_home=/usr/local/java/jdk1.8.0_251\nexport jre_home=${java_home}/jre\nexport classpath=.:${java_home}/lib:${jre_home}/lib\nexport path=${java_home}/bin:$path\n\n\n执行profile\n\nsource /etc/profile\n\n\n\n# 检查jdk是否生效\n\n[root@localhost src]# java -version \njava version "1.8.0_251"\njava(tm) se runtime environment (build 1.8.0_251-b08)\njava hotspot(tm) 64-bit server vm (build 25.251-b08, mixed mode)\n\n\n到此，安装配置成功',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 配置静态ip和动态ip",frontmatter:{title:"CentOS 7 配置静态ip和动态ip",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/8a00d6/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/06.CentOS%207%20%E9%85%8D%E7%BD%AE%E9%9D%99%E6%80%81ip%E5%92%8C%E5%8A%A8%E6%80%81ip.html",relativePath:"03.CentOS/01.入门/06.CentOS 7 配置静态ip和动态ip.md",key:"v-0a4aaf0e",path:"/pages/8a00d6/",headers:[{level:2,title:"配置动态ip",slug:"配置动态ip",normalizedTitle:"配置动态ip",charIndex:945},{level:3,title:"修改网络配置",slug:"修改网络配置",normalizedTitle:"修改网络配置",charIndex:979},{level:3,title:"重启网络服务",slug:"重启网络服务",normalizedTitle:"重启网络服务",charIndex:1450},{level:2,title:"配置静态IP",slug:"配置静态ip",normalizedTitle:"配置静态ip",charIndex:2910},{level:3,title:"重启网络服务",slug:"重启网络服务-2",normalizedTitle:"重启网络服务",charIndex:1450},{level:3,title:"完整配置",slug:"完整配置",normalizedTitle:"完整配置",charIndex:3224},{level:3,title:"测试",slug:"测试",normalizedTitle:"测试",charIndex:3746}],headersStr:"配置动态ip 修改网络配置 重启网络服务 配置静态IP 重启网络服务 完整配置 测试",content:'# CentOS 7 配置静态ip和动态ip\n\n有关于centos7获取IP地址的方法主要有两种，1：动态获取ip；2：设置静态IP地址\n\n在配置网络之前我们先要知道centos的网卡名称是什么，centos7不再使用ifconfig命令，可通过命令ip addr查看，如图，网卡名为ens32，是没有IP地址的 编辑网络配置文件之前，先查看自己的网卡名称，我的是ens192。\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n[root@localhost ~]# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 00:0c:29:39:a7:f7 brd ff:ff:ff:ff:ff:ff\n    inet 10.17.8.230/16 brd 10.17.255.255 scope global noprefixroute dynamic ens192\n       valid_lft 85701sec preferred_lft 85701sec\n    inet6 fe80::6f32:b40:a205:a076/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n\n\n\n# 配置动态ip\n\n提示\n\n前提是你的路由器已经开启了DHCP\n\n\n# 修改网络配置\n\n修改网卡配置文件 (ens192为网卡名称)\n\nvi /etc/sysconfig/network-scripts/ifcfg-ens192\n\n\n动态获取IP地址需要修改两处地方即可\n\n * bootproto=dhcp\n * onboot=yes\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n\n\nTYPE="Ethernet"\nPROXY_METHOD="none"\nBROWSER_ONLY="no"\nBOOTPROTO="dhcp"\nDEFROUTE="yes"\nIPV4_FAILURE_FATAL="no"\nIPV6INIT="yes"\nIPV6_AUTOCONF="yes"\nIPV6_DEFROUTE="yes"\nIPV6_FAILURE_FATAL="no"\nIPV6_ADDR_GEN_MODE="stable-privacy"\nNAME="ens192"\nUUID="fcc9095d-1ce4-421c-a2ca-a38e74b9622e"\nDEVICE="ens192"\nONBOOT="yes"\n\n\n\n# 重启网络服务\n\nsystemctl restart network.service\n\n\n这样动态配置IP地址就设置好了，这个时候再查看一下ip addr 就可以看到已经获取了IP地址，且可以上网（ping 百度）\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n[root@localhost ~]# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 00:0c:29:39:a7:f7 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.100/16 brd 10.17.255.255 scope global noprefixroute dynamic ens192\n       valid_lft 85112sec preferred_lft 85112sec\n    inet6 fe80::6f32:b40:a205:a076/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n[root@localhost ~]# ping www.baidu.com\nPING www.a.shifen.com (180.101.49.11) 56(84) bytes of data.\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=1 ttl=47 time=24.6 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=2 ttl=47 time=24.4 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=3 ttl=47 time=24.5 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=4 ttl=47 time=24.5 ms\n^C\n--- www.a.shifen.com ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3004ms\nrtt min/avg/max/mdev = 24.408/24.530/24.600/0.206 ms\n[root@localhost ~]# \n\n\n\n# 配置静态IP\n\n设置静态IP地址与动态IP差不多，也是要修改网卡配置文件。(ens192为网卡名称)\n\nvi /etc/sysconfig/network-scripts/ifcfg-ens192\n\n\n * bootproto=static\n * onboot=yes\n\n在最后加上几行，IP地址、子网掩码、网关、dns服务器\n\nIPADDR=192.168.1.100\nNETMASK=255.255.255.0\nGATEWAY=192.168.1.1\nDNS1=114.114.114.114\nDNS2=8.8.8.8\n\n\n\n# 重启网络服务\n\nsystemctl restart network.service\n\n\n\n# 完整配置\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n\n[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens192 \nTYPE="Ethernet"\nPROXY_METHOD="none"\nBROWSER_ONLY="no"\nBOOTPROTO="static"\nDEFROUTE="yes"\nIPV4_FAILURE_FATAL="no"\nIPV6INIT="yes"\nIPV6_AUTOCONF="yes"\nIPV6_DEFROUTE="yes"\nIPV6_FAILURE_FATAL="no"\nIPV6_ADDR_GEN_MODE="stable-privacy"\nNAME="ens192"\nUUID="fcc9095d-1ce4-421c-a2ca-a38e74b9622e"\nDEVICE="ens192"\nONBOOT="yes"\nIPADDR=192.168.1.100\nNETMASK=255.255.255.0\nGATEWAY=192.168.1.1\nDNS1=114.114.114.114\nDNS2=8.8.8.8\n\n\n\n# 测试\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n[root@localhost ~]# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 00:0c:29:39:a7:f7 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.100/16 brd 10.17.255.255 scope global noprefixroute dynamic ens192\n       valid_lft 85112sec preferred_lft 85112sec\n    inet6 fe80::6f32:b40:a205:a076/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n[root@localhost ~]# ping www.baidu.com\nPING www.a.shifen.com (180.101.49.11) 56(84) bytes of data.\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=1 ttl=47 time=24.6 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=2 ttl=47 time=24.4 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=3 ttl=47 time=24.5 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=4 ttl=47 time=24.5 ms\n^C\n--- www.a.shifen.com ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3004ms\nrtt min/avg/max/mdev = 24.408/24.530/24.600/0.206 ms\n[root@localhost ~]# \n',normalizedContent:'# centos 7 配置静态ip和动态ip\n\n有关于centos7获取ip地址的方法主要有两种，1：动态获取ip；2：设置静态ip地址\n\n在配置网络之前我们先要知道centos的网卡名称是什么，centos7不再使用ifconfig命令，可通过命令ip addr查看，如图，网卡名为ens32，是没有ip地址的 编辑网络配置文件之前，先查看自己的网卡名称，我的是ens192。\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n[root@localhost ~]# ip addr\n1: lo: <loopback,up,lower_up> mtu 65536 qdisc noqueue state unknown group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: ens192: <broadcast,multicast,up,lower_up> mtu 1500 qdisc mq state up group default qlen 1000\n    link/ether 00:0c:29:39:a7:f7 brd ff:ff:ff:ff:ff:ff\n    inet 10.17.8.230/16 brd 10.17.255.255 scope global noprefixroute dynamic ens192\n       valid_lft 85701sec preferred_lft 85701sec\n    inet6 fe80::6f32:b40:a205:a076/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n\n\n\n# 配置动态ip\n\n提示\n\n前提是你的路由器已经开启了dhcp\n\n\n# 修改网络配置\n\n修改网卡配置文件 (ens192为网卡名称)\n\nvi /etc/sysconfig/network-scripts/ifcfg-ens192\n\n\n动态获取ip地址需要修改两处地方即可\n\n * bootproto=dhcp\n * onboot=yes\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n\n\ntype="ethernet"\nproxy_method="none"\nbrowser_only="no"\nbootproto="dhcp"\ndefroute="yes"\nipv4_failure_fatal="no"\nipv6init="yes"\nipv6_autoconf="yes"\nipv6_defroute="yes"\nipv6_failure_fatal="no"\nipv6_addr_gen_mode="stable-privacy"\nname="ens192"\nuuid="fcc9095d-1ce4-421c-a2ca-a38e74b9622e"\ndevice="ens192"\nonboot="yes"\n\n\n\n# 重启网络服务\n\nsystemctl restart network.service\n\n\n这样动态配置ip地址就设置好了，这个时候再查看一下ip addr 就可以看到已经获取了ip地址，且可以上网（ping 百度）\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n[root@localhost ~]# ip addr\n1: lo: <loopback,up,lower_up> mtu 65536 qdisc noqueue state unknown group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: ens192: <broadcast,multicast,up,lower_up> mtu 1500 qdisc mq state up group default qlen 1000\n    link/ether 00:0c:29:39:a7:f7 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.100/16 brd 10.17.255.255 scope global noprefixroute dynamic ens192\n       valid_lft 85112sec preferred_lft 85112sec\n    inet6 fe80::6f32:b40:a205:a076/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n[root@localhost ~]# ping www.baidu.com\nping www.a.shifen.com (180.101.49.11) 56(84) bytes of data.\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=1 ttl=47 time=24.6 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=2 ttl=47 time=24.4 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=3 ttl=47 time=24.5 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=4 ttl=47 time=24.5 ms\n^c\n--- www.a.shifen.com ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3004ms\nrtt min/avg/max/mdev = 24.408/24.530/24.600/0.206 ms\n[root@localhost ~]# \n\n\n\n# 配置静态ip\n\n设置静态ip地址与动态ip差不多，也是要修改网卡配置文件。(ens192为网卡名称)\n\nvi /etc/sysconfig/network-scripts/ifcfg-ens192\n\n\n * bootproto=static\n * onboot=yes\n\n在最后加上几行，ip地址、子网掩码、网关、dns服务器\n\nipaddr=192.168.1.100\nnetmask=255.255.255.0\ngateway=192.168.1.1\ndns1=114.114.114.114\ndns2=8.8.8.8\n\n\n\n# 重启网络服务\n\nsystemctl restart network.service\n\n\n\n# 完整配置\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n\n[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens192 \ntype="ethernet"\nproxy_method="none"\nbrowser_only="no"\nbootproto="static"\ndefroute="yes"\nipv4_failure_fatal="no"\nipv6init="yes"\nipv6_autoconf="yes"\nipv6_defroute="yes"\nipv6_failure_fatal="no"\nipv6_addr_gen_mode="stable-privacy"\nname="ens192"\nuuid="fcc9095d-1ce4-421c-a2ca-a38e74b9622e"\ndevice="ens192"\nonboot="yes"\nipaddr=192.168.1.100\nnetmask=255.255.255.0\ngateway=192.168.1.1\ndns1=114.114.114.114\ndns2=8.8.8.8\n\n\n\n# 测试\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n[root@localhost ~]# ip addr\n1: lo: <loopback,up,lower_up> mtu 65536 qdisc noqueue state unknown group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: ens192: <broadcast,multicast,up,lower_up> mtu 1500 qdisc mq state up group default qlen 1000\n    link/ether 00:0c:29:39:a7:f7 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.100/16 brd 10.17.255.255 scope global noprefixroute dynamic ens192\n       valid_lft 85112sec preferred_lft 85112sec\n    inet6 fe80::6f32:b40:a205:a076/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n[root@localhost ~]# ping www.baidu.com\nping www.a.shifen.com (180.101.49.11) 56(84) bytes of data.\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=1 ttl=47 time=24.6 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=2 ttl=47 time=24.4 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=3 ttl=47 time=24.5 ms\n64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=4 ttl=47 time=24.5 ms\n^c\n--- www.a.shifen.com ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3004ms\nrtt min/avg/max/mdev = 24.408/24.530/24.600/0.206 ms\n[root@localhost ~]# \n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 安装 Nginx",frontmatter:{title:"CentOS 7 安装 Nginx",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/2bd107/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/07.CentOS%207%20%E5%AE%89%E8%A3%85%20Nginx.html",relativePath:"03.CentOS/01.入门/07.CentOS 7 安装 Nginx.md",key:"v-93569b68",path:"/pages/2bd107/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:24},{level:2,title:"添加源",slug:"添加源",normalizedTitle:"添加源",charIndex:704},{level:2,title:"安装Nginx",slug:"安装nginx",normalizedTitle:"安装nginx",charIndex:881}],headersStr:"概述 添加源 安装Nginx",content:"# CentOS 7 安装 Nginx\n\n\n# 概述\n\nNginx (engine x) 是一个高性能的HTTP (opens new window)和反向代理 (opens new window)web服务器，同时也提供了IMAP/POP3/SMTP服务 (opens new window)。Nginx是由伊戈尔·赛索耶夫为俄罗斯 (opens new window)访问量第二的Rambler.ru站点（俄文：Рамблер）开发的，第一个公开版本0.1.0发布于2004年10月4日。\n\n其将源代码 (opens new window)以类BSD许可证的形式发布，因它的稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名 (opens new window)。2011年6月1日，nginx 1.0.4发布。\n\nNginx是一款轻量级 (opens new window)的Web (opens new window)服务器/反向代理 (opens new window)服务器及电子邮件 (opens new window)（IMAP/POP3）代理服务器，在BSD-like 协议下发行。其特点是占有内存少，并发 (opens new window)能力强，事实上nginx的并发能力在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东 (opens new window)、新浪 (opens new window)、网易 (opens new window)、腾讯 (opens new window)、淘宝 (opens new window)等。\n\n\n# 添加源\n\n默认情况Centos7中无Nginx的源，最近发现Nginx官网提供了Centos的源地址。因此可以如下执行命令添加源：\n\nsudo rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm\n\n\n\n# 安装Nginx\n\n通过yum search nginx看看是否已经添加源成功。如果成功则执行下列命令安装Nginx。\n\nyum install -y nginx\n\n\n启动Nginx\n\nsystemctl start nginx.service\n\n\n设置开机自动运行\n\nsystemctl enable nginx.service\n\n\n浏览器访问：http://serverIP,即可看到nginx的默认欢迎页面\n\n至此，nginx部署完成 (゜-゜)つロ 干杯~",normalizedContent:"# centos 7 安装 nginx\n\n\n# 概述\n\nnginx (engine x) 是一个高性能的http (opens new window)和反向代理 (opens new window)web服务器，同时也提供了imap/pop3/smtp服务 (opens new window)。nginx是由伊戈尔·赛索耶夫为俄罗斯 (opens new window)访问量第二的rambler.ru站点（俄文：рамблер）开发的，第一个公开版本0.1.0发布于2004年10月4日。\n\n其将源代码 (opens new window)以类bsd许可证的形式发布，因它的稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名 (opens new window)。2011年6月1日，nginx 1.0.4发布。\n\nnginx是一款轻量级 (opens new window)的web (opens new window)服务器/反向代理 (opens new window)服务器及电子邮件 (opens new window)（imap/pop3）代理服务器，在bsd-like 协议下发行。其特点是占有内存少，并发 (opens new window)能力强，事实上nginx的并发能力在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东 (opens new window)、新浪 (opens new window)、网易 (opens new window)、腾讯 (opens new window)、淘宝 (opens new window)等。\n\n\n# 添加源\n\n默认情况centos7中无nginx的源，最近发现nginx官网提供了centos的源地址。因此可以如下执行命令添加源：\n\nsudo rpm -uvh http://nginx.org/packages/centos/7/noarch/rpms/nginx-release-centos-7-0.el7.ngx.noarch.rpm\n\n\n\n# 安装nginx\n\n通过yum search nginx看看是否已经添加源成功。如果成功则执行下列命令安装nginx。\n\nyum install -y nginx\n\n\n启动nginx\n\nsystemctl start nginx.service\n\n\n设置开机自动运行\n\nsystemctl enable nginx.service\n\n\n浏览器访问：http://serverip,即可看到nginx的默认欢迎页面\n\n至此，nginx部署完成 (゜-゜)つロ 干杯~",charsets:{cyrillic:!0,cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 安装 Tomcat",frontmatter:{title:"CentOS 7 安装 Tomcat",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/b0c71d/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/08.CentOS%207%20%E5%AE%89%E8%A3%85%20Tomcat.html",relativePath:"03.CentOS/01.入门/08.CentOS 7 安装 Tomcat.md",key:"v-eba23aa0",path:"/pages/b0c71d/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:25},{level:2,title:"Tomcat 下载和解压",slug:"tomcat-下载和解压",normalizedTitle:"tomcat 下载和解压",charIndex:290},{level:2,title:"设置开启启动",slug:"设置开启启动",normalizedTitle:"设置开启启动",charIndex:639},{level:3,title:"为Tomcat添加启动参数",slug:"为tomcat添加启动参数",normalizedTitle:"为tomcat添加启动参数",charIndex:679},{level:3,title:"编写tomcat.service文件",slug:"编写tomcat-service文件",normalizedTitle:"编写tomcat.service文件",charIndex:1018},{level:3,title:"将Tomcat加入服务管理",slug:"将tomcat加入服务管理",normalizedTitle:"将tomcat加入服务管理",charIndex:1549}],headersStr:"概述 Tomcat 下载和解压 设置开启启动 为Tomcat添加启动参数 编写tomcat.service文件 将Tomcat加入服务管理",content:'# CentOS 7 安装 Tomcat\n\n\n# 概述\n\nTomcat是Apache 软件基金会（Apache Software Foundation）的Jakarta 项目中的一个核心项目，由Apache、Sun 和其他一些公司及个人共同开发而成。由于有了Sun 的参与和支持，最新的Servlet 和JSP 规范总是能在Tomcat 中得到体现，Tomcat 5支持最新的Servlet 2.4 和JSP 2.0 规范。因为Tomcat 技术先进、性能稳定，而且免费，因而深受Java 爱好者的喜爱并得到了部分软件开发商的认可，成为目前比较流行的Web 应用服务器。\n\n\n# Tomcat 下载和解压\n\nSetup 1 :切换目录\n\ncd /opt\n\n\nSetup 2 :下载tomcat8\n\nwget https://archive.apache.org/dist/tomcat/tomcat-8/v8.0.53/bin/apache-tomcat-8.0.53.tar.gz\n\n\nSetup 3 :解压\n\ntar -zxvf apache-tomcat-8.0.53.tar.gz\n\n\nSetup 4 :创建符号链接tomcat8，因为解压出来的包名比较长，为了方便\n\nln -s apache-tomcat-8.0.53 tomcat\n\n\nSetup 5 :可以选择删除tomcat包\n\nrm -rf apache-tomcat-8.0.53.tar.gz\n\n\n\n# 设置开启启动\n\n提示\n\nTomcat的安装路径为/opt/tomcat\n\n\n# 为Tomcat添加启动参数\n\ncatalina.sh在执行的时候会调用同级路径下的setenv.sh来设置额外的环境变量，因此在/opt/tomcat/bin路径下创建setenv.sh文件，内容如下：\n\nexport CATALINA_HOME=/opt/tomcat\nexport CATALINA_BASE=/opt/tomcat\n#设置Tomcat的PID文件\nCATALINA_PID="$CATALINA_BASE/tomcat.pid"\n#添加JVM选项\nJAVA_OPTS="-server -XX:PermSize=256M -XX:MaxPermSize=1024m -Xms512M -Xmx1024M -XX:MaxNewSize=256m"\n\n\n\n# 编写tomcat.service文件\n\n在/usr/lib/systemd/system路径下添加tomcat.service文件，内容如下：\n\n[Unit]\nDescription=Tomcat\nAfter=syslog.target network.target remote-fs.target nss-lookup.target\n\n[Service]\nType=forking\nPIDFile=/opt/tomcat/tomcat.pid\nExecStart=/opt/tomcat/bin/startup.sh\nExecReload=/bin/kill -s HUP $MAINPID\nExecStop=/bin/kill -s QUIT $MAINPID\nPrivateTmp=true\n\n[Install]\nWantedBy=multi-user.target\n\n\n提示\n\n定义控制单元 [Unit] 配置了服务的描述，规定了在network启动之后执行\n\n声明服务类型 [Service] 配置服务的pid，服务的启动，停止，重启\n\n安装服务 [install] 在 multi-user.target 启用时，我们的服务也就会被启用了\n\n\n# 将Tomcat加入服务管理\n\n# 开机自动启动\nsystemctl enable tomcat.service\n# 禁用开机自启东\nsystemctl disable tomcat.service\n# 启动服务\nsystemctl start tomcat.service\n# 停止服务\nsystemctl stop tomcat.service\n# 重启服务\nsystemctl restart tomcat.service\n\n\n浏览器访问：http://serverIP:8080,即可看到tomcat的默认欢迎页面\n\n至此，tomcat部署完成 (゜-゜)つロ 干杯~',normalizedContent:'# centos 7 安装 tomcat\n\n\n# 概述\n\ntomcat是apache 软件基金会（apache software foundation）的jakarta 项目中的一个核心项目，由apache、sun 和其他一些公司及个人共同开发而成。由于有了sun 的参与和支持，最新的servlet 和jsp 规范总是能在tomcat 中得到体现，tomcat 5支持最新的servlet 2.4 和jsp 2.0 规范。因为tomcat 技术先进、性能稳定，而且免费，因而深受java 爱好者的喜爱并得到了部分软件开发商的认可，成为目前比较流行的web 应用服务器。\n\n\n# tomcat 下载和解压\n\nsetup 1 :切换目录\n\ncd /opt\n\n\nsetup 2 :下载tomcat8\n\nwget https://archive.apache.org/dist/tomcat/tomcat-8/v8.0.53/bin/apache-tomcat-8.0.53.tar.gz\n\n\nsetup 3 :解压\n\ntar -zxvf apache-tomcat-8.0.53.tar.gz\n\n\nsetup 4 :创建符号链接tomcat8，因为解压出来的包名比较长，为了方便\n\nln -s apache-tomcat-8.0.53 tomcat\n\n\nsetup 5 :可以选择删除tomcat包\n\nrm -rf apache-tomcat-8.0.53.tar.gz\n\n\n\n# 设置开启启动\n\n提示\n\ntomcat的安装路径为/opt/tomcat\n\n\n# 为tomcat添加启动参数\n\ncatalina.sh在执行的时候会调用同级路径下的setenv.sh来设置额外的环境变量，因此在/opt/tomcat/bin路径下创建setenv.sh文件，内容如下：\n\nexport catalina_home=/opt/tomcat\nexport catalina_base=/opt/tomcat\n#设置tomcat的pid文件\ncatalina_pid="$catalina_base/tomcat.pid"\n#添加jvm选项\njava_opts="-server -xx:permsize=256m -xx:maxpermsize=1024m -xms512m -xmx1024m -xx:maxnewsize=256m"\n\n\n\n# 编写tomcat.service文件\n\n在/usr/lib/systemd/system路径下添加tomcat.service文件，内容如下：\n\n[unit]\ndescription=tomcat\nafter=syslog.target network.target remote-fs.target nss-lookup.target\n\n[service]\ntype=forking\npidfile=/opt/tomcat/tomcat.pid\nexecstart=/opt/tomcat/bin/startup.sh\nexecreload=/bin/kill -s hup $mainpid\nexecstop=/bin/kill -s quit $mainpid\nprivatetmp=true\n\n[install]\nwantedby=multi-user.target\n\n\n提示\n\n定义控制单元 [unit] 配置了服务的描述，规定了在network启动之后执行\n\n声明服务类型 [service] 配置服务的pid，服务的启动，停止，重启\n\n安装服务 [install] 在 multi-user.target 启用时，我们的服务也就会被启用了\n\n\n# 将tomcat加入服务管理\n\n# 开机自动启动\nsystemctl enable tomcat.service\n# 禁用开机自启东\nsystemctl disable tomcat.service\n# 启动服务\nsystemctl start tomcat.service\n# 停止服务\nsystemctl stop tomcat.service\n# 重启服务\nsystemctl restart tomcat.service\n\n\n浏览器访问：http://serverip:8080,即可看到tomcat的默认欢迎页面\n\n至此，tomcat部署完成 (゜-゜)つロ 干杯~',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 安装 MySQL",frontmatter:{title:"CentOS 7 安装 MySQL",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/3748a2/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/09.CentOS%207%20%E5%AE%89%E8%A3%85%20MySQL.html",relativePath:"03.CentOS/01.入门/09.CentOS 7 安装 MySQL.md",key:"v-2d9e6cf8",path:"/pages/3748a2/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:24},{level:2,title:"设置Yum源",slug:"设置yum源",normalizedTitle:"设置yum源",charIndex:385},{level:3,title:"下载 MySQL Yum Repository",slug:"下载-mysql-yum-repository",normalizedTitle:"下载 mysql yum repository",charIndex:396},{level:3,title:"安装Yum源",slug:"安装yum源",normalizedTitle:"安装yum源",charIndex:497},{level:3,title:"验证下是否添加成功",slug:"验证下是否添加成功",normalizedTitle:"验证下是否添加成功",charIndex:617},{level:3,title:"选择要启用 MySQL 版本",slug:"选择要启用-mysql-版本",normalizedTitle:"选择要启用 mysql 版本",charIndex:683},{level:2,title:"通过 Yum 来安装 MySQL",slug:"通过-yum-来安装-mysql",normalizedTitle:"通过 yum 来安装 mysql",charIndex:750},{level:2,title:"启动和关闭 MySQL Server",slug:"启动和关闭-mysql-server",normalizedTitle:"启动和关闭 mysql server",charIndex:853},{level:3,title:"启动 MySQL Server",slug:"启动-mysql-server",normalizedTitle:"启动 mysql server",charIndex:876},{level:3,title:"查看 MySQL Server 状态",slug:"查看-mysql-server-状态",normalizedTitle:"查看 mysql server 状态",charIndex:929},{level:3,title:"关闭 MySQL Server",slug:"关闭-mysql-server",normalizedTitle:"关闭 mysql server",charIndex:856},{level:3,title:"开机自启动",slug:"开机自启动",normalizedTitle:"开机自启动",charIndex:1038},{level:3,title:"测试是否安装成功",slug:"测试是否安装成功",normalizedTitle:"测试是否安装成功",charIndex:1082},{level:2,title:"MySQL 安全设置",slug:"mysql-安全设置",normalizedTitle:"mysql 安全设置",charIndex:1103},{level:2,title:"远程访问设置",slug:"远程访问设置",normalizedTitle:"远程访问设置",charIndex:1159},{level:2,title:"修改配置文件",slug:"修改配置文件",normalizedTitle:"修改配置文件",charIndex:1407},{level:2,title:"重启 MySQL Server",slug:"重启-mysql-server",normalizedTitle:"重启 mysql server",charIndex:1683}],headersStr:"概述 设置Yum源 下载 MySQL Yum Repository 安装Yum源 验证下是否添加成功 选择要启用 MySQL 版本 通过 Yum 来安装 MySQL 启动和关闭 MySQL Server 启动 MySQL Server 查看 MySQL Server 状态 关闭 MySQL Server 开机自启动 测试是否安装成功 MySQL 安全设置 远程访问设置 修改配置文件 重启 MySQL Server",content:"# CentOS 7 安装 MySQL\n\n\n# 概述\n\nMySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一。\n\nMySQL是一种关系型数据库管理系统，关系数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。\n\nMySQL所使用的 SQL 语言是用于访问数据库的最常用标准化语言。MySQL 软件采用了双授权政策，分为社区版和商业版，由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，一般中小型网站的开发都选择 MySQL 作为网站数据库。\n\n\n# 设置Yum源\n\n\n# 下载 MySQL Yum Repository\n\nwget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm\n\n\n\n# 安装Yum源\n\n添加 MySQL Yum Repository 到你的系统 repository 列表中，执行\n\nyum localinstall mysql-community-release-el7-5.noarch.rpm\n\n\n\n# 验证下是否添加成功\n\nyum repolist enabled | grep \"mysql.*-community.*\"\n\n\n\n# 选择要启用 MySQL 版本\n\n查看 MySQL 版本，执行\n\nyum repolist all | grep mysql\n\n\n\n# 通过 Yum 来安装 MySQL\n\nyum install mysql-community-server\n\n\n可以看到 MySQL 的安装目录是 /usr/bin/\n\nwhereis mysql\n\n\n\n# 启动和关闭 MySQL Server\n\n\n# 启动 MySQL Server\n\nsystemctl start mysqld.service\n\n\n\n# 查看 MySQL Server 状态\n\nsystemctl status mysqld.service\n\n\n\n# 关闭 MySQL Server\n\nsystemctl stop mysqld.service\n\n\n\n# 开机自启动\n\nsystemctl enable mysqld.service\n\n\n\n# 测试是否安装成功\n\nmysql\n\n\n\n# MySQL 安全设置\n\n服务器启动后，可以执行\n\nmysql_secure_installation\n\n\n\n# 远程访问设置\n\n允许mysql远程访问，mysql默认是不允许远程访问的.\n\nmysql -u root -p xxxx\nmysql>use mysql\nmysql>grant all privileges on *.* to 'root'@'%' identified by 'xxxxx(密码)';\nmysql>flush privileges;\n\n\n注意\n\n注意一定要替换xxxxx(密码)，密码可以和上一步安全配置过程中设置的密码相同，也可以不同，这个密码用于远程访问，注意区分\n\n\n# 修改配置文件\n\n编辑/etc/my.cnf在[mysqld]节点下添加如下内容：\n\nvi /etc/my.cnf\n\n\n[mysqld]\n# 开启计划任务\nevent_scheduler = on\n# 空闲连接超时时间 默认值是8小时(60*60*8=28800)\nwait_timeout=288000\ninteractive_timeout=288000\n# 最大数据包\nmax_allowed_packet=256M\n# 全文检索\nft_min_word_len=1\n# 区分大小写设置\nlower_case_table_names=1\n\n\n\n# 重启 MySQL Server\n\nsystemctl restart mysqld.service\n\n\n至此，MySQL 部署完成 (゜-゜)つロ 干杯~",normalizedContent:"# centos 7 安装 mysql\n\n\n# 概述\n\nmysql是一个关系型数据库管理系统，由瑞典mysql ab 公司开发，属于 oracle 旗下产品。mysql 是最流行的关系型数据库管理系统之一，在 web 应用方面，mysql是最好的 rdbms (relational database management system，关系数据库管理系统) 应用软件之一。\n\nmysql是一种关系型数据库管理系统，关系数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。\n\nmysql所使用的 sql 语言是用于访问数据库的最常用标准化语言。mysql 软件采用了双授权政策，分为社区版和商业版，由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，一般中小型网站的开发都选择 mysql 作为网站数据库。\n\n\n# 设置yum源\n\n\n# 下载 mysql yum repository\n\nwget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm\n\n\n\n# 安装yum源\n\n添加 mysql yum repository 到你的系统 repository 列表中，执行\n\nyum localinstall mysql-community-release-el7-5.noarch.rpm\n\n\n\n# 验证下是否添加成功\n\nyum repolist enabled | grep \"mysql.*-community.*\"\n\n\n\n# 选择要启用 mysql 版本\n\n查看 mysql 版本，执行\n\nyum repolist all | grep mysql\n\n\n\n# 通过 yum 来安装 mysql\n\nyum install mysql-community-server\n\n\n可以看到 mysql 的安装目录是 /usr/bin/\n\nwhereis mysql\n\n\n\n# 启动和关闭 mysql server\n\n\n# 启动 mysql server\n\nsystemctl start mysqld.service\n\n\n\n# 查看 mysql server 状态\n\nsystemctl status mysqld.service\n\n\n\n# 关闭 mysql server\n\nsystemctl stop mysqld.service\n\n\n\n# 开机自启动\n\nsystemctl enable mysqld.service\n\n\n\n# 测试是否安装成功\n\nmysql\n\n\n\n# mysql 安全设置\n\n服务器启动后，可以执行\n\nmysql_secure_installation\n\n\n\n# 远程访问设置\n\n允许mysql远程访问，mysql默认是不允许远程访问的.\n\nmysql -u root -p xxxx\nmysql>use mysql\nmysql>grant all privileges on *.* to 'root'@'%' identified by 'xxxxx(密码)';\nmysql>flush privileges;\n\n\n注意\n\n注意一定要替换xxxxx(密码)，密码可以和上一步安全配置过程中设置的密码相同，也可以不同，这个密码用于远程访问，注意区分\n\n\n# 修改配置文件\n\n编辑/etc/my.cnf在[mysqld]节点下添加如下内容：\n\nvi /etc/my.cnf\n\n\n[mysqld]\n# 开启计划任务\nevent_scheduler = on\n# 空闲连接超时时间 默认值是8小时(60*60*8=28800)\nwait_timeout=288000\ninteractive_timeout=288000\n# 最大数据包\nmax_allowed_packet=256m\n# 全文检索\nft_min_word_len=1\n# 区分大小写设置\nlower_case_table_names=1\n\n\n\n# 重启 mysql server\n\nsystemctl restart mysqld.service\n\n\n至此，mysql 部署完成 (゜-゜)つロ 干杯~",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 安装 MongoDB",frontmatter:{title:"CentOS 7 安装 MongoDB",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/866b2e/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/10.CentOS%207%20%E5%AE%89%E8%A3%85%20MongoDB.html",relativePath:"03.CentOS/01.入门/10.CentOS 7 安装 MongoDB.md",key:"v-7d6fc128",path:"/pages/866b2e/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:26},{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:279},{level:2,title:"配置yum源",slug:"配置yum源",normalizedTitle:"配置yum源",charIndex:377},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:11},{level:3,title:"修改配置文件",slug:"修改配置文件",normalizedTitle:"修改配置文件",charIndex:733},{level:3,title:"启动服务",slug:"启动服务",normalizedTitle:"启动服务",charIndex:1799},{level:3,title:"连接MongoDB数据库",slug:"连接mongodb数据库",normalizedTitle:"连接mongodb数据库",charIndex:1846},{level:3,title:"创建验证用户, 提示rootpassword替换成你自己的密码",slug:"创建验证用户-提示rootpassword替换成你自己的密码",normalizedTitle:"创建验证用户, 提示rootpassword替换成你自己的密码",charIndex:1899},{level:3,title:"修改配置文件，开启认证",slug:"修改配置文件-开启认证",normalizedTitle:"修改配置文件，开启认证",charIndex:2032},{level:3,title:"添加上验证，重启mongodb服务",slug:"添加上验证-重启mongodb服务",normalizedTitle:"添加上验证，重启mongodb服务",charIndex:2112},{level:3,title:"登录验证",slug:"登录验证",normalizedTitle:"登录验证",charIndex:2169},{level:3,title:"添加普通用户",slug:"添加普通用户",normalizedTitle:"添加普通用户",charIndex:2270},{level:3,title:"建立普通账户",slug:"建立普通账户",normalizedTitle:"建立普通账户",charIndex:2381},{level:3,title:"普通用户登录",slug:"普通用户登录",normalizedTitle:"普通用户登录",charIndex:2573}],headersStr:"概述 前言 配置yum源 安装 修改配置文件 启动服务 连接MongoDB数据库 创建验证用户, 提示rootpassword替换成你自己的密码 修改配置文件，开启认证 添加上验证，重启mongodb服务 登录验证 添加普通用户 建立普通账户 普通用户登录",content:'# CentOS 7 安装 MongoDB\n\n\n# 概述\n\nMongoDB是一个基于分布式文件存储 [1] 的数据库。由C++语言编写。旨在为WEB应用提供可扩展的高性能数据存储解决方案。\n\nMongoDB是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似json的bson格式，因此可以存储比较复杂的数据类型。Mongo最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。\n\n\n# 前言\n\nCentOS上安装epel-release的yum源之后就可以安装MongoDB，但是版本都是比较老的，如果使用MongoDB官方的yum就可以安装到比较新版本的MongoDB。\n\n\n# 配置yum源\n\n创建一个/etc/yum.repos.d/mongodb-org-3.6.repo文件\n\nsudo vi /etc/yum.repos.d/mongodb-org-3.6.repo\n\n\n添加如下内容：\n\n[mongodb-org-3.6]\nname=MongoDB Repository\nbaseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.6/x86_64/\ngpgcheck=1\nenabled=1\ngpgkey=https://www.mongodb.org/static/pgp/server-3.6.asc\n\n\n\n# 安装\n\nsudo yum install -y mongodb-org\n\n\n\n# 修改配置文件\n\nsudo vi /etc/mongod.conf\n\n\n将net.bindIp设置为0.0.0.0允许远程访问，主要用户部署过程中的数据导入和调试。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# mongod.conf\n\n# for documentation of all options, see:\n#   http://docs.mongodb.org/manual/reference/configuration-options/\n\n# where to write logging data.\nsystemLog:\n  destination: file\n  logAppend: true\n  path: /var/log/mongodb/mongod.log\n\n# Where and how to store data.\nstorage:\n  dbPath: /var/lib/mongo\n  journal:\n    enabled: true\n#  engine:\n#  mmapv1:\n#  wiredTiger:\n\n# how the process runs\nprocessManagement:\n  fork: true  # fork and run in background\n  pidFilePath: /var/run/mongodb/mongod.pid  # location of pidfile\n  timeZoneInfo: /usr/share/zoneinfo\n\n# network interfaces\nnet:\n  port: 27017\n  bindIp: 0.0.0.0  # Listen to local interface only, comment to listen on all interfaces.\n\n#security:\n#  authorization: enabled  # 这里是开启验证功能，暂时先关闭，等创建完root用户再开起来进行验证\n#operationProfiling:\n\n#replication:\n\n#sharding:\n\n## Enterprise-Only Options\n\n#auditLog:\n\n#snmp:\n\n\n注意\n\n先注释掉security.authorization配置，这个配置用于启用权限认证，后续创建完用户时再打开，注意空格缩进\n\n\n# 启动服务\n\nsudo systemctl start mongod.service\n\n\n\n# 连接MongoDB数据库\n\n直接使用mongo命令进行连接，默认端口是27017\n\nmongo\n\n\n\n# 创建验证用户, 提示rootpassword替换成你自己的密码\n\n> use admin\n> db.createUser({user:"root",pwd:"rootpassword",roles:[{role:"root",db:"admin"}]})\n\n\n\n# 修改配置文件，开启认证\n\nsudo vi /etc/mongod.conf\n\n\nsecurity:\n  authorization: enabled\n\n\n\n# 添加上验证，重启mongodb服务\n\nsystemctl restart mongod.service\n\n\n\n# 登录验证\n\n提示\n\n提示rootpassword替换成你自己的密码\n\nmongo -u root -p rootpassword --authenticationDatabase admin\n\n\n\n# 添加普通用户\n\n提示\n\n使用用户管理账户登录认证，提示rootpassword替换成你自己的密码\n\n# mongo\n> use admin\n> db.auth(\'root\', \'rootpassword\')\n1\n\n\n\n# 建立普通账户\n\nuse portal\n\n\ndb.createUser({\n    "user": "portal",\n    "pwd": "heyuqiang",\n    "roles": [\n        {\n            "role": "readWrite",\n            "db": "portal"\n        }\n    ]\n})\n\n\n\n# 普通用户登录\n\ndb.auth(\'portal\', \'heyuqiang\')\n1\n\n\n至此，MongoDB 部署完成 (゜-゜)つロ 干杯~',normalizedContent:'# centos 7 安装 mongodb\n\n\n# 概述\n\nmongodb是一个基于分布式文件存储 [1] 的数据库。由c++语言编写。旨在为web应用提供可扩展的高性能数据存储解决方案。\n\nmongodb是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似json的bson格式，因此可以存储比较复杂的数据类型。mongo最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。\n\n\n# 前言\n\ncentos上安装epel-release的yum源之后就可以安装mongodb，但是版本都是比较老的，如果使用mongodb官方的yum就可以安装到比较新版本的mongodb。\n\n\n# 配置yum源\n\n创建一个/etc/yum.repos.d/mongodb-org-3.6.repo文件\n\nsudo vi /etc/yum.repos.d/mongodb-org-3.6.repo\n\n\n添加如下内容：\n\n[mongodb-org-3.6]\nname=mongodb repository\nbaseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.6/x86_64/\ngpgcheck=1\nenabled=1\ngpgkey=https://www.mongodb.org/static/pgp/server-3.6.asc\n\n\n\n# 安装\n\nsudo yum install -y mongodb-org\n\n\n\n# 修改配置文件\n\nsudo vi /etc/mongod.conf\n\n\n将net.bindip设置为0.0.0.0允许远程访问，主要用户部署过程中的数据导入和调试。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# mongod.conf\n\n# for documentation of all options, see:\n#   http://docs.mongodb.org/manual/reference/configuration-options/\n\n# where to write logging data.\nsystemlog:\n  destination: file\n  logappend: true\n  path: /var/log/mongodb/mongod.log\n\n# where and how to store data.\nstorage:\n  dbpath: /var/lib/mongo\n  journal:\n    enabled: true\n#  engine:\n#  mmapv1:\n#  wiredtiger:\n\n# how the process runs\nprocessmanagement:\n  fork: true  # fork and run in background\n  pidfilepath: /var/run/mongodb/mongod.pid  # location of pidfile\n  timezoneinfo: /usr/share/zoneinfo\n\n# network interfaces\nnet:\n  port: 27017\n  bindip: 0.0.0.0  # listen to local interface only, comment to listen on all interfaces.\n\n#security:\n#  authorization: enabled  # 这里是开启验证功能，暂时先关闭，等创建完root用户再开起来进行验证\n#operationprofiling:\n\n#replication:\n\n#sharding:\n\n## enterprise-only options\n\n#auditlog:\n\n#snmp:\n\n\n注意\n\n先注释掉security.authorization配置，这个配置用于启用权限认证，后续创建完用户时再打开，注意空格缩进\n\n\n# 启动服务\n\nsudo systemctl start mongod.service\n\n\n\n# 连接mongodb数据库\n\n直接使用mongo命令进行连接，默认端口是27017\n\nmongo\n\n\n\n# 创建验证用户, 提示rootpassword替换成你自己的密码\n\n> use admin\n> db.createuser({user:"root",pwd:"rootpassword",roles:[{role:"root",db:"admin"}]})\n\n\n\n# 修改配置文件，开启认证\n\nsudo vi /etc/mongod.conf\n\n\nsecurity:\n  authorization: enabled\n\n\n\n# 添加上验证，重启mongodb服务\n\nsystemctl restart mongod.service\n\n\n\n# 登录验证\n\n提示\n\n提示rootpassword替换成你自己的密码\n\nmongo -u root -p rootpassword --authenticationdatabase admin\n\n\n\n# 添加普通用户\n\n提示\n\n使用用户管理账户登录认证，提示rootpassword替换成你自己的密码\n\n# mongo\n> use admin\n> db.auth(\'root\', \'rootpassword\')\n1\n\n\n\n# 建立普通账户\n\nuse portal\n\n\ndb.createuser({\n    "user": "portal",\n    "pwd": "heyuqiang",\n    "roles": [\n        {\n            "role": "readwrite",\n            "db": "portal"\n        }\n    ]\n})\n\n\n\n# 普通用户登录\n\ndb.auth(\'portal\', \'heyuqiang\')\n1\n\n\n至此，mongodb 部署完成 (゜-゜)つロ 干杯~',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 安装 NodeJS",frontmatter:{title:"CentOS 7 安装 NodeJS",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/39ed97/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/11.CentOS%207%20%E5%AE%89%E8%A3%85%20NodeJS.html",relativePath:"03.CentOS/01.入门/11.CentOS 7 安装 NodeJS.md",key:"v-98da2ee0",path:"/pages/39ed97/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:25},{level:2,title:"下载",slug:"下载",normalizedTitle:"下载",charIndex:330},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:11},{level:3,title:"下载完成后解压",slug:"下载完成后解压",normalizedTitle:"下载完成后解压",charIndex:518},{level:3,title:"重命名",slug:"重命名",normalizedTitle:"重命名",charIndex:573},{level:3,title:"配置环境变量",slug:"配置环境变量",normalizedTitle:"配置环境变量",charIndex:624},{level:3,title:"保存退出（:wq）执行命令是更改生效",slug:"保存退出-wq-执行命令是更改生效",normalizedTitle:"保存退出（:wq）执行命令是更改生效",charIndex:748},{level:3,title:"检查",slug:"检查",normalizedTitle:"检查",charIndex:793}],headersStr:"概述 下载 安装 下载完成后解压 重命名 配置环境变量 保存退出（:wq）执行命令是更改生效 检查",content:"# CentOS 7 安装 NodeJS\n\n\n# 概述\n\nNode.js发布于2009年5月，由Ryan Dahl开发，是一个基于Chrome V8引擎的JavaScript运行环境，使用了一个事件驱动、非阻塞式I/O模型， [1] 让JavaScript 运行在服务端的开发平台，它让JavaScript成为与PHP、Python、Perl、Ruby等服务端语言平起平坐的脚本语言。 [2]\n\nNode.js对一些特殊用例进行优化，提供替代的API，使得V8在非浏览器环境下运行得更好，V8引擎执行Javascript的速度非常快，性能非常好，基于Chrome JavaScript运行时建立的平台， 用于方便地搭建响应速度快、易于扩展的网络应用。\n\n\n# 下载\n\n首先打开官网下载地址：https://nodejs.org/dist/v11.15.0/\n\n我们下载 v11.15.0 版本：node-v11.15.0-linux-x64.tar.gz\n\ncd /opt\nwget https://nodejs.org/dist/v11.15.0/node-v11.15.0-linux-x64.tar.gz\n\n\n\n# 安装\n\n\n# 下载完成后解压\n\ntar -zxvf node-v11.15.0-linux-x64.tar.gz\n\n\n\n# 重命名\n\nmv node-v11.15.0-linux-x64 node-v11.15.0\n\n\n\n# 配置环境变量\n\nvi /etc/profile\n\n\n在最后边添加\n\n#set for nodejs\nexport NODE_HOME=/opt/node-v11.15.0\nexport PATH=$NODE_HOME/bin:$PATH\n\n\n\n# 保存退出（:wq）执行命令是更改生效\n\nsource /etc/profile\n\n\n\n# 检查\n\n使用命令查看版本，出现v11.15.0版本号则表示成功\n\nnode -v\n\n\n至此，NodeJS 部署完成 (゜-゜)つロ 干杯~",normalizedContent:"# centos 7 安装 nodejs\n\n\n# 概述\n\nnode.js发布于2009年5月，由ryan dahl开发，是一个基于chrome v8引擎的javascript运行环境，使用了一个事件驱动、非阻塞式i/o模型， [1] 让javascript 运行在服务端的开发平台，它让javascript成为与php、python、perl、ruby等服务端语言平起平坐的脚本语言。 [2]\n\nnode.js对一些特殊用例进行优化，提供替代的api，使得v8在非浏览器环境下运行得更好，v8引擎执行javascript的速度非常快，性能非常好，基于chrome javascript运行时建立的平台， 用于方便地搭建响应速度快、易于扩展的网络应用。\n\n\n# 下载\n\n首先打开官网下载地址：https://nodejs.org/dist/v11.15.0/\n\n我们下载 v11.15.0 版本：node-v11.15.0-linux-x64.tar.gz\n\ncd /opt\nwget https://nodejs.org/dist/v11.15.0/node-v11.15.0-linux-x64.tar.gz\n\n\n\n# 安装\n\n\n# 下载完成后解压\n\ntar -zxvf node-v11.15.0-linux-x64.tar.gz\n\n\n\n# 重命名\n\nmv node-v11.15.0-linux-x64 node-v11.15.0\n\n\n\n# 配置环境变量\n\nvi /etc/profile\n\n\n在最后边添加\n\n#set for nodejs\nexport node_home=/opt/node-v11.15.0\nexport path=$node_home/bin:$path\n\n\n\n# 保存退出（:wq）执行命令是更改生效\n\nsource /etc/profile\n\n\n\n# 检查\n\n使用命令查看版本，出现v11.15.0版本号则表示成功\n\nnode -v\n\n\n至此，nodejs 部署完成 (゜-゜)つロ 干杯~",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 安装 Gitlab",frontmatter:{title:"CentOS 7 安装 Gitlab",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/5cddbb/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/12.CentOS%207%20%E5%AE%89%E8%A3%85%20Gitlab.html",relativePath:"03.CentOS/01.入门/12.CentOS 7 安装 Gitlab.md",key:"v-9bb044a0",path:"/pages/5cddbb/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:25},{level:2,title:"下载",slug:"下载",normalizedTitle:"下载",charIndex:118},{level:3,title:"安装并配置必要的依赖项",slug:"安装并配置必要的依赖项",normalizedTitle:"安装并配置必要的依赖项",charIndex:282},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:11},{level:3,title:"拷贝安装包并安装",slug:"拷贝安装包并安装",normalizedTitle:"拷贝安装包并安装",charIndex:504},{level:3,title:"修改gitlab配置文件指定服务器ip和自定义端口：",slug:"修改gitlab配置文件指定服务器ip和自定义端口",normalizedTitle:"修改gitlab配置文件指定服务器ip和自定义端口：",charIndex:665},{level:3,title:"重置并启动GitLab",slug:"重置并启动gitlab",normalizedTitle:"重置并启动gitlab",charIndex:792},{level:2,title:"创建Access Token",slug:"创建access-token",normalizedTitle:"创建access token",charIndex:877},{level:2,title:"完全卸载删除gitlab",slug:"完全卸载删除gitlab",normalizedTitle:"完全卸载删除gitlab",charIndex:1172},{level:3,title:"停止gitlab",slug:"停止gitlab",normalizedTitle:"停止gitlab",charIndex:1218},{level:3,title:"卸载gitlab（注意这里写的是gitlab-ce）",slug:"卸载gitlab-注意这里写的是gitlab-ce",normalizedTitle:"卸载gitlab（注意这里写的是gitlab-ce）",charIndex:1249},{level:3,title:"查看gitlab进程",slug:"查看gitlab进程",normalizedTitle:"查看gitlab进程",charIndex:1299},{level:3,title:"杀掉第一个进程（就是带有好多.............的进程）",slug:"杀掉第一个进程-就是带有好多-的进程",normalizedTitle:"杀掉第一个进程（就是带有好多.............的进程）",charIndex:1337},{level:3,title:"删除所有包含gitlab文件",slug:"删除所有包含gitlab文件",normalizedTitle:"删除所有包含gitlab文件",charIndex:1434},{level:2,title:"验证安装",slug:"验证安装",normalizedTitle:"验证安装",charIndex:1490}],headersStr:"概述 下载 安装并配置必要的依赖项 安装 拷贝安装包并安装 修改gitlab配置文件指定服务器ip和自定义端口： 重置并启动GitLab 创建Access Token 完全卸载删除gitlab 停止gitlab 卸载gitlab（注意这里写的是gitlab-ce） 查看gitlab进程 杀掉第一个进程（就是带有好多.............的进程） 删除所有包含gitlab文件 验证安装",content:"# CentOS 7 安装 Gitlab\n\n\n# 概述\n\nGitLab 是一个用于仓库管理系统的开源项目，使用Git作为代码管理工具，并在此基础上搭建起来的Web服务。安装方法是参考GitLab在GitHub上的Wiki页面。\n\n\n# 下载\n\n访问官方下载地址：https://packages.gitlab.com/gitlab/gitlab-ce\n\n打开下载地址，在Search by package name...一栏输入gitlab-ce-11.8.1-ce.0.el7.x86_64.rpm，下载Distro/Version为el/7的rpm包\n\n\n# 安装并配置必要的依赖项\n\nsudo yum install -y curl policycoreutils-python openssh-server\nsudo systemctl enable sshd\nsudo systemctl start sshd\nsudo firewall-cmd --permanent --add-service=http\nsudo systemctl reload firewalld\n\n\n\n# 安装\n\n\n# 拷贝安装包并安装\n\n将下载的gitlab-ce-11.8.1-ce.0.el7.x86_64.rpm安装包，拷贝到服务器任意目录，执行一下命令进行安装。\n\nrpm -i gitlab-ce-11.8.1-ce.0.el7.x86_64.rpm\n\n\n安装过程需要些时间，如果出现日志，则说明安装成功。\n\n安装日志\n\n\n\n# 修改gitlab配置文件指定服务器ip和自定义端口：\n\nvi /etc/gitlab/gitlab.rb\n\n\n找到external_url 'http://gitlab.example.com'并修改为当前服务器IP或域名地址\n\n配置文件内容\n\n\n\n# 重置并启动GitLab\n\n# gitlab-ctl reconfigure\n# gitlab-ctl restart\n\n\n提示 \"ok: run:\"表示启动成功\n\n\n# 创建Access Token\n\n浏览器打开页面：http://主机ip/profile/personal_access_tokens\n\n图片\n\nName随便填写。过期时间不填默认永久有效，勾选所有权限，点击Create personal access token按钮生成token。\n\n图片\n\n点击Create personal access token按钮完成按钮置灰后，页面上方位置显示了我们新创建了access token。请复制保存，部署后端项目portal-web配置gitlab相关配置时使用。\n\n注意\n\n需要注意的是该页面只显示一次，如果没有及时保存可重新创建即可。\n\n\n# 完全卸载删除gitlab\n\n提示\n\n如安装过程中遇到问题可以尝试完全卸载后重新安装\n\n\n# 停止gitlab\n\ngitlab-ctl stop\n\n\n\n# 卸载gitlab（注意这里写的是gitlab-ce）\n\nrpm -e gitlab-ce\n\n\n\n# 查看gitlab进程\n\nps aux | grep gitlab\n\n\n\n# 杀掉第一个进程（就是带有好多.............的进程）\n\nkill -9 18777\n\n\n杀掉后，在ps aux | grep gitlab确认一遍，还有没有gitlab的进程\n\n\n# 删除所有包含gitlab文件\n\nfind / -name gitlab | xargs rm -rf\n\n\n\n# 验证安装\n\n浏览器访问：http://serverIP,即可看到gitlab的登录页面，首次登录输入两次新密码，即设置root用户的密码。\n\n至此，gitlab部署完成 (゜-゜)つロ 干杯~",normalizedContent:"# centos 7 安装 gitlab\n\n\n# 概述\n\ngitlab 是一个用于仓库管理系统的开源项目，使用git作为代码管理工具，并在此基础上搭建起来的web服务。安装方法是参考gitlab在github上的wiki页面。\n\n\n# 下载\n\n访问官方下载地址：https://packages.gitlab.com/gitlab/gitlab-ce\n\n打开下载地址，在search by package name...一栏输入gitlab-ce-11.8.1-ce.0.el7.x86_64.rpm，下载distro/version为el/7的rpm包\n\n\n# 安装并配置必要的依赖项\n\nsudo yum install -y curl policycoreutils-python openssh-server\nsudo systemctl enable sshd\nsudo systemctl start sshd\nsudo firewall-cmd --permanent --add-service=http\nsudo systemctl reload firewalld\n\n\n\n# 安装\n\n\n# 拷贝安装包并安装\n\n将下载的gitlab-ce-11.8.1-ce.0.el7.x86_64.rpm安装包，拷贝到服务器任意目录，执行一下命令进行安装。\n\nrpm -i gitlab-ce-11.8.1-ce.0.el7.x86_64.rpm\n\n\n安装过程需要些时间，如果出现日志，则说明安装成功。\n\n安装日志\n\n\n\n# 修改gitlab配置文件指定服务器ip和自定义端口：\n\nvi /etc/gitlab/gitlab.rb\n\n\n找到external_url 'http://gitlab.example.com'并修改为当前服务器ip或域名地址\n\n配置文件内容\n\n\n\n# 重置并启动gitlab\n\n# gitlab-ctl reconfigure\n# gitlab-ctl restart\n\n\n提示 \"ok: run:\"表示启动成功\n\n\n# 创建access token\n\n浏览器打开页面：http://主机ip/profile/personal_access_tokens\n\n图片\n\nname随便填写。过期时间不填默认永久有效，勾选所有权限，点击create personal access token按钮生成token。\n\n图片\n\n点击create personal access token按钮完成按钮置灰后，页面上方位置显示了我们新创建了access token。请复制保存，部署后端项目portal-web配置gitlab相关配置时使用。\n\n注意\n\n需要注意的是该页面只显示一次，如果没有及时保存可重新创建即可。\n\n\n# 完全卸载删除gitlab\n\n提示\n\n如安装过程中遇到问题可以尝试完全卸载后重新安装\n\n\n# 停止gitlab\n\ngitlab-ctl stop\n\n\n\n# 卸载gitlab（注意这里写的是gitlab-ce）\n\nrpm -e gitlab-ce\n\n\n\n# 查看gitlab进程\n\nps aux | grep gitlab\n\n\n\n# 杀掉第一个进程（就是带有好多.............的进程）\n\nkill -9 18777\n\n\n杀掉后，在ps aux | grep gitlab确认一遍，还有没有gitlab的进程\n\n\n# 删除所有包含gitlab文件\n\nfind / -name gitlab | xargs rm -rf\n\n\n\n# 验证安装\n\n浏览器访问：http://serverip,即可看到gitlab的登录页面，首次登录输入两次新密码，即设置root用户的密码。\n\n至此，gitlab部署完成 (゜-゜)つロ 干杯~",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 安装 SonarQube",frontmatter:{title:"CentOS 7 安装 SonarQube",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/1dfc9f/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/13.CentOS%207%20%E5%AE%89%E8%A3%85%20SonarQube.html",relativePath:"03.CentOS/01.入门/13.CentOS 7 安装 SonarQube.md",key:"v-12e16df6",path:"/pages/1dfc9f/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:28},{level:2,title:"前置条件",slug:"前置条件",normalizedTitle:"前置条件",charIndex:145},{level:3,title:"查看MySQL引擎",slug:"查看mysql引擎",normalizedTitle:"查看mysql引擎",charIndex:200},{level:3,title:"看你的 mysql 当前默认的存储引擎:",slug:"看你的-mysql-当前默认的存储引擎",normalizedTitle:"看你的 mysql 当前默认的存储引擎:",charIndex:305},{level:3,title:"修改 MySQL 存储引擎为 InnoDB",slug:"修改-mysql-存储引擎为-innodb",normalizedTitle:"修改 mysql 存储引擎为 innodb",charIndex:408},{level:3,title:"重启 mysql 服务器",slug:"重启-mysql-服务器",normalizedTitle:"重启 mysql 服务器",charIndex:562},{level:3,title:"再次登录 MySQL 查看默认引擎设置是否生效",slug:"再次登录-mysql-查看默认引擎设置是否生效",normalizedTitle:"再次登录 mysql 查看默认引擎设置是否生效",charIndex:613},{level:2,title:"配置innodbbufferpool_size",slug:"配置innodb-buffer-pool-size",normalizedTitle:"配置innodbbufferpool_size",charIndex:null},{level:2,title:"配置querycachesize",slug:"配置query-cache-size",normalizedTitle:"配置querycachesize",charIndex:null},{level:3,title:"设置 MySQL 的查询缓存 querycachesize, 最少设置 15M",slug:"设置-mysql-的查询缓存-query-cache-size-最少设置-15m",normalizedTitle:"设置 mysql 的查询缓存 querycachesize, 最少设置 15m",charIndex:null},{level:3,title:"重启 mysql 服务器",slug:"重启-mysql-服务器-2",normalizedTitle:"重启 mysql 服务器",charIndex:562},{level:3,title:"验证缓存设置是否生效:",slug:"验证缓存设置是否生效",normalizedTitle:"验证缓存设置是否生效:",charIndex:1201},{level:2,title:"创建 sonarqube 数据库(UTF-8 编码)",slug:"创建-sonarqube-数据库-utf-8-编码",normalizedTitle:"创建 sonarqube 数据库(utf-8 编码)",charIndex:1264},{level:2,title:"下载 SonarQube",slug:"下载-sonarqube",normalizedTitle:"下载 sonarqube",charIndex:1354},{level:3,title:"切换到普通用户",slug:"切换到普通用户",normalizedTitle:"切换到普通用户",charIndex:1438},{level:3,title:"解压安装",slug:"解压安装",normalizedTitle:"解压安装",charIndex:1556},{level:2,title:"配置 SonarQube",slug:"配置-sonarqube",normalizedTitle:"配置 sonarqube",charIndex:1626},{level:2,title:"启动和测试",slug:"启动和测试",normalizedTitle:"启动和测试",charIndex:2183},{level:3,title:"切换到普通用户启动SonarQube Web Server",slug:"切换到普通用户启动sonarqube-web-server",normalizedTitle:"切换到普通用户启动sonarqube web server",charIndex:2193},{level:3,title:"查看日志",slug:"查看日志",normalizedTitle:"查看日志",charIndex:2312},{level:2,title:"初次登录 SonarQube 设置",slug:"初次登录-sonarqube-设置",normalizedTitle:"初次登录 sonarqube 设置",charIndex:2374},{level:2,title:"设置 SonarQube 开机启动",slug:"设置-sonarqube-开机启动",normalizedTitle:"设置 sonarqube 开机启动",charIndex:2466},{level:3,title:"使用root用户编辑/etc/rc.local文件，执行命令：",slug:"使用root用户编辑-etc-rc-local文件-执行命令",normalizedTitle:"使用root用户编辑/etc/rc.local文件，执行命令：",charIndex:2488}],headersStr:"概述 前置条件 查看MySQL引擎 看你的 mysql 当前默认的存储引擎: 修改 MySQL 存储引擎为 InnoDB 重启 mysql 服务器 再次登录 MySQL 查看默认引擎设置是否生效 配置innodbbufferpool_size 配置querycachesize 设置 MySQL 的查询缓存 querycachesize, 最少设置 15M 重启 mysql 服务器 验证缓存设置是否生效: 创建 sonarqube 数据库(UTF-8 编码) 下载 SonarQube 切换到普通用户 解压安装 配置 SonarQube 启动和测试 切换到普通用户启动SonarQube Web Server 查看日志 初次登录 SonarQube 设置 设置 SonarQube 开机启动 使用root用户编辑/etc/rc.local文件，执行命令：",content:"# CentOS 7 安装 SonarQube\n\n\n# 概述\n\nSonar（SonarQube）是一个开源平台，用于管理源代码的质量。Sonar 不只是一个质量数据报告工具，更是代码质量管理平台。支持的语言包括：Java、PHP、C#、C、Cobol、PL/SQL、Flex 等。\n\n\n# 前置条件\n\n提示\n\nSonarQube 依赖后端数据存储，本教程选择 MySQL 作为后端数据存储。\n\n\n# 查看MySQL引擎\n\n结合 SonarQube，MySQL 数据库最好使用 InnoDB 引擎，可提高性能。 看你的 mysql 现在已提供什么存储引擎:\n\nmysql> show engines;\n\n\n\n# 看你的 mysql 当前默认的存储引擎:\n\nmysql> show variables like '%storage_engine%';\n\n\n如果查询的结果和上图显示的效果不一致，需要做一下配置：\n\n\n# 修改 MySQL 存储引擎为 InnoDB\n\n编辑MySQL配置文件 /etc/my.cnf找到[mysqld]下面加入default-storage-engine=INNODB\n\nvi /etc/my.cnf\n\n\n[mysqld]\ndefault-storage-engine = INNODB\n\n\n\n# 重启 mysql 服务器\n\nsystemctl restart mysql.service\n\n\n\n# 再次登录 MySQL 查看默认引擎设置是否生效\n\nmysql> show variables like '%storage_engine%';\n\n\n\n# 配置innodb_buffer_pool_size\n\ninnodb_buffer_pool_size 参数值设置得尽可能大一点 这个参数主要作用是缓存 innodb 表的索引，数据，插入数据时的缓冲 默认值:128M，专用 mysql 服务器设置的大小:操作系统内存的 70%-80%最佳。\n\n设置方法:my.cnf 文件[mysqld] 下面加入 innodb_buffer_pool_size 参数\n\nvi /etc/my.cnf\n\n\n[mysqld]\ninnodb_buffer_pool_size=256M\n\n\n提示\n\n我们这里设置为 256M，因为我们的不是专用的 MySQL 数据库服务器，还有很多其他的服务需要占用系统内存\n\n\n# 配置query_cache_size\n\n\n# 设置 MySQL 的查询缓存 query_cache_size, 最少设置 15M\n\nvi /etc/my.cnf\n\n\n[mysqld]\nquery_cache_type=1\nquery_cache_size=32M\n\n\n\n# 重启 mysql 服务器\n\nsystemctl restart mysql.service\n\n\n\n# 验证缓存设置是否生效:\n\nmysql> show variables like '%query_cache%'; \n\n\n\n# 创建 sonarqube 数据库(UTF-8 编码)\n\nmysql> CREATE SCHEMA `sonar` DEFAULT CHARACTER SET utf8;\n\n\n\n# 下载 SonarQube\n\n提示\n\n下载地址: http://www.sonarqube.org/downloads/ 下载SonarQube 6.7.7版本\n\n\n# 切换到普通用户\n\n$ cd /home/heyuqiang\n$ wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-6.7.7.zip\n\n\n\n# 解压安装\n\n$ unzip sonarqube-6.7.7.zip\n$ mv sonarqube-6.7.7 sonarqube\n\n\n\n# 配置 SonarQube\n\n$ vi /home/heyuqiang/sonarqube/conf/sonar.properties\n\n\nsonar.jdbc.username=root # MySQL用户名\nsonar.jdbc.password=root # MySQL密码\n#----- MySQL 5.6 or greater\n# Only InnoDB storage engine is supported (not myISAM).\n# Only the bundled driver is supported. It can not be changed.\nsonar.jdbc.url=jdbc:mysql://mysql-ip:3306/sonarqube?useUnicode=true&characterEncoding=utf8&rewriteBatchedStatements=true&useConfigs=maxPerformance&useSSL=false\nsonar.web.host=0.0.0.0\nsonar.web.context=/sonar\nsonar.web.port=9000\n\n\n注意\n\n保存以上配置(注意，要看看默认的 9000 端口是否已被占用)\n\n\n# 启动和测试\n\n\n# 切换到普通用户启动SonarQube Web Server\n\n$ /home/heyuqiang/sonarqube/bin/linux-x86-64/sonar.sh start\n\n\n提示\n\n初次启动会自动建表和做相应的初始化\n\n\n# 查看日志\n\n$ tail -f /home/heyuqiang/sonarqube/logs/sonar.log\n\n\n\n# 初次登录 SonarQube 设置\n\nsonarqube默认用户名：admin 密码：admin，浏览器访问：http://server-ip:9000/sonarqube/\n\n\n# 设置 SonarQube 开机启动\n\n\n# 使用root用户编辑/etc/rc.local文件，执行命令：\n\n# vi /etc/rc.local\n\n\n加入：\n\n## sonarqube\nsu - heyuqiang -c '/home/heyuqiang/sonarqube/bin/linux-x86-64/sonar.sh start'\n\n\n提示\n\n第一次使用开机启动时需要为/etc/rc.local 文件赋权限\n\nchmod +x /etc/rc.local\nchmod +x /etc/rc.d/rc.local\n\n\n至此，SonarQube部署完成 (゜-゜)つロ 干杯~",normalizedContent:"# centos 7 安装 sonarqube\n\n\n# 概述\n\nsonar（sonarqube）是一个开源平台，用于管理源代码的质量。sonar 不只是一个质量数据报告工具，更是代码质量管理平台。支持的语言包括：java、php、c#、c、cobol、pl/sql、flex 等。\n\n\n# 前置条件\n\n提示\n\nsonarqube 依赖后端数据存储，本教程选择 mysql 作为后端数据存储。\n\n\n# 查看mysql引擎\n\n结合 sonarqube，mysql 数据库最好使用 innodb 引擎，可提高性能。 看你的 mysql 现在已提供什么存储引擎:\n\nmysql> show engines;\n\n\n\n# 看你的 mysql 当前默认的存储引擎:\n\nmysql> show variables like '%storage_engine%';\n\n\n如果查询的结果和上图显示的效果不一致，需要做一下配置：\n\n\n# 修改 mysql 存储引擎为 innodb\n\n编辑mysql配置文件 /etc/my.cnf找到[mysqld]下面加入default-storage-engine=innodb\n\nvi /etc/my.cnf\n\n\n[mysqld]\ndefault-storage-engine = innodb\n\n\n\n# 重启 mysql 服务器\n\nsystemctl restart mysql.service\n\n\n\n# 再次登录 mysql 查看默认引擎设置是否生效\n\nmysql> show variables like '%storage_engine%';\n\n\n\n# 配置innodb_buffer_pool_size\n\ninnodb_buffer_pool_size 参数值设置得尽可能大一点 这个参数主要作用是缓存 innodb 表的索引，数据，插入数据时的缓冲 默认值:128m，专用 mysql 服务器设置的大小:操作系统内存的 70%-80%最佳。\n\n设置方法:my.cnf 文件[mysqld] 下面加入 innodb_buffer_pool_size 参数\n\nvi /etc/my.cnf\n\n\n[mysqld]\ninnodb_buffer_pool_size=256m\n\n\n提示\n\n我们这里设置为 256m，因为我们的不是专用的 mysql 数据库服务器，还有很多其他的服务需要占用系统内存\n\n\n# 配置query_cache_size\n\n\n# 设置 mysql 的查询缓存 query_cache_size, 最少设置 15m\n\nvi /etc/my.cnf\n\n\n[mysqld]\nquery_cache_type=1\nquery_cache_size=32m\n\n\n\n# 重启 mysql 服务器\n\nsystemctl restart mysql.service\n\n\n\n# 验证缓存设置是否生效:\n\nmysql> show variables like '%query_cache%'; \n\n\n\n# 创建 sonarqube 数据库(utf-8 编码)\n\nmysql> create schema `sonar` default character set utf8;\n\n\n\n# 下载 sonarqube\n\n提示\n\n下载地址: http://www.sonarqube.org/downloads/ 下载sonarqube 6.7.7版本\n\n\n# 切换到普通用户\n\n$ cd /home/heyuqiang\n$ wget https://binaries.sonarsource.com/distribution/sonarqube/sonarqube-6.7.7.zip\n\n\n\n# 解压安装\n\n$ unzip sonarqube-6.7.7.zip\n$ mv sonarqube-6.7.7 sonarqube\n\n\n\n# 配置 sonarqube\n\n$ vi /home/heyuqiang/sonarqube/conf/sonar.properties\n\n\nsonar.jdbc.username=root # mysql用户名\nsonar.jdbc.password=root # mysql密码\n#----- mysql 5.6 or greater\n# only innodb storage engine is supported (not myisam).\n# only the bundled driver is supported. it can not be changed.\nsonar.jdbc.url=jdbc:mysql://mysql-ip:3306/sonarqube?useunicode=true&characterencoding=utf8&rewritebatchedstatements=true&useconfigs=maxperformance&usessl=false\nsonar.web.host=0.0.0.0\nsonar.web.context=/sonar\nsonar.web.port=9000\n\n\n注意\n\n保存以上配置(注意，要看看默认的 9000 端口是否已被占用)\n\n\n# 启动和测试\n\n\n# 切换到普通用户启动sonarqube web server\n\n$ /home/heyuqiang/sonarqube/bin/linux-x86-64/sonar.sh start\n\n\n提示\n\n初次启动会自动建表和做相应的初始化\n\n\n# 查看日志\n\n$ tail -f /home/heyuqiang/sonarqube/logs/sonar.log\n\n\n\n# 初次登录 sonarqube 设置\n\nsonarqube默认用户名：admin 密码：admin，浏览器访问：http://server-ip:9000/sonarqube/\n\n\n# 设置 sonarqube 开机启动\n\n\n# 使用root用户编辑/etc/rc.local文件，执行命令：\n\n# vi /etc/rc.local\n\n\n加入：\n\n## sonarqube\nsu - heyuqiang -c '/home/heyuqiang/sonarqube/bin/linux-x86-64/sonar.sh start'\n\n\n提示\n\n第一次使用开机启动时需要为/etc/rc.local 文件赋权限\n\nchmod +x /etc/rc.local\nchmod +x /etc/rc.d/rc.local\n\n\n至此，sonarqube部署完成 (゜-゜)つロ 干杯~",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 安装 Jenkins",frontmatter:{title:"CentOS 7 安装 Jenkins",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/893d19/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/14.CentOS%207%20%E5%AE%89%E8%A3%85%20Jenkins.html",relativePath:"03.CentOS/01.入门/14.CentOS 7 安装 Jenkins.md",key:"v-6a35a7dc",path:"/pages/893d19/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:26},{level:2,title:"环境准备",slug:"环境准备",normalizedTitle:"环境准备",charIndex:112},{level:3,title:"安装maven",slug:"安装maven",normalizedTitle:"安装maven",charIndex:121},{level:3,title:"解压",slug:"解压",normalizedTitle:"解压",charIndex:181},{level:3,title:"备份",slug:"备份",normalizedTitle:"备份",charIndex:244},{level:3,title:"配置maven",slug:"配置maven",normalizedTitle:"配置maven",charIndex:302},{level:3,title:"验证maven",slug:"验证maven",normalizedTitle:"验证maven",charIndex:498},{level:2,title:"部署Tomcat8 运行Jenkins.war",slug:"部署tomcat8-运行jenkins-war",normalizedTitle:"部署tomcat8 运行jenkins.war",charIndex:960},{level:3,title:"准备安装",slug:"准备安装",normalizedTitle:"准备安装",charIndex:1096},{level:3,title:"解压",slug:"解压-2",normalizedTitle:"解压",charIndex:181},{level:3,title:"修改端口与配置",slug:"修改端口与配置",normalizedTitle:"修改端口与配置",charIndex:1385},{level:3,title:"内存调优",slug:"内存调优",normalizedTitle:"内存调优",charIndex:1642},{level:2,title:"启动并验证",slug:"启动并验证",normalizedTitle:"启动并验证",charIndex:1924},{level:2,title:"设置 Jenkins-tomcat 开机启动",slug:"设置-jenkins-tomcat-开机启动",normalizedTitle:"设置 jenkins-tomcat 开机启动",charIndex:2093},{level:2,title:"部署jenkins.war",slug:"部署jenkins-war",normalizedTitle:"部署jenkins.war",charIndex:2342},{level:2,title:"启动jenkins安装程序",slug:"启动jenkins安装程序",normalizedTitle:"启动jenkins安装程序",charIndex:2617},{level:2,title:"插件安装",slug:"插件安装",normalizedTitle:"插件安装",charIndex:2910},{level:3,title:"经典安装模式",slug:"经典安装模式",normalizedTitle:"经典安装模式",charIndex:2919},{level:3,title:"自定义安装模式",slug:"自定义安装模式",normalizedTitle:"自定义安装模式",charIndex:3071},{level:2,title:"创建Admin用户",slug:"创建admin用户",normalizedTitle:"创建admin用户",charIndex:3181}],headersStr:"概述 环境准备 安装maven 解压 备份 配置maven 验证maven 部署Tomcat8 运行Jenkins.war 准备安装 解压 修改端口与配置 内存调优 启动并验证 设置 Jenkins-tomcat 开机启动 部署jenkins.war 启动jenkins安装程序 插件安装 经典安装模式 自定义安装模式 创建Admin用户",content:'# CentOS 7 安装 Jenkins\n\n\n# 概述\n\nJenkins是一个开源软件项目，是基于Java开发的一种持续集成工具，用于监控持续重复的工作，旨在提供一个开放易用的软件平台，使软件项目可以进行持续集成\n\n\n# 环境准备\n\n\n# 安装maven\n\n复制文件apache-maven-3.5.0-bin.tar.gz 到 /usr/local\n\n\n# 解压\n\ncd /usr/local\ntar -zxvf apache-maven-3.5.0-bin.tar.gz\n\n\n\n# 备份\n\ncp apache-maven-3.5.0-bin.tar.gz /root/software/\n\n\n\n# 配置maven\n\n设置环境变量 编辑 /etc/profile 在文档最后追加一下内容：\n\nvi /etc/profile\n\n\n## maven env\nexport MAVEN_HOME=/usr/local/apache-maven-3.5.0\nexport PATH=$PATH:$MAVEN_HOME/bin\n\n\n使配置文件生效：\n\nsource /etc/profile\n\n\n\n# 验证maven\n\nmvn -v\n\n\n正确配置jdk会输出一下内容：\n\n[root@localhost apache-maven-3.5.0]# mvn -v\nApache Maven 3.5.0 (ff8f5e7444045639af65f6095c62210b5713f426; 2017-04-04T03:39:06+08:00)\nMaven home: /usr/local/apache-maven-3.5.0\nJava version: 1.8.0_141, vendor: Oracle Corporation\nJava home: /usr/local/java/jdk1.8.0_221/jre\nDefault locale: en_US, platform encoding: UTF-8\nOS name: "linux", version: "3.10.0-514.21.1.el7.x86_64", arch: "amd64", family: "unix"\n\n\n大功告成maven安装成功！\n\n\n# 部署Tomcat8 运行Jenkins.war\n\ncd /root\nmkdir software\nmkdir servers\n\n\n提示\n\n目录用途说明：software 用于存放所有使用到的安装包文件；servers 用于部署所有tomcat等各种服务器运行程序\n\n\n# 准备安装\n\n复制apache-tomcat-8.5.16.tar.gz 到 /root/servers 目录下。\n\n\n# 解压\n\ncd /root/servers\ntar -zxvf apache-tomcat-8.5.16.tar.gz\n\n\n备份安装包到 /root/software 以便以后使用。\n\ncp apache-tomcat-8.5.16.tar.gz /root/software\n\n\n重命名apache-tomcat-8.5.16 为 jenkins-tomcat8\n\nmv apache-tomcat-8.5.16 jenkins-tomcat8\n\n\n\n# 修改端口与配置\n\n编辑conf/server.xml修改端口共3处：\n\nvi /root/servers/jenkins-tomcat8/conf/server.xml\n\n\n增加 URIEncoding="UTF-8" 解决tomcat 中文乱码问题。\n\n<Connector port="8080" protocol="HTTP/1.1"\n      connectionTimeout="20000"\n      redirectPort="8443" URIEncoding="UTF-8" />\n\n\n\n# 内存调优\n\n编辑bin/catalina.sh 设置JAVA_OPTS 提高JVM栈内存Increase JVM heap memory\n\nvi /root/servers/jenkins-tomcat8/bin/catalina.sh\n\n\n增加如下内容：\n\nJAVA_OPTS="-Djava.awt.headless=true -Dfile.encoding=UTF-8 -server -Xms1024m -Xmx2048m -XX:NewSize=512m -XX:MaxNewSize=512m -XX:+DisableExplicitGC"\n\n\n\n# 启动并验证\n\n启动tomcat服务\n\n/opt/jenkins-tomcat8/bin/startup.sh\n\n\n查看日志\n\ntail -f /opt/jenkins-tomcat8/logs/catalina.out\n\n\n浏览器访问地址：http://server_ip:8080，看到tomcat默认欢迎页面说明部署成功了！\n\n\n# 设置 Jenkins-tomcat 开机启动\n\n使用root用户编辑 /etc/rc.local 文件，执行命令：\n\nvi /etc/rc.local\n\n\n加入：\n\n## jenkins-tomcat8\nsu - root -c \'/opt/jenkins-tomcat8/bin/startup.sh\'\n\n\n提示\n\n第一次使用开机启动时需要为/etc/rc.local 文件赋权限\n\nchmod +x /etc/rc.local\nchmod +x /etc/rc.d/rc.local\n\n\n\n# 部署jenkins.war\n\n删除 tomcat-tomcat8/webapps/ 里面所有项目文件\n\nrm -rf /root/servers/jenkins-tomcat8/webapps/*\n\n\n复制 jenkins.war 到 /root/servers/jenkins-tomcat8/webapps/ 目录下\n\n停止并重新启动tomcat服务\n\n/root/servers/jenkins-tomcat8/bin/shutdown.sh\n/root/servers/jenkins-tomcat8/bin/startup.sh\n\n\n\n# 启动jenkins安装程序\n\n浏览器访问地址：http://server_ip:8080/jenkins 运行jenkins安装程序。\n\n提示\n\n首次访问jenkins我们需要提供超级管理员密码，超级管理员密码串在jenkins首次启动后会自动生成，存放在/root/.jenkins/secrets/initialAdminPassword 文件中。\n\n获取超级管理密码执行命令：\n\ncat /root/.jenkins/secrets/initialAdminPassword\n\n\n复制密码串并粘贴到Administrator password输入框内点击Continue\n\n\n# 插件安装\n\n\n# 经典安装模式\n\n鼠标单击左侧蓝色框安装社区推荐插件包Install suggested plugins Install plugins the Jenkins community finds most useful 安装插件，这一步安装插件受网络影响安装时间可能比较长，建议普通用户选择推荐安装\n\n\n# 自定义安装模式\n\n右侧灰色背景框是插件可选框，可以理解为插件自定义选择安装，建议有一些经验的用户使用；本演示教程已经典安装模式为例。\n\n提示\n\n开始安装插件包，在右侧可以看到事实的安装日志。绿色对勾表示安装成功\n\n\n# 创建Admin用户\n\n这里我们先简单设置一个用户，后续的配置中我们会创建一个超级管理员账户。\n\n点击Save and Finish 完成用户创建。\n\n至此，Jenkins部署完成 (゜-゜)つロ 干杯~',normalizedContent:'# centos 7 安装 jenkins\n\n\n# 概述\n\njenkins是一个开源软件项目，是基于java开发的一种持续集成工具，用于监控持续重复的工作，旨在提供一个开放易用的软件平台，使软件项目可以进行持续集成\n\n\n# 环境准备\n\n\n# 安装maven\n\n复制文件apache-maven-3.5.0-bin.tar.gz 到 /usr/local\n\n\n# 解压\n\ncd /usr/local\ntar -zxvf apache-maven-3.5.0-bin.tar.gz\n\n\n\n# 备份\n\ncp apache-maven-3.5.0-bin.tar.gz /root/software/\n\n\n\n# 配置maven\n\n设置环境变量 编辑 /etc/profile 在文档最后追加一下内容：\n\nvi /etc/profile\n\n\n## maven env\nexport maven_home=/usr/local/apache-maven-3.5.0\nexport path=$path:$maven_home/bin\n\n\n使配置文件生效：\n\nsource /etc/profile\n\n\n\n# 验证maven\n\nmvn -v\n\n\n正确配置jdk会输出一下内容：\n\n[root@localhost apache-maven-3.5.0]# mvn -v\napache maven 3.5.0 (ff8f5e7444045639af65f6095c62210b5713f426; 2017-04-04t03:39:06+08:00)\nmaven home: /usr/local/apache-maven-3.5.0\njava version: 1.8.0_141, vendor: oracle corporation\njava home: /usr/local/java/jdk1.8.0_221/jre\ndefault locale: en_us, platform encoding: utf-8\nos name: "linux", version: "3.10.0-514.21.1.el7.x86_64", arch: "amd64", family: "unix"\n\n\n大功告成maven安装成功！\n\n\n# 部署tomcat8 运行jenkins.war\n\ncd /root\nmkdir software\nmkdir servers\n\n\n提示\n\n目录用途说明：software 用于存放所有使用到的安装包文件；servers 用于部署所有tomcat等各种服务器运行程序\n\n\n# 准备安装\n\n复制apache-tomcat-8.5.16.tar.gz 到 /root/servers 目录下。\n\n\n# 解压\n\ncd /root/servers\ntar -zxvf apache-tomcat-8.5.16.tar.gz\n\n\n备份安装包到 /root/software 以便以后使用。\n\ncp apache-tomcat-8.5.16.tar.gz /root/software\n\n\n重命名apache-tomcat-8.5.16 为 jenkins-tomcat8\n\nmv apache-tomcat-8.5.16 jenkins-tomcat8\n\n\n\n# 修改端口与配置\n\n编辑conf/server.xml修改端口共3处：\n\nvi /root/servers/jenkins-tomcat8/conf/server.xml\n\n\n增加 uriencoding="utf-8" 解决tomcat 中文乱码问题。\n\n<connector port="8080" protocol="http/1.1"\n      connectiontimeout="20000"\n      redirectport="8443" uriencoding="utf-8" />\n\n\n\n# 内存调优\n\n编辑bin/catalina.sh 设置java_opts 提高jvm栈内存increase jvm heap memory\n\nvi /root/servers/jenkins-tomcat8/bin/catalina.sh\n\n\n增加如下内容：\n\njava_opts="-djava.awt.headless=true -dfile.encoding=utf-8 -server -xms1024m -xmx2048m -xx:newsize=512m -xx:maxnewsize=512m -xx:+disableexplicitgc"\n\n\n\n# 启动并验证\n\n启动tomcat服务\n\n/opt/jenkins-tomcat8/bin/startup.sh\n\n\n查看日志\n\ntail -f /opt/jenkins-tomcat8/logs/catalina.out\n\n\n浏览器访问地址：http://server_ip:8080，看到tomcat默认欢迎页面说明部署成功了！\n\n\n# 设置 jenkins-tomcat 开机启动\n\n使用root用户编辑 /etc/rc.local 文件，执行命令：\n\nvi /etc/rc.local\n\n\n加入：\n\n## jenkins-tomcat8\nsu - root -c \'/opt/jenkins-tomcat8/bin/startup.sh\'\n\n\n提示\n\n第一次使用开机启动时需要为/etc/rc.local 文件赋权限\n\nchmod +x /etc/rc.local\nchmod +x /etc/rc.d/rc.local\n\n\n\n# 部署jenkins.war\n\n删除 tomcat-tomcat8/webapps/ 里面所有项目文件\n\nrm -rf /root/servers/jenkins-tomcat8/webapps/*\n\n\n复制 jenkins.war 到 /root/servers/jenkins-tomcat8/webapps/ 目录下\n\n停止并重新启动tomcat服务\n\n/root/servers/jenkins-tomcat8/bin/shutdown.sh\n/root/servers/jenkins-tomcat8/bin/startup.sh\n\n\n\n# 启动jenkins安装程序\n\n浏览器访问地址：http://server_ip:8080/jenkins 运行jenkins安装程序。\n\n提示\n\n首次访问jenkins我们需要提供超级管理员密码，超级管理员密码串在jenkins首次启动后会自动生成，存放在/root/.jenkins/secrets/initialadminpassword 文件中。\n\n获取超级管理密码执行命令：\n\ncat /root/.jenkins/secrets/initialadminpassword\n\n\n复制密码串并粘贴到administrator password输入框内点击continue\n\n\n# 插件安装\n\n\n# 经典安装模式\n\n鼠标单击左侧蓝色框安装社区推荐插件包install suggested plugins install plugins the jenkins community finds most useful 安装插件，这一步安装插件受网络影响安装时间可能比较长，建议普通用户选择推荐安装\n\n\n# 自定义安装模式\n\n右侧灰色背景框是插件可选框，可以理解为插件自定义选择安装，建议有一些经验的用户使用；本演示教程已经典安装模式为例。\n\n提示\n\n开始安装插件包，在右侧可以看到事实的安装日志。绿色对勾表示安装成功\n\n\n# 创建admin用户\n\n这里我们先简单设置一个用户，后续的配置中我们会创建一个超级管理员账户。\n\n点击save and finish 完成用户创建。\n\n至此，jenkins部署完成 (゜-゜)つロ 干杯~',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 安装 Redis",frontmatter:{title:"CentOS 7 安装 Redis",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/54c917/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/15.CentOS%207%20%E5%AE%89%E8%A3%85%20Redis.html",relativePath:"03.CentOS/01.入门/15.CentOS 7 安装 Redis.md",key:"v-4c567690",path:"/pages/54c917/",headersStr:null,content:"# CentOS 7 安装 Redis\n\n提示\n\n文档迁移至基于源码编译安装 Redis",normalizedContent:"# centos 7 安装 redis\n\n提示\n\n文档迁移至基于源码编译安装 redis",charsets:{cjk:!0},lastUpdated:"2022/02/16, 16:19:51",lastUpdatedTimestamp:1644999591e3},{title:"CentOS 7 安装 TigerVNC Server",frontmatter:{title:"CentOS 7 安装 TigerVNC Server",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/8c6aad/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/16.CentOS%207%20%E5%AE%89%E8%A3%85%20TigerVNC%20Server.html",relativePath:"03.CentOS/01.入门/16.CentOS 7 安装 TigerVNC Server.md",key:"v-3fe1ff73",path:"/pages/8c6aad/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:34},{level:2,title:"依赖",slug:"依赖",normalizedTitle:"依赖",charIndex:115},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:11},{level:2,title:"配置（以root用户登录）",slug:"配置-以root用户登录",normalizedTitle:"配置（以root用户登录）",charIndex:376},{level:2,title:"防火墙设置",slug:"防火墙设置",normalizedTitle:"防火墙设置",charIndex:1132},{level:2,title:"设置VNC密码",slug:"设置vnc密码",normalizedTitle:"设置vnc密码",charIndex:1412},{level:2,title:"启动VNC Server",slug:"启动vnc-server",normalizedTitle:"启动vnc server",charIndex:1443},{level:2,title:"systemctl启动异常解决",slug:"systemctl启动异常解决",normalizedTitle:"systemctl启动异常解决",charIndex:1501}],headersStr:"概述 依赖 安装 配置（以root用户登录） 防火墙设置 设置VNC密码 启动VNC Server systemctl启动异常解决",content:'# CentOS 7 安装 TigerVNC Server\n\n\n# 概述\n\nTigerVNC是一个高级的 VNC 远程访问的实现，基于第四代VNC，包含一个 JPEG 压缩加速器。支持 X.Org 和 X server。\n\n\n# 依赖\n\nTigerVNC依赖图形化桌面，如果使用最小安装版本的centos需要先安装图形化桌面GNOME的程序包。\n\nyum groupinstall "GNOME Desktop"\n\n\n修改CentOS默认启动模式为图形化模式,以命令systemctl get-default 可查看当前默认的模式为multi-user.target，即命令行模式\n\nsystemctl set-default graphical.target\n\n\n\n# 安装\n\nyum install -y tigervnc-server\n\n\n\n# 配置（以root用户登录）\n\n1.拷贝\n\ncp /lib/systemd/system/vncserver@.service /lib/systemd/system/vncserver@:1.service\n\n\n2.修改配置文件：（主要是修改root用户）\n\nvim /lib/systemd/system/vncserver@:1.service\n\n\n[Unit]\nDescription=Remote desktop service (VNC)\nAfter=syslog.target network.target\n \n[Service]\nType=forking\nUser=root\n# Clean any existing files in /tmp/.X11-unix environment\nExecStartPre=/bin/sh -c \'/usr/bin/vncserver -kill %i > /dev/null 2>&1 || :\'\nExecStart=/sbin/runuser -l root -c "/usr/bin/vncserver %i -geometry 1280x720 -SecurityTypes None"\nPIDFile=/root/.vnc/%H%i.pid\nExecStop=/bin/sh -c \'/usr/bin/vncserver -kill %i > /dev/null 2>&1 || :\'\n \n[Install]\nWantedBy=multi-user.target\n\n\n3.加载配置：\n\nsystemctl daemon-reload\n\n\n4.设置开机启动：\n\nsystemctl enable vncserver@:1.service\n\n\n\n# 防火墙设置\n\n提示\n\n由于我们初始的centos系统默认关闭了防火墙，所以可以跳过此步骤。\n\n# 查看防火墙状态：\nfirewall-cmd --state\n \n# 关闭防火墙：\nsystemctl stop firewalld\nsystemctl disable firewalld\n \n# 或者开启防火墙添加5901端口（这里只开启一个端口，如有多个界面可以开启多个端口）：\nsystemctl start firewalld\nfirewall-cmd --permanent --zone=public --add-port=5901/tcp\n\n\n\n# 设置VNC密码\n\nvncserver passwd\n\n\n\n# 启动VNC Server\n\nsystemctl restart vncserver@:1.service\n\n\n\n# systemctl启动异常解决\n\n[root@localhost ~]# systemctl start vncserver@:1.service\nJob for vncserver@:1.service failed because the control process exited with error code.\n See "systemctl status vncserver@:1.service" and "journalctl -xe" for details.\n \n[root@localhost ~]# systemctl status vncserver@:1.service\n● vncserver@:1.service - Remote desktop service (VNC)\n   Loaded: loaded (/usr/lib/systemd/system/vncserver@:1.service; enabled; vendor preset:\n disabled)\n   Active: failed (Result: exit-code) since Fri 2021-05-28 19:46:46 CST; 1min 55s ago\n  Process: 5655 ExecStart=/sbin/runuser -l oracle -c /usr/bin/vncserver %i -geometry \n1280x720 (code=exited, status=1/FAILURE)\n  Process: 5650 ExecStartPre=/bin/sh -c /usr/bin/vncserver -kill %i > /dev/null 2>&1 \n|| : (code=exited, status=0/SUCCESS)\n \nJul 27 19:46:46 wyx.pc.com systemd[1]: Starting Remote desktop service (VNC)...\nJul 27 19:46:46 wyx.pc.com runuser[5655]: runuser: user oracle does not exist\nJul 27 19:46:46 wyx.pc.com systemd[1]: vncserver@:1.service: control process...=1\nJul 27 19:46:46 wyx.pc.com systemd[1]: Failed to start Remote desktop servic...).\nJul 27 19:46:46 wyx.pc.com systemd[1]: Unit vncserver@:1.service entered fai...e.\nJul 27 19:46:46 wyx.pc.com systemd[1]: vncserver@:1.service failed.\n\n\n如果启动过程中遇到异常，报错已经有进程存在，可以通过以下命令查看到\n\n[root@localhost ~]# netstat -antulp | grep 5901\ntcp        0      0 0.0.0.0:5901            0.0.0.0:*               LISTEN      5008/Xvnc           \ntcp6       0      0 :::5901                 :::*                    LISTEN      5008/Xvnc           \n[root@localhost ~]# ps -ef | grep vnc\nroot      5008     1  0 19:45 pts/0    00:00:00 /usr/bin/Xvnc :1 -auth /root/.Xauthority -\ndesktop wyx.pc.com:1 (root) -fp catalogue:/etc/X11/fontpath.d -geometry 1024x768 -pn \n-rfbauth /root/.vnc/passwd -rfbport 5901 -rfbwait 30000\nroot      5678  1640  0 19:48 pts/0    00:00:00 grep --color=auto vnc\n',normalizedContent:'# centos 7 安装 tigervnc server\n\n\n# 概述\n\ntigervnc是一个高级的 vnc 远程访问的实现，基于第四代vnc，包含一个 jpeg 压缩加速器。支持 x.org 和 x server。\n\n\n# 依赖\n\ntigervnc依赖图形化桌面，如果使用最小安装版本的centos需要先安装图形化桌面gnome的程序包。\n\nyum groupinstall "gnome desktop"\n\n\n修改centos默认启动模式为图形化模式,以命令systemctl get-default 可查看当前默认的模式为multi-user.target，即命令行模式\n\nsystemctl set-default graphical.target\n\n\n\n# 安装\n\nyum install -y tigervnc-server\n\n\n\n# 配置（以root用户登录）\n\n1.拷贝\n\ncp /lib/systemd/system/vncserver@.service /lib/systemd/system/vncserver@:1.service\n\n\n2.修改配置文件：（主要是修改root用户）\n\nvim /lib/systemd/system/vncserver@:1.service\n\n\n[unit]\ndescription=remote desktop service (vnc)\nafter=syslog.target network.target\n \n[service]\ntype=forking\nuser=root\n# clean any existing files in /tmp/.x11-unix environment\nexecstartpre=/bin/sh -c \'/usr/bin/vncserver -kill %i > /dev/null 2>&1 || :\'\nexecstart=/sbin/runuser -l root -c "/usr/bin/vncserver %i -geometry 1280x720 -securitytypes none"\npidfile=/root/.vnc/%h%i.pid\nexecstop=/bin/sh -c \'/usr/bin/vncserver -kill %i > /dev/null 2>&1 || :\'\n \n[install]\nwantedby=multi-user.target\n\n\n3.加载配置：\n\nsystemctl daemon-reload\n\n\n4.设置开机启动：\n\nsystemctl enable vncserver@:1.service\n\n\n\n# 防火墙设置\n\n提示\n\n由于我们初始的centos系统默认关闭了防火墙，所以可以跳过此步骤。\n\n# 查看防火墙状态：\nfirewall-cmd --state\n \n# 关闭防火墙：\nsystemctl stop firewalld\nsystemctl disable firewalld\n \n# 或者开启防火墙添加5901端口（这里只开启一个端口，如有多个界面可以开启多个端口）：\nsystemctl start firewalld\nfirewall-cmd --permanent --zone=public --add-port=5901/tcp\n\n\n\n# 设置vnc密码\n\nvncserver passwd\n\n\n\n# 启动vnc server\n\nsystemctl restart vncserver@:1.service\n\n\n\n# systemctl启动异常解决\n\n[root@localhost ~]# systemctl start vncserver@:1.service\njob for vncserver@:1.service failed because the control process exited with error code.\n see "systemctl status vncserver@:1.service" and "journalctl -xe" for details.\n \n[root@localhost ~]# systemctl status vncserver@:1.service\n● vncserver@:1.service - remote desktop service (vnc)\n   loaded: loaded (/usr/lib/systemd/system/vncserver@:1.service; enabled; vendor preset:\n disabled)\n   active: failed (result: exit-code) since fri 2021-05-28 19:46:46 cst; 1min 55s ago\n  process: 5655 execstart=/sbin/runuser -l oracle -c /usr/bin/vncserver %i -geometry \n1280x720 (code=exited, status=1/failure)\n  process: 5650 execstartpre=/bin/sh -c /usr/bin/vncserver -kill %i > /dev/null 2>&1 \n|| : (code=exited, status=0/success)\n \njul 27 19:46:46 wyx.pc.com systemd[1]: starting remote desktop service (vnc)...\njul 27 19:46:46 wyx.pc.com runuser[5655]: runuser: user oracle does not exist\njul 27 19:46:46 wyx.pc.com systemd[1]: vncserver@:1.service: control process...=1\njul 27 19:46:46 wyx.pc.com systemd[1]: failed to start remote desktop servic...).\njul 27 19:46:46 wyx.pc.com systemd[1]: unit vncserver@:1.service entered fai...e.\njul 27 19:46:46 wyx.pc.com systemd[1]: vncserver@:1.service failed.\n\n\n如果启动过程中遇到异常，报错已经有进程存在，可以通过以下命令查看到\n\n[root@localhost ~]# netstat -antulp | grep 5901\ntcp        0      0 0.0.0.0:5901            0.0.0.0:*               listen      5008/xvnc           \ntcp6       0      0 :::5901                 :::*                    listen      5008/xvnc           \n[root@localhost ~]# ps -ef | grep vnc\nroot      5008     1  0 19:45 pts/0    00:00:00 /usr/bin/xvnc :1 -auth /root/.xauthority -\ndesktop wyx.pc.com:1 (root) -fp catalogue:/etc/x11/fontpath.d -geometry 1024x768 -pn \n-rfbauth /root/.vnc/passwd -rfbport 5901 -rfbwait 30000\nroot      5678  1640  0 19:48 pts/0    00:00:00 grep --color=auto vnc\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS 7 安装 NFS",frontmatter:{title:"CentOS 7 安装 NFS",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/50595e/"},regularPath:"/03.CentOS/01.%E5%85%A5%E9%97%A8/17.CentOS%207%20%E5%AE%89%E8%A3%85%20NFS.html",relativePath:"03.CentOS/01.入门/17.CentOS 7 安装 NFS.md",key:"v-20b52a0c",path:"/pages/50595e/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:22},{level:2,title:"NFS服务端安装：",slug:"nfs服务端安装",normalizedTitle:"nfs服务端安装：",charIndex:143},{level:3,title:"创建共享",slug:"创建共享",normalizedTitle:"创建共享",charIndex:184},{level:3,title:"创建共享目录并给予写的权限",slug:"创建共享目录并给予写的权限",normalizedTitle:"创建共享目录并给予写的权限",charIndex:360},{level:3,title:"启动NFS服务",slug:"启动nfs服务",normalizedTitle:"启动nfs服务",charIndex:428},{level:3,title:"防火墙配置",slug:"防火墙配置",normalizedTitle:"防火墙配置",charIndex:600},{level:2,title:"NFS客户端安装：",slug:"nfs客户端安装",normalizedTitle:"nfs客户端安装：",charIndex:813},{level:3,title:"创建挂载目录",slug:"创建挂载目录",normalizedTitle:"创建挂载目录",charIndex:854},{level:3,title:"挂载NFS共享",slug:"挂载nfs共享",normalizedTitle:"挂载nfs共享",charIndex:892},{level:3,title:"查看客户端挂载信息",slug:"查看客户端挂载信息",normalizedTitle:"查看客户端挂载信息",charIndex:966}],headersStr:"概述 NFS服务端安装： 创建共享 创建共享目录并给予写的权限 启动NFS服务 防火墙配置 NFS客户端安装： 创建挂载目录 挂载NFS共享 查看客户端挂载信息",content:"# CentOS 7 安装 NFS\n\n\n# 概述\n\n网络文件系统，英文Network File System(NFS)，是由SUN公司研制的UNIX表示层协议(presentation layer protocol)，能使使用者访问网络上别处的文件就像在使用自己的计算机一样。\n\n\n# NFS服务端安装：\n\nyum install nfs-utils -y\n\n\n\n# 创建共享\n\n1.在/etc/exports配置文件中添加以下内容，保存退出；\n\n/home/ITP/share 172.16.134.252/255.255.255.0(rw,sync,no_root_squash)\n\n\n或者\n\n/home/ITP/share 172.16.134.0/24(rw,sync,no_root_squash)\n\n\n\n# 创建共享目录并给予写的权限\n\nmkdir /home/ITP/share\nchmod o+w /home/ITP/share\n\n\n\n# 启动NFS服务\n\nsystemctl restart rpcbind\nsystemctl restart nfs-server\nsystemctl enable rpcbind\nsystemctl enable nfs-server\n\n\n注意\n\nnfs其实还依赖nfs-lock、nfs-idmap两个服务，但是无需我们去做相关配置。\n\n\n# 防火墙配置\n\nfirewall-cmd --permanent --zone=public --add-service=nfs\nfirewall-cmd --permanent --zone=public --add-service=mountd\nfirewall-cmd --permanent --zone=public --add-service=rpc-bind\nfirewall-cmd --reload\n\n\n\n# NFS客户端安装：\n\nyum install nfs-utils -y\n\n\n\n# 创建挂载目录\n\nmkdir -p /home/ITP/share\n\n\n\n# 挂载NFS共享\n\nmount -t nfs 172.16.134.248:/home/ITP/share /home/ITP/share\n\n\n\n# 查看客户端挂载信息\n\ndf -h\n\n\n至此，NFS 部署完成 (゜-゜)つロ 干杯~",normalizedContent:"# centos 7 安装 nfs\n\n\n# 概述\n\n网络文件系统，英文network file system(nfs)，是由sun公司研制的unix表示层协议(presentation layer protocol)，能使使用者访问网络上别处的文件就像在使用自己的计算机一样。\n\n\n# nfs服务端安装：\n\nyum install nfs-utils -y\n\n\n\n# 创建共享\n\n1.在/etc/exports配置文件中添加以下内容，保存退出；\n\n/home/itp/share 172.16.134.252/255.255.255.0(rw,sync,no_root_squash)\n\n\n或者\n\n/home/itp/share 172.16.134.0/24(rw,sync,no_root_squash)\n\n\n\n# 创建共享目录并给予写的权限\n\nmkdir /home/itp/share\nchmod o+w /home/itp/share\n\n\n\n# 启动nfs服务\n\nsystemctl restart rpcbind\nsystemctl restart nfs-server\nsystemctl enable rpcbind\nsystemctl enable nfs-server\n\n\n注意\n\nnfs其实还依赖nfs-lock、nfs-idmap两个服务，但是无需我们去做相关配置。\n\n\n# 防火墙配置\n\nfirewall-cmd --permanent --zone=public --add-service=nfs\nfirewall-cmd --permanent --zone=public --add-service=mountd\nfirewall-cmd --permanent --zone=public --add-service=rpc-bind\nfirewall-cmd --reload\n\n\n\n# nfs客户端安装：\n\nyum install nfs-utils -y\n\n\n\n# 创建挂载目录\n\nmkdir -p /home/itp/share\n\n\n\n# 挂载nfs共享\n\nmount -t nfs 172.16.134.248:/home/itp/share /home/itp/share\n\n\n\n# 查看客户端挂载信息\n\ndf -h\n\n\n至此，nfs 部署完成 (゜-゜)つロ 干杯~",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"CentOS7 LVM操作实战",frontmatter:{title:"CentOS7 LVM操作实战",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/619aa3/"},regularPath:"/03.CentOS/02.%E9%AB%98%E7%BA%A7/01.CentOS7%20LVM%E6%93%8D%E4%BD%9C%E5%AE%9E%E6%88%98.html",relativePath:"03.CentOS/02.高级/01.CentOS7 LVM操作实战.md",key:"v-1a563de3",path:"/pages/619aa3/",headers:[{level:2,title:"LVM简介",slug:"lvm简介",normalizedTitle:"lvm简介",charIndex:22},{level:2,title:"LVM相关概念和机制",slug:"lvm相关概念和机制",normalizedTitle:"lvm相关概念和机制",charIndex:410},{level:3,title:"PV(Physical Volume)即物理卷",slug:"pv-physical-volume-即物理卷",normalizedTitle:"pv(physical volume)即物理卷",charIndex:535},{level:3,title:"VG(Volume Group)即卷组",slug:"vg-volume-group-即卷组",normalizedTitle:"vg(volume group)即卷组",charIndex:637},{level:3,title:"PE(Physical Extend)",slug:"pe-physical-extend",normalizedTitle:"pe(physical extend)",charIndex:805},{level:3,title:"LV(Logical Volume)",slug:"lv-logical-volume",normalizedTitle:"lv(logical volume)",charIndex:860},{level:3,title:"LE(logical extent)",slug:"le-logical-extent",normalizedTitle:"le(logical extent)",charIndex:1169},{level:2,title:"LVM的写入机制",slug:"lvm的写入机制",normalizedTitle:"lvm的写入机制",charIndex:1338},{level:2,title:"LVM详解图",slug:"lvm详解图",normalizedTitle:"lvm详解图",charIndex:1620},{level:2,title:"操作实战",slug:"操作实战",normalizedTitle:"操作实战",charIndex:13},{level:2,title:"创建LVM逻辑卷",slug:"创建lvm逻辑卷",normalizedTitle:"创建lvm逻辑卷",charIndex:3865},{level:2,title:"格式化并使用逻辑卷",slug:"格式化并使用逻辑卷",normalizedTitle:"格式化并使用逻辑卷",charIndex:6362},{level:2,title:"扩容逻辑卷",slug:"扩容逻辑卷",normalizedTitle:"扩容逻辑卷",charIndex:8104},{level:3,title:"第一种情况，扩的空间大小在vg的容量范围之内",slug:"第一种情况-扩的空间大小在vg的容量范围之内",normalizedTitle:"第一种情况，扩的空间大小在vg的容量范围之内",charIndex:8205},{level:3,title:"第二种情况，扩容的大小超过了vg的大小",slug:"第二种情况-扩容的大小超过了vg的大小",normalizedTitle:"第二种情况，扩容的大小超过了vg的大小",charIndex:10157},{level:2,title:"缩小逻辑卷",slug:"缩小逻辑卷",normalizedTitle:"缩小逻辑卷",charIndex:14360}],headersStr:"LVM简介 LVM相关概念和机制 PV(Physical Volume)即物理卷 VG(Volume Group)即卷组 PE(Physical Extend) LV(Logical Volume) LE(logical extent) LVM的写入机制 LVM详解图 操作实战 创建LVM逻辑卷 格式化并使用逻辑卷 扩容逻辑卷 第一种情况，扩的空间大小在vg的容量范围之内 第二种情况，扩容的大小超过了vg的大小 缩小逻辑卷",content:'# CentOS7 LVM操作实战\n\n\n# LVM简介\n\nLVM是逻辑盘卷管理（LogicalVolumeManager）的简称，它是Linux环境下对磁盘分区进行管理的一种机制，LVM是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。通过LVM系统管理员可以轻松管理磁盘分区，如：将若干个磁盘分区连接为一个整块的卷组 （volumegroup），形成一个存储池。管理员可以在卷组上随意创建逻辑卷组（logicalvolumes），并进一步在逻辑卷组上创建文件系统。\n\n * LVM是 Logical Volume Manager(逻辑卷管理)的简写\n\n * PV:是物理的磁盘分区\n\n * VG:LVM中的物理的磁盘分区，也就是PV，必须加入VG，可以将VG理解为一个仓库统一管理了几个大的硬盘，形成了一个统一虚拟的存储资源池。\n\n * LV：也就是从VG中划分的逻辑分区\n\n抽象模型如下:\n\n\n\n\n# LVM相关概念和机制\n\nLVM(Logical Volume Manager)可以让分区变得弹性，可以随时随地的扩大和缩小分区大小，前提是该分区是LVM格式的。\n\nlvm需要使用的软件包为lvm2，一般在CentOS发行版中都已经预安装了。\n\n\n# PV(Physical Volume)即物理卷\n\n硬盘分区后(还未格式化为文件系统)使用pvcreate命令可以将分区创建为PV，要求分区的system ID为8e，即为LVM格式的系统标识符。\n\n\n# VG(Volume Group)即卷组\n\n将多个PV组合起来，使用vgcreate命令创建成卷组，这样卷组包含了多个PV就比较大了，相当于重新整合了多个分区后得到的磁盘。虽然VG是整合多个PV的，但是创建VG时会将VG所有的空间根据指定的PE大小划分为多个PE，在LVM模式下的存储都以PE为单元，类似于文件系统的Block。\n\n\n# PE(Physical Extend)\n\nPE是VG中的存储单元。实际存储的数据都是存储在这里面的。\n\n\n# LV(Logical Volume)\n\nVG相当于整合过的硬盘，那么LV就相当于分区，只不过该分区是通过VG来划分的。VG中有很多PE单元，可以指定将多少个PE划分给一个LV，也可以直接指定大小(如多少兆)来划分。划分为LV之后就相当于划分了分区，只需再对LV进行格式化即可变成普通的文件系统。\n\n通俗地讲，非LVM管理的分区步骤是将硬盘分区，然后将分区格式化为文件系统。而使用LVM，则是在硬盘分区为特定的LVM标识符的分区后将其转变为LVM可管理的PV，其实PV仍然类似于分区，然后将几个PV整合为类似于磁盘的VG，最后划分VG为LV，此时LV就成了LVM可管理的分区，只需再对其格式化即可成为文件系统。\n\n\n# LE(logical extent)\n\nPE是物理存储单元，而LE则是逻辑存储单元，也即为LV中的逻辑存储单元，和PE的大小是一样的。从VG中划分LV，实际上是从VG中划分VG中的PE，只不过划分LV后它不再称为PE，而是成为le。\n\nLVM之所以能够伸缩容量，其实现的方法就是将LV里空闲的PE移出，或向LV中添加空闲的PE。\n\n\n# LVM的写入机制\n\nLV是从VG中划分出来的，LV中的PE很可能来自于多个PV。在向LV存储数据时，有多种存储机制，其中两种是：\n\n线性模式(linear)：先写完来自于同一个PV的PE，再写来自于下一个PV的PE。\n\n条带模式(striped)：一份数据拆分成多份，分别写入该LV对应的每个PV中，所以读写性能较好，类似于RAID 0。\n\n尽管striped读写性能较好也不建议使用该模式，因为lvm的着重点在于弹性容量扩展而非性能，要实现性能应该使用RAID来实现，而且使用striped模式时要进行容量的扩展和收缩将比较麻烦。默认的是使用线性模式。\n\n\n# LVM详解图\n\n\n\n\n# 操作实战\n\n通过在虚拟机的CentOS7上创建LVM ，使用LV，扩容LV，缩减LV实战来了解LVM及熟悉对LVM的操作。\n\n准备工作准备虚拟机，操作系统为CentOS7，初始40G的硬盘\n\n\n\n初始状态共20G的系统盘\n\n[root@localhost ~]# df -h\nFilesystem               Size  Used Avail Use% Mounted on\ndevtmpfs                 2.0G     0  2.0G   0% /dev\ntmpfs                    2.0G     0  2.0G   0% /dev/shm\ntmpfs                    2.0G  8.6M  2.0G   1% /run\ntmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35G  1.3G   32G   4% /\n/dev/sda1                976M  108M  802M  12% /boot\ntmpfs                    396M     0  396M   0% /run/user/0\n\n\n接下来，关闭虚拟机并添加两块硬盘，大小设置100GB\n\n\n\n启动虚拟机通过pvs命令查看物理卷的情况，目前只看到有虚拟机初始安装时有个pv为/dv/sda2 vg为centso的物理卷 大小为40G\n\n[root@localhost ~]# pvs\n  PV         VG     Fmt  Attr PSize   PFree\n  /dev/sda2  centos lvm2 a--  <39.00g 4.00m\n\n\n通过fdisk -l 可以看到新加的两个盘大小分别都是100G。我们将用这两个盘组成一个vg，进行lvm的管理。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[root@localhost ~]# fdisk -l\n\nDisk /dev/sda: 42.9 GB, 42949672960 bytes, 83886080 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk label type: dos\nDisk identifier: 0x000cbb0e\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048     2099199     1048576   83  Linux\n/dev/sda2         2099200    83886079    40893440   8e  Linux LVM\n\nDisk /dev/sdb: 107.4 GB, 107374182400 bytes, 209715200 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/sdc: 107.4 GB, 107374182400 bytes, 209715200 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/mapper/centos-root: 37.7 GB, 37706792960 bytes, 73646080 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/mapper/centos-swap: 4160 MB, 4160749568 bytes, 8126464 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\n\n# 创建LVM逻辑卷\n\n1、将物理硬盘格式化成PV（物理卷） 使用的是pvcreate命令pvcreate /dev/sdb /dev/sdc (将/dev/sdb /dev/sdc两块硬盘创建为物理卷)\n\n[root@localhost ~]# pvcreate /dev/sdb /dev/sdc\n  Physical volume "/dev/sdb" successfully created.\n  Physical volume "/dev/sdc" successfully created.\n\n\n通过pvdisplay或pvs查看当前的PV信息，可以看到两块100G的物理卷已经成功创建\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n\n[root@localhost ~]# pvdisplay\n  --- Physical volume ---\n  PV Name               /dev/sda2\n  VG Name               centos\n  PV Size               <39.00 GiB / not usable 3.00 MiB\n  Allocatable           yes \n  PE Size               4.00 MiB\n  Total PE              9983\n  Free PE               1\n  Allocated PE          9982\n  PV UUID               uT8J7D-rCmM-npfd-Cfzc-HDXf-3oGz-cx0sI8\n   \n  "/dev/sdb" is a new physical volume of "100.00 GiB"\n  --- NEW Physical volume ---\n  PV Name               /dev/sdb\n  VG Name               \n  PV Size               100.00 GiB\n  Allocatable           NO\n  PE Size               0   \n  Total PE              0\n  Free PE               0\n  Allocated PE          0\n  PV UUID               XcglBd-zjL5-SGRC-4F5i-huhz-2mac-DECnQA\n   \n  "/dev/sdc" is a new physical volume of "100.00 GiB"\n  --- NEW Physical volume ---\n  PV Name               /dev/sdc\n  VG Name               \n  PV Size               100.00 GiB\n  Allocatable           NO\n  PE Size               0   \n  Total PE              0\n  Free PE               0\n  Allocated PE          0\n  PV UUID               bgqoKV-1ncH-VqjQ-tpT3-KxEb-oiQl-UPvq9h\n\n\n2、创建卷组（VG），并将PV加入到卷组中 通过vgcreate命令vgcreate vg /dev/sdb /dev/sdc\n\n[root@localhost ~]# vgcreate vg /dev/sdb /dev/sdc\n  Volume group "vg" successfully created\n\n\n通过vgdisplay或vgs命令查看vg的信息。看到vg已经创建好了，大小是两个pv的大大小也就是100G+100G，大概是200G的样子，这里显示199.99G\n\n[root@localhost ~]# vgs\n  VG     #PV #LV #SN Attr   VSize   VFree  \n  centos   1   2   0 wz--n- <39.00g   4.00m\n  vg       2   0   0 wz--n- 199.99g 199.99g\n\n\n3、基于卷组（VG）创建逻辑卷(LV) 通过lvcreate命令lvcreate -n app -L 50G vg (基于vg创建逻辑卷lv,名字为app,大小为50G)\n\n[root@localhost ~]# lvcreate -n app -L 50G vg\n  Logical volume "app" created.\n\n\n用lvdisplay或lvs命令查看创建好的逻辑卷。可以看到名字为app的逻辑卷lv已经创建好了，它是基于vg创建的，大小为50G\n\n[root@localhost ~]# lvs\n  LV   VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n  root centos -wi-ao---- <35.12g                                                    \n  swap centos -wi-ao----  <3.88g                                                    \n  app  vg     -wi-a-----  50.00g   \n\n\n到这里，lv就创建好了，但是要用起来，还得格式化并挂载到我们的文件系统。\n\n\n# 格式化并使用逻辑卷\n\n1、格式化 mkfs -t ext4 /dev/vg/app 用ext4的格式格式化/dev/vg/app\n\n[root@localhost ~]# mkfs -t ext4 /dev/vg/app\nmke2fs 1.42.9 (28-Dec-2013)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\nStride=0 blocks, Stripe width=0 blocks\n3276800 inodes, 13107200 blocks\n655360 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=2162163712\n400 block groups\n32768 blocks per group, 32768 fragments per group\n8192 inodes per group\nSuperblock backups stored on blocks: \n        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, \n        4096000, 7962624, 11239424\n\nAllocating group tables: done                            \nWriting inode tables: done                            \nCreating journal (32768 blocks): done\nWriting superblocks and filesystem accounting information: done  \n\n\n2、挂载 创建挂载点 mkdir /app 将/dev/vg/app 挂载到/app\n\nmount /dev/vg/app /app\n\n\n然后df -h 可以看到已经挂载到/app下了 ，大小为lv 的大小50G\n\n[root@localhost app]# df -h\nFilesystem               Size  Used Avail Use% Mounted on\ndevtmpfs                 2.0G     0  2.0G   0% /dev\ntmpfs                    2.0G     0  2.0G   0% /dev/shm\ntmpfs                    2.0G  8.6M  2.0G   1% /run\ntmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35G  1.3G   32G   4% /\n/dev/sda1                976M  108M  802M  12% /boot\ntmpfs                    396M     0  396M   0% /run/user/0\n/dev/mapper/vg-app        50G   53M   47G   1% /app\n\n\n进入/app目录执行命令touch test.txt 创建一个测试文件test.txt，可以看到该挂载点是可以用了。\n\n[root@localhost app]# touch test.txt\n[root@localhost app]# ll\ntotal 0\n-rw-r--r--. 1 root root 0 May 28 17:50 test.txt\n\n\n设置开机加载\n\necho "/dev/vg/app /app ext4 defaults 0 0" >>/etc/fstab\n\n\n\n# 扩容逻辑卷\n\n这里扩容分两种情况，一种情况是vg还有足够的空间，那么就可以直接扩lv就可以了。另外一种情况是要扩的空间已经超过了vg的大小，那么就可以通过加物理磁盘扩充到vg里，然后再扩lv。\n\n\n# 第一种情况，扩的空间大小在vg的容量范围之内\n\n[root@localhost app]# vgs\n  VG     #PV #LV #SN Attr   VSize   VFree  \n  centos   1   2   0 wz--n- <39.00g   4.00m\n  vg       2   1   0 wz--n- 199.99g 149.99g\n\n\n现在vg的大小为200G，现在/app是50G，计划扩到80G，没有超过vg的大小那么可以直接扩lv就可以了。\n\n第一步：首先卸载设备和挂载点的关联\n\numount /app\n\n\n第二步：将逻辑卷/dev/vg/app 扩展到80G lvextend -L 80G /dev/vg/app 可以清楚的看到vg/app从50G扩容到了80G\n\n[root@localhost app]# lvextend -L 80G /dev/vg/app\n  Size of logical volume vg/app changed from 50.00 GiB (12800 extents) to 80.00 GiB (20480 extents).\n  Logical volume vg/app successfully resized.\n\n\n第三步：检查硬盘（lv）完整性，并重置硬盘(lv)容量 e2fsck -f /dev/vg/app 检查硬盘完整性\n\n[root@localhost app]# e2fsck -f /dev/vg/app\ne2fsck 1.42.9 (28-Dec-2013)\nPass 1: Checking inodes, blocks, and sizes\nPass 2: Checking directory structure\nPass 3: Checking directory connectivity\nPass 4: Checking reference counts\nPass 5: Checking group summary information\n/dev/vg/app: 11/3276800 files (0.0% non-contiguous), 251790/13107200 blocks\n\n\nresize2fs /dev/vg/app 重置硬盘(lv)容量，这一步必需要做，否则即使扩了容量，但看到的还是扩容之前的容量。\n\n[root@localhost app]# resize2fs /dev/vg/app\nresize2fs 1.42.9 (28-Dec-2013)\nResizing the filesystem on /dev/vg/app to 20971520 (4k) blocks.\nThe filesystem on /dev/vg/app is now 20971520 blocks long.\n\n\n第四步：重新挂载硬盘并查看 mount -a并执行df -h 可以看到/app已经成功扩容到80G了\n\n[root@localhost app]# mount -a\n[root@localhost app]# df -h\nFilesystem               Size  Used Avail Use% Mounted on\ndevtmpfs                 2.0G     0  2.0G   0% /dev\ntmpfs                    2.0G     0  2.0G   0% /dev/shm\ntmpfs                    2.0G  8.6M  2.0G   1% /run\ntmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35G  1.3G   32G   4% /\n/dev/sda1                976M  108M  802M  12% /boot\ntmpfs                    396M     0  396M   0% /run/user/0\n/dev/mapper/vg-app        79G   56M   75G   1% /app\n\n\nls 查看/app里面的文件还在，说明扩容对文件数据没有啥影响。\n\n[root@localhost app]# ls\ntest.txt\n\n\n如果扩容的大小超过了vg的大小怎么办呢？可以通过扩硬件的方式，加块硬盘到vg然后再扩lv。\n\n\n# 第二种情况，扩容的大小超过了vg的大小\n\n现在vg的大小为200G，现在/app是80G，计划扩到350G，已经没有超过vg的200G大小那么就需要先加硬盘，然后扩vg，再扩lv。\n\n第一步：添加硬盘\n\n\n\n通过fdisk -l命令查看添加\n\n\n\n\n \n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[root@localhost ~]# fdisk -l\n\nDisk /dev/sdd: 214.7 GB, 214748364800 bytes, 419430400 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/sdb: 107.4 GB, 107374182400 bytes, 209715200 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/sda: 42.9 GB, 42949672960 bytes, 83886080 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk label type: dos\nDisk identifier: 0x000cbb0e\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048     2099199     1048576   83  Linux\n/dev/sda2         2099200    83886079    40893440   8e  Linux LVM\n\nDisk /dev/sdc: 107.4 GB, 107374182400 bytes, 209715200 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/mapper/centos-root: 37.7 GB, 37706792960 bytes, 73646080 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/mapper/centos-swap: 4160 MB, 4160749568 bytes, 8126464 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/mapper/vg-app: 85.9 GB, 85899345920 bytes, 167772160 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\n第二步：扩容vg 将新的硬盘扩到vg卷组里，卸载/app\n\numount /app\n\n\n执行命令vgextend vg /dev/sdd 将新添加的硬盘/dev/sdd添加到vg卷组里\n\n[root@localhost ~]# vgextend vg /dev/sdd\n  Physical volume "/dev/sdd" successfully created.\n  Volume group "vg" successfully extended\n\n\n第三步：扩容lv 将逻辑卷/dev/vg/app 扩展到350G，执行命令 lvextend -L 11G /dev/vg/app 可以看到vg/app从原来的8G扩到了350G\n\n[root@localhost ~]# lvextend -L 350G /dev/vg/app\n  Size of logical volume vg/app changed from 80.00 GiB (20480 extents) to 350.00 GiB (89600 extents).\n  Logical volume vg/app successfully resized.\n\n\n第四步：同样检查硬盘（lv）完整性，并重置硬盘(lv)容量e2fsck -f /dev/vg/app 检查硬盘完整性，resize2fs /dev/vg/app 重置硬盘(lv)容量\n\n[root@localhost ~]# umount /app\n[root@localhost ~]# df -h\nFilesystem               Size  Used Avail Use% Mounted on\ndevtmpfs                 2.0G     0  2.0G   0% /dev\ntmpfs                    2.0G     0  2.0G   0% /dev/shm\ntmpfs                    2.0G  8.6M  2.0G   1% /run\ntmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35G  1.3G   32G   4% /\n/dev/sda1                976M  108M  802M  12% /boot\ntmpfs                    396M     0  396M   0% /run/user/0\n[root@localhost ~]# e2fsck -f /dev/vg/app\ne2fsck 1.42.9 (28-Dec-2013)\nPass 1: Checking inodes, blocks, and sizes\nPass 2: Checking directory structure\nPass 3: Checking directory connectivity\nPass 4: Checking reference counts\nPass 5: Checking group summary information\n/dev/vg/app: 11/5242880 files (0.0% non-contiguous), 376182/20971520 blocks\n[root@localhost ~]# resize2fs /dev/vg/app\nresize2fs 1.42.9 (28-Dec-2013)\nResizing the filesystem on /dev/vg/app to 91750400 (4k) blocks.\nThe filesystem on /dev/vg/app is now 91750400 blocks long.\n\n\n第五步：重新挂载硬盘并查看 mount -a并执行df -h 可以看到/app已经成功扩容到350G了\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n[root@localhost ~]# mount -a\n[root@localhost ~]# df -h\nFilesystem               Size  Used Avail Use% Mounted on\ndevtmpfs                 2.0G     0  2.0G   0% /dev\ntmpfs                    2.0G     0  2.0G   0% /dev/shm\ntmpfs                    2.0G  8.6M  2.0G   1% /run\ntmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35G  1.3G   32G   4% /\n/dev/sda1                976M  108M  802M  12% /boot\ntmpfs                    396M     0  396M   0% /run/user/0\n/dev/mapper/vg-app       345G   66M  327G   1% /app\n\n\n\n# 缩小逻辑卷\n\n相对于逻辑卷扩容，缩小逻辑卷，数据丢失的风险更大。所以在生产环境中操作一定要注意提前备份好数据。在对LVM逻辑卷进行缩小操作之前，先把要缩小的文件系统卸载并检查文件系统的完整性。 现在我们将/app由现在的350G缩到100G\n\n第一步：卸载/app并检查文件系统完整性\n\n[root@localhost ~]# umount /app\n[root@localhost ~]# e2fsck -f /dev/vg/app\ne2fsck 1.42.9 (28-Dec-2013)\nPass 1: Checking inodes, blocks, and sizes\nPass 2: Checking directory structure\nPass 3: Checking directory connectivity\nPass 4: Checking reference counts\nPass 5: Checking group summary information\n/dev/vg/app: 11/22937600 files (0.0% non-contiguous), 1489518/91750400 blocks\n\n\n第二步：把逻辑卷缩容到100G，执行命令：resize2fs /dev/vg/app 100G和lvreduce -L 100G /dev/vg/app\n\n[root@localhost ~]# resize2fs /dev/vg/app 100G\nresize2fs 1.42.9 (28-Dec-2013)\nResizing the filesystem on /dev/vg/app to 26214400 (4k) blocks.\nThe filesystem on /dev/vg/app is now 26214400 blocks long.\n\n[root@localhost ~]# lvreduce -L 100G /dev/vg/app\n  WARNING: Reducing active logical volume to 100.00 GiB.\n  THIS MAY DESTROY YOUR DATA (filesystem etc.)\nDo you really want to reduce vg/app? [y/n]: y\n  Size of logical volume vg/app changed from 350.00 GiB (89600 extents) to 100.00 GiB (25600 extents).\n  Logical volume vg/app successfully resized.\n\n\n第三步：重新挂载并查看状态\n\n[root@localhost ~]# mount -a \n[root@localhost ~]# df -h \nFilesystem               Size  Used Avail Use% Mounted on\ndevtmpfs                 2.0G     0  2.0G   0% /dev\ntmpfs                    2.0G     0  2.0G   0% /dev/shm\ntmpfs                    2.0G  8.6M  2.0G   1% /run\ntmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35G  1.3G   32G   4% /\n/dev/sda1                976M  108M  802M  12% /boot\ntmpfs                    396M     0  396M   0% /run/user/0\n/dev/mapper/vg-app        99G   60M   94G   1% /app\n\n\n通过LVM的管理，创建、扩容、缩容，可以看到通过LVM技术可以实现系统存储空间的动态的调整。\n\n基本原理是将多个物理硬盘创建成pv(物理卷)，这些物理卷是动态调整的物理基础，通过vg将pv管理起来形成一个整体的资源池。在vg中划分lv来动态调整逻辑卷的大小。\n\n至此，CentOS7 LVM操作实战完成 (゜-゜)つロ 干杯~',normalizedContent:'# centos7 lvm操作实战\n\n\n# lvm简介\n\nlvm是逻辑盘卷管理（logicalvolumemanager）的简称，它是linux环境下对磁盘分区进行管理的一种机制，lvm是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。通过lvm系统管理员可以轻松管理磁盘分区，如：将若干个磁盘分区连接为一个整块的卷组 （volumegroup），形成一个存储池。管理员可以在卷组上随意创建逻辑卷组（logicalvolumes），并进一步在逻辑卷组上创建文件系统。\n\n * lvm是 logical volume manager(逻辑卷管理)的简写\n\n * pv:是物理的磁盘分区\n\n * vg:lvm中的物理的磁盘分区，也就是pv，必须加入vg，可以将vg理解为一个仓库统一管理了几个大的硬盘，形成了一个统一虚拟的存储资源池。\n\n * lv：也就是从vg中划分的逻辑分区\n\n抽象模型如下:\n\n\n\n\n# lvm相关概念和机制\n\nlvm(logical volume manager)可以让分区变得弹性，可以随时随地的扩大和缩小分区大小，前提是该分区是lvm格式的。\n\nlvm需要使用的软件包为lvm2，一般在centos发行版中都已经预安装了。\n\n\n# pv(physical volume)即物理卷\n\n硬盘分区后(还未格式化为文件系统)使用pvcreate命令可以将分区创建为pv，要求分区的system id为8e，即为lvm格式的系统标识符。\n\n\n# vg(volume group)即卷组\n\n将多个pv组合起来，使用vgcreate命令创建成卷组，这样卷组包含了多个pv就比较大了，相当于重新整合了多个分区后得到的磁盘。虽然vg是整合多个pv的，但是创建vg时会将vg所有的空间根据指定的pe大小划分为多个pe，在lvm模式下的存储都以pe为单元，类似于文件系统的block。\n\n\n# pe(physical extend)\n\npe是vg中的存储单元。实际存储的数据都是存储在这里面的。\n\n\n# lv(logical volume)\n\nvg相当于整合过的硬盘，那么lv就相当于分区，只不过该分区是通过vg来划分的。vg中有很多pe单元，可以指定将多少个pe划分给一个lv，也可以直接指定大小(如多少兆)来划分。划分为lv之后就相当于划分了分区，只需再对lv进行格式化即可变成普通的文件系统。\n\n通俗地讲，非lvm管理的分区步骤是将硬盘分区，然后将分区格式化为文件系统。而使用lvm，则是在硬盘分区为特定的lvm标识符的分区后将其转变为lvm可管理的pv，其实pv仍然类似于分区，然后将几个pv整合为类似于磁盘的vg，最后划分vg为lv，此时lv就成了lvm可管理的分区，只需再对其格式化即可成为文件系统。\n\n\n# le(logical extent)\n\npe是物理存储单元，而le则是逻辑存储单元，也即为lv中的逻辑存储单元，和pe的大小是一样的。从vg中划分lv，实际上是从vg中划分vg中的pe，只不过划分lv后它不再称为pe，而是成为le。\n\nlvm之所以能够伸缩容量，其实现的方法就是将lv里空闲的pe移出，或向lv中添加空闲的pe。\n\n\n# lvm的写入机制\n\nlv是从vg中划分出来的，lv中的pe很可能来自于多个pv。在向lv存储数据时，有多种存储机制，其中两种是：\n\n线性模式(linear)：先写完来自于同一个pv的pe，再写来自于下一个pv的pe。\n\n条带模式(striped)：一份数据拆分成多份，分别写入该lv对应的每个pv中，所以读写性能较好，类似于raid 0。\n\n尽管striped读写性能较好也不建议使用该模式，因为lvm的着重点在于弹性容量扩展而非性能，要实现性能应该使用raid来实现，而且使用striped模式时要进行容量的扩展和收缩将比较麻烦。默认的是使用线性模式。\n\n\n# lvm详解图\n\n\n\n\n# 操作实战\n\n通过在虚拟机的centos7上创建lvm ，使用lv，扩容lv，缩减lv实战来了解lvm及熟悉对lvm的操作。\n\n准备工作准备虚拟机，操作系统为centos7，初始40g的硬盘\n\n\n\n初始状态共20g的系统盘\n\n[root@localhost ~]# df -h\nfilesystem               size  used avail use% mounted on\ndevtmpfs                 2.0g     0  2.0g   0% /dev\ntmpfs                    2.0g     0  2.0g   0% /dev/shm\ntmpfs                    2.0g  8.6m  2.0g   1% /run\ntmpfs                    2.0g     0  2.0g   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35g  1.3g   32g   4% /\n/dev/sda1                976m  108m  802m  12% /boot\ntmpfs                    396m     0  396m   0% /run/user/0\n\n\n接下来，关闭虚拟机并添加两块硬盘，大小设置100gb\n\n\n\n启动虚拟机通过pvs命令查看物理卷的情况，目前只看到有虚拟机初始安装时有个pv为/dv/sda2 vg为centso的物理卷 大小为40g\n\n[root@localhost ~]# pvs\n  pv         vg     fmt  attr psize   pfree\n  /dev/sda2  centos lvm2 a--  <39.00g 4.00m\n\n\n通过fdisk -l 可以看到新加的两个盘大小分别都是100g。我们将用这两个盘组成一个vg，进行lvm的管理。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[root@localhost ~]# fdisk -l\n\ndisk /dev/sda: 42.9 gb, 42949672960 bytes, 83886080 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk label type: dos\ndisk identifier: 0x000cbb0e\n\n   device boot      start         end      blocks   id  system\n/dev/sda1   *        2048     2099199     1048576   83  linux\n/dev/sda2         2099200    83886079    40893440   8e  linux lvm\n\ndisk /dev/sdb: 107.4 gb, 107374182400 bytes, 209715200 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\n\n\ndisk /dev/sdc: 107.4 gb, 107374182400 bytes, 209715200 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\n\n\ndisk /dev/mapper/centos-root: 37.7 gb, 37706792960 bytes, 73646080 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\n\n\ndisk /dev/mapper/centos-swap: 4160 mb, 4160749568 bytes, 8126464 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\n\n\n\n# 创建lvm逻辑卷\n\n1、将物理硬盘格式化成pv（物理卷） 使用的是pvcreate命令pvcreate /dev/sdb /dev/sdc (将/dev/sdb /dev/sdc两块硬盘创建为物理卷)\n\n[root@localhost ~]# pvcreate /dev/sdb /dev/sdc\n  physical volume "/dev/sdb" successfully created.\n  physical volume "/dev/sdc" successfully created.\n\n\n通过pvdisplay或pvs查看当前的pv信息，可以看到两块100g的物理卷已经成功创建\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n\n[root@localhost ~]# pvdisplay\n  --- physical volume ---\n  pv name               /dev/sda2\n  vg name               centos\n  pv size               <39.00 gib / not usable 3.00 mib\n  allocatable           yes \n  pe size               4.00 mib\n  total pe              9983\n  free pe               1\n  allocated pe          9982\n  pv uuid               ut8j7d-rcmm-npfd-cfzc-hdxf-3ogz-cx0si8\n   \n  "/dev/sdb" is a new physical volume of "100.00 gib"\n  --- new physical volume ---\n  pv name               /dev/sdb\n  vg name               \n  pv size               100.00 gib\n  allocatable           no\n  pe size               0   \n  total pe              0\n  free pe               0\n  allocated pe          0\n  pv uuid               xcglbd-zjl5-sgrc-4f5i-huhz-2mac-decnqa\n   \n  "/dev/sdc" is a new physical volume of "100.00 gib"\n  --- new physical volume ---\n  pv name               /dev/sdc\n  vg name               \n  pv size               100.00 gib\n  allocatable           no\n  pe size               0   \n  total pe              0\n  free pe               0\n  allocated pe          0\n  pv uuid               bgqokv-1nch-vqjq-tpt3-kxeb-oiql-upvq9h\n\n\n2、创建卷组（vg），并将pv加入到卷组中 通过vgcreate命令vgcreate vg /dev/sdb /dev/sdc\n\n[root@localhost ~]# vgcreate vg /dev/sdb /dev/sdc\n  volume group "vg" successfully created\n\n\n通过vgdisplay或vgs命令查看vg的信息。看到vg已经创建好了，大小是两个pv的大大小也就是100g+100g，大概是200g的样子，这里显示199.99g\n\n[root@localhost ~]# vgs\n  vg     #pv #lv #sn attr   vsize   vfree  \n  centos   1   2   0 wz--n- <39.00g   4.00m\n  vg       2   0   0 wz--n- 199.99g 199.99g\n\n\n3、基于卷组（vg）创建逻辑卷(lv) 通过lvcreate命令lvcreate -n app -l 50g vg (基于vg创建逻辑卷lv,名字为app,大小为50g)\n\n[root@localhost ~]# lvcreate -n app -l 50g vg\n  logical volume "app" created.\n\n\n用lvdisplay或lvs命令查看创建好的逻辑卷。可以看到名字为app的逻辑卷lv已经创建好了，它是基于vg创建的，大小为50g\n\n[root@localhost ~]# lvs\n  lv   vg     attr       lsize   pool origin data%  meta%  move log cpy%sync convert\n  root centos -wi-ao---- <35.12g                                                    \n  swap centos -wi-ao----  <3.88g                                                    \n  app  vg     -wi-a-----  50.00g   \n\n\n到这里，lv就创建好了，但是要用起来，还得格式化并挂载到我们的文件系统。\n\n\n# 格式化并使用逻辑卷\n\n1、格式化 mkfs -t ext4 /dev/vg/app 用ext4的格式格式化/dev/vg/app\n\n[root@localhost ~]# mkfs -t ext4 /dev/vg/app\nmke2fs 1.42.9 (28-dec-2013)\nfilesystem label=\nos type: linux\nblock size=4096 (log=2)\nfragment size=4096 (log=2)\nstride=0 blocks, stripe width=0 blocks\n3276800 inodes, 13107200 blocks\n655360 blocks (5.00%) reserved for the super user\nfirst data block=0\nmaximum filesystem blocks=2162163712\n400 block groups\n32768 blocks per group, 32768 fragments per group\n8192 inodes per group\nsuperblock backups stored on blocks: \n        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, \n        4096000, 7962624, 11239424\n\nallocating group tables: done                            \nwriting inode tables: done                            \ncreating journal (32768 blocks): done\nwriting superblocks and filesystem accounting information: done  \n\n\n2、挂载 创建挂载点 mkdir /app 将/dev/vg/app 挂载到/app\n\nmount /dev/vg/app /app\n\n\n然后df -h 可以看到已经挂载到/app下了 ，大小为lv 的大小50g\n\n[root@localhost app]# df -h\nfilesystem               size  used avail use% mounted on\ndevtmpfs                 2.0g     0  2.0g   0% /dev\ntmpfs                    2.0g     0  2.0g   0% /dev/shm\ntmpfs                    2.0g  8.6m  2.0g   1% /run\ntmpfs                    2.0g     0  2.0g   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35g  1.3g   32g   4% /\n/dev/sda1                976m  108m  802m  12% /boot\ntmpfs                    396m     0  396m   0% /run/user/0\n/dev/mapper/vg-app        50g   53m   47g   1% /app\n\n\n进入/app目录执行命令touch test.txt 创建一个测试文件test.txt，可以看到该挂载点是可以用了。\n\n[root@localhost app]# touch test.txt\n[root@localhost app]# ll\ntotal 0\n-rw-r--r--. 1 root root 0 may 28 17:50 test.txt\n\n\n设置开机加载\n\necho "/dev/vg/app /app ext4 defaults 0 0" >>/etc/fstab\n\n\n\n# 扩容逻辑卷\n\n这里扩容分两种情况，一种情况是vg还有足够的空间，那么就可以直接扩lv就可以了。另外一种情况是要扩的空间已经超过了vg的大小，那么就可以通过加物理磁盘扩充到vg里，然后再扩lv。\n\n\n# 第一种情况，扩的空间大小在vg的容量范围之内\n\n[root@localhost app]# vgs\n  vg     #pv #lv #sn attr   vsize   vfree  \n  centos   1   2   0 wz--n- <39.00g   4.00m\n  vg       2   1   0 wz--n- 199.99g 149.99g\n\n\n现在vg的大小为200g，现在/app是50g，计划扩到80g，没有超过vg的大小那么可以直接扩lv就可以了。\n\n第一步：首先卸载设备和挂载点的关联\n\numount /app\n\n\n第二步：将逻辑卷/dev/vg/app 扩展到80g lvextend -l 80g /dev/vg/app 可以清楚的看到vg/app从50g扩容到了80g\n\n[root@localhost app]# lvextend -l 80g /dev/vg/app\n  size of logical volume vg/app changed from 50.00 gib (12800 extents) to 80.00 gib (20480 extents).\n  logical volume vg/app successfully resized.\n\n\n第三步：检查硬盘（lv）完整性，并重置硬盘(lv)容量 e2fsck -f /dev/vg/app 检查硬盘完整性\n\n[root@localhost app]# e2fsck -f /dev/vg/app\ne2fsck 1.42.9 (28-dec-2013)\npass 1: checking inodes, blocks, and sizes\npass 2: checking directory structure\npass 3: checking directory connectivity\npass 4: checking reference counts\npass 5: checking group summary information\n/dev/vg/app: 11/3276800 files (0.0% non-contiguous), 251790/13107200 blocks\n\n\nresize2fs /dev/vg/app 重置硬盘(lv)容量，这一步必需要做，否则即使扩了容量，但看到的还是扩容之前的容量。\n\n[root@localhost app]# resize2fs /dev/vg/app\nresize2fs 1.42.9 (28-dec-2013)\nresizing the filesystem on /dev/vg/app to 20971520 (4k) blocks.\nthe filesystem on /dev/vg/app is now 20971520 blocks long.\n\n\n第四步：重新挂载硬盘并查看 mount -a并执行df -h 可以看到/app已经成功扩容到80g了\n\n[root@localhost app]# mount -a\n[root@localhost app]# df -h\nfilesystem               size  used avail use% mounted on\ndevtmpfs                 2.0g     0  2.0g   0% /dev\ntmpfs                    2.0g     0  2.0g   0% /dev/shm\ntmpfs                    2.0g  8.6m  2.0g   1% /run\ntmpfs                    2.0g     0  2.0g   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35g  1.3g   32g   4% /\n/dev/sda1                976m  108m  802m  12% /boot\ntmpfs                    396m     0  396m   0% /run/user/0\n/dev/mapper/vg-app        79g   56m   75g   1% /app\n\n\nls 查看/app里面的文件还在，说明扩容对文件数据没有啥影响。\n\n[root@localhost app]# ls\ntest.txt\n\n\n如果扩容的大小超过了vg的大小怎么办呢？可以通过扩硬件的方式，加块硬盘到vg然后再扩lv。\n\n\n# 第二种情况，扩容的大小超过了vg的大小\n\n现在vg的大小为200g，现在/app是80g，计划扩到350g，已经没有超过vg的200g大小那么就需要先加硬盘，然后扩vg，再扩lv。\n\n第一步：添加硬盘\n\n\n\n通过fdisk -l命令查看添加\n\n\n\n\n \n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[root@localhost ~]# fdisk -l\n\ndisk /dev/sdd: 214.7 gb, 214748364800 bytes, 419430400 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\n\n\ndisk /dev/sdb: 107.4 gb, 107374182400 bytes, 209715200 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\n\n\ndisk /dev/sda: 42.9 gb, 42949672960 bytes, 83886080 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk label type: dos\ndisk identifier: 0x000cbb0e\n\n   device boot      start         end      blocks   id  system\n/dev/sda1   *        2048     2099199     1048576   83  linux\n/dev/sda2         2099200    83886079    40893440   8e  linux lvm\n\ndisk /dev/sdc: 107.4 gb, 107374182400 bytes, 209715200 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\n\n\ndisk /dev/mapper/centos-root: 37.7 gb, 37706792960 bytes, 73646080 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\n\n\ndisk /dev/mapper/centos-swap: 4160 mb, 4160749568 bytes, 8126464 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\n\n\ndisk /dev/mapper/vg-app: 85.9 gb, 85899345920 bytes, 167772160 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\n\n\n第二步：扩容vg 将新的硬盘扩到vg卷组里，卸载/app\n\numount /app\n\n\n执行命令vgextend vg /dev/sdd 将新添加的硬盘/dev/sdd添加到vg卷组里\n\n[root@localhost ~]# vgextend vg /dev/sdd\n  physical volume "/dev/sdd" successfully created.\n  volume group "vg" successfully extended\n\n\n第三步：扩容lv 将逻辑卷/dev/vg/app 扩展到350g，执行命令 lvextend -l 11g /dev/vg/app 可以看到vg/app从原来的8g扩到了350g\n\n[root@localhost ~]# lvextend -l 350g /dev/vg/app\n  size of logical volume vg/app changed from 80.00 gib (20480 extents) to 350.00 gib (89600 extents).\n  logical volume vg/app successfully resized.\n\n\n第四步：同样检查硬盘（lv）完整性，并重置硬盘(lv)容量e2fsck -f /dev/vg/app 检查硬盘完整性，resize2fs /dev/vg/app 重置硬盘(lv)容量\n\n[root@localhost ~]# umount /app\n[root@localhost ~]# df -h\nfilesystem               size  used avail use% mounted on\ndevtmpfs                 2.0g     0  2.0g   0% /dev\ntmpfs                    2.0g     0  2.0g   0% /dev/shm\ntmpfs                    2.0g  8.6m  2.0g   1% /run\ntmpfs                    2.0g     0  2.0g   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35g  1.3g   32g   4% /\n/dev/sda1                976m  108m  802m  12% /boot\ntmpfs                    396m     0  396m   0% /run/user/0\n[root@localhost ~]# e2fsck -f /dev/vg/app\ne2fsck 1.42.9 (28-dec-2013)\npass 1: checking inodes, blocks, and sizes\npass 2: checking directory structure\npass 3: checking directory connectivity\npass 4: checking reference counts\npass 5: checking group summary information\n/dev/vg/app: 11/5242880 files (0.0% non-contiguous), 376182/20971520 blocks\n[root@localhost ~]# resize2fs /dev/vg/app\nresize2fs 1.42.9 (28-dec-2013)\nresizing the filesystem on /dev/vg/app to 91750400 (4k) blocks.\nthe filesystem on /dev/vg/app is now 91750400 blocks long.\n\n\n第五步：重新挂载硬盘并查看 mount -a并执行df -h 可以看到/app已经成功扩容到350g了\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n[root@localhost ~]# mount -a\n[root@localhost ~]# df -h\nfilesystem               size  used avail use% mounted on\ndevtmpfs                 2.0g     0  2.0g   0% /dev\ntmpfs                    2.0g     0  2.0g   0% /dev/shm\ntmpfs                    2.0g  8.6m  2.0g   1% /run\ntmpfs                    2.0g     0  2.0g   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35g  1.3g   32g   4% /\n/dev/sda1                976m  108m  802m  12% /boot\ntmpfs                    396m     0  396m   0% /run/user/0\n/dev/mapper/vg-app       345g   66m  327g   1% /app\n\n\n\n# 缩小逻辑卷\n\n相对于逻辑卷扩容，缩小逻辑卷，数据丢失的风险更大。所以在生产环境中操作一定要注意提前备份好数据。在对lvm逻辑卷进行缩小操作之前，先把要缩小的文件系统卸载并检查文件系统的完整性。 现在我们将/app由现在的350g缩到100g\n\n第一步：卸载/app并检查文件系统完整性\n\n[root@localhost ~]# umount /app\n[root@localhost ~]# e2fsck -f /dev/vg/app\ne2fsck 1.42.9 (28-dec-2013)\npass 1: checking inodes, blocks, and sizes\npass 2: checking directory structure\npass 3: checking directory connectivity\npass 4: checking reference counts\npass 5: checking group summary information\n/dev/vg/app: 11/22937600 files (0.0% non-contiguous), 1489518/91750400 blocks\n\n\n第二步：把逻辑卷缩容到100g，执行命令：resize2fs /dev/vg/app 100g和lvreduce -l 100g /dev/vg/app\n\n[root@localhost ~]# resize2fs /dev/vg/app 100g\nresize2fs 1.42.9 (28-dec-2013)\nresizing the filesystem on /dev/vg/app to 26214400 (4k) blocks.\nthe filesystem on /dev/vg/app is now 26214400 blocks long.\n\n[root@localhost ~]# lvreduce -l 100g /dev/vg/app\n  warning: reducing active logical volume to 100.00 gib.\n  this may destroy your data (filesystem etc.)\ndo you really want to reduce vg/app? [y/n]: y\n  size of logical volume vg/app changed from 350.00 gib (89600 extents) to 100.00 gib (25600 extents).\n  logical volume vg/app successfully resized.\n\n\n第三步：重新挂载并查看状态\n\n[root@localhost ~]# mount -a \n[root@localhost ~]# df -h \nfilesystem               size  used avail use% mounted on\ndevtmpfs                 2.0g     0  2.0g   0% /dev\ntmpfs                    2.0g     0  2.0g   0% /dev/shm\ntmpfs                    2.0g  8.6m  2.0g   1% /run\ntmpfs                    2.0g     0  2.0g   0% /sys/fs/cgroup\n/dev/mapper/centos-root   35g  1.3g   32g   4% /\n/dev/sda1                976m  108m  802m  12% /boot\ntmpfs                    396m     0  396m   0% /run/user/0\n/dev/mapper/vg-app        99g   60m   94g   1% /app\n\n\n通过lvm的管理，创建、扩容、缩容，可以看到通过lvm技术可以实现系统存储空间的动态的调整。\n\n基本原理是将多个物理硬盘创建成pv(物理卷)，这些物理卷是动态调整的物理基础，通过vg将pv管理起来形成一个整体的资源池。在vg中划分lv来动态调整逻辑卷的大小。\n\n至此，centos7 lvm操作实战完成 (゜-゜)つロ 干杯~',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"部署后台portal-web项目",frontmatter:{title:"部署后台portal-web项目",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/1b51c8/"},regularPath:"/03.CentOS/03.%E5%AE%9E%E6%88%98/01.%E9%83%A8%E7%BD%B2%E5%90%8E%E5%8F%B0portal-web%E9%A1%B9%E7%9B%AE.html",relativePath:"03.CentOS/03.实战/01.部署后台portal-web项目.md",key:"v-1ce7bb32",path:"/pages/1b51c8/",headers:[{level:2,title:"拷贝portal-web.zip到tomcat并解压",slug:"拷贝portal-web-zip到tomcat并解压",normalizedTitle:"拷贝portal-web.zip到tomcat并解压",charIndex:23},{level:2,title:"修改配置文件 config.properties",slug:"修改配置文件-config-properties",normalizedTitle:"修改配置文件 config.properties",charIndex:159},{level:3,title:"修改数据库连接",slug:"修改数据库连接",normalizedTitle:"修改数据库连接",charIndex:307},{level:3,title:"修改静态资源请求地址（默认本机）",slug:"修改静态资源请求地址-默认本机",normalizedTitle:"修改静态资源请求地址（默认本机）",charIndex:496},{level:3,title:"修改视频播放地址和上传视频资源地址（默认本机）",slug:"修改视频播放地址和上传视频资源地址-默认本机",normalizedTitle:"修改视频播放地址和上传视频资源地址（默认本机）",charIndex:556},{level:3,title:"修改Gitlab项目地址和token（Token获取方式参考Gitlab部署章节）",slug:"修改gitlab项目地址和token-token获取方式参考gitlab部署章节",normalizedTitle:"修改gitlab项目地址和token（token获取方式参考gitlab部署章节）",charIndex:687},{level:3,title:"修改domain",slug:"修改domain",normalizedTitle:"修改domain",charIndex:805},{level:3,title:"修改Jenkins配置 填写安装时配置的用户名和密码，credentialsId是我们手动创建的，请根据《创建gitlab-root凭据》章节配置",slug:"修改jenkins配置-填写安装时配置的用户名和密码-credentialsid是我们手动创建的-请根据《创建gitlab-root凭据》章节配置",normalizedTitle:"修改jenkins配置 填写安装时配置的用户名和密码，credentialsid是我们手动创建的，请根据《创建gitlab-root凭据》章节配置",charIndex:844},{level:3,title:"修改sonarqube配置",slug:"修改sonarqube配置",normalizedTitle:"修改sonarqube配置",charIndex:1077},{level:2,title:"修改配置文件 mongodb.properties",slug:"修改配置文件-mongodb-properties",normalizedTitle:"修改配置文件 mongodb.properties",charIndex:1200},{level:2,title:"修改配置文件 license-config.properties",slug:"修改配置文件-license-config-properties",normalizedTitle:"修改配置文件 license-config.properties",charIndex:1442},{level:2,title:"修改配置文件 upload.properties",slug:"修改配置文件-upload-properties",normalizedTitle:"修改配置文件 upload.properties",charIndex:1804},{level:2,title:"创建文件目录",slug:"创建文件目录",normalizedTitle:"创建文件目录",charIndex:2025}],headersStr:"拷贝portal-web.zip到tomcat并解压 修改配置文件 config.properties 修改数据库连接 修改静态资源请求地址（默认本机） 修改视频播放地址和上传视频资源地址（默认本机） 修改Gitlab项目地址和token（Token获取方式参考Gitlab部署章节） 修改domain 修改Jenkins配置 填写安装时配置的用户名和密码，credentialsId是我们手动创建的，请根据《创建gitlab-root凭据》章节配置 修改sonarqube配置 修改配置文件 mongodb.properties 修改配置文件 license-config.properties 修改配置文件 upload.properties 创建文件目录",content:"# 部署后台portal-web项目\n\n\n# 拷贝portal-web.zip到tomcat并解压\n\n# cd /opt/tomcat8/webapps/\n\n\n拷贝部署目录下的portal-web.zip到/opt/tomcat8/webapps下。\n\n解压\n\n# unzip portal-web.zip\n\n\n\n# 修改配置文件 config.properties\n\n编辑config.properties配置文件修改如下配置（修改红色部分）\n\n# vi /opt/tomcat8/webapps/portal-web/WEB-INF/classes/config/config.properties\n\n\n\n# 修改数据库连接\n\njdbc.url=jdbc:mysql://mysql-ip/portal?useUnicode=true&characterEncoding=utf-8&zeroDateTimeBehavior=convertToNull&autoReconnect=true\njdbc.username=root\njdbc.password=heyuqiang\n\n\n\n# 修改静态资源请求地址（默认本机）\n\nresource.host=http://10.0.0.81:18087\n\n\n\n# 修改视频播放地址和上传视频资源地址（默认本机）\n\nred5.ip=10.0.0.81\nred5Rtmp=rtmp://10.0.0.81/vod\nred5Host=http://10.0.0.81:18080/fileUpload\nplaySeq=1\n\n\n\n# 修改Gitlab项目地址和token（Token获取方式参考Gitlab部署章节）\n\ngitlab.api.url=http://10.0.0.81\ngitlab.api.token=yB9e1V85CfNNSa8RupEF\n\n\n\n# 修改domain\n\ndomain=http://10.0.0.81\n\n\n\n# 修改Jenkins配置 填写安装时配置的用户名和密码，credentialsId是我们手动创建的，请根据《创建gitlab-root凭据》章节配置\n\njenkins.server.url=http://10.0.0.86:8080/jenkins\njenkins.server.username=heyuqiang\njenkins.server.passwd=heyuqiang\njenkins.job.credentialsId=gitlab-root\n\n\n\n# 修改sonarqube配置\n\nsonar.server.url=http://10.0.0.85:9000/sonar\nsonar.server.username=admin\nsonar.server.passwd=heyuqiang\n\n\n\n# 修改配置文件 mongodb.properties\n\n# vi /opt/tomcat8/webapps/portal-web/WEB-INF/classes/config/mongodb.properties\n\n\n以下是mongodb数据配置 包括数据库名，地址,端口，用户名和密码。\n\nmongo.db=portal\nmongo.host=10.0.0.83\nmongo.port=27017\nmongo.user=portal\nmongo.pwd=heyuqiang\n\n\n\n# 修改配置文件 license-config.properties\n\n# vi /opt/tomcat8/webapps/portal-web/WEB-INF/classes/config/license-config.properties\n\n\n#License相关配置\nlicense.subject=license_portal\nlicense.publicAlias=publicCert\nlicense.storePass=public_password1234\nlicense.licensePath=/opt/license_portal/license.lic\nlicense.publicKeysStorePath=/opt/licesnse_portal/publicCerts.keystore\n\n\n\n# 修改配置文件 upload.properties\n\n# vi /opt/tomcat8/webapps/portal-web/WEB-INF/classes/config/upload.properties\n\n\nred5Path=/home/ITP/share/resource\ntempPath=/home/ITP/share/resource/temp\njudgedata=/home/ITP/share/judgedatas\n\n\n\n# 创建文件目录\n\n默认不用修改使用如下命令创建相关目录即可，然后将资源放入对应目录。\n\nmkdir -p /home/ITP/share/resource\nmkdir -p /home/ITP/share/resource/temp\nmkdir -p /home/ITP/share/upload\nmkdir -p /home/ITP/share/judgedata\n",normalizedContent:"# 部署后台portal-web项目\n\n\n# 拷贝portal-web.zip到tomcat并解压\n\n# cd /opt/tomcat8/webapps/\n\n\n拷贝部署目录下的portal-web.zip到/opt/tomcat8/webapps下。\n\n解压\n\n# unzip portal-web.zip\n\n\n\n# 修改配置文件 config.properties\n\n编辑config.properties配置文件修改如下配置（修改红色部分）\n\n# vi /opt/tomcat8/webapps/portal-web/web-inf/classes/config/config.properties\n\n\n\n# 修改数据库连接\n\njdbc.url=jdbc:mysql://mysql-ip/portal?useunicode=true&characterencoding=utf-8&zerodatetimebehavior=converttonull&autoreconnect=true\njdbc.username=root\njdbc.password=heyuqiang\n\n\n\n# 修改静态资源请求地址（默认本机）\n\nresource.host=http://10.0.0.81:18087\n\n\n\n# 修改视频播放地址和上传视频资源地址（默认本机）\n\nred5.ip=10.0.0.81\nred5rtmp=rtmp://10.0.0.81/vod\nred5host=http://10.0.0.81:18080/fileupload\nplayseq=1\n\n\n\n# 修改gitlab项目地址和token（token获取方式参考gitlab部署章节）\n\ngitlab.api.url=http://10.0.0.81\ngitlab.api.token=yb9e1v85cfnnsa8rupef\n\n\n\n# 修改domain\n\ndomain=http://10.0.0.81\n\n\n\n# 修改jenkins配置 填写安装时配置的用户名和密码，credentialsid是我们手动创建的，请根据《创建gitlab-root凭据》章节配置\n\njenkins.server.url=http://10.0.0.86:8080/jenkins\njenkins.server.username=heyuqiang\njenkins.server.passwd=heyuqiang\njenkins.job.credentialsid=gitlab-root\n\n\n\n# 修改sonarqube配置\n\nsonar.server.url=http://10.0.0.85:9000/sonar\nsonar.server.username=admin\nsonar.server.passwd=heyuqiang\n\n\n\n# 修改配置文件 mongodb.properties\n\n# vi /opt/tomcat8/webapps/portal-web/web-inf/classes/config/mongodb.properties\n\n\n以下是mongodb数据配置 包括数据库名，地址,端口，用户名和密码。\n\nmongo.db=portal\nmongo.host=10.0.0.83\nmongo.port=27017\nmongo.user=portal\nmongo.pwd=heyuqiang\n\n\n\n# 修改配置文件 license-config.properties\n\n# vi /opt/tomcat8/webapps/portal-web/web-inf/classes/config/license-config.properties\n\n\n#license相关配置\nlicense.subject=license_portal\nlicense.publicalias=publiccert\nlicense.storepass=public_password1234\nlicense.licensepath=/opt/license_portal/license.lic\nlicense.publickeysstorepath=/opt/licesnse_portal/publiccerts.keystore\n\n\n\n# 修改配置文件 upload.properties\n\n# vi /opt/tomcat8/webapps/portal-web/web-inf/classes/config/upload.properties\n\n\nred5path=/home/itp/share/resource\ntemppath=/home/itp/share/resource/temp\njudgedata=/home/itp/share/judgedatas\n\n\n\n# 创建文件目录\n\n默认不用修改使用如下命令创建相关目录即可，然后将资源放入对应目录。\n\nmkdir -p /home/itp/share/resource\nmkdir -p /home/itp/share/resource/temp\nmkdir -p /home/itp/share/upload\nmkdir -p /home/itp/share/judgedata\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"部署前端ITP项目",frontmatter:{title:"部署前端ITP项目",date:"2022-02-09T10:37:09.000Z",permalink:"/pages/7ba23c/"},regularPath:"/03.CentOS/03.%E5%AE%9E%E6%88%98/02.%E9%83%A8%E7%BD%B2%E5%89%8D%E7%AB%AFITP%E9%A1%B9%E7%9B%AE.html",relativePath:"03.CentOS/03.实战/02.部署前端ITP项目.md",key:"v-095cc5ff",path:"/pages/7ba23c/",headers:[{level:2,title:"拷贝itp.zip到/opt并解压",slug:"拷贝itp-zip到-opt并解压",normalizedTitle:"拷贝itp.zip到/opt并解压",charIndex:16},{level:2,title:"检查修改node服务配置和项目配置",slug:"检查修改node服务配置和项目配置",normalizedTitle:"检查修改node服务配置和项目配置",charIndex:38},{level:2,title:"淘宝镜像加速",slug:"淘宝镜像加速",normalizedTitle:"淘宝镜像加速",charIndex:141},{level:2,title:"安装最新本pm2",slug:"安装最新本pm2",normalizedTitle:"安装最新本pm2",charIndex:209},{level:2,title:"安装最新本fis3",slug:"安装最新本fis3",normalizedTitle:"安装最新本fis3",charIndex:250},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:209},{level:2,title:"启动服务命令",slug:"启动服务命令",normalizedTitle:"启动服务命令",charIndex:315},{level:2,title:"查看pm2服务列表命令",slug:"查看pm2服务列表命令",normalizedTitle:"查看pm2服务列表命令",charIndex:432},{level:2,title:"重启命令",slug:"重启命令",normalizedTitle:"重启命令",charIndex:459},{level:2,title:"查看日志 pm2 log",slug:"查看日志-pm2-log",normalizedTitle:"查看日志 pm2 log",charIndex:491},{level:2,title:"创建开机启动脚本",slug:"创建开机启动脚本",normalizedTitle:"创建开机启动脚本",charIndex:518},{level:2,title:"添加开机启动项目",slug:"添加开机启动项目",normalizedTitle:"添加开机启动项目",charIndex:665}],headersStr:"拷贝itp.zip到/opt并解压 检查修改node服务配置和项目配置 淘宝镜像加速 安装最新本pm2 安装最新本fis3 安装 启动服务命令 查看pm2服务列表命令 重启命令 查看日志 pm2 log 创建开机启动脚本 添加开机启动项目",content:"# 部署前端ITP项目\n\n\n# 拷贝itp.zip到/opt并解压\n\n\n# 检查修改node服务配置和项目配置\n\nnode服务配置：/config/config.js 主要修改端口和api地址 项目配置：webApp/js/common/config_89dc744.js\n\n\n# 淘宝镜像加速\n\nnpm install --registry https://registry.npm.taobao.org\n\n\n\n# 安装最新本pm2\n\nnpm install pm2@latest -g\n\n\n\n# 安装最新本fis3\n\nnpm install -g fis3\n\n\n\n# 安装\n\nnpm  i --production\n\n\n\n# 启动服务命令\n\ncd /opt\npm2 start ./bin/www -n ITP\n\n\n提示\n\n注：-n 服务名称，详细说明：http://pm2.keymetrics.io/docs/usage/quick-start/\n\n\n# 查看pm2服务列表命令\n\npm2 list\n\n\n\n# 重启命令\n\npm2 restart (服务名或ID)\n\n\n\n# 查看日志 pm2 log\n\npm2 log\n\n\n\n# 创建开机启动脚本\n\n# touch /opt/start-service.sh\n\n\n添加如下内容：\n\n#!/bin/sh\ncd /opt/ITP\npm2 start ./bin/www -n ITP\n\n\n赋予脚本文件执行权限\n\n# chmod +x start-service.sh\n\n\n\n# 添加开机启动项目\n\n# vi /etc/rc.local\n\n\n添加\n\nsu - root -c '/opt/start-service.sh'\n",normalizedContent:"# 部署前端itp项目\n\n\n# 拷贝itp.zip到/opt并解压\n\n\n# 检查修改node服务配置和项目配置\n\nnode服务配置：/config/config.js 主要修改端口和api地址 项目配置：webapp/js/common/config_89dc744.js\n\n\n# 淘宝镜像加速\n\nnpm install --registry https://registry.npm.taobao.org\n\n\n\n# 安装最新本pm2\n\nnpm install pm2@latest -g\n\n\n\n# 安装最新本fis3\n\nnpm install -g fis3\n\n\n\n# 安装\n\nnpm  i --production\n\n\n\n# 启动服务命令\n\ncd /opt\npm2 start ./bin/www -n itp\n\n\n提示\n\n注：-n 服务名称，详细说明：http://pm2.keymetrics.io/docs/usage/quick-start/\n\n\n# 查看pm2服务列表命令\n\npm2 list\n\n\n\n# 重启命令\n\npm2 restart (服务名或id)\n\n\n\n# 查看日志 pm2 log\n\npm2 log\n\n\n\n# 创建开机启动脚本\n\n# touch /opt/start-service.sh\n\n\n添加如下内容：\n\n#!/bin/sh\ncd /opt/itp\npm2 start ./bin/www -n itp\n\n\n赋予脚本文件执行权限\n\n# chmod +x start-service.sh\n\n\n\n# 添加开机启动项目\n\n# vi /etc/rc.local\n\n\n添加\n\nsu - root -c '/opt/start-service.sh'\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"什么是 Ubuntu",frontmatter:{title:"什么是 Ubuntu",date:"2022-02-09T10:51:04.000Z",permalink:"/pages/49420c/"},regularPath:"/04.Ubuntu/01.Ubuntu/01.%E4%BB%80%E4%B9%88%E6%98%AF%20Ubuntu.html",relativePath:"04.Ubuntu/01.Ubuntu/01.什么是 Ubuntu.md",key:"v-19944e88",path:"/pages/49420c/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:17},{level:2,title:"诞生与定位",slug:"诞生与定位",normalizedTitle:"诞生与定位",charIndex:193},{level:2,title:"特点",slug:"特点",normalizedTitle:"特点",charIndex:1040}],headersStr:"概述 诞生与定位 特点",content:"# 什么是 Ubuntu\n\n\n# 概述\n\n作为Linux发行版中的后起之秀，Ubuntu Linux在短短几年时间里便迅速成长为从Linux初学者到资深专家都十分青睐的发行版。由于Ubuntu Linux是开放源代码的自由软件，用户可以登录Ubuntu Linux的官方网址免费下载该软件的安装包。用户在使用过程中，没有人对该软件进行技术维护，用户只能自己解决遇到的技术故障。\n\n\n# 诞生与定位\n\nUbuntu Linux是由南非人Mark Shuttleworth创办的基于Debian Linux的操作系统，开于2004年10月公布Ubuntu的第一个版本(Ubuntu4.10 Warty Warthog)。Ubuntu适用于笔记本电脑、桌面电脑和服务器，特别是为桌面用户提供尽善尽美的使用体验。Ubuntu几乎包含了所有常用的应用软件：文字处理、电子邮件、软件开发工具和Web服务等。用户下载、使用、分享Ubuntu系统，以及获得技术支持与服务，无需支付任何许可费用。\n\nUbuntu提供了一个健壮、功能丰富的计算环境，既适合家庭使用又适用于商业环境。Ubuntu社区承诺每6个月发布一个新版本，以提供最新最强大的软件。 [3]\n\nUbuntu被视为一种传统的非洲民族理念，同时也被认为是南非共和国的建国准则之一，并且与非洲复兴的理想密切相关。该词源于祖鲁语和科萨语，它的核心理念是“人道待人”，着眼于人们之间相互的忠诚与交流。南非总统曼德拉这样解释：Ubuntu是一个概念，它包含了尊重、互助、分享、交流、关怀、信任、无私的众多内涵：Ubuntu是一种生活方式，提倡宽容和同情他人。可见，Ubuntu精神已经渗透到了南非的政治和日常生活当中。 [3]\n\nUbuntu精神与软件开源精神恰恰不谋而合。作为一个基于Linux的操作系统，Ubuntu Linux试图将这种精神延伸到计算机世界“软件应当被分享，井能够为任何需要的人所获得”。Ubuntu的目标是让世界上的每个人都能得到一个易于使用的Linux版本，不论他所处的地理位置和身体状况。 [3]\n\n在这种Ubuntu精神的指导下，Ubuntu Linux承诺如下所示：\n\nUbuntu将永远免费，包括企业版和安全升级。\n\nUbuntu将全球数百个公司提供商业支持。\n\nUbuntu包含由自由软件团体提供的最佳翻译和本地化。\n\nUbuntu光盘仅仅包含自由软件，鼓励用户使用自由和开源软件，并改善和传播它。\n\n\n# 特点\n\nUbuntu在桌面办公、服务器方面有着不俗的表现，总能够将最新的应用特性囊括其中，主要包括以下几方面： [3]\n\n1、桌面系统使用最新的Gnome、KDE、Xfce等桌面环境组件。 [3]\n\n2、集成搜索工具Tracker，为用户提供方便、智能的桌面资源搜索。 [3]\n\n3、抛弃繁琐的X桌面配置流程，可以轻松使用图形化界面完成复杂的配置。 [3]\n\n4、集成最新的Compiz稳定版本，让用户体验酷炫的3D桌面。 [3]\n\n5、“语言选择”程序提供了常用语言支持的安装功能，让用户可以在系统安装后，方便地安装多语言支持软件包。\n\n6、提供了全套的多媒体应用软件工具，包括处理音频、视频、图形、图像的工具。 [3]\n\n7、集成了Openffice办公套件，帮助用户完成文字处理、电子表格、幻灯片播放等日常办公任务。 [3]\n\n8、含有辅助功能，为残障人士提供辅助性服务，例如，为存在弱视力的用户提供屏显键盘，能够支持Windows NTFS分区的读/写操作，使Windows资源完全共享成为可能。 [3]\n\n9、支持蓝牙(Bluetooth)输入设备，如蓝牙鼠标、蓝牙键盘。 [3]\n\n10、拥有成熟的网络应用工具，从网络配置工具到Firefox网页浏览器、Gaim即时聊天工具、电子邮件工具、BT下载工具等。 [3]\n\n11、加入更多的打印机驱动，包括对HP的一体机(打印机、扫描仪集成)的支持。 [3]\n\n12、进一步加强系统对笔记本电脑的支持，包括系统热键以及更多型号笔记本电脑的休眠与唤醒功能。 [3]\n\n13、与著名的开源软件项目LTSP合作，内置了Linux终端服务器功能，提供对以瘦客户机作为图形终端的支持，大大提高老式PC机的利用率。",normalizedContent:"# 什么是 ubuntu\n\n\n# 概述\n\n作为linux发行版中的后起之秀，ubuntu linux在短短几年时间里便迅速成长为从linux初学者到资深专家都十分青睐的发行版。由于ubuntu linux是开放源代码的自由软件，用户可以登录ubuntu linux的官方网址免费下载该软件的安装包。用户在使用过程中，没有人对该软件进行技术维护，用户只能自己解决遇到的技术故障。\n\n\n# 诞生与定位\n\nubuntu linux是由南非人mark shuttleworth创办的基于debian linux的操作系统，开于2004年10月公布ubuntu的第一个版本(ubuntu4.10 warty warthog)。ubuntu适用于笔记本电脑、桌面电脑和服务器，特别是为桌面用户提供尽善尽美的使用体验。ubuntu几乎包含了所有常用的应用软件：文字处理、电子邮件、软件开发工具和web服务等。用户下载、使用、分享ubuntu系统，以及获得技术支持与服务，无需支付任何许可费用。\n\nubuntu提供了一个健壮、功能丰富的计算环境，既适合家庭使用又适用于商业环境。ubuntu社区承诺每6个月发布一个新版本，以提供最新最强大的软件。 [3]\n\nubuntu被视为一种传统的非洲民族理念，同时也被认为是南非共和国的建国准则之一，并且与非洲复兴的理想密切相关。该词源于祖鲁语和科萨语，它的核心理念是“人道待人”，着眼于人们之间相互的忠诚与交流。南非总统曼德拉这样解释：ubuntu是一个概念，它包含了尊重、互助、分享、交流、关怀、信任、无私的众多内涵：ubuntu是一种生活方式，提倡宽容和同情他人。可见，ubuntu精神已经渗透到了南非的政治和日常生活当中。 [3]\n\nubuntu精神与软件开源精神恰恰不谋而合。作为一个基于linux的操作系统，ubuntu linux试图将这种精神延伸到计算机世界“软件应当被分享，井能够为任何需要的人所获得”。ubuntu的目标是让世界上的每个人都能得到一个易于使用的linux版本，不论他所处的地理位置和身体状况。 [3]\n\n在这种ubuntu精神的指导下，ubuntu linux承诺如下所示：\n\nubuntu将永远免费，包括企业版和安全升级。\n\nubuntu将全球数百个公司提供商业支持。\n\nubuntu包含由自由软件团体提供的最佳翻译和本地化。\n\nubuntu光盘仅仅包含自由软件，鼓励用户使用自由和开源软件，并改善和传播它。\n\n\n# 特点\n\nubuntu在桌面办公、服务器方面有着不俗的表现，总能够将最新的应用特性囊括其中，主要包括以下几方面： [3]\n\n1、桌面系统使用最新的gnome、kde、xfce等桌面环境组件。 [3]\n\n2、集成搜索工具tracker，为用户提供方便、智能的桌面资源搜索。 [3]\n\n3、抛弃繁琐的x桌面配置流程，可以轻松使用图形化界面完成复杂的配置。 [3]\n\n4、集成最新的compiz稳定版本，让用户体验酷炫的3d桌面。 [3]\n\n5、“语言选择”程序提供了常用语言支持的安装功能，让用户可以在系统安装后，方便地安装多语言支持软件包。\n\n6、提供了全套的多媒体应用软件工具，包括处理音频、视频、图形、图像的工具。 [3]\n\n7、集成了openffice办公套件，帮助用户完成文字处理、电子表格、幻灯片播放等日常办公任务。 [3]\n\n8、含有辅助功能，为残障人士提供辅助性服务，例如，为存在弱视力的用户提供屏显键盘，能够支持windows ntfs分区的读/写操作，使windows资源完全共享成为可能。 [3]\n\n9、支持蓝牙(bluetooth)输入设备，如蓝牙鼠标、蓝牙键盘。 [3]\n\n10、拥有成熟的网络应用工具，从网络配置工具到firefox网页浏览器、gaim即时聊天工具、电子邮件工具、bt下载工具等。 [3]\n\n11、加入更多的打印机驱动，包括对hp的一体机(打印机、扫描仪集成)的支持。 [3]\n\n12、进一步加强系统对笔记本电脑的支持，包括系统热键以及更多型号笔记本电脑的休眠与唤醒功能。 [3]\n\n13、与著名的开源软件项目ltsp合作，内置了linux终端服务器功能，提供对以瘦客户机作为图形终端的支持，大大提高老式pc机的利用率。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"Ubuntu 安装教程",frontmatter:{title:"Ubuntu 安装教程",date:"2022-02-09T10:51:04.000Z",permalink:"/pages/075d45/"},regularPath:"/04.Ubuntu/02.Ubuntu%20%E4%BD%BF%E7%94%A8/01.Ubuntu%20%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B.html",relativePath:"04.Ubuntu/02.Ubuntu 使用/01.Ubuntu 安装教程.md",key:"v-35935e9c",path:"/pages/075d45/",headersStr:null,content:"# Ubuntu 18.04 安装教程\n\n进入系统安装的第一个界面，开始系统的安装操作。\n\n1.选择系统语言-English\n\n2.如提示有更新，这里选择Continue without updating\n\n3.键盘设置-English\n\n4.网络选择\n\n5.代理配置\n\n6.修改阿里云镜像地址：http://mirrors.aliyun.com/ubuntu\n\n7.设置文件系统，选择Use An Entire Disk And Set Up LVM\n\n8.选择硬盘，默认选择，直接回车\n\n9.选择ubuntu-lv new, to be ...，回车，选择Edit，回车\n\n10.修改ubuntu-lv默认可用磁盘空间大小，Save\n\n11.选择Done\n\n13.选择继续Continue\n\n14.输入主机和用户信息\n\n15.按空格勾选[x] Install OpenSSH server\n\n15.直接选择Done\n\n16.Reboot Now\n\n17.这里按回车,弹出光盘\n\n18.安装完成进入界面,用户登入\n\n18.安装完成进入界面,用户登入\n\n19.设置时区\n\ndpkg-reconfigure tzdata\n\n\n出现一个对话框，选择Asia，然后按下回车\n\n\n\n再次弹出对话框，选择Shanghai，再次按下回车\n\n\n\n终端显示下图则表示OK！系统重启后仍然时间是正确的\n\nroot@k8s-master:/home/heyuqiang# dpkg-reconfigure tzdata\n\nCurrent default time zone: 'Asia/Shanghai'\nLocal time is now:      Tue Jun  1 10:58:10 CST 2021.\nUniversal Time is now:  Tue Jun  1 02:58:10 UTC 2021.\n",normalizedContent:"# ubuntu 18.04 安装教程\n\n进入系统安装的第一个界面，开始系统的安装操作。\n\n1.选择系统语言-english\n\n2.如提示有更新，这里选择continue without updating\n\n3.键盘设置-english\n\n4.网络选择\n\n5.代理配置\n\n6.修改阿里云镜像地址：http://mirrors.aliyun.com/ubuntu\n\n7.设置文件系统，选择use an entire disk and set up lvm\n\n8.选择硬盘，默认选择，直接回车\n\n9.选择ubuntu-lv new, to be ...，回车，选择edit，回车\n\n10.修改ubuntu-lv默认可用磁盘空间大小，save\n\n11.选择done\n\n13.选择继续continue\n\n14.输入主机和用户信息\n\n15.按空格勾选[x] install openssh server\n\n15.直接选择done\n\n16.reboot now\n\n17.这里按回车,弹出光盘\n\n18.安装完成进入界面,用户登入\n\n18.安装完成进入界面,用户登入\n\n19.设置时区\n\ndpkg-reconfigure tzdata\n\n\n出现一个对话框，选择asia，然后按下回车\n\n\n\n再次弹出对话框，选择shanghai，再次按下回车\n\n\n\n终端显示下图则表示ok！系统重启后仍然时间是正确的\n\nroot@k8s-master:/home/heyuqiang# dpkg-reconfigure tzdata\n\ncurrent default time zone: 'asia/shanghai'\nlocal time is now:      tue jun  1 10:58:10 cst 2021.\nuniversal time is now:  tue jun  1 02:58:10 utc 2021.\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Ubuntu 更换国内源",frontmatter:{title:"Ubuntu 更换国内源",date:"2022-02-09T10:56:45.000Z",permalink:"/pages/5d8a7e/"},regularPath:"/04.Ubuntu/02.Ubuntu%20%E4%BD%BF%E7%94%A8/02.Ubuntu%20%E6%9B%B4%E6%8D%A2%E5%9B%BD%E5%86%85%E6%BA%90.html",relativePath:"04.Ubuntu/02.Ubuntu 使用/02.Ubuntu 更换国内源.md",key:"v-0a2f2246",path:"/pages/5d8a7e/",headers:[{level:3,title:"备份/etc/apt/sources.list文件",slug:"备份-etc-apt-sources-list文件",normalizedTitle:"备份/etc/apt/sources.list文件",charIndex:106},{level:3,title:"新建/etc/apt/sources.list文件并添加以下内容",slug:"新建-etc-apt-sources-list文件并添加以下内容",normalizedTitle:"新建/etc/apt/sources.list文件并添加以下内容",charIndex:192},{level:3,title:"更改完成之后执行以下命令",slug:"更改完成之后执行以下命令",normalizedTitle:"更改完成之后执行以下命令",charIndex:1139},{level:3,title:"其他的一些apt命令",slug:"其他的一些apt命令",normalizedTitle:"其他的一些apt命令",charIndex:1182},{level:3,title:"其他几个国内的源：",slug:"其他几个国内的源",normalizedTitle:"其他几个国内的源：",charIndex:1807}],headersStr:"备份/etc/apt/sources.list文件 新建/etc/apt/sources.list文件并添加以下内容 更改完成之后执行以下命令 其他的一些apt命令 其他几个国内的源：",content:"# Ubuntu 18.04 更换国内源\n\nUbuntu本身的源使用的是国内的源，下载速度比较慢，不像CentOS一样yum安装的时候对镜像站点进项选择， 所以选择了更换成国内的源。 以下内容整合自网络\n\n\n# 备份/etc/apt/sources.list文件\n\nmv /etc/apt/sources.list /etc/apt/sourses.list.backup\n\n\n\n# 新建/etc/apt/sources.list文件并添加以下内容\n\n#阿里云源\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n\n\n\n# 更改完成之后执行以下命令\n\napt update \napt upgrade\n\n\n\n# 其他的一些apt命令\n\nsudo apt-get update  更新源\nsudo apt-get install package 安装包\nsudo apt-get remove package 删除包\nsudo apt-cache search package 搜索软件包\nsudo apt-cache show package  获取包的相关信息，如说明、大小、版本等\nsudo apt-get install package --reinstall  重新安装包\nsudo apt-get -f install  修复安装\nsudo apt-get remove package --purge 删除包，包括配置文件等\nsudo apt-get build-dep package 安装相关的编译环境\nsudo apt-get upgrade 更新已安装的包\nsudo apt-get dist-upgrade 升级系统\nsudo apt-cache depends package 了解使用该包依赖那些包\nsudo apt-cache rdepends package 查看该包被哪些包依赖\nsudo apt-get source package  下载该包的源代码\nsudo apt-get clean && sudo apt-get autoclean 清理无用的包\nsudo apt-get check 检查是否有损坏的依赖\n\n\n\n# 其他几个国内的源：\n\n#中科大源\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n\n#163源\ndeb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\n\n#清华源\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n",normalizedContent:"# ubuntu 18.04 更换国内源\n\nubuntu本身的源使用的是国内的源，下载速度比较慢，不像centos一样yum安装的时候对镜像站点进项选择， 所以选择了更换成国内的源。 以下内容整合自网络\n\n\n# 备份/etc/apt/sources.list文件\n\nmv /etc/apt/sources.list /etc/apt/sourses.list.backup\n\n\n\n# 新建/etc/apt/sources.list文件并添加以下内容\n\n#阿里云源\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n\n\n\n# 更改完成之后执行以下命令\n\napt update \napt upgrade\n\n\n\n# 其他的一些apt命令\n\nsudo apt-get update  更新源\nsudo apt-get install package 安装包\nsudo apt-get remove package 删除包\nsudo apt-cache search package 搜索软件包\nsudo apt-cache show package  获取包的相关信息，如说明、大小、版本等\nsudo apt-get install package --reinstall  重新安装包\nsudo apt-get -f install  修复安装\nsudo apt-get remove package --purge 删除包，包括配置文件等\nsudo apt-get build-dep package 安装相关的编译环境\nsudo apt-get upgrade 更新已安装的包\nsudo apt-get dist-upgrade 升级系统\nsudo apt-cache depends package 了解使用该包依赖那些包\nsudo apt-cache rdepends package 查看该包被哪些包依赖\nsudo apt-get source package  下载该包的源代码\nsudo apt-get clean && sudo apt-get autoclean 清理无用的包\nsudo apt-get check 检查是否有损坏的依赖\n\n\n\n# 其他几个国内的源：\n\n#中科大源\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n\n#163源\ndeb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\n\n#清华源\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"macOS开发环境安装",frontmatter:{title:"macOS开发环境安装",date:"2022-02-09T10:59:04.000Z",permalink:"/pages/d6e0ac/"},regularPath:"/05.MacOS/01.macOS/01.macOS%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html",relativePath:"05.MacOS/01.macOS/01.macOS开发环境安装.md",key:"v-150d8184",path:"/pages/d6e0ac/",headers:[{level:2,title:"JDK 环境变量配置",slug:"jdk-环境变量配置",normalizedTitle:"jdk 环境变量配置",charIndex:18},{level:2,title:"Maven 环境变量配置",slug:"maven-环境变量配置",normalizedTitle:"maven 环境变量配置",charIndex:575}],headersStr:"JDK 环境变量配置 Maven 环境变量配置",content:'# macOS开发环境安装\n\n\n# JDK 环境变量配置\n\n如果你是第一次配置环境变量，可以使用touch .bash_profile 创建一个.bash_profile的隐藏配置文件(如果你是为编辑已存在的配置文件，则使用open -e .bash_profile命令)：\n\ntouch .bash_profile\nopen -e .bash_profile\n\n\n输入如下配置：\n\nJAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_251.jdk/Contents/Home\nPATH=$JAVA_HOME/bin:$PATH:.\nCLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:.\nexport JAVA_HOME\nexport PATH\nexport CLASSPATH\n\n\n然后保存关闭该窗口。\n\n使用source .bash_profile使配置生效\n\nsource .bash_profile\n\n\n输入 echo $JAVA_HOME 显示刚才配置的路径\n\n/Library/Java/JavaVirtualMachines/jdk1.8.0_251.jdk/Contents/Home\n\n\nJDK安装完成\n\n\n# Maven 环境变量配置\n\nMaven 除了以程序构建能力为特色之外，还提供高级项目管理工具。由于 Maven 的缺省构建规则有较高的可重用性，所以常常用两三行 Maven 构建脚本就可以构建简单的项目。由于 Maven 的面向项目的方法，许多 Apache Jakarta 项目发文时使用 Maven，而且公司项目采用 Maven 的比例在持续增长。 Maven这个单词来自于意第绪语（犹太语），意为知识的积累，最初在Jakata Turbine项目中用来简化构建过程。当时有一些项目（有各自Ant build文件），仅有细微的差别，而JAR文件都由CVS来维护。于是希望有一种标准化的方式构建项目，一个清晰的方式定义项目的组成，一个容易的方式发布项目的信息，以及一种简单的方式在多个项目中共享JARs。\n\nMaven 下载地址\n\n下载apache-maven-3.6.3-bin.tar.gz文件解压到/Users/heyuqiang/Tools/apache-maven-3.6.3\n\n编辑.bash_profile文件，添加maven环境变量配置。\n\nsudo vi .bash_profile \n# 粘贴如下内容：\nJAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_251.jdk/Contents/Home\nCLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:.\nMAVEN_HOME=/Users/heyuqiang/Tools/apache-maven-3.6.3\nPATH=$JAVA_HOME/bin:$MAVEN_HOME/bin:$PATH:.\nexport JAVA_HOME\nexport PATH\nexport CLASSPATH\nexport MAVEN_HOME\n\n\n保存并退出编辑。执行如下命令，使配置文件生效。\n\nsource .bash_profile \n\n\n最后一步，执行 mvn -v 命令，验证环境变量配置是否正确。\n\nheyuqiangdeMBP:~ heyuqiang$ mvn -v \n# 出现一下内容表示环境变量配置成功\nApache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)\nMaven home: /Users/heyuqiang/Tools/apache-maven-3.6.3\nJava version: 1.8.0_251, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk1.8.0_251.jdk/Contents/Home/jre\nDefault locale: zh_CN, platform encoding: UTF-8\nOS name: "mac os x", version: "10.14.6", arch: "x86_64", family: "mac"\n\n\n至此，macOS上配置Maven环境变量完成。',normalizedContent:'# macos开发环境安装\n\n\n# jdk 环境变量配置\n\n如果你是第一次配置环境变量，可以使用touch .bash_profile 创建一个.bash_profile的隐藏配置文件(如果你是为编辑已存在的配置文件，则使用open -e .bash_profile命令)：\n\ntouch .bash_profile\nopen -e .bash_profile\n\n\n输入如下配置：\n\njava_home=/library/java/javavirtualmachines/jdk1.8.0_251.jdk/contents/home\npath=$java_home/bin:$path:.\nclasspath=$java_home/lib/tools.jar:$java_home/lib/dt.jar:.\nexport java_home\nexport path\nexport classpath\n\n\n然后保存关闭该窗口。\n\n使用source .bash_profile使配置生效\n\nsource .bash_profile\n\n\n输入 echo $java_home 显示刚才配置的路径\n\n/library/java/javavirtualmachines/jdk1.8.0_251.jdk/contents/home\n\n\njdk安装完成\n\n\n# maven 环境变量配置\n\nmaven 除了以程序构建能力为特色之外，还提供高级项目管理工具。由于 maven 的缺省构建规则有较高的可重用性，所以常常用两三行 maven 构建脚本就可以构建简单的项目。由于 maven 的面向项目的方法，许多 apache jakarta 项目发文时使用 maven，而且公司项目采用 maven 的比例在持续增长。 maven这个单词来自于意第绪语（犹太语），意为知识的积累，最初在jakata turbine项目中用来简化构建过程。当时有一些项目（有各自ant build文件），仅有细微的差别，而jar文件都由cvs来维护。于是希望有一种标准化的方式构建项目，一个清晰的方式定义项目的组成，一个容易的方式发布项目的信息，以及一种简单的方式在多个项目中共享jars。\n\nmaven 下载地址\n\n下载apache-maven-3.6.3-bin.tar.gz文件解压到/users/heyuqiang/tools/apache-maven-3.6.3\n\n编辑.bash_profile文件，添加maven环境变量配置。\n\nsudo vi .bash_profile \n# 粘贴如下内容：\njava_home=/library/java/javavirtualmachines/jdk1.8.0_251.jdk/contents/home\nclasspath=$java_home/lib/tools.jar:$java_home/lib/dt.jar:.\nmaven_home=/users/heyuqiang/tools/apache-maven-3.6.3\npath=$java_home/bin:$maven_home/bin:$path:.\nexport java_home\nexport path\nexport classpath\nexport maven_home\n\n\n保存并退出编辑。执行如下命令，使配置文件生效。\n\nsource .bash_profile \n\n\n最后一步，执行 mvn -v 命令，验证环境变量配置是否正确。\n\nheyuqiangdembp:~ heyuqiang$ mvn -v \n# 出现一下内容表示环境变量配置成功\napache maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)\nmaven home: /users/heyuqiang/tools/apache-maven-3.6.3\njava version: 1.8.0_251, vendor: oracle corporation, runtime: /library/java/javavirtualmachines/jdk1.8.0_251.jdk/contents/home/jre\ndefault locale: zh_cn, platform encoding: utf-8\nos name: "mac os x", version: "10.14.6", arch: "x86_64", family: "mac"\n\n\n至此，macos上配置maven环境变量完成。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"macOS终端利器iTerm2",frontmatter:{title:"macOS终端利器iTerm2",date:"2022-02-09T10:59:04.000Z",permalink:"/pages/3b1a68/"},regularPath:"/05.MacOS/01.macOS/02.macOS%E7%BB%88%E7%AB%AF%E5%88%A9%E5%99%A8iTerm2.html",relativePath:"05.MacOS/01.macOS/02.macOS终端利器iTerm2.md",key:"v-e5b55ae4",path:"/pages/3b1a68/",headers:[{level:2,title:"下载iTerm2",slug:"下载iterm2",normalizedTitle:"下载iterm2",charIndex:79},{level:2,title:"下载oh my zsh 并切换shell为zsh",slug:"下载oh-my-zsh-并切换shell为zsh",normalizedTitle:"下载oh my zsh 并切换shell为zsh",charIndex:155},{level:3,title:"下载oh-my-zsh",slug:"下载oh-my-zsh",normalizedTitle:"下载oh-my-zsh",charIndex:184},{level:3,title:"创建一个新的配置文件",slug:"创建一个新的配置文件",normalizedTitle:"创建一个新的配置文件",charIndex:521},{level:3,title:"切换默认shell为zsh",slug:"切换默认shell为zsh",normalizedTitle:"切换默认shell为zsh",charIndex:594},{level:2,title:"更换zsh的主题为Dracula",slug:"更换zsh的主题为dracula",normalizedTitle:"更换zsh的主题为dracula",charIndex:633},{level:3,title:"下载主题文件",slug:"下载主题文件",normalizedTitle:"下载主题文件",charIndex:654},{level:3,title:"创建一个指向Oh my zsh主题文件夹的符号链接",slug:"创建一个指向oh-my-zsh主题文件夹的符号链接",normalizedTitle:"创建一个指向oh my zsh主题文件夹的符号链接",charIndex:714},{level:2,title:"更换iterm2的主题为Dracula",slug:"更换iterm2的主题为dracula",normalizedTitle:"更换iterm2的主题为dracula",charIndex:1386},{level:2,title:"命令高亮插件zsh-syntax-highlighting",slug:"命令高亮插件zsh-syntax-highlighting",normalizedTitle:"命令高亮插件zsh-syntax-highlighting",charIndex:1660},{level:2,title:"命令提示插件zsh-autosuggestions",slug:"命令提示插件zsh-autosuggestions",normalizedTitle:"命令提示插件zsh-autosuggestions",charIndex:2328},{level:2,title:"提示技巧",slug:"提示技巧",normalizedTitle:"提示技巧",charIndex:3006},{level:2,title:"iTerm2 升级 3.0 后字体变细的解决方法",slug:"iterm2-升级-3-0-后字体变细的解决方法",normalizedTitle:"iterm2 升级 3.0 后字体变细的解决方法",charIndex:3765}],headersStr:"下载iTerm2 下载oh my zsh 并切换shell为zsh 下载oh-my-zsh 创建一个新的配置文件 切换默认shell为zsh 更换zsh的主题为Dracula 下载主题文件 创建一个指向Oh my zsh主题文件夹的符号链接 更换iterm2的主题为Dracula 命令高亮插件zsh-syntax-highlighting 命令提示插件zsh-autosuggestions 提示技巧 iTerm2 升级 3.0 后字体变细的解决方法",content:"# macOS终端利器iTerm2\n\n提示\n\n利用iTerm2+oh-my-zsh+Dracula主题打造我的Mac终端利器\n\n首先附上效果图：\n\n\n\n\n# 下载iTerm2\n\n我是在官网下载安装的https://www.iterm2.com/index.html 直接点击Download下载。\n\n\n\n\n# 下载oh my zsh 并切换shell为zsh\n\n\n# 下载oh-my-zsh\n\n方式一: 使用git 这里下载到~/.oh-my-zsh下\n\n$ git clone https://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh\n\n\n方式二: 使用curl\n\n$ sh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n\n\n方式三: 使用wget\n\n$ sh -c \"$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\"\n\n\n\n# 创建一个新的配置文件\n\n$ cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc\n\n\n\n# 切换默认shell为zsh\n\n$ chsh -s /bin/zsh\n\n\n\n# 更换zsh的主题为Dracula\n\n\n# 下载主题文件\n\n$ git clone https://github.com/dracula/zsh.git\n\n\n\n# 创建一个指向Oh my zsh主题文件夹的符号链接\n\nDRACULA_THEME是你刚才下载主题的目录\n\n$ ln -s $DRACULA_THEME/dracula.zsh-theme ~/.oh-my-zsh/themes/dracula.zsh-theme\n\n\n或者你可以移动主题文件dracula.zsh-theme到~/.oh-my-zsh/themes/下\n\n提示\n\n显示隐藏文件夹的快捷键为command+shift+.\n\n修改zsh主题。编辑~(用户名)下.zshrc文件，修改ZSH_THEME为\"dracula\"\n\n$ vim ~/.zshrc\n\n\n\n\n\n\n\n\n \n \n\n\n\n...\n# Set name of the theme to load --- if set to \"random\", it will\n# load a random theme each time oh-my-zsh is loaded, in which case,\n# to know which specific one was loaded, run: echo $RANDOM_THEME\n# See https://github.com/ohmyzsh/ohmyzsh/wiki/Themes\n#ZSH_THEME=\"robbyrussell\"\nZSH_THEME=\"dracula\"\n...\n\n\n!wq保存退出。\n\n到这里，已经完成了zsh主题的配置，但是距离效果图还差那么一点点，那就是iterm2的主题和命令高亮插件。\n\n\n# 更换iterm2的主题为Dracula\n\n下载iterm2的Dracula主题\n\n$ git clone https://github.com/dracula/iterm.git\n\n\n设置主题：\n\n 1. iTerm2 > Preferences > Profiles > Colors Tab\n 2. Open the Color Presets...\n 3. 从列表中选择import\n 4. 选择刚才下载主题中Dracula.itermcolors 文件，确定\n\n再次打开 Color Presets... ，选择Dracula\n\n\n\n\n# 命令高亮插件zsh-syntax-highlighting\n\n提示\n\n命令正确绿色，命令错误红色\n\n效果图:\n\n下载命令高亮插件 这里下载到用户名下.zsh文件夹下\n\n$ sudo git clone https://github.com/zsh-users/zsh-syntax-highlighting ~/.zsh/zsh-syntax-highlighting\n\n\n编辑配置文件，使用插件\n\n$ vim ~/.zshrc\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n...\n# Which plugins would you like to load?\n# Standard plugins can be found in $ZSH/plugins/\n# Custom plugins may be added to $ZSH_CUSTOM/plugins/\n# Example format: plugins=(rails git textmate ruby lighthouse)\n# Add wisely, as too many plugins slow down shell startup.\nplugins=(git)\n\nsource $ZSH/oh-my-zsh.sh\n\n# 命令高亮\nsource ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\n...\n\n\n!wq 保存退出。\n\nOK，到这里就全部完成了，重启你的iTerm2，享受吧！\n\n\n# 命令提示插件zsh-autosuggestions\n\n提示\n\n效果:输入g会出现相应提示，按↑即可补全\n\n\n\n下载命令提示插件\n\n$ sudo git clone https://github.com/zsh-users/zsh-autosuggestions ~/.zsh/zsh-autosuggestions\n\n\n编辑配置文件，使用插件\n\n$ vim ~/.zshrc\n\n\n添加以下内容:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n...\n# Which plugins would you like to load?\n# Standard plugins can be found in $ZSH/plugins/\n# Custom plugins may be added to $ZSH_CUSTOM/plugins/\n# Example format: plugins=(rails git textmate ruby lighthouse)\n# Add wisely, as too many plugins slow down shell startup.\nplugins=(git)\n\nsource $ZSH/oh-my-zsh.sh\n\n# 命令高亮\nsource ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\n\n# 命令提示\nsource ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh\n...\n\n\n\n# 提示技巧\n\n提示\n\n将shell从bash切换到zsh之后，可能有的环境变量会失效，需要将原来在bash配置文件中的配置转移到zsh配置文件中，这里提供两个方式\n\n 1. 将bash配置文件~/.bash_profile中的环境变量等配置复制到zsh配置文件～/.zshrc中\n\n 2. 在zsh配置文件～/.zshrc中添加下面这行\n\nsource ~/.bash_profile\n\n\n提示\n\nitem2有很多技巧很好用，我列举一些我常用的技巧，读者可以自行搜索或者阅读官方文档来查看完整的技巧\n\n 1. 一些功能和快捷键:\n\n * 鼠标选中即复制;\n * command + d 垂直分屏\n * command + shift + d 水平分屏\n * command + shift + h 打开剪切板(复制历史)\n * command + ; 命令自动完成\n * command + shift + ; 查看历史命令\n * command + option + b 按键回放(输入命令回放, 通过时间线)\n\n提示\n\noh-my-zsh是很强大的，它用于管理zsh的配置，如果你是第一次使用可以参照oh-my-zsh官方文档，同样的，我会列举一些我常用的技巧\n\n 1. 可以使用alias命令查看一些命令的简写，用熟悉了会很方便\n\n➜ ~ alias\n-='cd -'\n...=../..\n....=../../..\n.....=../../../..\n......=../../../../..\n1='cd -'\n2='cd -2'\n3='cd -3'\n4='cd -4'\n5='cd -5'\n6='cd -6'\n7='cd -7'\n8='cd -8'\n9='cd -9'\n_='sudo '\n...\n\n\n\n# iTerm2 升级 3.0 后字体变细的解决方法\n\n提示\n\niTerm2 升级了之后发现我设置的 Courier New 字体变细了 查了一些 issues ，说并不是 bug 只是一个特性，虽然并不知道这个特性有什么好处，但是看惯了老版本粗字体的还是感觉不习惯， Prefs>profiles>text>Use thin strokes for anti-aliasd text 设置为 Never 即可恢复",normalizedContent:"# macos终端利器iterm2\n\n提示\n\n利用iterm2+oh-my-zsh+dracula主题打造我的mac终端利器\n\n首先附上效果图：\n\n\n\n\n# 下载iterm2\n\n我是在官网下载安装的https://www.iterm2.com/index.html 直接点击download下载。\n\n\n\n\n# 下载oh my zsh 并切换shell为zsh\n\n\n# 下载oh-my-zsh\n\n方式一: 使用git 这里下载到~/.oh-my-zsh下\n\n$ git clone https://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh\n\n\n方式二: 使用curl\n\n$ sh -c \"$(curl -fssl https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n\n\n方式三: 使用wget\n\n$ sh -c \"$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -o -)\"\n\n\n\n# 创建一个新的配置文件\n\n$ cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc\n\n\n\n# 切换默认shell为zsh\n\n$ chsh -s /bin/zsh\n\n\n\n# 更换zsh的主题为dracula\n\n\n# 下载主题文件\n\n$ git clone https://github.com/dracula/zsh.git\n\n\n\n# 创建一个指向oh my zsh主题文件夹的符号链接\n\ndracula_theme是你刚才下载主题的目录\n\n$ ln -s $dracula_theme/dracula.zsh-theme ~/.oh-my-zsh/themes/dracula.zsh-theme\n\n\n或者你可以移动主题文件dracula.zsh-theme到~/.oh-my-zsh/themes/下\n\n提示\n\n显示隐藏文件夹的快捷键为command+shift+.\n\n修改zsh主题。编辑~(用户名)下.zshrc文件，修改zsh_theme为\"dracula\"\n\n$ vim ~/.zshrc\n\n\n\n\n\n\n\n\n \n \n\n\n\n...\n# set name of the theme to load --- if set to \"random\", it will\n# load a random theme each time oh-my-zsh is loaded, in which case,\n# to know which specific one was loaded, run: echo $random_theme\n# see https://github.com/ohmyzsh/ohmyzsh/wiki/themes\n#zsh_theme=\"robbyrussell\"\nzsh_theme=\"dracula\"\n...\n\n\n!wq保存退出。\n\n到这里，已经完成了zsh主题的配置，但是距离效果图还差那么一点点，那就是iterm2的主题和命令高亮插件。\n\n\n# 更换iterm2的主题为dracula\n\n下载iterm2的dracula主题\n\n$ git clone https://github.com/dracula/iterm.git\n\n\n设置主题：\n\n 1. iterm2 > preferences > profiles > colors tab\n 2. open the color presets...\n 3. 从列表中选择import\n 4. 选择刚才下载主题中dracula.itermcolors 文件，确定\n\n再次打开 color presets... ，选择dracula\n\n\n\n\n# 命令高亮插件zsh-syntax-highlighting\n\n提示\n\n命令正确绿色，命令错误红色\n\n效果图:\n\n下载命令高亮插件 这里下载到用户名下.zsh文件夹下\n\n$ sudo git clone https://github.com/zsh-users/zsh-syntax-highlighting ~/.zsh/zsh-syntax-highlighting\n\n\n编辑配置文件，使用插件\n\n$ vim ~/.zshrc\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n...\n# which plugins would you like to load?\n# standard plugins can be found in $zsh/plugins/\n# custom plugins may be added to $zsh_custom/plugins/\n# example format: plugins=(rails git textmate ruby lighthouse)\n# add wisely, as too many plugins slow down shell startup.\nplugins=(git)\n\nsource $zsh/oh-my-zsh.sh\n\n# 命令高亮\nsource ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\n...\n\n\n!wq 保存退出。\n\nok，到这里就全部完成了，重启你的iterm2，享受吧！\n\n\n# 命令提示插件zsh-autosuggestions\n\n提示\n\n效果:输入g会出现相应提示，按↑即可补全\n\n\n\n下载命令提示插件\n\n$ sudo git clone https://github.com/zsh-users/zsh-autosuggestions ~/.zsh/zsh-autosuggestions\n\n\n编辑配置文件，使用插件\n\n$ vim ~/.zshrc\n\n\n添加以下内容:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n...\n# which plugins would you like to load?\n# standard plugins can be found in $zsh/plugins/\n# custom plugins may be added to $zsh_custom/plugins/\n# example format: plugins=(rails git textmate ruby lighthouse)\n# add wisely, as too many plugins slow down shell startup.\nplugins=(git)\n\nsource $zsh/oh-my-zsh.sh\n\n# 命令高亮\nsource ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\n\n# 命令提示\nsource ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh\n...\n\n\n\n# 提示技巧\n\n提示\n\n将shell从bash切换到zsh之后，可能有的环境变量会失效，需要将原来在bash配置文件中的配置转移到zsh配置文件中，这里提供两个方式\n\n 1. 将bash配置文件~/.bash_profile中的环境变量等配置复制到zsh配置文件～/.zshrc中\n\n 2. 在zsh配置文件～/.zshrc中添加下面这行\n\nsource ~/.bash_profile\n\n\n提示\n\nitem2有很多技巧很好用，我列举一些我常用的技巧，读者可以自行搜索或者阅读官方文档来查看完整的技巧\n\n 1. 一些功能和快捷键:\n\n * 鼠标选中即复制;\n * command + d 垂直分屏\n * command + shift + d 水平分屏\n * command + shift + h 打开剪切板(复制历史)\n * command + ; 命令自动完成\n * command + shift + ; 查看历史命令\n * command + option + b 按键回放(输入命令回放, 通过时间线)\n\n提示\n\noh-my-zsh是很强大的，它用于管理zsh的配置，如果你是第一次使用可以参照oh-my-zsh官方文档，同样的，我会列举一些我常用的技巧\n\n 1. 可以使用alias命令查看一些命令的简写，用熟悉了会很方便\n\n➜ ~ alias\n-='cd -'\n...=../..\n....=../../..\n.....=../../../..\n......=../../../../..\n1='cd -'\n2='cd -2'\n3='cd -3'\n4='cd -4'\n5='cd -5'\n6='cd -6'\n7='cd -7'\n8='cd -8'\n9='cd -9'\n_='sudo '\n...\n\n\n\n# iterm2 升级 3.0 后字体变细的解决方法\n\n提示\n\niterm2 升级了之后发现我设置的 courier new 字体变细了 查了一些 issues ，说并不是 bug 只是一个特性，虽然并不知道这个特性有什么好处，但是看惯了老版本粗字体的还是感觉不习惯， prefs>profiles>text>use thin strokes for anti-aliasd text 设置为 never 即可恢复",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"macOS自带终端美化",frontmatter:{title:"macOS自带终端美化",date:"2022-02-09T10:59:04.000Z",permalink:"/pages/078f4d/"},regularPath:"/05.MacOS/01.macOS/03.macOS%E8%87%AA%E5%B8%A6%E7%BB%88%E7%AB%AF%E7%BE%8E%E5%8C%96.html",relativePath:"05.MacOS/01.macOS/03.macOS自带终端美化.md",key:"v-4fa0bc5f",path:"/pages/078f4d/",headers:[{level:2,title:"Terminal.app)",slug:"terminal-app",normalizedTitle:"terminal.app)",charIndex:null},{level:3,title:"Install using Git",slug:"install-using-git",normalizedTitle:"install using git",charIndex:42},{level:3,title:"Install manually",slug:"install-manually",normalizedTitle:"install manually",charIndex:213},{level:3,title:"Activating theme",slug:"activating-theme",normalizedTitle:"activating theme",charIndex:298}],headersStr:"Terminal.app) Install using Git Install manually Activating theme",content:'# macOS自带终端美化Dracula\n\n\n# Terminal.app\n\n\n# Install using Git\n\nIf you are a git user, you can install the theme and keep up to date by cloning the repo:\n\n$ git clone https://github.com/dracula/terminal-app.git\n\n\n\n# Install manually\n\nDownload using the GitHub .zip download option and unzip them.\n\n\n# Activating theme\n\n 1. Terminal > Settings Tab\n 2. Click "Gear" icon\n 3. Click Import...\n 4. Select the Dracula.terminal file\n 5. Click Default',normalizedContent:'# macos自带终端美化dracula\n\n\n# terminal.app\n\n\n# install using git\n\nif you are a git user, you can install the theme and keep up to date by cloning the repo:\n\n$ git clone https://github.com/dracula/terminal-app.git\n\n\n\n# install manually\n\ndownload using the github .zip download option and unzip them.\n\n\n# activating theme\n\n 1. terminal > settings tab\n 2. click "gear" icon\n 3. click import...\n 4. select the dracula.terminal file\n 5. click default',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"macOS常用软件",frontmatter:{title:"macOS常用软件",date:"2022-02-09T10:59:04.000Z",permalink:"/pages/9ee77d/"},regularPath:"/05.MacOS/01.macOS/04.macOS%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6.html",relativePath:"05.MacOS/01.macOS/04.macOS常用软件.md",key:"v-536252ab",path:"/pages/9ee77d/",headers:[{level:2,title:"打开来自身份不明开发者的 Mac App",slug:"打开来自身份不明开发者的-mac-app",normalizedTitle:"打开来自身份不明开发者的 mac app",charIndex:16},{level:2,title:"macOS 10.15 关闭开机提示音",slug:"macos-10-15-关闭开机提示音",normalizedTitle:"macos 10.15 关闭开机提示音",charIndex:71},{level:2,title:"Paragon 驱动程序",slug:"paragon-驱动程序",normalizedTitle:"paragon 驱动程序",charIndex:171},{level:2,title:"Typora",slug:"typora",normalizedTitle:"typora",charIndex:250},{level:2,title:"Visual Studio Code",slug:"visual-studio-code",normalizedTitle:"visual studio code",charIndex:289},{level:2,title:"IINA",slug:"iina",normalizedTitle:"iina",charIndex:349},{level:2,title:"MacZip",slug:"maczip",normalizedTitle:"maczip",charIndex:381},{level:2,title:"iTerm2",slug:"iterm2",normalizedTitle:"iterm2",charIndex:424},{level:2,title:"SwitchHosts",slug:"switchhosts",normalizedTitle:"switchhosts",charIndex:461},{level:2,title:"Docker",slug:"docker",normalizedTitle:"docker",charIndex:499},{level:2,title:"VirtualBox",slug:"virtualbox",normalizedTitle:"virtualbox",charIndex:540},{level:2,title:"Chrome",slug:"chrome",normalizedTitle:"chrome",charIndex:589},{level:2,title:"KeyManager",slug:"keymanager",normalizedTitle:"keymanager",charIndex:636},{level:2,title:"ide-eval-resetter",slug:"ide-eval-resetter",normalizedTitle:"ide-eval-resetter",charIndex:684},{level:2,title:'介绍一个"牛逼闪闪"开源库：ja-netfilter',slug:"介绍一个-牛逼闪闪-开源库-ja-netfilter",normalizedTitle:"介绍一个&quot;牛逼闪闪&quot;开源库：ja-netfilter",charIndex:null},{level:2,title:"macOS 软件下载",slug:"macos-软件下载",normalizedTitle:"macos 软件下载",charIndex:1073},{level:2,title:"lx-music-desktop",slug:"lx-music-desktop",normalizedTitle:"lx-music-desktop",charIndex:1187}],headersStr:'打开来自身份不明开发者的 Mac App macOS 10.15 关闭开机提示音 Paragon 驱动程序 Typora Visual Studio Code IINA MacZip iTerm2 SwitchHosts Docker VirtualBox Chrome KeyManager ide-eval-resetter 介绍一个"牛逼闪闪"开源库：ja-netfilter macOS 软件下载 lx-music-desktop',content:'# macOS常用软件\n\n\n# 打开来自身份不明开发者的 Mac App\n\nsudo spctl --master-disable\n\n\n\n# macOS 10.15 关闭开机提示音\n\n# 关闭提示音：\nsudo nvram StartupMute=%01\n# 打开duang：\nsudo nvram StartupMute=%00\n\n\n\n# Paragon 驱动程序\n\n官网地址：https://www.seagate.com/cn/zh/support/software/paragon/\n\n\n# Typora\n\n官网地址：https://www.typora.io\n\n\n# Visual Studio Code\n\n官网地址：https://code.visualstudio.com/\n\n\n# IINA\n\n官网地址：https://iina.io/\n\n\n# MacZip\n\n官网地址：https://ezip.awehunt.com/\n\n\n# iTerm2\n\n官网地址：https://iterm2.com/\n\n\n# SwitchHosts\n\n官网地址：https://swh.app\n\n\n# Docker\n\n官网地址：https://www.docker.com/\n\n\n# VirtualBox\n\n官网地址：https://www.virtualbox.org/\n\n\n# Chrome\n\n官网地址：https://www.google.cn/chrome/\n\n\n# KeyManager\n\n官网地址：https://www.keymanager.org\n\n\n# ide-eval-resetter\n\n> Z 大的无限重置插件IDE Eval Resetter 在 2021.3 版本中彻底失效了，而这一次，Z大直接放大招了搞了个"牛逼闪闪"开源库：ja-netfilter Z 大的网站 https://zhile.io\n\n下载地址：https://gitee.com/pengzhile/ide-eval-resetter\n\n\n# 介绍一个"牛逼闪闪"开源库：ja-netfilter\n\n> 我们通常会使用防火墙来阻断这些软件的恶意访问。但防火墙也不是万能的，比如：跨平台问题、https下无法精准阻断某个url访问、部分防火墙不能阻断dns访问。 于是就有了我今天开源的这个项目：ja-netfilter！\n\n下载地址：https://github.com/ja-netfilter/ja-netfilter/releases\n\n\n# macOS 软件下载\n\n * 精品MAC应用分线 https://xclient.info/\n * 马可菠萝 https://www.macbl.com/\n * MacWk https://www.macwk.com/\n\n\n# lx-music-desktop\n\n> 一个基于 Electron + Vue 开发的音乐软件。\n\nGitHub：https://github.com/lyswhut/lx-music-desktop',normalizedContent:'# macos常用软件\n\n\n# 打开来自身份不明开发者的 mac app\n\nsudo spctl --master-disable\n\n\n\n# macos 10.15 关闭开机提示音\n\n# 关闭提示音：\nsudo nvram startupmute=%01\n# 打开duang：\nsudo nvram startupmute=%00\n\n\n\n# paragon 驱动程序\n\n官网地址：https://www.seagate.com/cn/zh/support/software/paragon/\n\n\n# typora\n\n官网地址：https://www.typora.io\n\n\n# visual studio code\n\n官网地址：https://code.visualstudio.com/\n\n\n# iina\n\n官网地址：https://iina.io/\n\n\n# maczip\n\n官网地址：https://ezip.awehunt.com/\n\n\n# iterm2\n\n官网地址：https://iterm2.com/\n\n\n# switchhosts\n\n官网地址：https://swh.app\n\n\n# docker\n\n官网地址：https://www.docker.com/\n\n\n# virtualbox\n\n官网地址：https://www.virtualbox.org/\n\n\n# chrome\n\n官网地址：https://www.google.cn/chrome/\n\n\n# keymanager\n\n官网地址：https://www.keymanager.org\n\n\n# ide-eval-resetter\n\n> z 大的无限重置插件ide eval resetter 在 2021.3 版本中彻底失效了，而这一次，z大直接放大招了搞了个"牛逼闪闪"开源库：ja-netfilter z 大的网站 https://zhile.io\n\n下载地址：https://gitee.com/pengzhile/ide-eval-resetter\n\n\n# 介绍一个"牛逼闪闪"开源库：ja-netfilter\n\n> 我们通常会使用防火墙来阻断这些软件的恶意访问。但防火墙也不是万能的，比如：跨平台问题、https下无法精准阻断某个url访问、部分防火墙不能阻断dns访问。 于是就有了我今天开源的这个项目：ja-netfilter！\n\n下载地址：https://github.com/ja-netfilter/ja-netfilter/releases\n\n\n# macos 软件下载\n\n * 精品mac应用分线 https://xclient.info/\n * 马可菠萝 https://www.macbl.com/\n * macwk https://www.macwk.com/\n\n\n# lx-music-desktop\n\n> 一个基于 electron + vue 开发的音乐软件。\n\ngithub：https://github.com/lyswhut/lx-music-desktop',charsets:{cjk:!0},lastUpdated:"2022/02/18, 16:05:14",lastUpdatedTimestamp:1645171514e3},{title:"什么是 Git",frontmatter:{title:"什么是 Git",date:"2022-02-09T11:03:58.000Z",permalink:"/pages/22c758/"},regularPath:"/06.GitLab/01.GitLab/01.%E4%BB%80%E4%B9%88%E6%98%AF%20Git.html",relativePath:"06.GitLab/01.GitLab/01.什么是 Git.md",key:"v-6694efc8",path:"/pages/22c758/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:14}],headersStr:"概述",content:"# 什么是 Git\n\n\n# 概述\n\n\n\n * Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。\n * Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。\n * Git 与常用的版本控制工具 CVS, Subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持。",normalizedContent:"# 什么是 git\n\n\n# 概述\n\n\n\n * git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。\n * git 是 linus torvalds 为了帮助管理 linux 内核开发而开发的一个开放源码的版本控制软件。\n * git 与常用的版本控制工具 cvs, subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"安装 Git",frontmatter:{title:"安装 Git",date:"2022-02-09T11:03:58.000Z",permalink:"/pages/75dfd9/"},regularPath:"/06.GitLab/01.GitLab/02.%E5%AE%89%E8%A3%85%20Git.html",relativePath:"06.GitLab/01.GitLab/02.安装 Git.md",key:"v-2835dba9",path:"/pages/75dfd9/",headers:[{level:2,title:"下载",slug:"下载",normalizedTitle:"下载",charIndex:13},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:2},{level:2,title:"测试",slug:"测试",normalizedTitle:"测试",charIndex:101}],headersStr:"下载 安装 测试",content:"# 安装 Git\n\n\n# 下载\n\nGit下载地址：https://git-scm.com/downloads\n\n\n\n\n# 安装\n\n双击安装文件，然后出现安装向导界面,点击下一步(Next)即可\n\n\n# 测试\n\nheyuqiangdeMBP:~ heyuqiang$ git --version\ngit version 2.23.0\n",normalizedContent:"# 安装 git\n\n\n# 下载\n\ngit下载地址：https://git-scm.com/downloads\n\n\n\n\n# 安装\n\n双击安装文件，然后出现安装向导界面,点击下一步(next)即可\n\n\n# 测试\n\nheyuqiangdembp:~ heyuqiang$ git --version\ngit version 2.23.0\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Git 的一般工作流程",frontmatter:{title:"Git 的一般工作流程",date:"2022-02-09T11:03:58.000Z",permalink:"/pages/8383bb/"},regularPath:"/06.GitLab/01.GitLab/03.Git%20%E7%9A%84%E4%B8%80%E8%88%AC%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.html",relativePath:"06.GitLab/01.GitLab/03.Git 的一般工作流程.md",key:"v-37c9db00",path:"/pages/8383bb/",headersStr:null,content:"# Git 的一般工作流程\n\n * 克隆 Git 资源作为工作目录。\n * 在克隆的资源上添加或修改文件。\n * 如果其他人修改了，你可以更新资源。\n * 在提交前查看修改。\n * 提交修改。\n * 在修改完成后，如果发现错误，可以撤回提交并再次修改并提交。\n\n",normalizedContent:"# git 的一般工作流程\n\n * 克隆 git 资源作为工作目录。\n * 在克隆的资源上添加或修改文件。\n * 如果其他人修改了，你可以更新资源。\n * 在提交前查看修改。\n * 提交修改。\n * 在修改完成后，如果发现错误，可以撤回提交并再次修改并提交。\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Git 的基本操作",frontmatter:{title:"Git 的基本操作",date:"2022-02-09T11:03:58.000Z",permalink:"/pages/20e9b1/"},regularPath:"/06.GitLab/01.GitLab/04.Git%20%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C.html",relativePath:"06.GitLab/01.GitLab/04.Git 的基本操作.md",key:"v-30105e6e",path:"/pages/20e9b1/",headers:[{level:2,title:"获取与创建项目命令",slug:"获取与创建项目命令",normalizedTitle:"获取与创建项目命令",charIndex:16},{level:3,title:"git init",slug:"git-init",normalizedTitle:"git init",charIndex:30},{level:3,title:"git clone",slug:"git-clone",normalizedTitle:"git clone",charIndex:110},{level:2,title:"基本快照",slug:"基本快照",normalizedTitle:"基本快照",charIndex:190},{level:3,title:"git add",slug:"git-add",normalizedTitle:"git add",charIndex:199},{level:3,title:"git status",slug:"git-status",normalizedTitle:"git status",charIndex:254},{level:3,title:"git diff",slug:"git-diff",normalizedTitle:"git diff",charIndex:326},{level:3,title:"git commit",slug:"git-commit",normalizedTitle:"git commit",charIndex:555},{level:3,title:"git reset HEAD",slug:"git-reset-head",normalizedTitle:"git reset head",charIndex:809},{level:2,title:"拉取与推送",slug:"拉取与推送",normalizedTitle:"拉取与推送",charIndex:889},{level:3,title:"git pull",slug:"git-pull",normalizedTitle:"git pull",charIndex:899},{level:3,title:"git push",slug:"git-push",normalizedTitle:"git push",charIndex:1171},{level:2,title:"标签",slug:"标签",normalizedTitle:"标签",charIndex:1268},{level:3,title:"git tag",slug:"git-tag",normalizedTitle:"git tag",charIndex:1275}],headersStr:"获取与创建项目命令 git init git clone 基本快照 git add git status git diff git commit git reset HEAD 拉取与推送 git pull git push 标签 git tag",content:'# Git 的基本操作\n\n\n# 获取与创建项目命令\n\n\n# git init\n\n用 git init 在目录中创建新的 Git 仓库。 你可以在任何时候、任何目录中这么做，完全是本地化的。\n\ngit init\n\n\n\n# git clone\n\n使用 git clone 拷贝一个 Git 仓库到本地，让自己能够查看该项目，或者进行修改。\n\ngit clone [url]\n\n\n\n# 基本快照\n\n\n# git add\n\ngit add 命令可将该文件添加到缓存\n\ngit add <filename>\n\n\n\n# git status\n\ngit status 以查看在你上次提交之后是否有修改。\n\ngit status\ngit status -s\n\n\n\n# git diff\n\n执行 git diff 来查看执行 git status 的结果的详细信息。\n\ngit diff 命令显示已写入缓存与已修改但尚未写入缓存的改动的区别。git diff 有两个主要的应用场景。\n\n * 尚未缓存的改动：git diff\n * 查看已缓存的改动： git diff --cached\n * 查看已缓存的与未缓存的所有改动：git diff HEAD\n * 显示摘要而非整个 diff：git diff --stat\n\n\n# git commit\n\n使用 git add 命令将想要快照的内容写入缓存区， 而执行 git commit 将缓存区内容添加到仓库中。\n\nGit 为你的每一个提交都记录你的名字与电子邮箱地址，所以第一步需要配置用户名和邮箱地址。\n\ngit config --global user.name \'yourname\'\ngit config --global user.email youremail\n\n\n将文件写入缓存区并提供提交注释\n\ngit commit -m \'update message\'\n\n\n\n# git reset HEAD\n\ngit reset HEAD 命令用于取消已缓存的内容。\n\ngit reset HEAD -- <filename>\n\n\n\n# 拉取与推送\n\n\n# git pull\n\ngit pull命令用于从另一个存储库或本地分支获取并集成(整合)。git pull命令的作用是：取回远程主机某个分支的更新，再与本地的指定分支合并，它的完整格式稍稍有点复杂。\n\ngit pull <远程主机名> <远程分支名>:<本地分支名>\n\n\n将远程存储库中的更改合并到当前分支中。在默认模式下，git pull是git fetch后跟git merge FETCH_HEAD的缩写。更准确地说，git pull使用给定的参数运行git fetch，并调用git merge将检索到的分支头合并到当前分支中。\n\n\n# git push\n\ngit push命令用于将本地分支的更新，推送到远程主机。它的格式与git pull命令相似。\n\ngit push <远程主机名> <本地分支名>:<远程分支名>\n\n\n\n# 标签\n\n\n# git tag\n\n如果你达到一个重要的阶段，并希望永远记住那个特别的提交快照，你可以使用 git tag 给它打上标签。\n\n比如说，我们想为我们的 商城 项目发布一个"1.0.0"版本。 我们可以用 git tag -a v1.0.0 命令给最新一次提交打上（HEAD） "v1.0.0" 的标签。\n\n-a 选项意为"创建一个带注解的标签"。 不用 -a 选项也可以执行的，但它不会记录这标签是啥时候打的，谁打的，也不会让你添加个标签的注解。 我推荐一直创建带注解的标签。\n\ngit tag -a v1.0.0\n\n\n如果我们要查看所有标签可以使用以下命令：\n\ngit tag\n',normalizedContent:'# git 的基本操作\n\n\n# 获取与创建项目命令\n\n\n# git init\n\n用 git init 在目录中创建新的 git 仓库。 你可以在任何时候、任何目录中这么做，完全是本地化的。\n\ngit init\n\n\n\n# git clone\n\n使用 git clone 拷贝一个 git 仓库到本地，让自己能够查看该项目，或者进行修改。\n\ngit clone [url]\n\n\n\n# 基本快照\n\n\n# git add\n\ngit add 命令可将该文件添加到缓存\n\ngit add <filename>\n\n\n\n# git status\n\ngit status 以查看在你上次提交之后是否有修改。\n\ngit status\ngit status -s\n\n\n\n# git diff\n\n执行 git diff 来查看执行 git status 的结果的详细信息。\n\ngit diff 命令显示已写入缓存与已修改但尚未写入缓存的改动的区别。git diff 有两个主要的应用场景。\n\n * 尚未缓存的改动：git diff\n * 查看已缓存的改动： git diff --cached\n * 查看已缓存的与未缓存的所有改动：git diff head\n * 显示摘要而非整个 diff：git diff --stat\n\n\n# git commit\n\n使用 git add 命令将想要快照的内容写入缓存区， 而执行 git commit 将缓存区内容添加到仓库中。\n\ngit 为你的每一个提交都记录你的名字与电子邮箱地址，所以第一步需要配置用户名和邮箱地址。\n\ngit config --global user.name \'yourname\'\ngit config --global user.email youremail\n\n\n将文件写入缓存区并提供提交注释\n\ngit commit -m \'update message\'\n\n\n\n# git reset head\n\ngit reset head 命令用于取消已缓存的内容。\n\ngit reset head -- <filename>\n\n\n\n# 拉取与推送\n\n\n# git pull\n\ngit pull命令用于从另一个存储库或本地分支获取并集成(整合)。git pull命令的作用是：取回远程主机某个分支的更新，再与本地的指定分支合并，它的完整格式稍稍有点复杂。\n\ngit pull <远程主机名> <远程分支名>:<本地分支名>\n\n\n将远程存储库中的更改合并到当前分支中。在默认模式下，git pull是git fetch后跟git merge fetch_head的缩写。更准确地说，git pull使用给定的参数运行git fetch，并调用git merge将检索到的分支头合并到当前分支中。\n\n\n# git push\n\ngit push命令用于将本地分支的更新，推送到远程主机。它的格式与git pull命令相似。\n\ngit push <远程主机名> <本地分支名>:<远程分支名>\n\n\n\n# 标签\n\n\n# git tag\n\n如果你达到一个重要的阶段，并希望永远记住那个特别的提交快照，你可以使用 git tag 给它打上标签。\n\n比如说，我们想为我们的 商城 项目发布一个"1.0.0"版本。 我们可以用 git tag -a v1.0.0 命令给最新一次提交打上（head） "v1.0.0" 的标签。\n\n-a 选项意为"创建一个带注解的标签"。 不用 -a 选项也可以执行的，但它不会记录这标签是啥时候打的，谁打的，也不会让你添加个标签的注解。 我推荐一直创建带注解的标签。\n\ngit tag -a v1.0.0\n\n\n如果我们要查看所有标签可以使用以下命令：\n\ngit tag\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:08:14",lastUpdatedTimestamp:1644376094e3},{title:"TortoiseGit 简化 Git 操作",frontmatter:{title:"TortoiseGit 简化 Git 操作",date:"2022-02-09T11:03:58.000Z",permalink:"/pages/7eca40/"},regularPath:"/06.GitLab/01.GitLab/05.TortoiseGit%20%E7%AE%80%E5%8C%96%20Git%20%E6%93%8D%E4%BD%9C.html",relativePath:"06.GitLab/01.GitLab/05.TortoiseGit 简化 Git 操作.md",key:"v-a983545e",path:"/pages/7eca40/",headers:[{level:2,title:"下载",slug:"下载",normalizedTitle:"下载",charIndex:165},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:214},{level:2,title:"安装语言包",slug:"安装语言包",normalizedTitle:"安装语言包",charIndex:231},{level:2,title:"配置",slug:"配置",normalizedTitle:"配置",charIndex:460}],headersStr:"下载 安装 安装语言包 配置",content:"# TortoiseGit 简化 Git 操作\n\nTortoiseGit, 中文名海龟 Git. 海龟 Git 只支持 Windows 系统, 有一个前辈海龟 SVN, TortoiseSVN 和 TortoiseGit 都是非常优秀的开源的版本库客户端. 分为 32 位版与 64 位版.并且支持各种语言,包括简体中文\n\n\n# 下载\n\n下载地址：https://tortoisegit.org/download/\n\n\n\n\n# 安装\n\n我们需要先安装程序包,然后安装语言包(LanguagePack). 因为TortoiseGit 只是一个程序壳,必须依赖一个 Git Core,也就是上一节我们安装的 Git. 所以安装前请确定已完成上一节的操作. 下面以64位版本为演示(64,32位除文件名不一样,其他的操作都一致)\n\n双击安装程序，直接点击下一步(Next)即可\n\n安装完成,点击 Finish 按钮即可\n\n\n# 安装语言包\n\n双击打开语言包安装程序\n\n点击下一步(Alt+N), 语言包会自动安装完成\n\n\n# 配置\n\n在空白处点击鼠标右键, 选择 --\x3e TortoiseGit --\x3e Settings, 然后就可以看到配置界面\n\n选中General,在右边的 Language中选择中文. 不勾选自动升级的复选框,可能还需要指定 Git.exe 文件的路径\n\n再次点击鼠标右键,可以看到弹出菜单中已经变成中文. 原来的 Settings 变成 设置; Clone 变为 克隆",normalizedContent:"# tortoisegit 简化 git 操作\n\ntortoisegit, 中文名海龟 git. 海龟 git 只支持 windows 系统, 有一个前辈海龟 svn, tortoisesvn 和 tortoisegit 都是非常优秀的开源的版本库客户端. 分为 32 位版与 64 位版.并且支持各种语言,包括简体中文\n\n\n# 下载\n\n下载地址：https://tortoisegit.org/download/\n\n\n\n\n# 安装\n\n我们需要先安装程序包,然后安装语言包(languagepack). 因为tortoisegit 只是一个程序壳,必须依赖一个 git core,也就是上一节我们安装的 git. 所以安装前请确定已完成上一节的操作. 下面以64位版本为演示(64,32位除文件名不一样,其他的操作都一致)\n\n双击安装程序，直接点击下一步(next)即可\n\n安装完成,点击 finish 按钮即可\n\n\n# 安装语言包\n\n双击打开语言包安装程序\n\n点击下一步(alt+n), 语言包会自动安装完成\n\n\n# 配置\n\n在空白处点击鼠标右键, 选择 --\x3e tortoisegit --\x3e settings, 然后就可以看到配置界面\n\n选中general,在右边的 language中选择中文. 不勾选自动升级的复选框,可能还需要指定 git.exe 文件的路径\n\n再次点击鼠标右键,可以看到弹出菜单中已经变成中文. 原来的 settings 变成 设置; clone 变为 克隆",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"什么是 GitLab",frontmatter:{title:"什么是 GitLab",date:"2022-02-09T11:03:58.000Z",permalink:"/pages/89c106/"},regularPath:"/06.GitLab/01.GitLab/06.%E4%BB%80%E4%B9%88%E6%98%AF%20GitLab.html",relativePath:"06.GitLab/01.GitLab/06.什么是 GitLab.md",key:"v-5adc669c",path:"/pages/89c106/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:17}],headersStr:"概述",content:"# 什么是 GitLab\n\n\n# 概述\n\nGitLab 是利用 Ruby on Rails 一个开源的版本管理系统，实现一个自托管的 Git 项目仓库，可通过 Web 界面进行访问公开的或者私人项目。它拥有与 Github 类似的功能，能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，它非常易于浏览提交过的版本并提供一个文件历史库。团队成员可以利用内置的简单聊天程序 (Wall) 进行交流。它还提供一个代码片段收集功能可以轻松实现代码复用，便于日后有需要的时候进行查找。",normalizedContent:"# 什么是 gitlab\n\n\n# 概述\n\ngitlab 是利用 ruby on rails 一个开源的版本管理系统，实现一个自托管的 git 项目仓库，可通过 web 界面进行访问公开的或者私人项目。它拥有与 github 类似的功能，能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，它非常易于浏览提交过的版本并提供一个文件历史库。团队成员可以利用内置的简单聊天程序 (wall) 进行交流。它还提供一个代码片段收集功能可以轻松实现代码复用，便于日后有需要的时候进行查找。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:08:14",lastUpdatedTimestamp:1644376094e3},{title:"基于 Docker 安装 GitLab",frontmatter:{title:"基于 Docker 安装 GitLab",date:"2022-02-09T11:03:58.000Z",permalink:"/pages/ffc177/"},regularPath:"/06.GitLab/01.GitLab/07.%E5%9F%BA%E4%BA%8E%20Docker%20%E5%AE%89%E8%A3%85%20GitLab.html",relativePath:"06.GitLab/01.GitLab/07.基于 Docker 安装 GitLab.md",key:"v-0670b552",path:"/pages/ffc177/",headers:[{level:2,title:"使用 docker-compose 安装 GitLab",slug:"使用-docker-compose-安装-gitlab",normalizedTitle:"使用 docker-compose 安装 gitlab",charIndex:26},{level:2,title:"使用 docker-compose 升级 GitLab",slug:"使用-docker-compose-升级-gitlab",normalizedTitle:"使用 docker-compose 升级 gitlab",charIndex:1764},{level:2,title:"安装完成首次使用",slug:"安装完成首次使用",normalizedTitle:"安装完成首次使用",charIndex:1891}],headersStr:"使用 docker-compose 安装 GitLab 使用 docker-compose 升级 GitLab 安装完成首次使用",content:"# 基于 Docker 安装 GitLab\n\n\n# 使用 docker-compose 安装 GitLab\n\n使用 Docker Compose 您可以轻松地配置、安装和升级基于 Docker 的 GitLab 安装。\n\n1.安装 Docker Compose\n\n2.创建docker-compose.yml文件。官方示例\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\nversion: '3'\nservices:\n  web:\n    image: 'gitlab/gitlab-ce:latest'\n    restart: always\n    hostname: '192.168.199.179'\n    environment:\n      TZ: 'Asia/Shanghai'\n      GITLAB_OMNIBUS_CONFIG: |\n        external_url 'http://192.168.199.179'\n    ports:\n      - '80:80'\n      - '443:443'\n      - '22:22'\n    volumes:\n      - '/usr/local/docker/gitlab/config:/etc/gitlab'\n      - '/usr/local/docker/gitlab/logs:/var/log/gitlab'\n      - '/usr/local/docker/gitlab/data:/var/opt/gitlab'\n\n\n提示\n\n需要替换192.168.199.179为你自己的域名或者IP地址。\n\n注意\n\nLinux主机开启SSH服务时，默认监听22端口，此时直接执行docker-compose up会报端口冲突，解决办法：修改主机SSH监听端口，或参考下面自定HTTP和SSH监听端口配置。\n\n\n\n\n\n\n\n \n\n\n\n \n \n \n \n\n\n\n\n\n\n\n\n\nversion: '3'\nservices:\n  web:\n    image: 'gitlab/gitlab-ce:latest'\n    restart: always\n    hostname: '192.168.199.179'\n    environment:\n      TZ: 'Asia/Shanghai'\n      GITLAB_OMNIBUS_CONFIG: |\n        external_url 'http://192.168.199.179:8080'\n        gitlab_rails['gitlab_shell_ssh_port'] = 2224\n        unicorn['port'] = 8888\n        nginx['listen_port'] = 8080\n    ports:\n      - '8080:8080'\n      - '2224:22'\n    volumes:\n      - '/usr/local/docker/gitlab/config:/etc/gitlab'\n      - '/usr/local/docker/gitlab/logs:/var/log/gitlab'\n      - '/usr/local/docker/gitlab/data:/var/opt/gitlab'\n\n\n提示\n\n以上配置相当与使用--publish 8080:8080 --publish 2224:22。\n\n由于unicorn（Ruby的服务器）默认监听8080端口，所以如果想使用8080端口，需要添加unicorn['port'] = 8888配置，如果使用非8080端口可以去掉该unicorn端口配置。\n\n注意\n\n在ports配置中- '2224:22',只修改主机对外开放端口2224即可，千万不要修改对应容器的22端口，否则SSH连接方式将无法认证成功。\n\n3.确保与docker-compose.yml在同一个目录中，并运行docker-compose up -d启动 GitLab\n\n提示\n\n如果服务器配置较低，启动运行可能需要较长时间，请耐心等待\n\n\n# 使用 docker-compose 升级 GitLab\n\n如果您使用docker-compose安装GitLab，那么您只需运行docker-compose pull和docker-compose up -d来下载新版本并升级GitLab实例。\n\n\n# 安装完成首次使用\n\n * 访问地址：http://192.168.199.179:8080\n * 端口 8080 是因为我在配置中设置的外部访问地址为 8080，默认是 80\n * 初始化安装完成后效果如下：\n\n\n\n * 设置管理员初始密码，这里的密码最好是 字母 + 数字 组合，并且 大于等于 8 位\n * 配置完成后登录，管理员账号是 root\n\n\n\n * 登录Gitlab，进入欢迎页面\n\n\n\n * Gitlab版本\n\n",normalizedContent:"# 基于 docker 安装 gitlab\n\n\n# 使用 docker-compose 安装 gitlab\n\n使用 docker compose 您可以轻松地配置、安装和升级基于 docker 的 gitlab 安装。\n\n1.安装 docker compose\n\n2.创建docker-compose.yml文件。官方示例\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\nversion: '3'\nservices:\n  web:\n    image: 'gitlab/gitlab-ce:latest'\n    restart: always\n    hostname: '192.168.199.179'\n    environment:\n      tz: 'asia/shanghai'\n      gitlab_omnibus_config: |\n        external_url 'http://192.168.199.179'\n    ports:\n      - '80:80'\n      - '443:443'\n      - '22:22'\n    volumes:\n      - '/usr/local/docker/gitlab/config:/etc/gitlab'\n      - '/usr/local/docker/gitlab/logs:/var/log/gitlab'\n      - '/usr/local/docker/gitlab/data:/var/opt/gitlab'\n\n\n提示\n\n需要替换192.168.199.179为你自己的域名或者ip地址。\n\n注意\n\nlinux主机开启ssh服务时，默认监听22端口，此时直接执行docker-compose up会报端口冲突，解决办法：修改主机ssh监听端口，或参考下面自定http和ssh监听端口配置。\n\n\n\n\n\n\n\n \n\n\n\n \n \n \n \n\n\n\n\n\n\n\n\n\nversion: '3'\nservices:\n  web:\n    image: 'gitlab/gitlab-ce:latest'\n    restart: always\n    hostname: '192.168.199.179'\n    environment:\n      tz: 'asia/shanghai'\n      gitlab_omnibus_config: |\n        external_url 'http://192.168.199.179:8080'\n        gitlab_rails['gitlab_shell_ssh_port'] = 2224\n        unicorn['port'] = 8888\n        nginx['listen_port'] = 8080\n    ports:\n      - '8080:8080'\n      - '2224:22'\n    volumes:\n      - '/usr/local/docker/gitlab/config:/etc/gitlab'\n      - '/usr/local/docker/gitlab/logs:/var/log/gitlab'\n      - '/usr/local/docker/gitlab/data:/var/opt/gitlab'\n\n\n提示\n\n以上配置相当与使用--publish 8080:8080 --publish 2224:22。\n\n由于unicorn（ruby的服务器）默认监听8080端口，所以如果想使用8080端口，需要添加unicorn['port'] = 8888配置，如果使用非8080端口可以去掉该unicorn端口配置。\n\n注意\n\n在ports配置中- '2224:22',只修改主机对外开放端口2224即可，千万不要修改对应容器的22端口，否则ssh连接方式将无法认证成功。\n\n3.确保与docker-compose.yml在同一个目录中，并运行docker-compose up -d启动 gitlab\n\n提示\n\n如果服务器配置较低，启动运行可能需要较长时间，请耐心等待\n\n\n# 使用 docker-compose 升级 gitlab\n\n如果您使用docker-compose安装gitlab，那么您只需运行docker-compose pull和docker-compose up -d来下载新版本并升级gitlab实例。\n\n\n# 安装完成首次使用\n\n * 访问地址：http://192.168.199.179:8080\n * 端口 8080 是因为我在配置中设置的外部访问地址为 8080，默认是 80\n * 初始化安装完成后效果如下：\n\n\n\n * 设置管理员初始密码，这里的密码最好是 字母 + 数字 组合，并且 大于等于 8 位\n * 配置完成后登录，管理员账号是 root\n\n\n\n * 登录gitlab，进入欢迎页面\n\n\n\n * gitlab版本\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"GitLab 的基本设置",frontmatter:{title:"GitLab 的基本设置",date:"2022-02-09T11:03:58.000Z",permalink:"/pages/655544/"},regularPath:"/06.GitLab/01.GitLab/08.GitLab%20%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%AE%BE%E7%BD%AE.html",relativePath:"06.GitLab/01.GitLab/08.GitLab 的基本设置.md",key:"v-685bb760",path:"/pages/655544/",headers:[{level:2,title:"GitLab 的基本设置",slug:"gitlab-的基本设置-2",normalizedTitle:"gitlab 的基本设置",charIndex:2},{level:3,title:"账户与限制设置",slug:"账户与限制设置",normalizedTitle:"账户与限制设置",charIndex:70},{level:3,title:"注册限制",slug:"注册限制",normalizedTitle:"注册限制",charIndex:129}],headersStr:"GitLab 的基本设置 账户与限制设置 注册限制",content:"# GitLab 的基本设置\n\n\n# GitLab 的基本设置\n\n第一次使用时需要做一些初始化设置，点击“管理区域”--\x3e“设置”\n\n\n# 账户与限制设置\n\n关闭头像功能，由于 Gravatar 头像为网络头像，在网络情况不理想时可能导致访问时卡顿\n\n\n# 注册限制\n\n由于是内部代码托管服务器，可以直接关闭注册功能，由管理员统一创建用户即可\n\n提示\n\n关闭注册功能后，使配置生效需重启Gitlab服务。",normalizedContent:"# gitlab 的基本设置\n\n\n# gitlab 的基本设置\n\n第一次使用时需要做一些初始化设置，点击“管理区域”--\x3e“设置”\n\n\n# 账户与限制设置\n\n关闭头像功能，由于 gravatar 头像为网络头像，在网络情况不理想时可能导致访问时卡顿\n\n\n# 注册限制\n\n由于是内部代码托管服务器，可以直接关闭注册功能，由管理员统一创建用户即可\n\n提示\n\n关闭注册功能后，使配置生效需重启gitlab服务。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:08:14",lastUpdatedTimestamp:1644376094e3},{title:"GitLab 的账户管理",frontmatter:{title:"GitLab 的账户管理",date:"2022-02-09T11:03:58.000Z",permalink:"/pages/0ba162/"},regularPath:"/06.GitLab/01.GitLab/09.GitLab%20%E7%9A%84%E8%B4%A6%E6%88%B7%E7%AE%A1%E7%90%86.html",relativePath:"06.GitLab/01.GitLab/09.GitLab 的账户管理.md",key:"v-7dad8b02",path:"/pages/0ba162/",headers:[{level:2,title:"创建用户",slug:"创建用户",normalizedTitle:"创建用户",charIndex:40},{level:2,title:"设置账户信息",slug:"设置账户信息",normalizedTitle:"设置账户信息",charIndex:111},{level:2,title:"修改用户密码",slug:"修改用户密码",normalizedTitle:"修改用户密码",charIndex:138},{level:2,title:"退出并使用新账户登录",slug:"退出并使用新账户登录",normalizedTitle:"退出并使用新账户登录",charIndex:186}],headersStr:"创建用户 设置账户信息 修改用户密码 退出并使用新账户登录",content:"# GitLab 的账户管理\n\n使用时请不要直接通过 root 用户操作，需要先创建用户，然后通过创建的用户操作，如果你是管理员还需要为其他开发人员分配账户\n\n\n# 创建用户\n\n点击“管理区域”--\x3e“新建用户”\n\n\n# 设置账户信息\n\n同时你可以将自己设置为管理员\n\n\n# 修改用户密码\n\n由于我们创建时并没有配置邮箱，所以还需要重新编辑用户信息并手动设置密码\n\n\n# 退出并使用新账户登录\n\n提示\n\n创建完账户，第一次登录时还会提示你修改登录密码",normalizedContent:"# gitlab 的账户管理\n\n使用时请不要直接通过 root 用户操作，需要先创建用户，然后通过创建的用户操作，如果你是管理员还需要为其他开发人员分配账户\n\n\n# 创建用户\n\n点击“管理区域”--\x3e“新建用户”\n\n\n# 设置账户信息\n\n同时你可以将自己设置为管理员\n\n\n# 修改用户密码\n\n由于我们创建时并没有配置邮箱，所以还需要重新编辑用户信息并手动设置密码\n\n\n# 退出并使用新账户登录\n\n提示\n\n创建完账户，第一次登录时还会提示你修改登录密码",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:08:14",lastUpdatedTimestamp:1644376094e3},{title:"GitLab 创建第一个项目",frontmatter:{title:"GitLab 创建第一个项目",date:"2022-02-09T11:03:58.000Z",permalink:"/pages/aa27ff/"},regularPath:"/06.GitLab/01.GitLab/10.GitLab%20%E5%88%9B%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AA%E9%A1%B9%E7%9B%AE.html",relativePath:"06.GitLab/01.GitLab/10.GitLab 创建第一个项目.md",key:"v-77b753e2",path:"/pages/aa27ff/",headers:[{level:2,title:"GitLab 创建第一个项目",slug:"gitlab-创建第一个项目-2",normalizedTitle:"gitlab 创建第一个项目",charIndex:2},{level:2,title:"初始化项目",slug:"初始化项目",normalizedTitle:"初始化项目",charIndex:93},{level:2,title:"使用 SSH 的方式拉取和推送项目",slug:"使用-ssh-的方式拉取和推送项目",normalizedTitle:"使用 ssh 的方式拉取和推送项目",charIndex:142},{level:3,title:"生成 SSH KEY",slug:"生成-ssh-key",normalizedTitle:"生成 ssh key",charIndex:164},{level:3,title:"复制 SSH-KEY 信息到 GitLab",slug:"复制-ssh-key-信息到-gitlab",normalizedTitle:"复制 ssh-key 信息到 gitlab",charIndex:1145},{level:3,title:"使用 TortoiseGit 克隆项目",slug:"使用-tortoisegit-克隆项目",normalizedTitle:"使用 tortoisegit 克隆项目",charIndex:1284},{level:3,title:"使用 TortoiseGit 推送项目（提交代码）",slug:"使用-tortoisegit-推送项目-提交代码",normalizedTitle:"使用 tortoisegit 推送项目（提交代码）",charIndex:1415},{level:2,title:"查看 GitLab 确认提交成功",slug:"查看-gitlab-确认提交成功",normalizedTitle:"查看 gitlab 确认提交成功",charIndex:2079}],headersStr:"GitLab 创建第一个项目 初始化项目 使用 SSH 的方式拉取和推送项目 生成 SSH KEY 复制 SSH-KEY 信息到 GitLab 使用 TortoiseGit 克隆项目 使用 TortoiseGit 推送项目（提交代码） 查看 GitLab 确认提交成功",content:'# GitLab 创建第一个项目\n\n\n# GitLab 创建第一个项目\n\n点击 + 号 --\x3e 新建项目\n\n输入项目名称及描述信息，设置可见等级为私有，这样别人就看不见你的项目\n\n\n# 初始化项目\n\n我们选择通过增加一个 README 的方式来初始化项目\n\n直接提交修改即可\n\n\n# 使用 SSH 的方式拉取和推送项目\n\n\n# 生成 SSH KEY\n\n使用 ssh-keygen 工具生成，位置在 Git 安装目录下，我的是 C:\\Program Files\\Git\\usr\\bin\n\n输入命令：\n\nssh-keygen -t rsa -C "your_email@example.com"\n\n\n执行成功后的效果：\n\nMicrosoft Windows [版本 10.0.14393]\n(c) 2016 Microsoft Corporation。保留所有权利。\n\nC:\\Program Files\\Git\\usr\\bin>ssh-keygen -t rsa -C "topsale@vip.qq.com"\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/c/Users/ivoov/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /c/Users/ivoov/.ssh/id_rsa.\nYour public key has been saved in /c/Users/ivoov/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:cVesJKa5VnQNihQOTotXUAIyphsqjb7Z9lqOji2704E topsale@vip.qq.com\nThe key\'s randomart image is:\n+---[RSA 2048]----+\n|  + ..=o=.  .+.  |\n| o o + B .+.o.o  |\n|o   . + +=o+..   |\n|.=   .  oo...    |\n|= o     So       |\n|oE .    o        |\n| .. .. .         |\n| o*o+            |\n| *B*oo           |\n+----[SHA256]-----+\n\nC:\\Program Files\\Git\\usr\\bin>\n\n\n\n# 复制 SSH-KEY 信息到 GitLab\n\n秘钥位置在：C:\\Users\\你的用户名\\.ssh 目录下，找到 id_rsa.pub 并使用编辑器打开，如：\n\n\n\n登录 GitLab，点击“用户头像”--\x3e“设置”--\x3e“SSH 密钥”\n\n\n\n成功增加密钥后的效果\n\n\n\n\n# 使用 TortoiseGit 克隆项目\n\n * 新建一个存放代码仓库的本地文件夹\n * 在文件夹空白处按右键\n * 选择“Git 克隆...”\n\n\n\n * 服务项目地址到 URL\n\n\n\n * 如果弹出连接信息请选择是\n\n\n\n * 成功克隆项目到本地\n\n\n\n\n# 使用 TortoiseGit 推送项目（提交代码）\n\n * 创建或修改文件（这里的文件为所有文件，包括：代码、图片等）\n * 我们以创建 .gitignore 过滤配置文件为例，该文件的主要作用为过滤不需要上传的文件，比如：IDE 生成的工程文件、编译后的 class 文件等\n * 在工程目录下，新建 .gitignore 文件，并填入如下配置：\n\n.gradle\n*.sw?\n.#*\n*#\n*~\n/build\n/code\n.classpath\n.project\n.settings\n.metadata\n.factorypath\n.recommenders\nbin\nbuild\ntarget\n.factorypath\n.springBeans\ninterpolated*.xml\ndependency-reduced-pom.xml\nbuild.log\n_site/\n.*.md.html\nmanifest.yml\nMANIFEST.MF\nsettings.xml\nactivemq-data\noverridedb.*\n*.iml\n*.ipr\n*.iws\n.idea\n.DS_Store\n.factorypath\ndump.rdb\ntransaction-logs\n**/overlays/\n**/logs/\n**/temp/\n**/classes/\n\n\n * 右键呼出菜单，选择“提交 Master...”\n\n\n\n * 点击“全部”并填入“日志信息”\n\n\n\n * 点击“提交并推送”\n\n\n\n * 成功后的效果图\n\n\n\n\n# 查看 GitLab 确认提交成功\n\n',normalizedContent:'# gitlab 创建第一个项目\n\n\n# gitlab 创建第一个项目\n\n点击 + 号 --\x3e 新建项目\n\n输入项目名称及描述信息，设置可见等级为私有，这样别人就看不见你的项目\n\n\n# 初始化项目\n\n我们选择通过增加一个 readme 的方式来初始化项目\n\n直接提交修改即可\n\n\n# 使用 ssh 的方式拉取和推送项目\n\n\n# 生成 ssh key\n\n使用 ssh-keygen 工具生成，位置在 git 安装目录下，我的是 c:\\program files\\git\\usr\\bin\n\n输入命令：\n\nssh-keygen -t rsa -c "your_email@example.com"\n\n\n执行成功后的效果：\n\nmicrosoft windows [版本 10.0.14393]\n(c) 2016 microsoft corporation。保留所有权利。\n\nc:\\program files\\git\\usr\\bin>ssh-keygen -t rsa -c "topsale@vip.qq.com"\ngenerating public/private rsa key pair.\nenter file in which to save the key (/c/users/ivoov/.ssh/id_rsa):\nenter passphrase (empty for no passphrase):\nenter same passphrase again:\nyour identification has been saved in /c/users/ivoov/.ssh/id_rsa.\nyour public key has been saved in /c/users/ivoov/.ssh/id_rsa.pub.\nthe key fingerprint is:\nsha256:cvesjka5vnqnihqototxuaiyphsqjb7z9lqoji2704e topsale@vip.qq.com\nthe key\'s randomart image is:\n+---[rsa 2048]----+\n|  + ..=o=.  .+.  |\n| o o + b .+.o.o  |\n|o   . + +=o+..   |\n|.=   .  oo...    |\n|= o     so       |\n|oe .    o        |\n| .. .. .         |\n| o*o+            |\n| *b*oo           |\n+----[sha256]-----+\n\nc:\\program files\\git\\usr\\bin>\n\n\n\n# 复制 ssh-key 信息到 gitlab\n\n秘钥位置在：c:\\users\\你的用户名\\.ssh 目录下，找到 id_rsa.pub 并使用编辑器打开，如：\n\n\n\n登录 gitlab，点击“用户头像”--\x3e“设置”--\x3e“ssh 密钥”\n\n\n\n成功增加密钥后的效果\n\n\n\n\n# 使用 tortoisegit 克隆项目\n\n * 新建一个存放代码仓库的本地文件夹\n * 在文件夹空白处按右键\n * 选择“git 克隆...”\n\n\n\n * 服务项目地址到 url\n\n\n\n * 如果弹出连接信息请选择是\n\n\n\n * 成功克隆项目到本地\n\n\n\n\n# 使用 tortoisegit 推送项目（提交代码）\n\n * 创建或修改文件（这里的文件为所有文件，包括：代码、图片等）\n * 我们以创建 .gitignore 过滤配置文件为例，该文件的主要作用为过滤不需要上传的文件，比如：ide 生成的工程文件、编译后的 class 文件等\n * 在工程目录下，新建 .gitignore 文件，并填入如下配置：\n\n.gradle\n*.sw?\n.#*\n*#\n*~\n/build\n/code\n.classpath\n.project\n.settings\n.metadata\n.factorypath\n.recommenders\nbin\nbuild\ntarget\n.factorypath\n.springbeans\ninterpolated*.xml\ndependency-reduced-pom.xml\nbuild.log\n_site/\n.*.md.html\nmanifest.yml\nmanifest.mf\nsettings.xml\nactivemq-data\noverridedb.*\n*.iml\n*.ipr\n*.iws\n.idea\n.ds_store\n.factorypath\ndump.rdb\ntransaction-logs\n**/overlays/\n**/logs/\n**/temp/\n**/classes/\n\n\n * 右键呼出菜单，选择“提交 master...”\n\n\n\n * 点击“全部”并填入“日志信息”\n\n\n\n * 点击“提交并推送”\n\n\n\n * 成功后的效果图\n\n\n\n\n# 查看 gitlab 确认提交成功\n\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:08:14",lastUpdatedTimestamp:1644376094e3},{title:"GitLab 使用 SSH 免密登录",frontmatter:{title:"GitLab 使用 SSH 免密登录",date:"2022-02-09T11:03:58.000Z",permalink:"/pages/2b230d/"},regularPath:"/06.GitLab/01.GitLab/11.GitLab%20%E4%BD%BF%E7%94%A8%20SSH%20%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95.html",relativePath:"06.GitLab/01.GitLab/11.GitLab 使用 SSH 免密登录.md",key:"v-220226bd",path:"/pages/2b230d/",headers:[{level:2,title:"待完善",slug:"待完善",normalizedTitle:"待完善",charIndex:25}],headersStr:"待完善",content:"# GitLab 使用 SSH 免密登录\n\n\n# 待完善",normalizedContent:"# gitlab 使用 ssh 免密登录\n\n\n# 待完善",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:08:14",lastUpdatedTimestamp:1644376094e3},{title:"什么是 Nexus",frontmatter:{title:"什么是 Nexus",date:"2022-02-09T11:09:28.000Z",permalink:"/pages/ceff17/"},regularPath:"/07.Nexus/01.Nexus/01.%E4%BB%80%E4%B9%88%E6%98%AF%20Nexus.html",relativePath:"07.Nexus/01.Nexus/01.什么是 Nexus.md",key:"v-d809a770",path:"/pages/ceff17/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:16}],headersStr:"概述",content:"# 什么是 Nexus\n\n\n# 概述\n\nNexus 是一个强大的仓库管理器，极大地简化了内部仓库的维护和外部仓库的访问。\n\n2016 年 4 月 6 日 Nexus 3.0 版本发布，相较 2.x 版本有了很大的改变：\n\n * 对低层代码进行了大规模重构，提升性能，增加可扩展性以及改善用户体验。\n * 升级界面，极大的简化了用户界面的操作和管理。\n * 提供新的安装包，让部署更加简单。\n * 增加对 Docker, NeGet, npm, Bower 的支持。\n * 提供新的管理接口，以及增强对自动任务的管理。",normalizedContent:"# 什么是 nexus\n\n\n# 概述\n\nnexus 是一个强大的仓库管理器，极大地简化了内部仓库的维护和外部仓库的访问。\n\n2016 年 4 月 6 日 nexus 3.0 版本发布，相较 2.x 版本有了很大的改变：\n\n * 对低层代码进行了大规模重构，提升性能，增加可扩展性以及改善用户体验。\n * 升级界面，极大的简化了用户界面的操作和管理。\n * 提供新的安装包，让部署更加简单。\n * 增加对 docker, neget, npm, bower 的支持。\n * 提供新的管理接口，以及增强对自动任务的管理。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:13:40",lastUpdatedTimestamp:164437642e4},{title:"基于 Docker 安装 Nexus",frontmatter:{title:"基于 Docker 安装 Nexus",date:"2022-02-09T11:09:28.000Z",permalink:"/pages/dc57a4/"},regularPath:"/07.Nexus/01.Nexus/02.%E5%9F%BA%E4%BA%8E%20Docker%20%E5%AE%89%E8%A3%85%20Nexus.html",relativePath:"07.Nexus/01.Nexus/02.基于 Docker 安装 Nexus.md",key:"v-e3e7063c",path:"/pages/dc57a4/",headers:[{level:2,title:"系统需求",slug:"系统需求",normalizedTitle:"系统需求",charIndex:25},{level:3,title:"CPU",slug:"cpu",normalizedTitle:"cpu",charIndex:34},{level:3,title:"内存",slug:"内存",normalizedTitle:"内存",charIndex:144},{level:2,title:"版本说明",slug:"版本说明",normalizedTitle:"版本说明",charIndex:170},{level:2,title:"使用 Docker 安装 Nexus",slug:"使用-docker-安装-nexus",normalizedTitle:"使用 docker 安装 nexus",charIndex:288},{level:3,title:"方法一 使用docker 数据卷方式",slug:"方法一-使用docker-数据卷方式",normalizedTitle:"方法一 使用docker 数据卷方式",charIndex:365},{level:3,title:"方法二 使用挂载主机目录方式",slug:"方法二-使用挂载主机目录方式",normalizedTitle:"方法二 使用挂载主机目录方式",charIndex:843},{level:2,title:"启动服务",slug:"启动服务",normalizedTitle:"启动服务",charIndex:1366},{level:2,title:"查看日志",slug:"查看日志",normalizedTitle:"查看日志",charIndex:1454},{level:2,title:"登录控制台验证安装",slug:"登录控制台验证安装",normalizedTitle:"登录控制台验证安装",charIndex:1494},{level:2,title:"获取管理员密码",slug:"获取管理员密码",normalizedTitle:"获取管理员密码",charIndex:1574},{level:2,title:"登录Nexus控制台",slug:"登录nexus控制台",normalizedTitle:"登录nexus控制台",charIndex:1742},{level:2,title:"删除 Nexus",slug:"删除-nexus",normalizedTitle:"删除 nexus",charIndex:1842},{level:2,title:"Nginx 反向代理 Nexus",slug:"nginx-反向代理-nexus",normalizedTitle:"nginx 反向代理 nexus",charIndex:1950}],headersStr:"系统需求 CPU 内存 版本说明 使用 Docker 安装 Nexus 方法一 使用docker 数据卷方式 方法二 使用挂载主机目录方式 启动服务 查看日志 登录控制台验证安装 获取管理员密码 登录Nexus控制台 删除 Nexus Nginx 反向代理 Nexus",content:'# 基于 Docker 安装 Nexus\n\n\n# 系统需求\n\n\n# CPU\n\n性能主要受 I/O (磁盘和网络)而不是 CPU 的限制。 可用的 cpu 会影响较长时间运行的操作，也会影响 web 容器的线程分配算法。\n\n * 最小 CPUs: 4\n * 推荐 CPUs: 8+\n\n\n# 内存\n\n * 最小 4GB\n * 官方资料\n\n\n# 版本说明\n\nSonatype Nexus Repository Manager 3的 Dockerfile，从3.18开始，这个映像基于 Red Hat Universal Base Image，而早期版本使用 CentOS。\n\n\n# 使用 Docker 安装 Nexus\n\n我们使用 Docker 来安装和运行 Nexus，使用 Docker 处理持久性存储需求通常有两种方法。\n\n\n# 方法一 使用docker 数据卷方式\n\n使用docker 数据卷方式docker-compose.yml 配置如下：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \n\n\nversion: "3.7"\nservices:\n\n  nexus:\n    restart: always\n    image: sonatype/nexus3\n    container_name: nexus\n    environment:\n      NEXUS_CONTEXT: nexus\n    ports:\n      - "8081:8081"\n    volumes:\n      - nexus-data:/nexus-data\n\nvolumes:\n  nexus-data:\n\n\n * nexus 默认对外暴露端口8081.\n * 使用环境变量自定义 Nexus 的上下文路径NEXUS_CONTEXT，可选 默认：/。\n * 持久目录 /nexus-data 用于配置、日志和存储。Nexus 进程需要对这个目录进行写操作，它以 UID 200的形式运行。\n\n\n# 方法二 使用挂载主机目录方式\n\n将主机目录挂载为卷。 这是不可移植的，因为它依赖于对主机具有正确权限的现有目录。 但是，在某些情况下，如果需要将该卷分配给某些特定的底层存储，那么它会很有用。\n\n1.创建数据卷目录，并修改目录权限。\n\nmkdir /usr/local/docker/nexus/nexus-data && chown -R 200 /usr/local/docker/nexus/nexus-data\n\n\n2.使用docker 挂载主机目录方式docker-compose.yml 配置如下：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nversion: "3.7"\nservices:\n\n  nexus:\n    restart: always\n    image: sonatype/nexus3\n    container_name: nexus\n    environment:\n      NEXUS_CONTEXT: nexus\n    ports:\n      - "8081:8081"\n    volumes:\n      - /usr/local/docker/nexus/data:/nexus-data\n\n\n\n# 启动服务\n\ndocker-compose up -d\n\n\n在一个新的容器中启动服务可能需要一些时间(2-3分钟)。你可以通过跟踪日志来确定 Nexus 是否准备好了。\n\n\n# 查看日志\n\ndocker-compose logs -f nexus\n\n\n\n# 登录控制台验证安装\n\n * 浏览器访问地址：http://ivoov.com:8081/nexus，看到如下页面说明 Nexus 服务启动成功了。\n\n\n\n\n# 获取管理员密码\n\n默认用户是 admin，生成的密码可以在卷中的 admin.password 文件中找到。\n\ncat /var/lib/docker/volumes/nexus_nexus-data/_data/admin.password\n\n\n例如：23896e46-96bf-4b7f-8332-1bd45e32bea5\n\n\n# 登录Nexus控制台\n\n首次登录nexus控制台，管理用户是admin，密码输入上一步admin.password中获取的初始密码。\n\n\n\n进入系统后，根据系统提示重新设置管理密码即可。\n\n\n\n\n# 删除 Nexus\n\n进入docker-compose.yml文件所在目录，执行如下命令：\n\ndocker-compose down --volume\n\n\n * 参数--volume可选，指定是否删除数据卷。\n\n\n# Nginx 反向代理 Nexus\n\nNginx 反向代理 nexus 的服务, 一直卡在 Initialize...\n\n解决方式是添加一个 header X-Forwarded-Proto:\n\nproxy_set_header X-Forwarded-Proto "https";\n\n\nnexus.conf\n\nupstream nexus-server{\n    server 127.0.0.1:8081;\n}\n\nserver {\n    listen 80;\n    server_name xx.xx;\n    location / {\n        return 301 https://xx.xx;\n    }\n\n    location ~ /.well-known {\n        root /tmp;\n    }\n}\n\nserver {\n    listen 443 ssl;\n    server_name xx.xx;\n\n    ssl_certificate cert/xx.pem;\n    ssl_certificate_key cert/xx.key;\n\n    location / {\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-Proto "https";\n        proxy_pass http://nexus-server;\n    }\n}\n',normalizedContent:'# 基于 docker 安装 nexus\n\n\n# 系统需求\n\n\n# cpu\n\n性能主要受 i/o (磁盘和网络)而不是 cpu 的限制。 可用的 cpu 会影响较长时间运行的操作，也会影响 web 容器的线程分配算法。\n\n * 最小 cpus: 4\n * 推荐 cpus: 8+\n\n\n# 内存\n\n * 最小 4gb\n * 官方资料\n\n\n# 版本说明\n\nsonatype nexus repository manager 3的 dockerfile，从3.18开始，这个映像基于 red hat universal base image，而早期版本使用 centos。\n\n\n# 使用 docker 安装 nexus\n\n我们使用 docker 来安装和运行 nexus，使用 docker 处理持久性存储需求通常有两种方法。\n\n\n# 方法一 使用docker 数据卷方式\n\n使用docker 数据卷方式docker-compose.yml 配置如下：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \n\n\nversion: "3.7"\nservices:\n\n  nexus:\n    restart: always\n    image: sonatype/nexus3\n    container_name: nexus\n    environment:\n      nexus_context: nexus\n    ports:\n      - "8081:8081"\n    volumes:\n      - nexus-data:/nexus-data\n\nvolumes:\n  nexus-data:\n\n\n * nexus 默认对外暴露端口8081.\n * 使用环境变量自定义 nexus 的上下文路径nexus_context，可选 默认：/。\n * 持久目录 /nexus-data 用于配置、日志和存储。nexus 进程需要对这个目录进行写操作，它以 uid 200的形式运行。\n\n\n# 方法二 使用挂载主机目录方式\n\n将主机目录挂载为卷。 这是不可移植的，因为它依赖于对主机具有正确权限的现有目录。 但是，在某些情况下，如果需要将该卷分配给某些特定的底层存储，那么它会很有用。\n\n1.创建数据卷目录，并修改目录权限。\n\nmkdir /usr/local/docker/nexus/nexus-data && chown -r 200 /usr/local/docker/nexus/nexus-data\n\n\n2.使用docker 挂载主机目录方式docker-compose.yml 配置如下：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nversion: "3.7"\nservices:\n\n  nexus:\n    restart: always\n    image: sonatype/nexus3\n    container_name: nexus\n    environment:\n      nexus_context: nexus\n    ports:\n      - "8081:8081"\n    volumes:\n      - /usr/local/docker/nexus/data:/nexus-data\n\n\n\n# 启动服务\n\ndocker-compose up -d\n\n\n在一个新的容器中启动服务可能需要一些时间(2-3分钟)。你可以通过跟踪日志来确定 nexus 是否准备好了。\n\n\n# 查看日志\n\ndocker-compose logs -f nexus\n\n\n\n# 登录控制台验证安装\n\n * 浏览器访问地址：http://ivoov.com:8081/nexus，看到如下页面说明 nexus 服务启动成功了。\n\n\n\n\n# 获取管理员密码\n\n默认用户是 admin，生成的密码可以在卷中的 admin.password 文件中找到。\n\ncat /var/lib/docker/volumes/nexus_nexus-data/_data/admin.password\n\n\n例如：23896e46-96bf-4b7f-8332-1bd45e32bea5\n\n\n# 登录nexus控制台\n\n首次登录nexus控制台，管理用户是admin，密码输入上一步admin.password中获取的初始密码。\n\n\n\n进入系统后，根据系统提示重新设置管理密码即可。\n\n\n\n\n# 删除 nexus\n\n进入docker-compose.yml文件所在目录，执行如下命令：\n\ndocker-compose down --volume\n\n\n * 参数--volume可选，指定是否删除数据卷。\n\n\n# nginx 反向代理 nexus\n\nnginx 反向代理 nexus 的服务, 一直卡在 initialize...\n\n解决方式是添加一个 header x-forwarded-proto:\n\nproxy_set_header x-forwarded-proto "https";\n\n\nnexus.conf\n\nupstream nexus-server{\n    server 127.0.0.1:8081;\n}\n\nserver {\n    listen 80;\n    server_name xx.xx;\n    location / {\n        return 301 https://xx.xx;\n    }\n\n    location ~ /.well-known {\n        root /tmp;\n    }\n}\n\nserver {\n    listen 443 ssl;\n    server_name xx.xx;\n\n    ssl_certificate cert/xx.pem;\n    ssl_certificate_key cert/xx.key;\n\n    location / {\n        proxy_set_header host $host;\n        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n        proxy_set_header x-real-ip $remote_addr;\n        proxy_set_header x-forwarded-proto "https";\n        proxy_pass http://nexus-server;\n    }\n}\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Nexus配置阿里云代理仓库",frontmatter:{title:"Nexus配置阿里云代理仓库",date:"2022-02-09T11:09:28.000Z",permalink:"/pages/159c7d/"},regularPath:"/07.Nexus/01.Nexus/03.Nexus%E9%85%8D%E7%BD%AE%E9%98%BF%E9%87%8C%E4%BA%91%E4%BB%A3%E7%90%86%E4%BB%93%E5%BA%93.html",relativePath:"07.Nexus/01.Nexus/03.Nexus配置阿里云代理仓库.md",key:"v-704eb51c",path:"/pages/159c7d/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:21},{level:2,title:"添加阿里云代理仓库",slug:"添加阿里云代理仓库",normalizedTitle:"添加阿里云代理仓库",charIndex:110},{level:2,title:"配置public-repository",slug:"配置public-repository",normalizedTitle:"配置public-repository",charIndex:344}],headersStr:"概述 添加阿里云代理仓库 配置public-repository",content:"# Nexus配置阿里云代理仓库\n\n\n# 概述\n\nNexus默认远程仓库为https://repo1.maven.org/maven2/，国内访问速度慢死，还常连不上。设置阿里云代理仓库可加速访问速度。\n\n\n\n\n\n\n# 添加阿里云代理仓库\n\n创建仓库\n\n\n\n选择maven2 (proxy)\n\n\n\n仓库设置\n\n * Name: maven-aliyun\n * Remote storage: https://maven.aliyun.com/repository/public/\n\n\n\n> 阿里云已经更新了仓库地址，图片上为旧版本\n\n其他默认，翻到最下面，点create repository保存\n\n仓库状态显示Online - Remote Available表示配置成功\n\n\n\n\n# 配置public-repository\n\n\n\n点击maven-aliyun，再点击>按钮\n\n设置Nexus获取依赖的优先顺序。如下图，优先去阿里云的Maven\n\n至此，Nexus配置阿里云代理仓库完成。",normalizedContent:"# nexus配置阿里云代理仓库\n\n\n# 概述\n\nnexus默认远程仓库为https://repo1.maven.org/maven2/，国内访问速度慢死，还常连不上。设置阿里云代理仓库可加速访问速度。\n\n\n\n\n\n\n# 添加阿里云代理仓库\n\n创建仓库\n\n\n\n选择maven2 (proxy)\n\n\n\n仓库设置\n\n * name: maven-aliyun\n * remote storage: https://maven.aliyun.com/repository/public/\n\n\n\n> 阿里云已经更新了仓库地址，图片上为旧版本\n\n其他默认，翻到最下面，点create repository保存\n\n仓库状态显示online - remote available表示配置成功\n\n\n\n\n# 配置public-repository\n\n\n\n点击maven-aliyun，再点击>按钮\n\n设置nexus获取依赖的优先顺序。如下图，优先去阿里云的maven\n\n至此，nexus配置阿里云代理仓库完成。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Maven 仓库介绍",frontmatter:{title:"Maven 仓库介绍",date:"2022-02-09T11:09:28.000Z",permalink:"/pages/ee14ce/"},regularPath:"/07.Nexus/01.Nexus/04.Maven%20%E4%BB%93%E5%BA%93%E4%BB%8B%E7%BB%8D.html",relativePath:"07.Nexus/01.Nexus/04.Maven 仓库介绍.md",key:"v-48944d5a",path:"/pages/ee14ce/",headers:[{level:2,title:"代理仓库（Proxy Repository）",slug:"代理仓库-proxy-repository",normalizedTitle:"代理仓库（proxy repository）",charIndex:17},{level:2,title:"宿主仓库（Hosted Repository）",slug:"宿主仓库-hosted-repository",normalizedTitle:"宿主仓库（hosted repository）",charIndex:220},{level:2,title:"仓库组（Repository Group）",slug:"仓库组-repository-group",normalizedTitle:"仓库组（repository group）",charIndex:415}],headersStr:"代理仓库（Proxy Repository） 宿主仓库（Hosted Repository） 仓库组（Repository Group）",content:"# Maven 仓库介绍\n\n\n# 代理仓库（Proxy Repository）\n\n意为第三方仓库，如：\n\n * maven-central\n * nuget.org-proxy\n\n版本策略（Version Policy）：\n\n * Release: 正式版本\n * Snapshot: 快照版本\n * Mixed: 混合模式\n\n布局策略（Layout Policy）：\n\n * Strict：严格\n * Permissive：宽松\n\n\n# 宿主仓库（Hosted Repository）\n\n存储本地上传的组件和资源的，如：\n\n * maven-releases\n * maven-snapshots\n * nuget-hosted\n\n部署策略（Deployment Policy）：\n\n * Allow Redeploy：允许重新部署\n * Disable Redeploy：禁止重新部署\n * Read-Only：只读\n\n\n# 仓库组（Repository Group）\n\n通常包含了多个代理仓库和宿主仓库，在项目中只要引入仓库组就可以下载到代理仓库和宿主仓库中的包，如：\n\n * maven-public\n * nuget-group",normalizedContent:"# maven 仓库介绍\n\n\n# 代理仓库（proxy repository）\n\n意为第三方仓库，如：\n\n * maven-central\n * nuget.org-proxy\n\n版本策略（version policy）：\n\n * release: 正式版本\n * snapshot: 快照版本\n * mixed: 混合模式\n\n布局策略（layout policy）：\n\n * strict：严格\n * permissive：宽松\n\n\n# 宿主仓库（hosted repository）\n\n存储本地上传的组件和资源的，如：\n\n * maven-releases\n * maven-snapshots\n * nuget-hosted\n\n部署策略（deployment policy）：\n\n * allow redeploy：允许重新部署\n * disable redeploy：禁止重新部署\n * read-only：只读\n\n\n# 仓库组（repository group）\n\n通常包含了多个代理仓库和宿主仓库，在项目中只要引入仓库组就可以下载到代理仓库和宿主仓库中的包，如：\n\n * maven-public\n * nuget-group",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:13:40",lastUpdatedTimestamp:164437642e4},{title:"在项目中使用 Maven 私服",frontmatter:{title:"在项目中使用 Maven 私服",date:"2022-02-09T11:09:28.000Z",permalink:"/pages/433dea/"},regularPath:"/07.Nexus/01.Nexus/05.%E5%9C%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%BD%BF%E7%94%A8%20Maven%20%E7%A7%81%E6%9C%8D.html",relativePath:"07.Nexus/01.Nexus/05.在项目中使用 Maven 私服.md",key:"v-695dfcb6",path:"/pages/433dea/",headers:[{level:2,title:"配置认证信息",slug:"配置认证信息",normalizedTitle:"配置认证信息",charIndex:22},{level:3,title:"Snapshots 与 Releases 的区别",slug:"snapshots-与-releases-的区别",normalizedTitle:"snapshots 与 releases 的区别",charIndex:183},{level:2,title:"配置自动化部署",slug:"配置自动化部署",normalizedTitle:"配置自动化部署",charIndex:551},{level:2,title:"部署到仓库",slug:"部署到仓库",normalizedTitle:"部署到仓库",charIndex:1164},{level:2,title:"上传第三方 JAR 包",slug:"上传第三方-jar-包",normalizedTitle:"上传第三方 jar 包",charIndex:1187},{level:2,title:"settings.xml",slug:"settings-xml",normalizedTitle:"settings.xml",charIndex:38}],headersStr:"配置认证信息 Snapshots 与 Releases 的区别 配置自动化部署 部署到仓库 上传第三方 JAR 包 settings.xml",content:'# 在项目中使用 Maven 私服\n\n\n# 配置认证信息\n\n在 Maven settings.xml 中添加 Nexus 认证信息(servers 节点下)：\n\n<server>\n  <id>nexus</id>\n  <username>admin</username>\n  <password>admin123</password>\n</server>\n\n\n\n# Snapshots 与 Releases 的区别\n\n * nexus-releases: 用于发布 Release 版本\n * nexus-snapshots: 用于发布 Snapshot 版本（快照版）\n\nRelease 版本与 Snapshot 定义如下：\n\nRelease: 1.0.0/1.0.0-RELEASE\nSnapshot: 1.0.0-SNAPSHOT\n\n\n * 在项目 pom.xml 中设置的版本号添加 SNAPSHOT 标识的都会发布为 SNAPSHOT 版本，没有 SNAPSHOT 标识的都会发布为 RELEASE 版本。\n * SNAPSHOT 版本会自动加一个时间作为标识，如：1.0.0-SNAPSHOT 发布后为变成 1.0.0-SNAPSHOT-20180522.123456-1.jar\n\n\n# 配置自动化部署\n\n在 pom.xml 中添加如下代码：\n\n<distributionManagement>  \n  <repository>  \n    <id>nexus</id>  \n    <name>Nexus Release Repository</name>  \n    <url>http://192.168.199.110:8081/repository/maven-releases/</url>  \n  </repository>  \n  <snapshotRepository>  \n    <id>nexus</id>  \n    <name>Nexus Snapshot Repository</name>  \n    <url>http://192.168.199.110:8081/repository/maven-snapshots/</url>  \n  </snapshotRepository>  \n</distributionManagement> \n\n\n注意事项：\n\n * ID 名称必须要与 settings.xml 中 Servers 配置的 ID 名称保持一致。\n * 项目版本号中有 SNAPSHOT 标识的，会发布到 Nexus Snapshots Repository, 否则发布到 Nexus Release Repository，并根据 ID 去匹配授权账号。\n\n\n# 部署到仓库\n\nmvn deploy\n\n\n\n# 上传第三方 JAR 包\n\nnexus控制台提供上传功能，也可以使用 maven 命令：\n\n# 如第三方JAR包：aliyun-sdk-oss-2.2.3.jar\nmvn deploy:deploy-file \n  -DgroupId=com.aliyun.oss \n  -DartifactId=aliyun-sdk-oss \n  -Dversion=2.2.3 \n  -Dpackaging=jar \n  -Dfile=D:\\aliyun-sdk-oss-2.2.3.jar \n  -Durl=http://192.168.199.110:8081/repository/maven-3rd/ \n  -DrepositoryId=nexus\n\n\n注意事项：\n\n * 建议在上传第三方 JAR 包时，创建单独的第三方 JAR 包管理仓库，便于管理有维护。（maven-3rd）\n * -DrepositoryId=nexus 对应的是 settings.xml 中 Servers 配置的 ID 名称。（授权）\n\n\n# settings.xml\n\n<?xml version="1.0" encoding="UTF-8"?>\n<settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"\n          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n          xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd">\n    \n    <servers>\n        <server>\n            <id>nexus</id>\n            <username>admin</username>\n            <password>heyuqiang</password>\n        </server>\n    </servers>\n    <mirrors>\n        <mirror>\n            \x3c!--This sends everything else to /public --\x3e\n            <id>nexus</id>\n            <mirrorOf>*</mirrorOf>\n            <url>http://192.168.199.110:8081/repository/maven-public/</url>\n        </mirror>\n    </mirrors>\n\n    <profiles>\n        <profile>\n            <id>nexus</id>\n            \x3c!--Enable snapshots for the built in central repo to direct --\x3e\n            \x3c!--all requests to nexus via the mirror --\x3e\n            <repositories>\n                <repository>\n                    <id>central</id>\n                    <url>http://central</url>\n                    <releases>\n                        <enabled>true</enabled>\n                    </releases>\n                    <snapshots>\n                        <enabled>true</enabled>\n                    </snapshots>\n                </repository>\n            </repositories>\n            <pluginRepositories>\n                <pluginRepository>\n                    <id>central</id>\n                    <url>http://central</url>\n                    <releases>\n                        <enabled>true</enabled>\n                    </releases>\n                    <snapshots>\n                        <enabled>true</enabled>\n                    </snapshots>\n                </pluginRepository>\n            </pluginRepositories>\n        </profile>\n    </profiles>\n    <activeProfiles>\n        \x3c!--make the profile active all the time --\x3e\n        <activeProfile>nexus</activeProfile>\n    </activeProfiles>\n</settings>\n\n',normalizedContent:'# 在项目中使用 maven 私服\n\n\n# 配置认证信息\n\n在 maven settings.xml 中添加 nexus 认证信息(servers 节点下)：\n\n<server>\n  <id>nexus</id>\n  <username>admin</username>\n  <password>admin123</password>\n</server>\n\n\n\n# snapshots 与 releases 的区别\n\n * nexus-releases: 用于发布 release 版本\n * nexus-snapshots: 用于发布 snapshot 版本（快照版）\n\nrelease 版本与 snapshot 定义如下：\n\nrelease: 1.0.0/1.0.0-release\nsnapshot: 1.0.0-snapshot\n\n\n * 在项目 pom.xml 中设置的版本号添加 snapshot 标识的都会发布为 snapshot 版本，没有 snapshot 标识的都会发布为 release 版本。\n * snapshot 版本会自动加一个时间作为标识，如：1.0.0-snapshot 发布后为变成 1.0.0-snapshot-20180522.123456-1.jar\n\n\n# 配置自动化部署\n\n在 pom.xml 中添加如下代码：\n\n<distributionmanagement>  \n  <repository>  \n    <id>nexus</id>  \n    <name>nexus release repository</name>  \n    <url>http://192.168.199.110:8081/repository/maven-releases/</url>  \n  </repository>  \n  <snapshotrepository>  \n    <id>nexus</id>  \n    <name>nexus snapshot repository</name>  \n    <url>http://192.168.199.110:8081/repository/maven-snapshots/</url>  \n  </snapshotrepository>  \n</distributionmanagement> \n\n\n注意事项：\n\n * id 名称必须要与 settings.xml 中 servers 配置的 id 名称保持一致。\n * 项目版本号中有 snapshot 标识的，会发布到 nexus snapshots repository, 否则发布到 nexus release repository，并根据 id 去匹配授权账号。\n\n\n# 部署到仓库\n\nmvn deploy\n\n\n\n# 上传第三方 jar 包\n\nnexus控制台提供上传功能，也可以使用 maven 命令：\n\n# 如第三方jar包：aliyun-sdk-oss-2.2.3.jar\nmvn deploy:deploy-file \n  -dgroupid=com.aliyun.oss \n  -dartifactid=aliyun-sdk-oss \n  -dversion=2.2.3 \n  -dpackaging=jar \n  -dfile=d:\\aliyun-sdk-oss-2.2.3.jar \n  -durl=http://192.168.199.110:8081/repository/maven-3rd/ \n  -drepositoryid=nexus\n\n\n注意事项：\n\n * 建议在上传第三方 jar 包时，创建单独的第三方 jar 包管理仓库，便于管理有维护。（maven-3rd）\n * -drepositoryid=nexus 对应的是 settings.xml 中 servers 配置的 id 名称。（授权）\n\n\n# settings.xml\n\n<?xml version="1.0" encoding="utf-8"?>\n<settings xmlns="http://maven.apache.org/settings/1.0.0"\n          xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n          xsi:schemalocation="http://maven.apache.org/settings/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd">\n    \n    <servers>\n        <server>\n            <id>nexus</id>\n            <username>admin</username>\n            <password>heyuqiang</password>\n        </server>\n    </servers>\n    <mirrors>\n        <mirror>\n            \x3c!--this sends everything else to /public --\x3e\n            <id>nexus</id>\n            <mirrorof>*</mirrorof>\n            <url>http://192.168.199.110:8081/repository/maven-public/</url>\n        </mirror>\n    </mirrors>\n\n    <profiles>\n        <profile>\n            <id>nexus</id>\n            \x3c!--enable snapshots for the built in central repo to direct --\x3e\n            \x3c!--all requests to nexus via the mirror --\x3e\n            <repositories>\n                <repository>\n                    <id>central</id>\n                    <url>http://central</url>\n                    <releases>\n                        <enabled>true</enabled>\n                    </releases>\n                    <snapshots>\n                        <enabled>true</enabled>\n                    </snapshots>\n                </repository>\n            </repositories>\n            <pluginrepositories>\n                <pluginrepository>\n                    <id>central</id>\n                    <url>http://central</url>\n                    <releases>\n                        <enabled>true</enabled>\n                    </releases>\n                    <snapshots>\n                        <enabled>true</enabled>\n                    </snapshots>\n                </pluginrepository>\n            </pluginrepositories>\n        </profile>\n    </profiles>\n    <activeprofiles>\n        \x3c!--make the profile active all the time --\x3e\n        <activeprofile>nexus</activeprofile>\n    </activeprofiles>\n</settings>\n\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:13:40",lastUpdatedTimestamp:164437642e4},{title:"Maven pom文件标签大全详解",frontmatter:{title:"Maven pom文件标签大全详解",date:"2022-02-09T11:09:28.000Z",permalink:"/pages/d1f496/"},regularPath:"/07.Nexus/01.Nexus/06.Maven%20pom%E6%96%87%E4%BB%B6%E6%A0%87%E7%AD%BE%E5%A4%A7%E5%85%A8%E8%AF%A6%E8%A7%A3.html",relativePath:"07.Nexus/01.Nexus/06.Maven pom文件标签大全详解.md",key:"v-7ddb1434",path:"/pages/d1f496/",headers:[{level:2,title:"Tag的解释如下",slug:"tag的解释如下",normalizedTitle:"tag的解释如下",charIndex:28}],headersStr:"Tag的解释如下",content:'# Maven pom.xml内的标签大全详解\n\n\n# Tag的解释如下\n\n<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0http://maven.apache.org/maven-v4_0_0.xsd">\n    \x3c!--父项目的坐标。如果项目中没有规定某个元素的值，那么父项目中的对应值即为项目的默认值。 坐标包括group ID，artifact ID和 \n        version。 --\x3e\n    <parent>\n        \x3c!--被继承的父项目的构件标识符 --\x3e\n        <artifactId />\n        \x3c!--被继承的父项目的全球唯一标识符 --\x3e\n        <groupId />\n        \x3c!--被继承的父项目的版本 --\x3e\n        <version />\n        \x3c!-- 父项目的pom.xml文件的相对路径。相对路径允许你选择一个不同的路径。默认值是../pom.xml。Maven首先在构建当前项目的地方寻找父项 \n            目的pom，其次在文件系统的这个位置（relativePath位置），然后在本地仓库，最后在远程仓库寻找父项目的pom。 --\x3e\n        <relativePath />\n    </parent>\n    \x3c!--声明项目描述符遵循哪一个POM模型版本。模型本身的版本很少改变，虽然如此，但它仍然是必不可少的，这是为了当Maven引入了新的特性或者其他模型变更的时候，确保稳定性。 --\x3e\n    <modelVersion>4.0.0</modelVersion>\n    \x3c!--项目的全球唯一标识符，通常使用全限定的包名区分该项目和其他项目。并且构建时生成的路径也是由此生成， 如com.mycompany.app生成的相对路径为：/com/mycompany/app --\x3e\n    <groupId>asia.banseon</groupId>\n    \x3c!-- 构件的标识符，它和group ID一起唯一标识一个构件。换句话说，你不能有两个不同的项目拥有同样的artifact ID和groupID；在某个 \n        特定的group ID下，artifact ID也必须是唯一的。构件是项目产生的或使用的一个东西，Maven为项目产生的构件包括：JARs，源 码，二进制发布和WARs等。 --\x3e\n    <artifactId>banseon-maven2</artifactId>\n    \x3c!--项目产生的构件类型，例如jar、war、ear、pom。插件可以创建他们自己的构件类型，所以前面列的不是全部构件类型 --\x3e\n    <packaging>jar</packaging>\n    \x3c!--项目当前版本，格式为:主版本.次版本.增量版本-限定版本号 --\x3e\n    <version>1.0-SNAPSHOT</version>\n    \x3c!--项目的名称, Maven产生的文档用 --\x3e\n    <name>banseon-maven</name>\n    \x3c!--项目主页的URL, Maven产生的文档用 --\x3e\n    <url>http://www.baidu.com/banseon</url>\n    \x3c!-- 项目的详细描述, Maven 产生的文档用。 当这个元素能够用HTML格式描述时（例如，CDATA中的文本会被解析器忽略，就可以包含HTML标 \n        签）， 不鼓励使用纯文本描述。如果你需要修改产生的web站点的索引页面，你应该修改你自己的索引页文件，而不是调整这里的文档。 --\x3e\n    <description>A maven project to study maven.</description>\n    \x3c!--描述了这个项目构建环境中的前提条件。 --\x3e\n    <prerequisites>\n        \x3c!--构建该项目或使用该插件所需要的Maven的最低版本 --\x3e\n        <maven />\n    </prerequisites>\n    \x3c!--项目的问题管理系统(Bugzilla, Jira, Scarab,或任何你喜欢的问题管理系统)的名称和URL，本例为 jira --\x3e\n    <issueManagement>\n        \x3c!--问题管理系统（例如jira）的名字， --\x3e\n        <system>jira</system>\n        \x3c!--该项目使用的问题管理系统的URL --\x3e\n        <url>http://jira.baidu.com/banseon</url>\n    </issueManagement>\n    \x3c!--项目持续集成信息 --\x3e\n    <ciManagement>\n        \x3c!--持续集成系统的名字，例如continuum --\x3e\n        <system />\n        \x3c!--该项目使用的持续集成系统的URL（如果持续集成系统有web接口的话）。 --\x3e\n        <url />\n        \x3c!--构建完成时，需要通知的开发者/用户的配置项。包括被通知者信息和通知条件（错误，失败，成功，警告） --\x3e\n        <notifiers>\n            \x3c!--配置一种方式，当构建中断时，以该方式通知用户/开发者 --\x3e\n            <notifier>\n                \x3c!--传送通知的途径 --\x3e\n                <type />\n                \x3c!--发生错误时是否通知 --\x3e\n                <sendOnError />\n                \x3c!--构建失败时是否通知 --\x3e\n                <sendOnFailure />\n                \x3c!--构建成功时是否通知 --\x3e\n                <sendOnSuccess />\n                \x3c!--发生警告时是否通知 --\x3e\n                <sendOnWarning />\n                \x3c!--不赞成使用。通知发送到哪里 --\x3e\n                <address />\n                \x3c!--扩展配置项 --\x3e\n                <configuration />\n            </notifier>\n        </notifiers>\n    </ciManagement>\n    \x3c!--项目创建年份，4位数字。当产生版权信息时需要使用这个值。 --\x3e\n    <inceptionYear />\n    \x3c!--项目相关邮件列表信息 --\x3e\n    <mailingLists>\n        \x3c!--该元素描述了项目相关的所有邮件列表。自动产生的网站引用这些信息。 --\x3e\n        <mailingList>\n            \x3c!--邮件的名称 --\x3e\n            <name>Demo</name>\n            \x3c!--发送邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --\x3e\n            <post>banseon@126.com</post>\n            \x3c!--订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --\x3e\n            <subscribe>banseon@126.com</subscribe>\n            \x3c!--取消订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --\x3e\n            <unsubscribe>banseon@126.com</unsubscribe>\n            \x3c!--你可以浏览邮件信息的URL --\x3e\n            <archive>http:/hi.baidu.com/banseon/demo/dev/</archive>\n        </mailingList>\n    </mailingLists>\n    \x3c!--项目开发者列表 --\x3e\n    <developers>\n        \x3c!--某个项目开发者的信息 --\x3e\n        <developer>\n            \x3c!--SCM里项目开发者的唯一标识符 --\x3e\n            <id>HELLO WORLD</id>\n            \x3c!--项目开发者的全名 --\x3e\n            <name>banseon</name>\n            \x3c!--项目开发者的email --\x3e\n            <email>banseon@126.com</email>\n            \x3c!--项目开发者的主页的URL --\x3e\n            <url />\n            \x3c!--项目开发者在项目中扮演的角色，角色元素描述了各种角色 --\x3e\n            <roles>\n                <role>Project Manager</role>\n                <role>Architect</role>\n            </roles>\n            \x3c!--项目开发者所属组织 --\x3e\n            <organization>demo</organization>\n            \x3c!--项目开发者所属组织的URL --\x3e\n            <organizationUrl>http://hi.baidu.com/banseon</organizationUrl>\n            \x3c!--项目开发者属性，如即时消息如何处理等 --\x3e\n            <properties>\n                <dept>No</dept>\n            </properties>\n            \x3c!--项目开发者所在时区， -11到12范围内的整数。 --\x3e\n            <timezone>-5</timezone>\n        </developer>\n    </developers>\n    \x3c!--项目的其他贡献者列表 --\x3e\n    <contributors>\n        \x3c!--项目的其他贡献者。参见developers/developer元素 --\x3e\n        <contributor>\n            <name />\n            <email />\n            <url />\n            <organization />\n            <organizationUrl />\n            <roles />\n            <timezone />\n            <properties />\n        </contributor>\n    </contributors>\n    \x3c!--该元素描述了项目所有License列表。 应该只列出该项目的license列表，不要列出依赖项目的 license列表。如果列出多个license，用户可以选择它们中的一个而不是接受所有license。 --\x3e\n    <licenses>\n        \x3c!--描述了项目的license，用于生成项目的web站点的license页面，其他一些报表和validation也会用到该元素。 --\x3e\n        <license>\n            \x3c!--license用于法律上的名称 --\x3e\n            <name>Apache 2</name>\n            \x3c!--官方的license正文页面的URL --\x3e\n            <url>http://www.baidu.com/banseon/LICENSE-2.0.txt</url>\n            \x3c!--项目分发的主要方式： repo，可以从Maven库下载 manual， 用户必须手动下载和安装依赖 --\x3e\n            <distribution>repo</distribution>\n            \x3c!--关于license的补充信息 --\x3e\n            <comments>A business-friendly OSS license</comments>\n        </license>\n    </licenses>\n    \x3c!--SCM(Source Control Management)标签允许你配置你的代码库，供Maven web站点和其它插件使用。 --\x3e\n    <scm>\n        \x3c!--SCM的URL,该URL描述了版本库和如何连接到版本库。欲知详情，请看SCMs提供的URL格式和列表。该连接只读。 --\x3e\n        <connection>\n            scm:svn:http://svn.baidu.com/banseon/maven/banseon/banseon-maven2-trunk(dao-trunk)\n        </connection>\n        \x3c!--给开发者使用的，类似connection元素。即该连接不仅仅只读 --\x3e\n        <developerConnection>\n            scm:svn:http://svn.baidu.com/banseon/maven/banseon/dao-trunk\n        </developerConnection>\n        \x3c!--当前代码的标签，在开发阶段默认为HEAD --\x3e\n        <tag />\n        \x3c!--指向项目的可浏览SCM库（例如ViewVC或者Fisheye）的URL。 --\x3e\n        <url>http://svn.baidu.com/banseon</url>\n    </scm>\n    \x3c!--描述项目所属组织的各种属性。Maven产生的文档用 --\x3e\n    <organization>\n        \x3c!--组织的全名 --\x3e\n        <name>demo</name>\n        \x3c!--组织主页的URL --\x3e\n        <url>http://www.baidu.com/banseon</url>\n    </organization>\n    \x3c!--构建项目需要的信息 --\x3e\n    <build>\n        \x3c!--该元素设置了项目源码目录，当构建项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --\x3e\n        <sourceDirectory />\n        \x3c!--该元素设置了项目脚本源码目录，该目录和源码目录不同：绝大多数情况下，该目录下的内容 会被拷贝到输出目录(因为脚本是被解释的，而不是被编译的)。 --\x3e\n        <scriptSourceDirectory />\n        \x3c!--该元素设置了项目单元测试使用的源码目录，当测试项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --\x3e\n        <testSourceDirectory />\n        \x3c!--被编译过的应用程序class文件存放的目录。 --\x3e\n        <outputDirectory />\n        \x3c!--被编译过的测试class文件存放的目录。 --\x3e\n        <testOutputDirectory />\n        \x3c!--使用来自该项目的一系列构建扩展 --\x3e\n        <extensions>\n            \x3c!--描述使用到的构建扩展。 --\x3e\n            <extension>\n                \x3c!--构建扩展的groupId --\x3e\n                <groupId />\n                \x3c!--构建扩展的artifactId --\x3e\n                <artifactId />\n                \x3c!--构建扩展的版本 --\x3e\n                <version />\n            </extension>\n        </extensions>\n        \x3c!--当项目没有规定目标（Maven2 叫做阶段）时的默认值 --\x3e\n        <defaultGoal />\n        \x3c!--这个元素描述了项目相关的所有资源路径列表，例如和项目相关的属性文件，这些资源被包含在最终的打包文件里。 --\x3e\n        <resources>\n            \x3c!--这个元素描述了项目相关或测试相关的所有资源路径 --\x3e\n            <resource>\n                \x3c!-- 描述了资源的目标路径。该路径相对target/classes目录（例如${project.build.outputDirectory}）。举个例 \n                    子，如果你想资源在特定的包里(org.apache.maven.messages)，你就必须该元素设置为org/apache/maven /messages。然而，如果你只是想把资源放到源码目录结构里，就不需要该配置。 --\x3e\n                <targetPath />\n                \x3c!--是否使用参数值代替参数名。参数值取自properties元素或者文件里配置的属性，文件在filters元素里列出。 --\x3e\n                <filtering />\n                \x3c!--描述存放资源的目录，该路径相对POM路径 --\x3e\n                <directory />\n                \x3c!--包含的模式列表，例如**/*.xml. --\x3e\n                <includes />\n                \x3c!--排除的模式列表，例如**/*.xml --\x3e\n                <excludes />\n            </resource>\n        </resources>\n        \x3c!--这个元素描述了单元测试相关的所有资源路径，例如和单元测试相关的属性文件。 --\x3e\n        <testResources>\n            \x3c!--这个元素描述了测试相关的所有资源路径，参见build/resources/resource元素的说明 --\x3e\n            <testResource>\n                <targetPath />\n                <filtering />\n                <directory />\n                <includes />\n                <excludes />\n            </testResource>\n        </testResources>\n        \x3c!--构建产生的所有文件存放的目录 --\x3e\n        <directory />\n        \x3c!--产生的构件的文件名，默认值是${artifactId}-${version}。 --\x3e\n        <finalName />\n        \x3c!--当filtering开关打开时，使用到的过滤器属性文件列表 --\x3e\n        <filters />\n        \x3c!--子项目可以引用的默认插件信息。该插件配置项直到被引用时才会被解析或绑定到生命周期。给定插件的任何本地配置都会覆盖这里的配置 --\x3e\n        <pluginManagement>\n            \x3c!--使用的插件列表 。 --\x3e\n            <plugins>\n                \x3c!--plugin元素包含描述插件所需要的信息。 --\x3e\n                <plugin>\n                    \x3c!--插件在仓库里的group ID --\x3e\n                    <groupId />\n                    \x3c!--插件在仓库里的artifact ID --\x3e\n                    <artifactId />\n                    \x3c!--被使用的插件的版本（或版本范围） --\x3e\n                    <version />\n                    \x3c!--是否从该插件下载Maven扩展（例如打包和类型处理器），由于性能原因，只有在真需要下载时，该元素才被设置成enabled。 --\x3e\n                    <extensions />\n                    \x3c!--在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。 --\x3e\n                    <executions>\n                        \x3c!--execution元素包含了插件执行需要的信息 --\x3e\n                        <execution>\n                            \x3c!--执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标 --\x3e\n                            <id />\n                            \x3c!--绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段 --\x3e\n                            <phase />\n                            \x3c!--配置的执行目标 --\x3e\n                            <goals />\n                            \x3c!--配置是否被传播到子POM --\x3e\n                            <inherited />\n                            \x3c!--作为DOM对象的配置 --\x3e\n                            <configuration />\n                        </execution>\n                    </executions>\n                    \x3c!--项目引入插件所需要的额外依赖 --\x3e\n                    <dependencies>\n                        \x3c!--参见dependencies/dependency元素 --\x3e\n                        <dependency>\n                            ......\n                        </dependency>\n                    </dependencies>\n                    \x3c!--任何配置是否被传播到子项目 --\x3e\n                    <inherited />\n                    \x3c!--作为DOM对象的配置 --\x3e\n                    <configuration />\n                </plugin>\n            </plugins>\n        </pluginManagement>\n        \x3c!--使用的插件列表 --\x3e\n        <plugins>\n            \x3c!--参见build/pluginManagement/plugins/plugin元素 --\x3e\n            <plugin>\n                <groupId />\n                <artifactId />\n                <version />\n                <extensions />\n                <executions>\n                    <execution>\n                        <id />\n                        <phase />\n                        <goals />\n                        <inherited />\n                        <configuration />\n                    </execution>\n                </executions>\n                <dependencies>\n                    \x3c!--参见dependencies/dependency元素 --\x3e\n                    <dependency>\n                        ......\n                    </dependency>\n                </dependencies>\n                <goals />\n                <inherited />\n                <configuration />\n            </plugin>\n        </plugins>\n    </build>\n    \x3c!--在列的项目构建profile，如果被激活，会修改构建处理 --\x3e\n    <profiles>\n        \x3c!--根据环境参数或命令行参数激活某个构建处理 --\x3e\n        <profile>\n            \x3c!--构建配置的唯一标识符。即用于命令行激活，也用于在继承时合并具有相同标识符的profile。 --\x3e\n            <id />\n            \x3c!--自动触发profile的条件逻辑。Activation是profile的开启钥匙。profile的力量来自于它 能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。activation元素并不是激活profile的唯一方式。 --\x3e\n            <activation>\n                \x3c!--profile默认是否激活的标志 --\x3e\n                <activeByDefault />\n                \x3c!--当匹配的jdk被检测到，profile被激活。例如，1.4激活JDK1.4，1.4.0_2，而!1.4激活所有版本不是以1.4开头的JDK。 --\x3e\n                <jdk />\n                \x3c!--当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 --\x3e\n                <os>\n                    \x3c!--激活profile的操作系统的名字 --\x3e\n                    <name>Windows XP</name>\n                    \x3c!--激活profile的操作系统所属家族(如 \'windows\') --\x3e\n                    <family>Windows</family>\n                    \x3c!--激活profile的操作系统体系结构 --\x3e\n                    <arch>x86</arch>\n                    \x3c!--激活profile的操作系统版本 --\x3e\n                    <version>5.1.2600</version>\n                </os>\n                \x3c!--如果Maven检测到某一个属性（其值可以在POM中通过${名称}引用），其拥有对应的名称和值，Profile就会被激活。如果值 字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段 --\x3e\n                <property>\n                    \x3c!--激活profile的属性的名称 --\x3e\n                    <name>mavenVersion</name>\n                    \x3c!--激活profile的属性的值 --\x3e\n                    <value>2.0.3</value>\n                </property>\n                \x3c!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活 profile。另一方面，exists则会检查文件是否存在，如果存在则激活profile。 --\x3e\n                <file>\n                    \x3c!--如果指定的文件存在，则激活profile。 --\x3e\n                    <exists>/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/\n                    </exists>\n                    \x3c!--如果指定的文件不存在，则激活profile。 --\x3e\n                    <missing>/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/\n                    </missing>\n                </file>\n            </activation>\n            \x3c!--构建项目所需要的信息。参见build元素 --\x3e\n            <build>\n                <defaultGoal />\n                <resources>\n                    <resource>\n                        <targetPath />\n                        <filtering />\n                        <directory />\n                        <includes />\n                        <excludes />\n                    </resource>\n                </resources>\n                <testResources>\n                    <testResource>\n                        <targetPath />\n                        <filtering />\n                        <directory />\n                        <includes />\n                        <excludes />\n                    </testResource>\n                </testResources>\n                <directory />\n                <finalName />\n                <filters />\n                <pluginManagement>\n                    <plugins>\n                        \x3c!--参见build/pluginManagement/plugins/plugin元素 --\x3e\n                        <plugin>\n                            <groupId />\n                            <artifactId />\n                            <version />\n                            <extensions />\n                            <executions>\n                                <execution>\n                                    <id />\n                                    <phase />\n                                    <goals />\n                                    <inherited />\n                                    <configuration />\n                                </execution>\n                            </executions>\n                            <dependencies>\n                                \x3c!--参见dependencies/dependency元素 --\x3e\n                                <dependency>\n                                    ......\n                                </dependency>\n                            </dependencies>\n                            <goals />\n                            <inherited />\n                            <configuration />\n                        </plugin>\n                    </plugins>\n                </pluginManagement>\n                <plugins>\n                    \x3c!--参见build/pluginManagement/plugins/plugin元素 --\x3e\n                    <plugin>\n                        <groupId />\n                        <artifactId />\n                        <version />\n                        <extensions />\n                        <executions>\n                            <execution>\n                                <id />\n                                <phase />\n                                <goals />\n                                <inherited />\n                                <configuration />\n                            </execution>\n                        </executions>\n                        <dependencies>\n                            \x3c!--参见dependencies/dependency元素 --\x3e\n                            <dependency>\n                                ......\n                            </dependency>\n                        </dependencies>\n                        <goals />\n                        <inherited />\n                        <configuration />\n                    </plugin>\n                </plugins>\n            </build>\n            \x3c!--模块（有时称作子项目） 被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径 --\x3e\n            <modules />\n            \x3c!--发现依赖和扩展的远程仓库列表。 --\x3e\n            <repositories>\n                \x3c!--参见repositories/repository元素 --\x3e\n                <repository>\n                    <releases>\n                        <enabled />\n                        <updatePolicy />\n                        <checksumPolicy />\n                    </releases>\n                    <snapshots>\n                        <enabled />\n                        <updatePolicy />\n                        <checksumPolicy />\n                    </snapshots>\n                    <id />\n                    <name />\n                    <url />\n                    <layout />\n                </repository>\n            </repositories>\n            \x3c!--发现插件的远程仓库列表，这些插件用于构建和报表 --\x3e\n            <pluginRepositories>\n                \x3c!--包含需要连接到远程插件仓库的信息.参见repositories/repository元素 --\x3e\n                <pluginRepository>\n                    <releases>\n                        <enabled />\n                        <updatePolicy />\n                        <checksumPolicy />\n                    </releases>\n                    <snapshots>\n                        <enabled />\n                        <updatePolicy />\n                        <checksumPolicy />\n                    </snapshots>\n                    <id />\n                    <name />\n                    <url />\n                    <layout />\n                </pluginRepository>\n            </pluginRepositories>\n            \x3c!--该元素描述了项目相关的所有依赖。 这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。 --\x3e\n            <dependencies>\n                \x3c!--参见dependencies/dependency元素 --\x3e\n                <dependency>\n                    ......\n                </dependency>\n            </dependencies>\n            \x3c!--不赞成使用. 现在Maven忽略该元素. --\x3e\n            <reports />\n            \x3c!--该元素包括使用报表插件产生报表的规范。当用户执行“mvn site”，这些报表就会运行。 在页面导航栏能看到所有报表的链接。参见reporting元素 --\x3e\n            <reporting>\n                ......\n            </reporting>\n            \x3c!--参见dependencyManagement元素 --\x3e\n            <dependencyManagement>\n                <dependencies>\n                    \x3c!--参见dependencies/dependency元素 --\x3e\n                    <dependency>\n                        ......\n                    </dependency>\n                </dependencies>\n            </dependencyManagement>\n            \x3c!--参见distributionManagement元素 --\x3e\n            <distributionManagement>\n                ......\n            </distributionManagement>\n            \x3c!--参见properties元素 --\x3e\n            <properties />\n        </profile>\n    </profiles>\n    \x3c!--模块（有时称作子项目） 被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径 --\x3e\n    <modules />\n    \x3c!--发现依赖和扩展的远程仓库列表。 --\x3e\n    <repositories>\n        \x3c!--包含需要连接到远程仓库的信息 --\x3e\n        <repository>\n            \x3c!--如何处理远程仓库里发布版本的下载 --\x3e\n            <releases>\n                \x3c!--true或者false表示该仓库是否为下载某种类型构件（发布版，快照版）开启。 --\x3e\n                <enabled />\n                \x3c!--该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。这里的选项是：always（一直），daily（默认，每日），interval：X（这里X是以分钟为单位的时间间隔），或者never（从不）。 --\x3e\n                <updatePolicy />\n                \x3c!--当Maven验证构件校验文件失败时该怎么做：ignore（忽略），fail（失败），或者warn（警告）。 --\x3e\n                <checksumPolicy />\n            </releases>\n            \x3c!-- 如何处理远程仓库里快照版本的下载。有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的 \n                策略。例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素 --\x3e\n            <snapshots>\n                <enabled />\n                <updatePolicy />\n                <checksumPolicy />\n            </snapshots>\n            \x3c!--远程仓库唯一标识符。可以用来匹配在settings.xml文件里配置的远程仓库 --\x3e\n            <id>banseon-repository-proxy</id>\n            \x3c!--远程仓库名称 --\x3e\n            <name>banseon-repository-proxy</name>\n            \x3c!--远程仓库URL，按protocol://hostname/path形式 --\x3e\n            <url>http://192.168.1.169:9999/repository/</url>\n            \x3c!-- 用于定位和排序构件的仓库布局类型-可以是default（默认）或者legacy（遗留）。Maven 2为其仓库提供了一个默认的布局；然 \n                而，Maven 1.x有一种不同的布局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留）。 --\x3e\n            <layout>default</layout>\n        </repository>\n    </repositories>\n    \x3c!--发现插件的远程仓库列表，这些插件用于构建和报表 --\x3e\n    <pluginRepositories>\n        \x3c!--包含需要连接到远程插件仓库的信息.参见repositories/repository元素 --\x3e\n        <pluginRepository>\n            ......\n        </pluginRepository>\n    </pluginRepositories>\n\n    \x3c!--该元素描述了项目相关的所有依赖。 这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。 --\x3e\n    <dependencies>\n        <dependency>\n            \x3c!--依赖的group ID --\x3e\n            <groupId>org.apache.maven</groupId>\n            \x3c!--依赖的artifact ID --\x3e\n            <artifactId>maven-artifact</artifactId>\n            \x3c!--依赖的版本号。 在Maven 2里, 也可以配置成版本号的范围。 --\x3e\n            <version>3.8.1</version>\n            \x3c!-- 依赖类型，默认类型是jar。它通常表示依赖的文件的扩展名，但也有例外。一个类型可以被映射成另外一个扩展名或分类器。类型经常和使用的打包方式对应， \n                尽管这也有例外。一些类型的例子：jar，war，ejb-client和test-jar。如果设置extensions为 true，就可以在 plugin里定义新的类型。所以前面的类型的例子不完整。 --\x3e\n            <type>jar</type>\n            \x3c!-- 依赖的分类器。分类器可以区分属于同一个POM，但不同构建方式的构件。分类器名被附加到文件名的版本号后面。例如，如果你想要构建两个单独的构件成 \n                JAR，一个使用Java 1.4编译器，另一个使用Java 6编译器，你就可以使用分类器来生成两个单独的JAR构件。 --\x3e\n            <classifier></classifier>\n            \x3c!--依赖范围。在项目发布过程中，帮助决定哪些构件被包括进来。欲知详情请参考依赖机制。 - compile ：默认范围，用于编译 - provided：类似于编译，但支持你期待jdk或者容器提供，类似于classpath \n                - runtime: 在执行时需要使用 - test: 用于test任务时使用 - system: 需要外在提供相应的元素。通过systemPath来取得 \n                - systemPath: 仅用于范围为system。提供相应的路径 - optional: 当项目自身被依赖时，标注依赖是否传递。用于连续依赖时使用 --\x3e\n            <scope>test</scope>\n            \x3c!--仅供system范围使用。注意，不鼓励使用这个元素，并且在新的版本中该元素可能被覆盖掉。该元素为依赖规定了文件系统上的路径。需要绝对路径而不是相对路径。推荐使用属性匹配绝对路径，例如${java.home}。 --\x3e\n            <systemPath></systemPath>\n            \x3c!--当计算传递依赖时， 从依赖构件列表里，列出被排除的依赖构件集。即告诉maven你只依赖指定的项目，不依赖项目的依赖。此元素主要用于解决版本冲突问题 --\x3e\n            <exclusions>\n                <exclusion>\n                    <artifactId>spring-core</artifactId>\n                    <groupId>org.springframework</groupId>\n                </exclusion>\n            </exclusions>\n            \x3c!--可选依赖，如果你在项目B中把C依赖声明为可选，你就需要在依赖于B的项目（例如项目A）中显式的引用对C的依赖。可选依赖阻断依赖的传递性。 --\x3e\n            <optional>true</optional>\n        </dependency>\n    </dependencies>\n    \x3c!--不赞成使用. 现在Maven忽略该元素. --\x3e\n    <reports></reports>\n    \x3c!--该元素描述使用报表插件产生报表的规范。当用户执行“mvn site”，这些报表就会运行。 在页面导航栏能看到所有报表的链接。 --\x3e\n    <reporting>\n        \x3c!--true，则，网站不包括默认的报表。这包括“项目信息”菜单中的报表。 --\x3e\n        <excludeDefaults />\n        \x3c!--所有产生的报表存放到哪里。默认值是${project.build.directory}/site。 --\x3e\n        <outputDirectory />\n        \x3c!--使用的报表插件和他们的配置。 --\x3e\n        <plugins>\n            \x3c!--plugin元素包含描述报表插件需要的信息 --\x3e\n            <plugin>\n                \x3c!--报表插件在仓库里的group ID --\x3e\n                <groupId />\n                \x3c!--报表插件在仓库里的artifact ID --\x3e\n                <artifactId />\n                \x3c!--被使用的报表插件的版本（或版本范围） --\x3e\n                <version />\n                \x3c!--任何配置是否被传播到子项目 --\x3e\n                <inherited />\n                \x3c!--报表插件的配置 --\x3e\n                <configuration />\n                \x3c!--一组报表的多重规范，每个规范可能有不同的配置。一个规范（报表集）对应一个执行目标 。例如，有1，2，3，4，5，6，7，8，9个报表。1，2，5构成A报表集，对应一个执行目标。2，5，8构成B报表集，对应另一个执行目标 --\x3e\n                <reportSets>\n                    \x3c!--表示报表的一个集合，以及产生该集合的配置 --\x3e\n                    <reportSet>\n                        \x3c!--报表集合的唯一标识符，POM继承时用到 --\x3e\n                        <id />\n                        \x3c!--产生报表集合时，被使用的报表的配置 --\x3e\n                        <configuration />\n                        \x3c!--配置是否被继承到子POMs --\x3e\n                        <inherited />\n                        \x3c!--这个集合里使用到哪些报表 --\x3e\n                        <reports />\n                    </reportSet>\n                </reportSets>\n            </plugin>\n        </plugins>\n    </reporting>\n    \x3c!-- 继承自该项目的所有子项目的默认依赖信息。这部分的依赖信息不会被立即解析,而是当子项目声明一个依赖（必须描述group ID和 artifact \n        ID信息），如果group ID和artifact ID以外的一些信息没有描述，则通过group ID和artifact ID 匹配到这里的依赖，并使用这里的依赖信息。 --\x3e\n    <dependencyManagement>\n        <dependencies>\n            \x3c!--参见dependencies/dependency元素 --\x3e\n            <dependency>\n                ......\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n    \x3c!--项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。 --\x3e\n    <distributionManagement>\n        \x3c!--部署项目产生的构件到远程仓库需要的信息 --\x3e\n        <repository>\n            \x3c!--是分配给快照一个唯一的版本号（由时间戳和构建流水号）？还是每次都使用相同的版本号？参见repositories/repository元素 --\x3e\n            <uniqueVersion />\n            <id>banseon-maven2</id>\n            <name>banseon maven2</name>\n            <url>file://${basedir}/target/deploy</url>\n            <layout />\n        </repository>\n        \x3c!--构件的快照部署到哪里？如果没有配置该元素，默认部署到repository元素配置的仓库，参见distributionManagement/repository元素 --\x3e\n        <snapshotRepository>\n            <uniqueVersion />\n            <id>banseon-maven2</id>\n            <name>Banseon-maven2 Snapshot Repository</name>\n            <url>scp://svn.baidu.com/banseon:/usr/local/maven-snapshot</url>\n            <layout />\n        </snapshotRepository>\n        \x3c!--部署项目的网站需要的信息 --\x3e\n        <site>\n            \x3c!--部署位置的唯一标识符，用来匹配站点和settings.xml文件里的配置 --\x3e\n            <id>banseon-site</id>\n            \x3c!--部署位置的名称 --\x3e\n            <name>business api website</name>\n            \x3c!--部署位置的URL，按protocol://hostname/path形式 --\x3e\n            <url>\n                scp://svn.baidu.com/banseon:/var/www/localhost/banseon-web\n            </url>\n        </site>\n        \x3c!--项目下载页面的URL。如果没有该元素，用户应该参考主页。使用该元素的原因是：帮助定位那些不在仓库里的构件（由于license限制）。 --\x3e\n        <downloadUrl />\n        \x3c!--如果构件有了新的group ID和artifact ID（构件移到了新的位置），这里列出构件的重定位信息。 --\x3e\n        <relocation>\n            \x3c!--构件新的group ID --\x3e\n            <groupId />\n            \x3c!--构件新的artifact ID --\x3e\n            <artifactId />\n            \x3c!--构件新的版本号 --\x3e\n            <version />\n            \x3c!--显示给用户的，关于移动的额外信息，例如原因。 --\x3e\n            <message />\n        </relocation>\n        \x3c!-- 给出该构件在远程仓库的状态。不得在本地项目中设置该元素，因为这是工具自动更新的。有效的值有：none（默认），converted（仓库管理员从 \n            Maven 1 POM转换过来），partner（直接从伙伴Maven 2仓库同步过来），deployed（从Maven 2实例部 署），verified（被核实时正确的和最终的）。 --\x3e\n        <status />\n    </distributionManagement>\n    \x3c!--以值替代名称，Properties可以在整个POM中使用，也可以作为触发条件（见settings.xml配置文件里activation元素的说明）。格式是<name>value</name>。 --\x3e\n    <properties />\n</project>\n',normalizedContent:'# maven pom.xml内的标签大全详解\n\n\n# tag的解释如下\n\n<project xmlns="http://maven.apache.org/pom/4.0.0" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n    xsi:schemalocation="http://maven.apache.org/pom/4.0.0http://maven.apache.org/maven-v4_0_0.xsd">\n    \x3c!--父项目的坐标。如果项目中没有规定某个元素的值，那么父项目中的对应值即为项目的默认值。 坐标包括group id，artifact id和 \n        version。 --\x3e\n    <parent>\n        \x3c!--被继承的父项目的构件标识符 --\x3e\n        <artifactid />\n        \x3c!--被继承的父项目的全球唯一标识符 --\x3e\n        <groupid />\n        \x3c!--被继承的父项目的版本 --\x3e\n        <version />\n        \x3c!-- 父项目的pom.xml文件的相对路径。相对路径允许你选择一个不同的路径。默认值是../pom.xml。maven首先在构建当前项目的地方寻找父项 \n            目的pom，其次在文件系统的这个位置（relativepath位置），然后在本地仓库，最后在远程仓库寻找父项目的pom。 --\x3e\n        <relativepath />\n    </parent>\n    \x3c!--声明项目描述符遵循哪一个pom模型版本。模型本身的版本很少改变，虽然如此，但它仍然是必不可少的，这是为了当maven引入了新的特性或者其他模型变更的时候，确保稳定性。 --\x3e\n    <modelversion>4.0.0</modelversion>\n    \x3c!--项目的全球唯一标识符，通常使用全限定的包名区分该项目和其他项目。并且构建时生成的路径也是由此生成， 如com.mycompany.app生成的相对路径为：/com/mycompany/app --\x3e\n    <groupid>asia.banseon</groupid>\n    \x3c!-- 构件的标识符，它和group id一起唯一标识一个构件。换句话说，你不能有两个不同的项目拥有同样的artifact id和groupid；在某个 \n        特定的group id下，artifact id也必须是唯一的。构件是项目产生的或使用的一个东西，maven为项目产生的构件包括：jars，源 码，二进制发布和wars等。 --\x3e\n    <artifactid>banseon-maven2</artifactid>\n    \x3c!--项目产生的构件类型，例如jar、war、ear、pom。插件可以创建他们自己的构件类型，所以前面列的不是全部构件类型 --\x3e\n    <packaging>jar</packaging>\n    \x3c!--项目当前版本，格式为:主版本.次版本.增量版本-限定版本号 --\x3e\n    <version>1.0-snapshot</version>\n    \x3c!--项目的名称, maven产生的文档用 --\x3e\n    <name>banseon-maven</name>\n    \x3c!--项目主页的url, maven产生的文档用 --\x3e\n    <url>http://www.baidu.com/banseon</url>\n    \x3c!-- 项目的详细描述, maven 产生的文档用。 当这个元素能够用html格式描述时（例如，cdata中的文本会被解析器忽略，就可以包含html标 \n        签）， 不鼓励使用纯文本描述。如果你需要修改产生的web站点的索引页面，你应该修改你自己的索引页文件，而不是调整这里的文档。 --\x3e\n    <description>a maven project to study maven.</description>\n    \x3c!--描述了这个项目构建环境中的前提条件。 --\x3e\n    <prerequisites>\n        \x3c!--构建该项目或使用该插件所需要的maven的最低版本 --\x3e\n        <maven />\n    </prerequisites>\n    \x3c!--项目的问题管理系统(bugzilla, jira, scarab,或任何你喜欢的问题管理系统)的名称和url，本例为 jira --\x3e\n    <issuemanagement>\n        \x3c!--问题管理系统（例如jira）的名字， --\x3e\n        <system>jira</system>\n        \x3c!--该项目使用的问题管理系统的url --\x3e\n        <url>http://jira.baidu.com/banseon</url>\n    </issuemanagement>\n    \x3c!--项目持续集成信息 --\x3e\n    <cimanagement>\n        \x3c!--持续集成系统的名字，例如continuum --\x3e\n        <system />\n        \x3c!--该项目使用的持续集成系统的url（如果持续集成系统有web接口的话）。 --\x3e\n        <url />\n        \x3c!--构建完成时，需要通知的开发者/用户的配置项。包括被通知者信息和通知条件（错误，失败，成功，警告） --\x3e\n        <notifiers>\n            \x3c!--配置一种方式，当构建中断时，以该方式通知用户/开发者 --\x3e\n            <notifier>\n                \x3c!--传送通知的途径 --\x3e\n                <type />\n                \x3c!--发生错误时是否通知 --\x3e\n                <sendonerror />\n                \x3c!--构建失败时是否通知 --\x3e\n                <sendonfailure />\n                \x3c!--构建成功时是否通知 --\x3e\n                <sendonsuccess />\n                \x3c!--发生警告时是否通知 --\x3e\n                <sendonwarning />\n                \x3c!--不赞成使用。通知发送到哪里 --\x3e\n                <address />\n                \x3c!--扩展配置项 --\x3e\n                <configuration />\n            </notifier>\n        </notifiers>\n    </cimanagement>\n    \x3c!--项目创建年份，4位数字。当产生版权信息时需要使用这个值。 --\x3e\n    <inceptionyear />\n    \x3c!--项目相关邮件列表信息 --\x3e\n    <mailinglists>\n        \x3c!--该元素描述了项目相关的所有邮件列表。自动产生的网站引用这些信息。 --\x3e\n        <mailinglist>\n            \x3c!--邮件的名称 --\x3e\n            <name>demo</name>\n            \x3c!--发送邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --\x3e\n            <post>banseon@126.com</post>\n            \x3c!--订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --\x3e\n            <subscribe>banseon@126.com</subscribe>\n            \x3c!--取消订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto: 链接会被自动创建 --\x3e\n            <unsubscribe>banseon@126.com</unsubscribe>\n            \x3c!--你可以浏览邮件信息的url --\x3e\n            <archive>http:/hi.baidu.com/banseon/demo/dev/</archive>\n        </mailinglist>\n    </mailinglists>\n    \x3c!--项目开发者列表 --\x3e\n    <developers>\n        \x3c!--某个项目开发者的信息 --\x3e\n        <developer>\n            \x3c!--scm里项目开发者的唯一标识符 --\x3e\n            <id>hello world</id>\n            \x3c!--项目开发者的全名 --\x3e\n            <name>banseon</name>\n            \x3c!--项目开发者的email --\x3e\n            <email>banseon@126.com</email>\n            \x3c!--项目开发者的主页的url --\x3e\n            <url />\n            \x3c!--项目开发者在项目中扮演的角色，角色元素描述了各种角色 --\x3e\n            <roles>\n                <role>project manager</role>\n                <role>architect</role>\n            </roles>\n            \x3c!--项目开发者所属组织 --\x3e\n            <organization>demo</organization>\n            \x3c!--项目开发者所属组织的url --\x3e\n            <organizationurl>http://hi.baidu.com/banseon</organizationurl>\n            \x3c!--项目开发者属性，如即时消息如何处理等 --\x3e\n            <properties>\n                <dept>no</dept>\n            </properties>\n            \x3c!--项目开发者所在时区， -11到12范围内的整数。 --\x3e\n            <timezone>-5</timezone>\n        </developer>\n    </developers>\n    \x3c!--项目的其他贡献者列表 --\x3e\n    <contributors>\n        \x3c!--项目的其他贡献者。参见developers/developer元素 --\x3e\n        <contributor>\n            <name />\n            <email />\n            <url />\n            <organization />\n            <organizationurl />\n            <roles />\n            <timezone />\n            <properties />\n        </contributor>\n    </contributors>\n    \x3c!--该元素描述了项目所有license列表。 应该只列出该项目的license列表，不要列出依赖项目的 license列表。如果列出多个license，用户可以选择它们中的一个而不是接受所有license。 --\x3e\n    <licenses>\n        \x3c!--描述了项目的license，用于生成项目的web站点的license页面，其他一些报表和validation也会用到该元素。 --\x3e\n        <license>\n            \x3c!--license用于法律上的名称 --\x3e\n            <name>apache 2</name>\n            \x3c!--官方的license正文页面的url --\x3e\n            <url>http://www.baidu.com/banseon/license-2.0.txt</url>\n            \x3c!--项目分发的主要方式： repo，可以从maven库下载 manual， 用户必须手动下载和安装依赖 --\x3e\n            <distribution>repo</distribution>\n            \x3c!--关于license的补充信息 --\x3e\n            <comments>a business-friendly oss license</comments>\n        </license>\n    </licenses>\n    \x3c!--scm(source control management)标签允许你配置你的代码库，供maven web站点和其它插件使用。 --\x3e\n    <scm>\n        \x3c!--scm的url,该url描述了版本库和如何连接到版本库。欲知详情，请看scms提供的url格式和列表。该连接只读。 --\x3e\n        <connection>\n            scm:svn:http://svn.baidu.com/banseon/maven/banseon/banseon-maven2-trunk(dao-trunk)\n        </connection>\n        \x3c!--给开发者使用的，类似connection元素。即该连接不仅仅只读 --\x3e\n        <developerconnection>\n            scm:svn:http://svn.baidu.com/banseon/maven/banseon/dao-trunk\n        </developerconnection>\n        \x3c!--当前代码的标签，在开发阶段默认为head --\x3e\n        <tag />\n        \x3c!--指向项目的可浏览scm库（例如viewvc或者fisheye）的url。 --\x3e\n        <url>http://svn.baidu.com/banseon</url>\n    </scm>\n    \x3c!--描述项目所属组织的各种属性。maven产生的文档用 --\x3e\n    <organization>\n        \x3c!--组织的全名 --\x3e\n        <name>demo</name>\n        \x3c!--组织主页的url --\x3e\n        <url>http://www.baidu.com/banseon</url>\n    </organization>\n    \x3c!--构建项目需要的信息 --\x3e\n    <build>\n        \x3c!--该元素设置了项目源码目录，当构建项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --\x3e\n        <sourcedirectory />\n        \x3c!--该元素设置了项目脚本源码目录，该目录和源码目录不同：绝大多数情况下，该目录下的内容 会被拷贝到输出目录(因为脚本是被解释的，而不是被编译的)。 --\x3e\n        <scriptsourcedirectory />\n        \x3c!--该元素设置了项目单元测试使用的源码目录，当测试项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --\x3e\n        <testsourcedirectory />\n        \x3c!--被编译过的应用程序class文件存放的目录。 --\x3e\n        <outputdirectory />\n        \x3c!--被编译过的测试class文件存放的目录。 --\x3e\n        <testoutputdirectory />\n        \x3c!--使用来自该项目的一系列构建扩展 --\x3e\n        <extensions>\n            \x3c!--描述使用到的构建扩展。 --\x3e\n            <extension>\n                \x3c!--构建扩展的groupid --\x3e\n                <groupid />\n                \x3c!--构建扩展的artifactid --\x3e\n                <artifactid />\n                \x3c!--构建扩展的版本 --\x3e\n                <version />\n            </extension>\n        </extensions>\n        \x3c!--当项目没有规定目标（maven2 叫做阶段）时的默认值 --\x3e\n        <defaultgoal />\n        \x3c!--这个元素描述了项目相关的所有资源路径列表，例如和项目相关的属性文件，这些资源被包含在最终的打包文件里。 --\x3e\n        <resources>\n            \x3c!--这个元素描述了项目相关或测试相关的所有资源路径 --\x3e\n            <resource>\n                \x3c!-- 描述了资源的目标路径。该路径相对target/classes目录（例如${project.build.outputdirectory}）。举个例 \n                    子，如果你想资源在特定的包里(org.apache.maven.messages)，你就必须该元素设置为org/apache/maven /messages。然而，如果你只是想把资源放到源码目录结构里，就不需要该配置。 --\x3e\n                <targetpath />\n                \x3c!--是否使用参数值代替参数名。参数值取自properties元素或者文件里配置的属性，文件在filters元素里列出。 --\x3e\n                <filtering />\n                \x3c!--描述存放资源的目录，该路径相对pom路径 --\x3e\n                <directory />\n                \x3c!--包含的模式列表，例如**/*.xml. --\x3e\n                <includes />\n                \x3c!--排除的模式列表，例如**/*.xml --\x3e\n                <excludes />\n            </resource>\n        </resources>\n        \x3c!--这个元素描述了单元测试相关的所有资源路径，例如和单元测试相关的属性文件。 --\x3e\n        <testresources>\n            \x3c!--这个元素描述了测试相关的所有资源路径，参见build/resources/resource元素的说明 --\x3e\n            <testresource>\n                <targetpath />\n                <filtering />\n                <directory />\n                <includes />\n                <excludes />\n            </testresource>\n        </testresources>\n        \x3c!--构建产生的所有文件存放的目录 --\x3e\n        <directory />\n        \x3c!--产生的构件的文件名，默认值是${artifactid}-${version}。 --\x3e\n        <finalname />\n        \x3c!--当filtering开关打开时，使用到的过滤器属性文件列表 --\x3e\n        <filters />\n        \x3c!--子项目可以引用的默认插件信息。该插件配置项直到被引用时才会被解析或绑定到生命周期。给定插件的任何本地配置都会覆盖这里的配置 --\x3e\n        <pluginmanagement>\n            \x3c!--使用的插件列表 。 --\x3e\n            <plugins>\n                \x3c!--plugin元素包含描述插件所需要的信息。 --\x3e\n                <plugin>\n                    \x3c!--插件在仓库里的group id --\x3e\n                    <groupid />\n                    \x3c!--插件在仓库里的artifact id --\x3e\n                    <artifactid />\n                    \x3c!--被使用的插件的版本（或版本范围） --\x3e\n                    <version />\n                    \x3c!--是否从该插件下载maven扩展（例如打包和类型处理器），由于性能原因，只有在真需要下载时，该元素才被设置成enabled。 --\x3e\n                    <extensions />\n                    \x3c!--在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。 --\x3e\n                    <executions>\n                        \x3c!--execution元素包含了插件执行需要的信息 --\x3e\n                        <execution>\n                            \x3c!--执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标 --\x3e\n                            <id />\n                            \x3c!--绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段 --\x3e\n                            <phase />\n                            \x3c!--配置的执行目标 --\x3e\n                            <goals />\n                            \x3c!--配置是否被传播到子pom --\x3e\n                            <inherited />\n                            \x3c!--作为dom对象的配置 --\x3e\n                            <configuration />\n                        </execution>\n                    </executions>\n                    \x3c!--项目引入插件所需要的额外依赖 --\x3e\n                    <dependencies>\n                        \x3c!--参见dependencies/dependency元素 --\x3e\n                        <dependency>\n                            ......\n                        </dependency>\n                    </dependencies>\n                    \x3c!--任何配置是否被传播到子项目 --\x3e\n                    <inherited />\n                    \x3c!--作为dom对象的配置 --\x3e\n                    <configuration />\n                </plugin>\n            </plugins>\n        </pluginmanagement>\n        \x3c!--使用的插件列表 --\x3e\n        <plugins>\n            \x3c!--参见build/pluginmanagement/plugins/plugin元素 --\x3e\n            <plugin>\n                <groupid />\n                <artifactid />\n                <version />\n                <extensions />\n                <executions>\n                    <execution>\n                        <id />\n                        <phase />\n                        <goals />\n                        <inherited />\n                        <configuration />\n                    </execution>\n                </executions>\n                <dependencies>\n                    \x3c!--参见dependencies/dependency元素 --\x3e\n                    <dependency>\n                        ......\n                    </dependency>\n                </dependencies>\n                <goals />\n                <inherited />\n                <configuration />\n            </plugin>\n        </plugins>\n    </build>\n    \x3c!--在列的项目构建profile，如果被激活，会修改构建处理 --\x3e\n    <profiles>\n        \x3c!--根据环境参数或命令行参数激活某个构建处理 --\x3e\n        <profile>\n            \x3c!--构建配置的唯一标识符。即用于命令行激活，也用于在继承时合并具有相同标识符的profile。 --\x3e\n            <id />\n            \x3c!--自动触发profile的条件逻辑。activation是profile的开启钥匙。profile的力量来自于它 能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。activation元素并不是激活profile的唯一方式。 --\x3e\n            <activation>\n                \x3c!--profile默认是否激活的标志 --\x3e\n                <activebydefault />\n                \x3c!--当匹配的jdk被检测到，profile被激活。例如，1.4激活jdk1.4，1.4.0_2，而!1.4激活所有版本不是以1.4开头的jdk。 --\x3e\n                <jdk />\n                \x3c!--当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 --\x3e\n                <os>\n                    \x3c!--激活profile的操作系统的名字 --\x3e\n                    <name>windows xp</name>\n                    \x3c!--激活profile的操作系统所属家族(如 \'windows\') --\x3e\n                    <family>windows</family>\n                    \x3c!--激活profile的操作系统体系结构 --\x3e\n                    <arch>x86</arch>\n                    \x3c!--激活profile的操作系统版本 --\x3e\n                    <version>5.1.2600</version>\n                </os>\n                \x3c!--如果maven检测到某一个属性（其值可以在pom中通过${名称}引用），其拥有对应的名称和值，profile就会被激活。如果值 字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段 --\x3e\n                <property>\n                    \x3c!--激活profile的属性的名称 --\x3e\n                    <name>mavenversion</name>\n                    \x3c!--激活profile的属性的值 --\x3e\n                    <value>2.0.3</value>\n                </property>\n                \x3c!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活 profile。另一方面，exists则会检查文件是否存在，如果存在则激活profile。 --\x3e\n                <file>\n                    \x3c!--如果指定的文件存在，则激活profile。 --\x3e\n                    <exists>/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/\n                    </exists>\n                    \x3c!--如果指定的文件不存在，则激活profile。 --\x3e\n                    <missing>/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/\n                    </missing>\n                </file>\n            </activation>\n            \x3c!--构建项目所需要的信息。参见build元素 --\x3e\n            <build>\n                <defaultgoal />\n                <resources>\n                    <resource>\n                        <targetpath />\n                        <filtering />\n                        <directory />\n                        <includes />\n                        <excludes />\n                    </resource>\n                </resources>\n                <testresources>\n                    <testresource>\n                        <targetpath />\n                        <filtering />\n                        <directory />\n                        <includes />\n                        <excludes />\n                    </testresource>\n                </testresources>\n                <directory />\n                <finalname />\n                <filters />\n                <pluginmanagement>\n                    <plugins>\n                        \x3c!--参见build/pluginmanagement/plugins/plugin元素 --\x3e\n                        <plugin>\n                            <groupid />\n                            <artifactid />\n                            <version />\n                            <extensions />\n                            <executions>\n                                <execution>\n                                    <id />\n                                    <phase />\n                                    <goals />\n                                    <inherited />\n                                    <configuration />\n                                </execution>\n                            </executions>\n                            <dependencies>\n                                \x3c!--参见dependencies/dependency元素 --\x3e\n                                <dependency>\n                                    ......\n                                </dependency>\n                            </dependencies>\n                            <goals />\n                            <inherited />\n                            <configuration />\n                        </plugin>\n                    </plugins>\n                </pluginmanagement>\n                <plugins>\n                    \x3c!--参见build/pluginmanagement/plugins/plugin元素 --\x3e\n                    <plugin>\n                        <groupid />\n                        <artifactid />\n                        <version />\n                        <extensions />\n                        <executions>\n                            <execution>\n                                <id />\n                                <phase />\n                                <goals />\n                                <inherited />\n                                <configuration />\n                            </execution>\n                        </executions>\n                        <dependencies>\n                            \x3c!--参见dependencies/dependency元素 --\x3e\n                            <dependency>\n                                ......\n                            </dependency>\n                        </dependencies>\n                        <goals />\n                        <inherited />\n                        <configuration />\n                    </plugin>\n                </plugins>\n            </build>\n            \x3c!--模块（有时称作子项目） 被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径 --\x3e\n            <modules />\n            \x3c!--发现依赖和扩展的远程仓库列表。 --\x3e\n            <repositories>\n                \x3c!--参见repositories/repository元素 --\x3e\n                <repository>\n                    <releases>\n                        <enabled />\n                        <updatepolicy />\n                        <checksumpolicy />\n                    </releases>\n                    <snapshots>\n                        <enabled />\n                        <updatepolicy />\n                        <checksumpolicy />\n                    </snapshots>\n                    <id />\n                    <name />\n                    <url />\n                    <layout />\n                </repository>\n            </repositories>\n            \x3c!--发现插件的远程仓库列表，这些插件用于构建和报表 --\x3e\n            <pluginrepositories>\n                \x3c!--包含需要连接到远程插件仓库的信息.参见repositories/repository元素 --\x3e\n                <pluginrepository>\n                    <releases>\n                        <enabled />\n                        <updatepolicy />\n                        <checksumpolicy />\n                    </releases>\n                    <snapshots>\n                        <enabled />\n                        <updatepolicy />\n                        <checksumpolicy />\n                    </snapshots>\n                    <id />\n                    <name />\n                    <url />\n                    <layout />\n                </pluginrepository>\n            </pluginrepositories>\n            \x3c!--该元素描述了项目相关的所有依赖。 这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。 --\x3e\n            <dependencies>\n                \x3c!--参见dependencies/dependency元素 --\x3e\n                <dependency>\n                    ......\n                </dependency>\n            </dependencies>\n            \x3c!--不赞成使用. 现在maven忽略该元素. --\x3e\n            <reports />\n            \x3c!--该元素包括使用报表插件产生报表的规范。当用户执行“mvn site”，这些报表就会运行。 在页面导航栏能看到所有报表的链接。参见reporting元素 --\x3e\n            <reporting>\n                ......\n            </reporting>\n            \x3c!--参见dependencymanagement元素 --\x3e\n            <dependencymanagement>\n                <dependencies>\n                    \x3c!--参见dependencies/dependency元素 --\x3e\n                    <dependency>\n                        ......\n                    </dependency>\n                </dependencies>\n            </dependencymanagement>\n            \x3c!--参见distributionmanagement元素 --\x3e\n            <distributionmanagement>\n                ......\n            </distributionmanagement>\n            \x3c!--参见properties元素 --\x3e\n            <properties />\n        </profile>\n    </profiles>\n    \x3c!--模块（有时称作子项目） 被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径 --\x3e\n    <modules />\n    \x3c!--发现依赖和扩展的远程仓库列表。 --\x3e\n    <repositories>\n        \x3c!--包含需要连接到远程仓库的信息 --\x3e\n        <repository>\n            \x3c!--如何处理远程仓库里发布版本的下载 --\x3e\n            <releases>\n                \x3c!--true或者false表示该仓库是否为下载某种类型构件（发布版，快照版）开启。 --\x3e\n                <enabled />\n                \x3c!--该元素指定更新发生的频率。maven会比较本地pom和远程pom的时间戳。这里的选项是：always（一直），daily（默认，每日），interval：x（这里x是以分钟为单位的时间间隔），或者never（从不）。 --\x3e\n                <updatepolicy />\n                \x3c!--当maven验证构件校验文件失败时该怎么做：ignore（忽略），fail（失败），或者warn（警告）。 --\x3e\n                <checksumpolicy />\n            </releases>\n            \x3c!-- 如何处理远程仓库里快照版本的下载。有了releases和snapshots这两组配置，pom就可以在每个单独的仓库中，为每种类型的构件采取不同的 \n                策略。例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素 --\x3e\n            <snapshots>\n                <enabled />\n                <updatepolicy />\n                <checksumpolicy />\n            </snapshots>\n            \x3c!--远程仓库唯一标识符。可以用来匹配在settings.xml文件里配置的远程仓库 --\x3e\n            <id>banseon-repository-proxy</id>\n            \x3c!--远程仓库名称 --\x3e\n            <name>banseon-repository-proxy</name>\n            \x3c!--远程仓库url，按protocol://hostname/path形式 --\x3e\n            <url>http://192.168.1.169:9999/repository/</url>\n            \x3c!-- 用于定位和排序构件的仓库布局类型-可以是default（默认）或者legacy（遗留）。maven 2为其仓库提供了一个默认的布局；然 \n                而，maven 1.x有一种不同的布局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留）。 --\x3e\n            <layout>default</layout>\n        </repository>\n    </repositories>\n    \x3c!--发现插件的远程仓库列表，这些插件用于构建和报表 --\x3e\n    <pluginrepositories>\n        \x3c!--包含需要连接到远程插件仓库的信息.参见repositories/repository元素 --\x3e\n        <pluginrepository>\n            ......\n        </pluginrepository>\n    </pluginrepositories>\n\n    \x3c!--该元素描述了项目相关的所有依赖。 这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。 --\x3e\n    <dependencies>\n        <dependency>\n            \x3c!--依赖的group id --\x3e\n            <groupid>org.apache.maven</groupid>\n            \x3c!--依赖的artifact id --\x3e\n            <artifactid>maven-artifact</artifactid>\n            \x3c!--依赖的版本号。 在maven 2里, 也可以配置成版本号的范围。 --\x3e\n            <version>3.8.1</version>\n            \x3c!-- 依赖类型，默认类型是jar。它通常表示依赖的文件的扩展名，但也有例外。一个类型可以被映射成另外一个扩展名或分类器。类型经常和使用的打包方式对应， \n                尽管这也有例外。一些类型的例子：jar，war，ejb-client和test-jar。如果设置extensions为 true，就可以在 plugin里定义新的类型。所以前面的类型的例子不完整。 --\x3e\n            <type>jar</type>\n            \x3c!-- 依赖的分类器。分类器可以区分属于同一个pom，但不同构建方式的构件。分类器名被附加到文件名的版本号后面。例如，如果你想要构建两个单独的构件成 \n                jar，一个使用java 1.4编译器，另一个使用java 6编译器，你就可以使用分类器来生成两个单独的jar构件。 --\x3e\n            <classifier></classifier>\n            \x3c!--依赖范围。在项目发布过程中，帮助决定哪些构件被包括进来。欲知详情请参考依赖机制。 - compile ：默认范围，用于编译 - provided：类似于编译，但支持你期待jdk或者容器提供，类似于classpath \n                - runtime: 在执行时需要使用 - test: 用于test任务时使用 - system: 需要外在提供相应的元素。通过systempath来取得 \n                - systempath: 仅用于范围为system。提供相应的路径 - optional: 当项目自身被依赖时，标注依赖是否传递。用于连续依赖时使用 --\x3e\n            <scope>test</scope>\n            \x3c!--仅供system范围使用。注意，不鼓励使用这个元素，并且在新的版本中该元素可能被覆盖掉。该元素为依赖规定了文件系统上的路径。需要绝对路径而不是相对路径。推荐使用属性匹配绝对路径，例如${java.home}。 --\x3e\n            <systempath></systempath>\n            \x3c!--当计算传递依赖时， 从依赖构件列表里，列出被排除的依赖构件集。即告诉maven你只依赖指定的项目，不依赖项目的依赖。此元素主要用于解决版本冲突问题 --\x3e\n            <exclusions>\n                <exclusion>\n                    <artifactid>spring-core</artifactid>\n                    <groupid>org.springframework</groupid>\n                </exclusion>\n            </exclusions>\n            \x3c!--可选依赖，如果你在项目b中把c依赖声明为可选，你就需要在依赖于b的项目（例如项目a）中显式的引用对c的依赖。可选依赖阻断依赖的传递性。 --\x3e\n            <optional>true</optional>\n        </dependency>\n    </dependencies>\n    \x3c!--不赞成使用. 现在maven忽略该元素. --\x3e\n    <reports></reports>\n    \x3c!--该元素描述使用报表插件产生报表的规范。当用户执行“mvn site”，这些报表就会运行。 在页面导航栏能看到所有报表的链接。 --\x3e\n    <reporting>\n        \x3c!--true，则，网站不包括默认的报表。这包括“项目信息”菜单中的报表。 --\x3e\n        <excludedefaults />\n        \x3c!--所有产生的报表存放到哪里。默认值是${project.build.directory}/site。 --\x3e\n        <outputdirectory />\n        \x3c!--使用的报表插件和他们的配置。 --\x3e\n        <plugins>\n            \x3c!--plugin元素包含描述报表插件需要的信息 --\x3e\n            <plugin>\n                \x3c!--报表插件在仓库里的group id --\x3e\n                <groupid />\n                \x3c!--报表插件在仓库里的artifact id --\x3e\n                <artifactid />\n                \x3c!--被使用的报表插件的版本（或版本范围） --\x3e\n                <version />\n                \x3c!--任何配置是否被传播到子项目 --\x3e\n                <inherited />\n                \x3c!--报表插件的配置 --\x3e\n                <configuration />\n                \x3c!--一组报表的多重规范，每个规范可能有不同的配置。一个规范（报表集）对应一个执行目标 。例如，有1，2，3，4，5，6，7，8，9个报表。1，2，5构成a报表集，对应一个执行目标。2，5，8构成b报表集，对应另一个执行目标 --\x3e\n                <reportsets>\n                    \x3c!--表示报表的一个集合，以及产生该集合的配置 --\x3e\n                    <reportset>\n                        \x3c!--报表集合的唯一标识符，pom继承时用到 --\x3e\n                        <id />\n                        \x3c!--产生报表集合时，被使用的报表的配置 --\x3e\n                        <configuration />\n                        \x3c!--配置是否被继承到子poms --\x3e\n                        <inherited />\n                        \x3c!--这个集合里使用到哪些报表 --\x3e\n                        <reports />\n                    </reportset>\n                </reportsets>\n            </plugin>\n        </plugins>\n    </reporting>\n    \x3c!-- 继承自该项目的所有子项目的默认依赖信息。这部分的依赖信息不会被立即解析,而是当子项目声明一个依赖（必须描述group id和 artifact \n        id信息），如果group id和artifact id以外的一些信息没有描述，则通过group id和artifact id 匹配到这里的依赖，并使用这里的依赖信息。 --\x3e\n    <dependencymanagement>\n        <dependencies>\n            \x3c!--参见dependencies/dependency元素 --\x3e\n            <dependency>\n                ......\n            </dependency>\n        </dependencies>\n    </dependencymanagement>\n    \x3c!--项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。 --\x3e\n    <distributionmanagement>\n        \x3c!--部署项目产生的构件到远程仓库需要的信息 --\x3e\n        <repository>\n            \x3c!--是分配给快照一个唯一的版本号（由时间戳和构建流水号）？还是每次都使用相同的版本号？参见repositories/repository元素 --\x3e\n            <uniqueversion />\n            <id>banseon-maven2</id>\n            <name>banseon maven2</name>\n            <url>file://${basedir}/target/deploy</url>\n            <layout />\n        </repository>\n        \x3c!--构件的快照部署到哪里？如果没有配置该元素，默认部署到repository元素配置的仓库，参见distributionmanagement/repository元素 --\x3e\n        <snapshotrepository>\n            <uniqueversion />\n            <id>banseon-maven2</id>\n            <name>banseon-maven2 snapshot repository</name>\n            <url>scp://svn.baidu.com/banseon:/usr/local/maven-snapshot</url>\n            <layout />\n        </snapshotrepository>\n        \x3c!--部署项目的网站需要的信息 --\x3e\n        <site>\n            \x3c!--部署位置的唯一标识符，用来匹配站点和settings.xml文件里的配置 --\x3e\n            <id>banseon-site</id>\n            \x3c!--部署位置的名称 --\x3e\n            <name>business api website</name>\n            \x3c!--部署位置的url，按protocol://hostname/path形式 --\x3e\n            <url>\n                scp://svn.baidu.com/banseon:/var/www/localhost/banseon-web\n            </url>\n        </site>\n        \x3c!--项目下载页面的url。如果没有该元素，用户应该参考主页。使用该元素的原因是：帮助定位那些不在仓库里的构件（由于license限制）。 --\x3e\n        <downloadurl />\n        \x3c!--如果构件有了新的group id和artifact id（构件移到了新的位置），这里列出构件的重定位信息。 --\x3e\n        <relocation>\n            \x3c!--构件新的group id --\x3e\n            <groupid />\n            \x3c!--构件新的artifact id --\x3e\n            <artifactid />\n            \x3c!--构件新的版本号 --\x3e\n            <version />\n            \x3c!--显示给用户的，关于移动的额外信息，例如原因。 --\x3e\n            <message />\n        </relocation>\n        \x3c!-- 给出该构件在远程仓库的状态。不得在本地项目中设置该元素，因为这是工具自动更新的。有效的值有：none（默认），converted（仓库管理员从 \n            maven 1 pom转换过来），partner（直接从伙伴maven 2仓库同步过来），deployed（从maven 2实例部 署），verified（被核实时正确的和最终的）。 --\x3e\n        <status />\n    </distributionmanagement>\n    \x3c!--以值替代名称，properties可以在整个pom中使用，也可以作为触发条件（见settings.xml配置文件里activation元素的说明）。格式是<name>value</name>。 --\x3e\n    <properties />\n</project>\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:13:40",lastUpdatedTimestamp:164437642e4},{title:"将项目推送到 Maven 中央仓库实践",frontmatter:{title:"将项目推送到 Maven 中央仓库实践",date:"2022-02-09T11:09:28.000Z",permalink:"/pages/f28aa7/"},regularPath:"/07.Nexus/01.Nexus/07.%E5%B0%86%E9%A1%B9%E7%9B%AE%E6%8E%A8%E9%80%81%E5%88%B0%20Maven%20%E4%B8%AD%E5%A4%AE%E4%BB%93%E5%BA%93%E5%AE%9E%E8%B7%B5.html",relativePath:"07.Nexus/01.Nexus/07.将项目推送到 Maven 中央仓库实践.md",key:"v-f26a2222",path:"/pages/f28aa7/",headers:[{level:2,title:"一、将项目推送到 Github",slug:"一、将项目推送到-github",normalizedTitle:"一、将项目推送到 github",charIndex:26},{level:2,title:"二、注册 Sonatype 账户",slug:"二、注册-sonatype-账户",normalizedTitle:"二、注册 sonatype 账户",charIndex:115},{level:2,title:"三、登录 Sonatype 创建工单",slug:"三、登录-sonatype-创建工单",normalizedTitle:"三、登录 sonatype 创建工单",charIndex:215},{level:2,title:"四、确认域名",slug:"四、确认域名",normalizedTitle:"四、确认域名",charIndex:683},{level:2,title:"五、macOS配置 GPG",slug:"五、macos配置-gpg",normalizedTitle:"五、macos配置 gpg",charIndex:2017},{level:3,title:"1、下载 GPG",slug:"_1、下载-gpg",normalizedTitle:"1、下载 gpg",charIndex:2035},{level:3,title:"2、配置密钥对",slug:"_2、配置密钥对",normalizedTitle:"2、配置密钥对",charIndex:2091},{level:2,title:"六、Windows配置 GPG",slug:"六、windows配置-gpg",normalizedTitle:"六、windows配置 gpg",charIndex:2316},{level:3,title:"1、下载 GPG",slug:"_1、下载-gpg-2",normalizedTitle:"1、下载 gpg",charIndex:2035},{level:3,title:"2、配置密钥对",slug:"_2、配置密钥对-2",normalizedTitle:"2、配置密钥对",charIndex:2091},{level:3,title:"3、上传公钥到 GPS key-servers",slug:"_3、上传公钥到-gps-key-servers",normalizedTitle:"3、上传公钥到 gps key-servers",charIndex:2559},{level:2,title:"七、配置项目",slug:"七、配置项目",normalizedTitle:"七、配置项目",charIndex:2723},{level:3,title:"1、配置 Strings.xml",slug:"_1、配置-strings-xml",normalizedTitle:"1、配置 strings.xml",charIndex:2775},{level:3,title:"2、配置 Pom.xml",slug:"_2、配置-pom-xml",normalizedTitle:"2、配置 pom.xml",charIndex:3002},{level:3,title:"3、设置编码",slug:"_3、设置编码",normalizedTitle:"3、设置编码",charIndex:10674},{level:2,title:"八、发送项目到 Maven 中央仓库",slug:"八、发送项目到-maven-中央仓库",normalizedTitle:"八、发送项目到 maven 中央仓库",charIndex:10801},{level:3,title:"1、登录 Nexus Repositories Manager",slug:"_1、登录-nexus-repositories-manager",normalizedTitle:"1、登录 nexus repositories manager",charIndex:10824},{level:3,title:"2、发布项目",slug:"_2、发布项目",normalizedTitle:"2、发布项目",charIndex:10982},{level:3,title:"3、查看发布过程",slug:"_3、查看发布过程",normalizedTitle:"3、查看发布过程",charIndex:11081},{level:3,title:"4、在 Issues 中回复已经 Release 完成",slug:"_4、在-issues-中回复已经-release-完成",normalizedTitle:"4、在 issues 中回复已经 release 完成",charIndex:11858},{level:3,title:"5、查看自己发布的项目",slug:"_5、查看自己发布的项目",normalizedTitle:"5、查看自己发布的项目",charIndex:11926}],headersStr:"一、将项目推送到 Github 二、注册 Sonatype 账户 三、登录 Sonatype 创建工单 四、确认域名 五、macOS配置 GPG 1、下载 GPG 2、配置密钥对 六、Windows配置 GPG 1、下载 GPG 2、配置密钥对 3、上传公钥到 GPS key-servers 七、配置项目 1、配置 Strings.xml 2、配置 Pom.xml 3、设置编码 八、发送项目到 Maven 中央仓库 1、登录 Nexus Repositories Manager 2、发布项目 3、查看发布过程 4、在 Issues 中回复已经 Release 完成 5、查看自己发布的项目",content:'# 将项目推送到 Maven 中央仓库实践\n\n\n# 一、将项目推送到 Github\n\n在 GitHub 上新建一个项目仓库，然后将要发送到 Maven 中央仓库的代码推送到 Github。这方面资料较多，这里不过多叙述。\n\n\n# 二、注册 Sonatype 账户\n\n进入 https://issues.sonatype.org/secure/Signup!default.jspa 注册JIRA账号，按提示完善个人信息。\n\n\n# 三、登录 Sonatype 创建工单\n\n登录 https://issues.sonatype.org/secure/Dashboard.jspa 点击创建按钮来创建一个新的 issues 工单\n\n内容如下，填写必填部分即可，然后等待审核。\n\n提示\n\n * 项目： Community Support - Open Source Project Repository Hosting (OSSRH)\n * 问题类型： New Project\n * 概要： Simplify Cloud Build Dependencies\n * 描述： 可选\n * Group Id： com.ieooc\n * Project URL： https://github.com/simplify-cloud/simplify-cloud-dependencies-parent\n * SCM url： https://github.com/simplify-cloud/simplify-cloud-dependencies-parent.git\n\n\n# 四、确认域名\n\n等待一段时间就会收到一封邮件，内容如下：\n\n[ https://issues.sonatype.org/browse/OSSRH-59440 ]\n\nCentral OSSRH 更新了 OSSRH-59440:\n------------------------------\n状态: Waiting for Response  (原值: 开放)\n\nDo you own the domain ieooc.com? If so, please verify ownership via one of the following methods:\n\n*  Add a TXT record to your DNS referencing this JIRA ticket: [OSSRH-59440](Fastest)\n*  Setup a redirect to your Github page (if it does not already exist)\n\nIf you do not own this domain, please read:\nhttp://central.sonatype.org/pages/choosing-your-coordinates.html\nYou may also choose a groupId that reflects your project hosting, in this case, something like io.github.heyuqiang or com.github.heyuqiang\n\n----------------------------------------------------------------------------------------------------------------------\n\n关键字: OSSRH-59440\nURL: https://issues.sonatype.org/browse/OSSRH-59440\n项目: Community Support - Open Source Project Repository Hosting\n问题类型: New Project\n报告人: heyuqiang\n经办人: Joel Orlina\n优先级: 重要\n\n\n\n意思就是询问你该 GroupId 设置的域名是否为你自己所拥有，如果是就需要证明一下，两种方法\n\n * 1、设置github page当做域名。\n * 2、在域名 DNS 解析中添加一条类型为 Txt 的记录。\n\n由于本人有自己的域名，所以设置使用了第二种方法添加 NDS 解析记录，这里添加了记录为 OSSRH-59440，类型为 TxT，值为 https://issues.sonatype.org/browse/OSSRH-59440 的一条解析记录信息。\n\n\n\n然后打开邮件中提到的地址 https://issues.sonatype.org/browse/OSSRH-59440 进入该 issues，在里面回复自己拥有该域名，且设置了 DNS 解析。\n\n\n# 五、macOS配置 GPG\n\n\n# 1、下载 GPG\n\n打开 https://gpgtools.org 下载 GPG，然后一步步安装。\n\n\n\n\n# 2、配置密钥对\n\n提示\n\n过程中需要自己输入一个8位以上的密码，请记住该密码，后面还需要用到。\n\n打开 GPG Keychain 后，点击+按钮，输入姓名、电子邮件、密码和确认密码，点击生成密钥按钮。\n\n\n\n接下来，点击Upload Public Key按钮将证书发送到远程服务器。\n\n\n\n下图表示密钥已经成功上传至服务器。如果上传失败，鼠标右键点击Send Public Key to Key Server按钮重新上传即可，多试几次。\n\n\n\n\n# 六、Windows配置 GPG\n\n\n# 1、下载 GPG\n\n打开 https://www.gpg4win.org/ 下载 GPG，然后一步步安装。\n\n安装完成后会看到桌面上多了个 Kleopatra 图标，这个程序是 GnuPG 的前端程序，这里我们将它打开。\n\n\n# 2、配置密钥对\n\n打开 Kleopatra 后，点击新建密钥对\n\n输入姓名和邮箱\n\n最后确认新建，过程中需要自己输入一个8位以上的密码，请记住该密码，后面还需要用到。 输入完成以后就会提示下面信息。\n\n\n\n\n\n\n# 3、上传公钥到 GPS key-servers\n\n打开命令提示符窗口，输入下面命令将证书发送到远程服务器\n\ngpg --keyserver hkp://keyserver.ubuntu.com:11371 --send-keys  3C8AE63C486358D2090F565928821A74321B8CCD\n\n\n\n\n\n# 七、配置项目\n\n要想将项目发送到 Maven 中央仓库，需要对项目进行一些设置来符合发布条件。\n\n\n# 1、配置 Strings.xml\n\n配置 maven 目录 conf 文件夹中的 settings.xml 文件，设置一个 server,里面添加 JIRA 的账号和密码。\n\n<servers>\n   <server>\n      <id>sona</id>\n      <username>JIRA账号</username>\n      <password>JIRA密码</password>\n   </server>\n</servers>\n\n\n\n# 2、配置 Pom.xml\n\nPom.xml 配置一些必要的信息，例如 Maven 文档插件、打包插件、GPG验证插件等，如下：\n\n提示\n\n可以查看我的项目的 GitHub 地址： Maven 示例项目，pom 参数照着修改即可，顺便点下 Start 星，Thanks。\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n\n    <modelVersion>4.0.0</modelVersion>\n    <inceptionYear>2021</inceptionYear>\n    <groupId>com.ieooc.cloud</groupId>\n    <artifactId>simplify-cloud-dependencies-parent</artifactId>\n    <version>1.0.0.RELEASE</version>\n    <packaging>pom</packaging>\n\n    <name>simplify-cloud-dependencies-parent</name>\n    <description>Simplify Cloud Build Dependencies</description>\n    <url>https://www.ieooc.com/</url>\n\n    <organization>\n        <name>Simplify Software, Inc.</name>\n        <url>https://www.ieooc.com</url>\n    </organization>\n\n    <licenses>\n        <license>\n            <name>Apache License, Version 2.0</name>\n            <url>https://www.apache.org/licenses/LICENSE-2.0</url>\n            <comments>Copyright 2014-2015 the original author or authors.\n\n                Licensed under the Apache License, Version 2.0 (the "License");\n                you may not use this file except in compliance with the License.\n                You may obtain a copy of the License at\n\n                https://www.apache.org/licenses/LICENSE-2.0\n\n                Unless required by applicable law or agreed to in writing, software\n                distributed under the License is distributed on an "AS IS" BASIS,\n                WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n                implied.\n\n                See the License for the specific language governing permissions and\n                limitations under the License.</comments>\n        </license>\n    </licenses>\n\n    <developers>\n        <developer>\n            <id>heyuqiang</id>\n            <name>heyuqiang</name>\n            <email>785056500@qq.com</email>\n            <organization>Pivotal Software, Inc.</organization>\n            <organizationUrl>https://www.ieooc.com</organizationUrl>\n            <roles>\n                <role>Project lead</role>\n            </roles>\n        </developer>\n    </developers>\n\n    <scm>\n        <connection>scm:git:git://github.com/simplify-cloud/simplify-cloud-dependencies-parent.git</connection>\n        <developerConnection>scm:git:ssh://git@github.com/simplify-cloud/simplify-cloud-dependencies-parent.git</developerConnection>\n        <url>https://github.com/simplify-cloud/simplify-cloud-dependencies-parent</url>\n    </scm>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    </properties>\n\n    <build>\n        <pluginManagement>\n            <plugins>\n                <plugin>\n                    <artifactId>maven-enforcer-plugin</artifactId>\n                    <version>1.4.1</version>\n                    <executions>\n                        <execution>\n                            <id>enforce-versions</id>\n                            <goals>\n                                <goal>enforce</goal>\n                            </goals>\n                        </execution>\n                    </executions>\n                    <configuration>\n                        <fail>false</fail>\n                        <rules>\n                            <dependencyConvergence />\n                        </rules>\n                    </configuration>\n                </plugin>\n            </plugins>\n        </pluginManagement>\n    </build>\n    <profiles>\n        <profile>\n            <id>central</id>\n            <build>\n                <plugins>\n                    <plugin>\n                        <groupId>org.codehaus.mojo</groupId>\n                        <artifactId>flatten-maven-plugin</artifactId>\n                        <version>1.2.2</version>\n                        <executions>\n                            <execution>\n                                <id>flatten</id>\n                                <phase>process-resources</phase>\n                                <goals>\n                                    <goal>flatten</goal>\n                                </goals>\n                                <configuration>\n                                    <updatePomFile>true</updatePomFile>\n                                    <flattenMode>bom</flattenMode>\n                                    <pomElements>\n                                        <parent>expand</parent>\n                                        <pluginManagement>keep</pluginManagement>\n                                        <properties>keep</properties>\n                                        <repositories>remove</repositories>\n                                        <profiles>keep</profiles>\n                                    </pomElements>\n                                </configuration>\n                            </execution>\n                            <execution>\n                                <id>flatten-clean</id>\n                                <phase>clean</phase>\n                                <goals>\n                                    <goal>clean</goal>\n                                </goals>\n                            </execution>\n                        </executions>\n                        <inherited>true</inherited>\n                    </plugin>\n                    <plugin>\n                        <artifactId>maven-gpg-plugin</artifactId>\n                        <version>1.6</version>\n                        <executions>\n                            <execution>\n                                <id>sign-artifacts</id>\n                                <phase>verify</phase>\n                                <goals>\n                                    <goal>sign</goal>\n                                </goals>\n                            </execution>\n                        </executions>\n                    </plugin>\n                    \x3c!--staging puglin,用于自动执行发布阶段(免手动)--\x3e\n                    <plugin>\n                        <groupId>org.sonatype.plugins</groupId>\n                        <artifactId>nexus-staging-maven-plugin</artifactId>\n                        <version>1.6.7</version>\n                        <extensions>true</extensions>\n                        <configuration>\n                            <serverId>sona</serverId>\n                            <nexusUrl>https://oss.sonatype.org/</nexusUrl>\n                            <autoReleaseAfterClose>true</autoReleaseAfterClose>\n                        </configuration>\n                    </plugin>\n                    \x3c!-- release plugin,用于发布到release仓库部署插件 --\x3e\n                    <plugin>\n                        <groupId>org.apache.maven.plugins</groupId>\n                        <artifactId>maven-release-plugin</artifactId>\n                        <version>2.4.2</version>\n                    </plugin>\n                </plugins>\n            </build>\n            <distributionManagement>\n                <repository>\n                    <id>sonatype-nexus-staging</id>\n                    <name>Nexus Release Repository</name>\n                    <url>https://oss.sonatype.org/service/local/staging/deploy/maven2/</url>\n                </repository>\n                <snapshotRepository>\n                    <id>sonatype-nexus-snapshots</id>\n                    <name>Sonatype Nexus Snapshots</name>\n                    <url>https://oss.sonatype.org/content/repositories/snapshots/</url>\n                </snapshotRepository>\n            </distributionManagement>\n        </profile>\n    </profiles>\n</project>\n\n\n\n# 3、设置编码\n\n如果Maven编译doc因编码原因出错，可以在环境变量设置变量 JAVA_TOOL_OPTIONS，值为 UTF-8 来改变编码方式。\n\n * JAVA_TOOL_OPTIONS\n * -Dfile.encoding=UTF-8\n\n\n# 八、发送项目到 Maven 中央仓库\n\n\n# 1、登录 Nexus Repositories Manager\n\n在发布之前要求必须有登录 Nexus Repositories 的信息，所以发布前提前登录 Nexus Repositories\n\n登录 https://oss.sonatype.org 其中用户名、密码就是 JIRA 的用户名、密码。\n\n\n\n\n# 2、发布项目\n\n执行 maven 命令，部署项目到 Maven 仓库，过程中需要输入之前设置的 GPG 密码，输入一下即可。\n\n$ mvn clean deploy -P central\n\n\n\n# 3、查看发布过程\n\n回到 Nexus Repositories Manager，选择 Staging Repositories 暂存库，根据发布时间查找到你提交的 Jar 信息。然后查看 Actives 里面的执行信息，注意执行部署过程现在流行都交给 Maven 自动部署插件执行，网上一些老办法是进入这里面手动执行，非常不推荐这样做，因为这样非常容易部署发生问题。\n\n\n\n提示\n\n在 Maven 执行发布过程中才能看到，如果已经 release 完成，暂存库中可能找不到。\n\n正常的部署流程应该为：open阶段—>close阶段—>release阶段 执行完成这三个阶段就会发布到中央仓库，等会会受到邮件通知\n\n[ https://issues.sonatype.org/browse/OSSRH-59440?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ] Central OSSRH 更新了 OSSRH-59440: ------------------------------ Central sync is activated for club.mydlq. After you successfully release, your component will be published to Central, typically within 10 minutes, though updates to search.maven.org can take up to two hours.\n\n\n意思为 Release 成功，大概等十分钟或者几个就能在中央仓库查询到该 Jar 信息，不过对search.maven.org的更新可能需要两个小时。\n\n\n# 4、在 Issues 中回复已经 Release 完成\n\n打开之前提交 Issues 地址，后面回复已经 Release 完成\n\n\n# 5、查看自己发布的项目\n\n发布成功后可以进入 https://search.maven.org 网址查询自己发布的 Jar，如果未查到等几个小时再此查询。',normalizedContent:'# 将项目推送到 maven 中央仓库实践\n\n\n# 一、将项目推送到 github\n\n在 github 上新建一个项目仓库，然后将要发送到 maven 中央仓库的代码推送到 github。这方面资料较多，这里不过多叙述。\n\n\n# 二、注册 sonatype 账户\n\n进入 https://issues.sonatype.org/secure/signup!default.jspa 注册jira账号，按提示完善个人信息。\n\n\n# 三、登录 sonatype 创建工单\n\n登录 https://issues.sonatype.org/secure/dashboard.jspa 点击创建按钮来创建一个新的 issues 工单\n\n内容如下，填写必填部分即可，然后等待审核。\n\n提示\n\n * 项目： community support - open source project repository hosting (ossrh)\n * 问题类型： new project\n * 概要： simplify cloud build dependencies\n * 描述： 可选\n * group id： com.ieooc\n * project url： https://github.com/simplify-cloud/simplify-cloud-dependencies-parent\n * scm url： https://github.com/simplify-cloud/simplify-cloud-dependencies-parent.git\n\n\n# 四、确认域名\n\n等待一段时间就会收到一封邮件，内容如下：\n\n[ https://issues.sonatype.org/browse/ossrh-59440 ]\n\ncentral ossrh 更新了 ossrh-59440:\n------------------------------\n状态: waiting for response  (原值: 开放)\n\ndo you own the domain ieooc.com? if so, please verify ownership via one of the following methods:\n\n*  add a txt record to your dns referencing this jira ticket: [ossrh-59440](fastest)\n*  setup a redirect to your github page (if it does not already exist)\n\nif you do not own this domain, please read:\nhttp://central.sonatype.org/pages/choosing-your-coordinates.html\nyou may also choose a groupid that reflects your project hosting, in this case, something like io.github.heyuqiang or com.github.heyuqiang\n\n----------------------------------------------------------------------------------------------------------------------\n\n关键字: ossrh-59440\nurl: https://issues.sonatype.org/browse/ossrh-59440\n项目: community support - open source project repository hosting\n问题类型: new project\n报告人: heyuqiang\n经办人: joel orlina\n优先级: 重要\n\n\n\n意思就是询问你该 groupid 设置的域名是否为你自己所拥有，如果是就需要证明一下，两种方法\n\n * 1、设置github page当做域名。\n * 2、在域名 dns 解析中添加一条类型为 txt 的记录。\n\n由于本人有自己的域名，所以设置使用了第二种方法添加 nds 解析记录，这里添加了记录为 ossrh-59440，类型为 txt，值为 https://issues.sonatype.org/browse/ossrh-59440 的一条解析记录信息。\n\n\n\n然后打开邮件中提到的地址 https://issues.sonatype.org/browse/ossrh-59440 进入该 issues，在里面回复自己拥有该域名，且设置了 dns 解析。\n\n\n# 五、macos配置 gpg\n\n\n# 1、下载 gpg\n\n打开 https://gpgtools.org 下载 gpg，然后一步步安装。\n\n\n\n\n# 2、配置密钥对\n\n提示\n\n过程中需要自己输入一个8位以上的密码，请记住该密码，后面还需要用到。\n\n打开 gpg keychain 后，点击+按钮，输入姓名、电子邮件、密码和确认密码，点击生成密钥按钮。\n\n\n\n接下来，点击upload public key按钮将证书发送到远程服务器。\n\n\n\n下图表示密钥已经成功上传至服务器。如果上传失败，鼠标右键点击send public key to key server按钮重新上传即可，多试几次。\n\n\n\n\n# 六、windows配置 gpg\n\n\n# 1、下载 gpg\n\n打开 https://www.gpg4win.org/ 下载 gpg，然后一步步安装。\n\n安装完成后会看到桌面上多了个 kleopatra 图标，这个程序是 gnupg 的前端程序，这里我们将它打开。\n\n\n# 2、配置密钥对\n\n打开 kleopatra 后，点击新建密钥对\n\n输入姓名和邮箱\n\n最后确认新建，过程中需要自己输入一个8位以上的密码，请记住该密码，后面还需要用到。 输入完成以后就会提示下面信息。\n\n\n\n\n\n\n# 3、上传公钥到 gps key-servers\n\n打开命令提示符窗口，输入下面命令将证书发送到远程服务器\n\ngpg --keyserver hkp://keyserver.ubuntu.com:11371 --send-keys  3c8ae63c486358d2090f565928821a74321b8ccd\n\n\n\n\n\n# 七、配置项目\n\n要想将项目发送到 maven 中央仓库，需要对项目进行一些设置来符合发布条件。\n\n\n# 1、配置 strings.xml\n\n配置 maven 目录 conf 文件夹中的 settings.xml 文件，设置一个 server,里面添加 jira 的账号和密码。\n\n<servers>\n   <server>\n      <id>sona</id>\n      <username>jira账号</username>\n      <password>jira密码</password>\n   </server>\n</servers>\n\n\n\n# 2、配置 pom.xml\n\npom.xml 配置一些必要的信息，例如 maven 文档插件、打包插件、gpg验证插件等，如下：\n\n提示\n\n可以查看我的项目的 github 地址： maven 示例项目，pom 参数照着修改即可，顺便点下 start 星，thanks。\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n\n    <modelversion>4.0.0</modelversion>\n    <inceptionyear>2021</inceptionyear>\n    <groupid>com.ieooc.cloud</groupid>\n    <artifactid>simplify-cloud-dependencies-parent</artifactid>\n    <version>1.0.0.release</version>\n    <packaging>pom</packaging>\n\n    <name>simplify-cloud-dependencies-parent</name>\n    <description>simplify cloud build dependencies</description>\n    <url>https://www.ieooc.com/</url>\n\n    <organization>\n        <name>simplify software, inc.</name>\n        <url>https://www.ieooc.com</url>\n    </organization>\n\n    <licenses>\n        <license>\n            <name>apache license, version 2.0</name>\n            <url>https://www.apache.org/licenses/license-2.0</url>\n            <comments>copyright 2014-2015 the original author or authors.\n\n                licensed under the apache license, version 2.0 (the "license");\n                you may not use this file except in compliance with the license.\n                you may obtain a copy of the license at\n\n                https://www.apache.org/licenses/license-2.0\n\n                unless required by applicable law or agreed to in writing, software\n                distributed under the license is distributed on an "as is" basis,\n                without warranties or conditions of any kind, either express or\n                implied.\n\n                see the license for the specific language governing permissions and\n                limitations under the license.</comments>\n        </license>\n    </licenses>\n\n    <developers>\n        <developer>\n            <id>heyuqiang</id>\n            <name>heyuqiang</name>\n            <email>785056500@qq.com</email>\n            <organization>pivotal software, inc.</organization>\n            <organizationurl>https://www.ieooc.com</organizationurl>\n            <roles>\n                <role>project lead</role>\n            </roles>\n        </developer>\n    </developers>\n\n    <scm>\n        <connection>scm:git:git://github.com/simplify-cloud/simplify-cloud-dependencies-parent.git</connection>\n        <developerconnection>scm:git:ssh://git@github.com/simplify-cloud/simplify-cloud-dependencies-parent.git</developerconnection>\n        <url>https://github.com/simplify-cloud/simplify-cloud-dependencies-parent</url>\n    </scm>\n\n    <properties>\n        <project.build.sourceencoding>utf-8</project.build.sourceencoding>\n    </properties>\n\n    <build>\n        <pluginmanagement>\n            <plugins>\n                <plugin>\n                    <artifactid>maven-enforcer-plugin</artifactid>\n                    <version>1.4.1</version>\n                    <executions>\n                        <execution>\n                            <id>enforce-versions</id>\n                            <goals>\n                                <goal>enforce</goal>\n                            </goals>\n                        </execution>\n                    </executions>\n                    <configuration>\n                        <fail>false</fail>\n                        <rules>\n                            <dependencyconvergence />\n                        </rules>\n                    </configuration>\n                </plugin>\n            </plugins>\n        </pluginmanagement>\n    </build>\n    <profiles>\n        <profile>\n            <id>central</id>\n            <build>\n                <plugins>\n                    <plugin>\n                        <groupid>org.codehaus.mojo</groupid>\n                        <artifactid>flatten-maven-plugin</artifactid>\n                        <version>1.2.2</version>\n                        <executions>\n                            <execution>\n                                <id>flatten</id>\n                                <phase>process-resources</phase>\n                                <goals>\n                                    <goal>flatten</goal>\n                                </goals>\n                                <configuration>\n                                    <updatepomfile>true</updatepomfile>\n                                    <flattenmode>bom</flattenmode>\n                                    <pomelements>\n                                        <parent>expand</parent>\n                                        <pluginmanagement>keep</pluginmanagement>\n                                        <properties>keep</properties>\n                                        <repositories>remove</repositories>\n                                        <profiles>keep</profiles>\n                                    </pomelements>\n                                </configuration>\n                            </execution>\n                            <execution>\n                                <id>flatten-clean</id>\n                                <phase>clean</phase>\n                                <goals>\n                                    <goal>clean</goal>\n                                </goals>\n                            </execution>\n                        </executions>\n                        <inherited>true</inherited>\n                    </plugin>\n                    <plugin>\n                        <artifactid>maven-gpg-plugin</artifactid>\n                        <version>1.6</version>\n                        <executions>\n                            <execution>\n                                <id>sign-artifacts</id>\n                                <phase>verify</phase>\n                                <goals>\n                                    <goal>sign</goal>\n                                </goals>\n                            </execution>\n                        </executions>\n                    </plugin>\n                    \x3c!--staging puglin,用于自动执行发布阶段(免手动)--\x3e\n                    <plugin>\n                        <groupid>org.sonatype.plugins</groupid>\n                        <artifactid>nexus-staging-maven-plugin</artifactid>\n                        <version>1.6.7</version>\n                        <extensions>true</extensions>\n                        <configuration>\n                            <serverid>sona</serverid>\n                            <nexusurl>https://oss.sonatype.org/</nexusurl>\n                            <autoreleaseafterclose>true</autoreleaseafterclose>\n                        </configuration>\n                    </plugin>\n                    \x3c!-- release plugin,用于发布到release仓库部署插件 --\x3e\n                    <plugin>\n                        <groupid>org.apache.maven.plugins</groupid>\n                        <artifactid>maven-release-plugin</artifactid>\n                        <version>2.4.2</version>\n                    </plugin>\n                </plugins>\n            </build>\n            <distributionmanagement>\n                <repository>\n                    <id>sonatype-nexus-staging</id>\n                    <name>nexus release repository</name>\n                    <url>https://oss.sonatype.org/service/local/staging/deploy/maven2/</url>\n                </repository>\n                <snapshotrepository>\n                    <id>sonatype-nexus-snapshots</id>\n                    <name>sonatype nexus snapshots</name>\n                    <url>https://oss.sonatype.org/content/repositories/snapshots/</url>\n                </snapshotrepository>\n            </distributionmanagement>\n        </profile>\n    </profiles>\n</project>\n\n\n\n# 3、设置编码\n\n如果maven编译doc因编码原因出错，可以在环境变量设置变量 java_tool_options，值为 utf-8 来改变编码方式。\n\n * java_tool_options\n * -dfile.encoding=utf-8\n\n\n# 八、发送项目到 maven 中央仓库\n\n\n# 1、登录 nexus repositories manager\n\n在发布之前要求必须有登录 nexus repositories 的信息，所以发布前提前登录 nexus repositories\n\n登录 https://oss.sonatype.org 其中用户名、密码就是 jira 的用户名、密码。\n\n\n\n\n# 2、发布项目\n\n执行 maven 命令，部署项目到 maven 仓库，过程中需要输入之前设置的 gpg 密码，输入一下即可。\n\n$ mvn clean deploy -p central\n\n\n\n# 3、查看发布过程\n\n回到 nexus repositories manager，选择 staging repositories 暂存库，根据发布时间查找到你提交的 jar 信息。然后查看 actives 里面的执行信息，注意执行部署过程现在流行都交给 maven 自动部署插件执行，网上一些老办法是进入这里面手动执行，非常不推荐这样做，因为这样非常容易部署发生问题。\n\n\n\n提示\n\n在 maven 执行发布过程中才能看到，如果已经 release 完成，暂存库中可能找不到。\n\n正常的部署流程应该为：open阶段—>close阶段—>release阶段 执行完成这三个阶段就会发布到中央仓库，等会会受到邮件通知\n\n[ https://issues.sonatype.org/browse/ossrh-59440?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ] central ossrh 更新了 ossrh-59440: ------------------------------ central sync is activated for club.mydlq. after you successfully release, your component will be published to central, typically within 10 minutes, though updates to search.maven.org can take up to two hours.\n\n\n意思为 release 成功，大概等十分钟或者几个就能在中央仓库查询到该 jar 信息，不过对search.maven.org的更新可能需要两个小时。\n\n\n# 4、在 issues 中回复已经 release 完成\n\n打开之前提交 issues 地址，后面回复已经 release 完成\n\n\n# 5、查看自己发布的项目\n\n发布成功后可以进入 https://search.maven.org 网址查询自己发布的 jar，如果未查到等几个小时再此查询。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"安装 Docker Registry 私服",frontmatter:{title:"安装 Docker Registry 私服",date:"2022-02-09T11:14:29.000Z",permalink:"/pages/654ea2/"},regularPath:"/08.Registry/01.Registry/01.%E5%AE%89%E8%A3%85%20Docker%20Registry%20%E7%A7%81%E6%9C%8D.html",relativePath:"08.Registry/01.Registry/01.安装 Docker Registry 私服.md",key:"v-5f044039",path:"/pages/654ea2/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:28},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:2},{level:2,title:"验证",slug:"验证",normalizedTitle:"验证",charIndex:467}],headersStr:"概述 安装 验证",content:"# 安装 Docker Registry 私服\n\n\n# 概述\n\n官方的 Docker Hub 是一个用于管理公共镜像的地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候我们的服务器无法访问互联网，或者你不希望将自己的镜像放到公网当中，那么你就需要 Docker Registry，它可以用来存储和管理自己的镜像。\n\n\n# 安装\n\n在之前的 Docker 私有仓库 章节中已经提到过如何配置和使用容器运行私有仓库，这里我们使用 docker-compose 来安装，配置如下：\n\nversion: '3.7'\nservices:\n  registry:\n    image: registry\n    restart: always\n    container_name: registry\n    ports:\n      - 5000:5000\n    volumes:\n      - /usr/local/docker/registry/data:/var/lib/registry\n\n\n\n# 验证\n\n启动成功后需要测试服务端是否能够正常提供服务，有两种方式：\n\n * 浏览器端访问http://ivoov.com:5000/v2/\n * 返回{}表示Docker Registry 私服搭建成功。\n\n\n\n * 终端访问\n\ncurl http://ivoov.com:5000/v2/\n\n\n",normalizedContent:"# 安装 docker registry 私服\n\n\n# 概述\n\n官方的 docker hub 是一个用于管理公共镜像的地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候我们的服务器无法访问互联网，或者你不希望将自己的镜像放到公网当中，那么你就需要 docker registry，它可以用来存储和管理自己的镜像。\n\n\n# 安装\n\n在之前的 docker 私有仓库 章节中已经提到过如何配置和使用容器运行私有仓库，这里我们使用 docker-compose 来安装，配置如下：\n\nversion: '3.7'\nservices:\n  registry:\n    image: registry\n    restart: always\n    container_name: registry\n    ports:\n      - 5000:5000\n    volumes:\n      - /usr/local/docker/registry/data:/var/lib/registry\n\n\n\n# 验证\n\n启动成功后需要测试服务端是否能够正常提供服务，有两种方式：\n\n * 浏览器端访问http://ivoov.com:5000/v2/\n * 返回{}表示docker registry 私服搭建成功。\n\n\n\n * 终端访问\n\ncurl http://ivoov.com:5000/v2/\n\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"配置 Docker Registry 客户端",frontmatter:{title:"配置 Docker Registry 客户端",date:"2022-02-09T11:14:29.000Z",permalink:"/pages/87fa6f/"},regularPath:"/08.Registry/01.Registry/02.%E9%85%8D%E7%BD%AE%20Docker%20Registry%20%E5%AE%A2%E6%88%B7%E7%AB%AF.html",relativePath:"08.Registry/01.Registry/02.配置 Docker Registry 客户端.md",key:"v-310b44d6",path:"/pages/87fa6f/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:29},{level:2,title:"检查客户端配置是否生效",slug:"检查客户端配置是否生效",normalizedTitle:"检查客户端配置是否生效",charIndex:393},{level:2,title:"测试镜像上传",slug:"测试镜像上传",normalizedTitle:"测试镜像上传",charIndex:511},{level:2,title:"查看全部镜像",slug:"查看全部镜像",normalizedTitle:"查看全部镜像",charIndex:573},{level:2,title:"查看指定镜像",slug:"查看指定镜像",normalizedTitle:"查看指定镜像",charIndex:792},{level:2,title:"测试拉取镜像",slug:"测试拉取镜像",normalizedTitle:"测试拉取镜像",charIndex:878}],headersStr:"概述 检查客户端配置是否生效 测试镜像上传 查看全部镜像 查看指定镜像 测试拉取镜像",content:'# 配置 Docker Registry 客户端\n\n\n# 概述\n\n我们的教学案例使用的是 Ubuntu Server 18.04 LTS 版本，属于 systemd 系统，需要在 /etc/docker/daemon.json 中增加如下内容（如果文件不存在请新建该文件）\n\n\n\n\n\n\n \n \n \n\n\n\n{\n  "registry-mirrors": [\n    "https://registry.docker-cn.com"\n  ],\n  "insecure-registries": [\n    "ivoov.com:5000"\n  ]\n}\n\n\n提示\n\n该文件必须符合 json 规范，否则 Docker 将不能启动。\n\n之后重新启动服务。\n\n$ sudo systemctl daemon-reload\n$ sudo systemctl restart docker\n\n\n\n# 检查客户端配置是否生效\n\n使用 docker info 命令手动检查，如果从配置中看到如下内容，说明配置成功\n\n\n\n \n\n\n\nInsecure Registries:\n ivoov.com:5000\n 127.0.0.0/8\n\n\n\n# 测试镜像上传\n\n我们以 Nginx 为例测试镜像上传功能\n\n## 拉取一个镜像\ndocker pull nginx\n\n## 查看全部镜像\ndocker images\n\n## 标记本地镜像并指向目标仓库（ip:port/image_name:tag，该格式为标记版本号）\ndocker tag nginx ivoov.com:5000/nginx\n\n## 提交镜像到仓库\ndocker push ivoov.com:5000/nginx\n\n\n\n# 查看全部镜像\n\ncurl -XGET http://ivoov.com:5000/v2/_catalog\n\n\n\n# 查看指定镜像\n\n以 Nginx 为例，查看已提交的列表\n\ncurl -XGET http://ivoov.com:5000/v2/nginx/tags/list\n\n\n\n# 测试拉取镜像\n\n * 先删除镜像\n\ndocker rmi nginx\ndocker rmi ivoov.com:5000/nginx\n\n\n * 再拉取镜像\n\ndocker pull ivoov.com:5000/nginx\n',normalizedContent:'# 配置 docker registry 客户端\n\n\n# 概述\n\n我们的教学案例使用的是 ubuntu server 18.04 lts 版本，属于 systemd 系统，需要在 /etc/docker/daemon.json 中增加如下内容（如果文件不存在请新建该文件）\n\n\n\n\n\n\n \n \n \n\n\n\n{\n  "registry-mirrors": [\n    "https://registry.docker-cn.com"\n  ],\n  "insecure-registries": [\n    "ivoov.com:5000"\n  ]\n}\n\n\n提示\n\n该文件必须符合 json 规范，否则 docker 将不能启动。\n\n之后重新启动服务。\n\n$ sudo systemctl daemon-reload\n$ sudo systemctl restart docker\n\n\n\n# 检查客户端配置是否生效\n\n使用 docker info 命令手动检查，如果从配置中看到如下内容，说明配置成功\n\n\n\n \n\n\n\ninsecure registries:\n ivoov.com:5000\n 127.0.0.0/8\n\n\n\n# 测试镜像上传\n\n我们以 nginx 为例测试镜像上传功能\n\n## 拉取一个镜像\ndocker pull nginx\n\n## 查看全部镜像\ndocker images\n\n## 标记本地镜像并指向目标仓库（ip:port/image_name:tag，该格式为标记版本号）\ndocker tag nginx ivoov.com:5000/nginx\n\n## 提交镜像到仓库\ndocker push ivoov.com:5000/nginx\n\n\n\n# 查看全部镜像\n\ncurl -xget http://ivoov.com:5000/v2/_catalog\n\n\n\n# 查看指定镜像\n\n以 nginx 为例，查看已提交的列表\n\ncurl -xget http://ivoov.com:5000/v2/nginx/tags/list\n\n\n\n# 测试拉取镜像\n\n * 先删除镜像\n\ndocker rmi nginx\ndocker rmi ivoov.com:5000/nginx\n\n\n * 再拉取镜像\n\ndocker pull ivoov.com:5000/nginx\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:16:31",lastUpdatedTimestamp:1644376591e3},{title:"部署 Harbor 企业级Registry服务器",frontmatter:{title:"部署 Harbor 企业级Registry服务器",date:"2022-02-09T11:14:29.000Z",permalink:"/pages/22b02b/"},regularPath:"/08.Registry/01.Registry/03.%E9%83%A8%E7%BD%B2%20Harbor%20%E4%BC%81%E4%B8%9A%E7%BA%A7Registry%E6%9C%8D%E5%8A%A1%E5%99%A8.html",relativePath:"08.Registry/01.Registry/03.部署 Harbor 企业级Registry服务器.md",key:"v-0163bee7",path:"/pages/22b02b/",headers:[{level:2,title:"Harbor 简介",slug:"harbor-简介",normalizedTitle:"harbor 简介",charIndex:31},{level:2,title:"Harbor 特性",slug:"harbor-特性",normalizedTitle:"harbor 特性",charIndex:320},{level:2,title:"Harbor 组件",slug:"harbor-组件",normalizedTitle:"harbor 组件",charIndex:774},{level:2,title:"Harbor 实现",slug:"harbor-实现",normalizedTitle:"harbor 实现",charIndex:1552},{level:2,title:"安装 Docker 和 Docker-compose",slug:"安装-docker-和-docker-compose",normalizedTitle:"安装 docker 和 docker-compose",charIndex:3423},{level:3,title:"硬件要求",slug:"硬件要求",normalizedTitle:"硬件要求",charIndex:3494},{level:3,title:"软件要求",slug:"软件要求",normalizedTitle:"软件要求",charIndex:3614},{level:3,title:"网络端口",slug:"网络端口",normalizedTitle:"网络端口",charIndex:4071},{level:2,title:"安装 Harbor",slug:"安装-harbor",normalizedTitle:"安装 harbor",charIndex:4103},{level:2,title:"Harbor启动和停止",slug:"harbor启动和停止",normalizedTitle:"harbor启动和停止",charIndex:5830},{level:2,title:"测试 Harbor",slug:"测试-harbor",normalizedTitle:"测试 harbor",charIndex:6124},{level:2,title:"Harbor 基本使用",slug:"harbor-基本使用",normalizedTitle:"harbor 基本使用",charIndex:6219},{level:3,title:"创建用户",slug:"创建用户",normalizedTitle:"创建用户",charIndex:6235},{level:3,title:"docker私服配置",slug:"docker私服配置",normalizedTitle:"docker私服配置",charIndex:6316}],headersStr:"Harbor 简介 Harbor 特性 Harbor 组件 Harbor 实现 安装 Docker 和 Docker-compose 硬件要求 软件要求 网络端口 安装 Harbor Harbor启动和停止 测试 Harbor Harbor 基本使用 创建用户 docker私服配置",content:'# 部署 Harbor 企业级Registry服务器\n\n\n# Harbor 简介\n\n\n\nHarbor是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。\n\n\n# Harbor 特性\n\n * 基于角色的访问控制：用户与Docker镜像仓库通过“项目”进行组织管理，一个用户可以对多个镜像仓库在同一命名空间（project）里有不同的权限。\n\n * 镜像复制：镜像可以在多个Registry实例中复制（同步）。尤其适合于负载均衡，高可用，混合云和多云的场景。\n\n * 图形化用户界面：用户可以通过浏览器来浏览，检索当前Docker镜像仓库，管理项目和命名空间。\n\n * AD/LDAP 支持：Harbor可以集成企业内部已有的AD/LDAP，用于鉴权认证管理。\n\n * 审计管理：所有针对镜像仓库的操作都可以被记录追溯，用于审计管理。\n\n * 国际化：已拥有英文、中文、德文、日文和俄文的本地化版本。更多的语言将会添加进来。\n\n * RESTful API：RESTful API 提供给管理员对于Harbor更多的操控, 使得与其它管理软件集成变得更容易。\n\n * 部署简单： 提供在线和离线两种安装工具， 也可以安装到vSphere平台(OVA方式)虚拟设备。\n\n\n# Harbor 组件\n\nHarbor在架构上主要由6个组件构成：\n\n * Proxy：Harbor的registry, UI, token等服务，通过一个前置的反向代理统一接收浏览器、Docker客户端的请求，并将请求转发给后端不同的服务。\n\n * Registry： 负责储存Docker镜像，并处理docker push/pull 命令。由于我们要对用户进行访问控制，即不同用户对Docker image有不同的读写权限，Registry会指向一个token服务，强制用户的每次docker pull/push请求都要携带一个合法的token, Registry会通过公钥对token 进行解密验证。\n\n * Core services： 这是Harbor的核心功能，主要提供以下服务：\n\n * UI：提供图形化界面，帮助用户管理registry上的镜像（image）, 并对用户进行授权。\n\n * webhook：为了及时获取registry 上image状态变化的情况， 在Registry上配置webhook，把状态变化传递给UI模块。\n\n * token 服务：负责根据用户权限给每个docker push/pull命令签发token. Docker 客户端向Regiøstry服务发起的请求,如果不包含token，会被重定向到这里，获得token后再重新向Registry进行请求。\n\n * Database：为core services提供数据库服务，负责储存用户权限、审计日志、Docker image分组信息等数据。\n\n * Job Services：提供镜像远程复制功能，可以把本地镜像同步到其他Harbor实例中。\n\n * Log collector：为了帮助监控Harbor运行，负责收集其他组件的log，供日后进行分析。\n\n\n# Harbor 实现\n\nHarbor的每个组件都是以Docker容器的形式构建的，官方也是使用Docker Compose来对它进行部署。用于部署Harbor的Docker Compose模板位于 harbor/docker-compose.yml,打开这个模板文件，发现Harbor是由9个容器组成的；\n\ndocker-compose ps\n\n\n      Name                     Command                  State                 Ports          \n---------------------------------------------------------------------------------------------\nharbor-core         /harbor/harbor_core              Up (healthy)                            \nharbor-db           /docker-entrypoint.sh            Up (healthy)   5432/tcp                 \nharbor-jobservice   /harbor/harbor_jobservice  ...   Up (healthy)                            \nharbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514->10514/tcp\nharbor-portal       nginx -g daemon off;             Up (healthy)   8080/tcp                 \nnginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:80->8080/tcp     \nredis               redis-server /etc/redis.conf     Up (healthy)   6379/tcp                 \nregistry            /home/harbor/entrypoint.sh       Up (healthy)   5000/tcp                 \nregistryctl         /home/harbor/start.sh            Up (healthy)  \n\n\n * nginx：nginx负责流量转发和安全验证，对外提供的流量都是从nginx中转，所以开放https的443端口，它将流量分发到后端的ui和正在docker镜像存储的docker registry。\n * harbor-jobservice：harbor-jobservice 是harbor的job管理模块，job在harbor里面主要是为了镜像仓库之前同步使用的;\n * harbor-portal：harbor-portal是web管理页面，主要是前端的页面和后端CURD的接口;\n * registry：registry就是docker原生的仓库，负责保存镜像。\n * Harbor-ctl：harbor-ctlr是harbor系统管理接口，可以修改系统配置以及获取系统信息。 这几个容器通过Docker link的形式连接在一起，在容器之间通过容器名字互相访问。对终端用户而言，只需要暴露proxy （即Nginx）的服务端口。\n * harbor-db：harbor-db是harbor的数据库，这里保存了系统的job以及项目、人员权限管理。由于本harbor的认证也是通过数据，在生产环节大多对接到企业的ldap中；\n * harbor-log：harbor-log是harbor的日志服务，统一管理harbor的日志。通过inspect可以看出容器统一将日志输出的syslog。\n\n这几个容器通过Docker link的形式连接在一起，这样，在容器之间可以通过容器名字互相访问。对终端用户而言，只需要暴露proxy （即Nginx）的服务端口。\n\n\n# 安装 Docker 和 Docker-compose\n\n安装Harbor需要先安装 docker 和 docker-compose。\n\n\n# 硬件要求\n\n下表列出了部署Harbor的最低和建议的硬件配置。\n\n资源     最小配置    推荐配置\nCPU    2 CPU   4 CPU\nMem    4 GB    8 GB\nDisk   40 GB   160 GB\n\n\n# 软件要求\n\n下表列出了必须安装在目标主机上的软件版本。\n\n软件               版本                    描述\n操作系统             Ubuntu 20.04 LTS      或更高版本\nDocker engine    20.10.7               有关安装说明，请参阅 Docker Engine documentation\nDocker Compose   1.29.2                有关安装说明，请参阅 Docker Compose documentation\nOpenssl          Latest is preferred   用于生成Harbor证书和密钥\n安装方式             离线安装                  harbor-offline-installer-v2.3.0.tgz\n安装位置             /workspace/harbor     根据个人习惯自行配置\n\n\n# 网络端口\n\nHarbor要求在目标主机上打开以下端口。\n\n\n# 安装 Harbor\n\n安装方式分为在线安装和离线安装两种方式，我这里选择的是在线安装。访问 Harbor 的Github页面。\n\n注意\n\n以下安装过程请切换使用root用户进行操作\n\n在线安装\n\nwget -P /workspace https://github.com/goharbor/harbor/releases/download/v2.3.0/harbor-online-installer-v2.3.0.tgz\n\n\n离线安装\n\nwget -P /workspace https://github.com/goharbor/harbor/releases/download/v2.3.0/harbor-offline-installer-v2.3.0.tgz\n\n\ncd /workspace\ntar zxvf harbor-offline-installer-v2.3.0.tgz\ncd harbor\n\n\n复制配置文件模版\n\ncp harbor.yml.tmpl harbor.yml\n\n\n修改配置文件：harbor.yml，配置hostname、certificate、private_key。\n\nvim harbor.yml\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n# Configuration file of Harbor\n\n# The IP address or hostname to access admin UI and registry service.\n# DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients.\nhostname: harbor.ieooc.com\n\n# http related config\nhttp:\n  # port for http, default is 80. If https enabled, this port will redirect to https port\n  port: 80\n\n# https related config\nhttps:\n  # https port for harbor, default is 443\n  port: 443\n  # The path of cert and key files for nginx\n  certificate: /workspace/harbor/cert/ieooc.com_chain.crt\n  private_key: /workspace/harbor/cert/ieooc.com_key.key\n...\n\n\n提示\n\nhostname = 你的域名或IP，例如：harbor.ieooc.com 或者 192.168.199.110\n\n本地环境使用http时，注释掉https related config，否则需要为https配置证书\n\n执行安装脚本\n\n./install.sh\n\n\nharbor安装成功控制台日志输出如下：\n\nCreating network "harbor_harbor" with the default driver\nCreating harbor-log ... done\nCreating harbor-db     ... done\nCreating redis         ... done\nCreating registryctl   ... done\nCreating registry      ... done\nCreating harbor-portal ... done\nCreating harbor-core   ... done\nCreating nginx             ... done\nCreating harbor-jobservice ... done\n✔ ----Harbor has been installed and started successfully.----\n\n\n\n# Harbor启动和停止\n\nHarbor 的日常运维管理是通过docker-compose来完成的，Harbor本身有多个服务进程，都放在docker容器之中运行，我们可以通过docker ps命令查看。\n\ndocker ps\n\n\n或者docker-compose 来查看\n\ncd /workspace/harbor/\ndocker-compose ps\n\n\nHarbor的启动和停止\n\n启动Harbor\n# docker-compose start\n停止Harbor\n# docker-comose stop\n重启Harbor\n# docker-compose restart\n\n\n\n# 测试 Harbor\n\n使用你的域名或者IP直接访问Harbor\n\n\n\n\n\n提示\n\nHarbor 默认用户名：admin，密码：Harbor12345\n\n至此，Harbor安装完成。\n\n\n# Harbor 基本使用\n\n\n# 创建用户\n\n创建用户: 系统管理->用户管理->创建用户\n\n将新创建用户添加到项目: 项目->library>成员-> +用户 将创建好的用户添加进去。\n\n\n# docker私服配置\n\n注意\n\n如果harbor.yml中的port设置为80（http），则需要修改daemon.json，添加registry\n\nvi /etc/docker/daemon.json\n\n\n{\n    "insecure-registries": [\n       "192.168.199.110"\n    ]\n}\n\n\n提示\n\n如果port为443（https），则不需要修改daemon.json添加registry，必须配置cert，更多的信息可以参考官网。\n\n重启docker\n\nsystemctl restart docker\n\n\n登录registry\n\ndocker login 192.168.199.110\n\n\n为镜像打tag\n\ndocker tag SOURCE_IMAGE[:TAG] 192.168.199.110/library/IMAGE[:TAG]\n\n\n将镜像推送到harbor\n\ndocker push 192.168.199.110/library/IMAGE[:TAG]\n\n\n登出registry\n\ndocker logout 192.168.199.110\n\n\n拉取镜像\n\ndocker pull 192.168.199.110/library/IMAGE[:TAG]\n',normalizedContent:'# 部署 harbor 企业级registry服务器\n\n\n# harbor 简介\n\n\n\nharbor是一个用于存储和分发docker镜像的企业级registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源docker distribution。作为一个企业级私有registry服务器，harbor提供了更好的性能和安全。提升用户使用registry构建和运行环境传输镜像的效率。harbor支持安装在多个registry节点的镜像资源复制，镜像全部保存在私有registry中， 确保数据和知识产权在公司内部网络中管控。另外，harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。\n\n\n# harbor 特性\n\n * 基于角色的访问控制：用户与docker镜像仓库通过“项目”进行组织管理，一个用户可以对多个镜像仓库在同一命名空间（project）里有不同的权限。\n\n * 镜像复制：镜像可以在多个registry实例中复制（同步）。尤其适合于负载均衡，高可用，混合云和多云的场景。\n\n * 图形化用户界面：用户可以通过浏览器来浏览，检索当前docker镜像仓库，管理项目和命名空间。\n\n * ad/ldap 支持：harbor可以集成企业内部已有的ad/ldap，用于鉴权认证管理。\n\n * 审计管理：所有针对镜像仓库的操作都可以被记录追溯，用于审计管理。\n\n * 国际化：已拥有英文、中文、德文、日文和俄文的本地化版本。更多的语言将会添加进来。\n\n * restful api：restful api 提供给管理员对于harbor更多的操控, 使得与其它管理软件集成变得更容易。\n\n * 部署简单： 提供在线和离线两种安装工具， 也可以安装到vsphere平台(ova方式)虚拟设备。\n\n\n# harbor 组件\n\nharbor在架构上主要由6个组件构成：\n\n * proxy：harbor的registry, ui, token等服务，通过一个前置的反向代理统一接收浏览器、docker客户端的请求，并将请求转发给后端不同的服务。\n\n * registry： 负责储存docker镜像，并处理docker push/pull 命令。由于我们要对用户进行访问控制，即不同用户对docker image有不同的读写权限，registry会指向一个token服务，强制用户的每次docker pull/push请求都要携带一个合法的token, registry会通过公钥对token 进行解密验证。\n\n * core services： 这是harbor的核心功能，主要提供以下服务：\n\n * ui：提供图形化界面，帮助用户管理registry上的镜像（image）, 并对用户进行授权。\n\n * webhook：为了及时获取registry 上image状态变化的情况， 在registry上配置webhook，把状态变化传递给ui模块。\n\n * token 服务：负责根据用户权限给每个docker push/pull命令签发token. docker 客户端向regiøstry服务发起的请求,如果不包含token，会被重定向到这里，获得token后再重新向registry进行请求。\n\n * database：为core services提供数据库服务，负责储存用户权限、审计日志、docker image分组信息等数据。\n\n * job services：提供镜像远程复制功能，可以把本地镜像同步到其他harbor实例中。\n\n * log collector：为了帮助监控harbor运行，负责收集其他组件的log，供日后进行分析。\n\n\n# harbor 实现\n\nharbor的每个组件都是以docker容器的形式构建的，官方也是使用docker compose来对它进行部署。用于部署harbor的docker compose模板位于 harbor/docker-compose.yml,打开这个模板文件，发现harbor是由9个容器组成的；\n\ndocker-compose ps\n\n\n      name                     command                  state                 ports          \n---------------------------------------------------------------------------------------------\nharbor-core         /harbor/harbor_core              up (healthy)                            \nharbor-db           /docker-entrypoint.sh            up (healthy)   5432/tcp                 \nharbor-jobservice   /harbor/harbor_jobservice  ...   up (healthy)                            \nharbor-log          /bin/sh -c /usr/local/bin/ ...   up (healthy)   127.0.0.1:1514->10514/tcp\nharbor-portal       nginx -g daemon off;             up (healthy)   8080/tcp                 \nnginx               nginx -g daemon off;             up (healthy)   0.0.0.0:80->8080/tcp     \nredis               redis-server /etc/redis.conf     up (healthy)   6379/tcp                 \nregistry            /home/harbor/entrypoint.sh       up (healthy)   5000/tcp                 \nregistryctl         /home/harbor/start.sh            up (healthy)  \n\n\n * nginx：nginx负责流量转发和安全验证，对外提供的流量都是从nginx中转，所以开放https的443端口，它将流量分发到后端的ui和正在docker镜像存储的docker registry。\n * harbor-jobservice：harbor-jobservice 是harbor的job管理模块，job在harbor里面主要是为了镜像仓库之前同步使用的;\n * harbor-portal：harbor-portal是web管理页面，主要是前端的页面和后端curd的接口;\n * registry：registry就是docker原生的仓库，负责保存镜像。\n * harbor-ctl：harbor-ctlr是harbor系统管理接口，可以修改系统配置以及获取系统信息。 这几个容器通过docker link的形式连接在一起，在容器之间通过容器名字互相访问。对终端用户而言，只需要暴露proxy （即nginx）的服务端口。\n * harbor-db：harbor-db是harbor的数据库，这里保存了系统的job以及项目、人员权限管理。由于本harbor的认证也是通过数据，在生产环节大多对接到企业的ldap中；\n * harbor-log：harbor-log是harbor的日志服务，统一管理harbor的日志。通过inspect可以看出容器统一将日志输出的syslog。\n\n这几个容器通过docker link的形式连接在一起，这样，在容器之间可以通过容器名字互相访问。对终端用户而言，只需要暴露proxy （即nginx）的服务端口。\n\n\n# 安装 docker 和 docker-compose\n\n安装harbor需要先安装 docker 和 docker-compose。\n\n\n# 硬件要求\n\n下表列出了部署harbor的最低和建议的硬件配置。\n\n资源     最小配置    推荐配置\ncpu    2 cpu   4 cpu\nmem    4 gb    8 gb\ndisk   40 gb   160 gb\n\n\n# 软件要求\n\n下表列出了必须安装在目标主机上的软件版本。\n\n软件               版本                    描述\n操作系统             ubuntu 20.04 lts      或更高版本\ndocker engine    20.10.7               有关安装说明，请参阅 docker engine documentation\ndocker compose   1.29.2                有关安装说明，请参阅 docker compose documentation\nopenssl          latest is preferred   用于生成harbor证书和密钥\n安装方式             离线安装                  harbor-offline-installer-v2.3.0.tgz\n安装位置             /workspace/harbor     根据个人习惯自行配置\n\n\n# 网络端口\n\nharbor要求在目标主机上打开以下端口。\n\n\n# 安装 harbor\n\n安装方式分为在线安装和离线安装两种方式，我这里选择的是在线安装。访问 harbor 的github页面。\n\n注意\n\n以下安装过程请切换使用root用户进行操作\n\n在线安装\n\nwget -p /workspace https://github.com/goharbor/harbor/releases/download/v2.3.0/harbor-online-installer-v2.3.0.tgz\n\n\n离线安装\n\nwget -p /workspace https://github.com/goharbor/harbor/releases/download/v2.3.0/harbor-offline-installer-v2.3.0.tgz\n\n\ncd /workspace\ntar zxvf harbor-offline-installer-v2.3.0.tgz\ncd harbor\n\n\n复制配置文件模版\n\ncp harbor.yml.tmpl harbor.yml\n\n\n修改配置文件：harbor.yml，配置hostname、certificate、private_key。\n\nvim harbor.yml\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n# configuration file of harbor\n\n# the ip address or hostname to access admin ui and registry service.\n# do not use localhost or 127.0.0.1, because harbor needs to be accessed by external clients.\nhostname: harbor.ieooc.com\n\n# http related config\nhttp:\n  # port for http, default is 80. if https enabled, this port will redirect to https port\n  port: 80\n\n# https related config\nhttps:\n  # https port for harbor, default is 443\n  port: 443\n  # the path of cert and key files for nginx\n  certificate: /workspace/harbor/cert/ieooc.com_chain.crt\n  private_key: /workspace/harbor/cert/ieooc.com_key.key\n...\n\n\n提示\n\nhostname = 你的域名或ip，例如：harbor.ieooc.com 或者 192.168.199.110\n\n本地环境使用http时，注释掉https related config，否则需要为https配置证书\n\n执行安装脚本\n\n./install.sh\n\n\nharbor安装成功控制台日志输出如下：\n\ncreating network "harbor_harbor" with the default driver\ncreating harbor-log ... done\ncreating harbor-db     ... done\ncreating redis         ... done\ncreating registryctl   ... done\ncreating registry      ... done\ncreating harbor-portal ... done\ncreating harbor-core   ... done\ncreating nginx             ... done\ncreating harbor-jobservice ... done\n✔ ----harbor has been installed and started successfully.----\n\n\n\n# harbor启动和停止\n\nharbor 的日常运维管理是通过docker-compose来完成的，harbor本身有多个服务进程，都放在docker容器之中运行，我们可以通过docker ps命令查看。\n\ndocker ps\n\n\n或者docker-compose 来查看\n\ncd /workspace/harbor/\ndocker-compose ps\n\n\nharbor的启动和停止\n\n启动harbor\n# docker-compose start\n停止harbor\n# docker-comose stop\n重启harbor\n# docker-compose restart\n\n\n\n# 测试 harbor\n\n使用你的域名或者ip直接访问harbor\n\n\n\n\n\n提示\n\nharbor 默认用户名：admin，密码：harbor12345\n\n至此，harbor安装完成。\n\n\n# harbor 基本使用\n\n\n# 创建用户\n\n创建用户: 系统管理->用户管理->创建用户\n\n将新创建用户添加到项目: 项目->library>成员-> +用户 将创建好的用户添加进去。\n\n\n# docker私服配置\n\n注意\n\n如果harbor.yml中的port设置为80（http），则需要修改daemon.json，添加registry\n\nvi /etc/docker/daemon.json\n\n\n{\n    "insecure-registries": [\n       "192.168.199.110"\n    ]\n}\n\n\n提示\n\n如果port为443（https），则不需要修改daemon.json添加registry，必须配置cert，更多的信息可以参考官网。\n\n重启docker\n\nsystemctl restart docker\n\n\n登录registry\n\ndocker login 192.168.199.110\n\n\n为镜像打tag\n\ndocker tag source_image[:tag] 192.168.199.110/library/image[:tag]\n\n\n将镜像推送到harbor\n\ndocker push 192.168.199.110/library/image[:tag]\n\n\n登出registry\n\ndocker logout 192.168.199.110\n\n\n拉取镜像\n\ndocker pull 192.168.199.110/library/image[:tag]\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"部署 Docker 可视化管理工具Portainer",frontmatter:{title:"部署 Docker 可视化管理工具Portainer",date:"2022-02-09T11:14:29.000Z",permalink:"/pages/7ad507/"},regularPath:"/08.Registry/01.Registry/04.%E9%83%A8%E7%BD%B2%20Docker%20%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7Portainer.html",relativePath:"08.Registry/01.Registry/04.部署 Docker 可视化管理工具Portainer.md",key:"v-a4144134",path:"/pages/7ad507/",headers:[{level:2,title:"Portainer介绍",slug:"portainer介绍",normalizedTitle:"portainer介绍",charIndex:303},{level:2,title:"单机版运行",slug:"单机版运行",normalizedTitle:"单机版运行",charIndex:472},{level:2,title:"创建 docker-compose.yml",slug:"创建-docker-compose-yml",normalizedTitle:"创建 docker-compose.yml",charIndex:573},{level:2,title:"启动 Portainer",slug:"启动-portainer",normalizedTitle:"启动 portainer",charIndex:812},{level:2,title:"访问 Portainer",slug:"访问-portainer",normalizedTitle:"访问 portainer",charIndex:889},{level:2,title:"首页",slug:"首页",normalizedTitle:"首页",charIndex:1101},{level:2,title:"控制台",slug:"控制台",normalizedTitle:"控制台",charIndex:401},{level:2,title:"容器列表",slug:"容器列表",normalizedTitle:"容器列表",charIndex:1120},{level:2,title:"镜像列表",slug:"镜像列表",normalizedTitle:"镜像列表",charIndex:1180},{level:2,title:"仓库管理",slug:"仓库管理",normalizedTitle:"仓库管理",charIndex:1345},{level:2,title:"扩展：集群运行",slug:"扩展-集群运行",normalizedTitle:"扩展：集群运行",charIndex:1471}],headersStr:"Portainer介绍 单机版运行 创建 docker-compose.yml 启动 Portainer 访问 Portainer 首页 控制台 容器列表 镜像列表 仓库管理 扩展：集群运行",content:"# 部署 Docker 可视化管理工具Portainer\n\n私服安装成功后就可以使用 docker 命令行工具对 registry 做各种操作了。对于熟悉shell、技术开发人员而言，还是可以接受的，熟练之后，命令行毕竟是很方便的，便于操作及脚本化。但对于命令行过敏、非技术人员，进行docker部署、管理是比较头疼的，学习成本是很高的。而市面上的可视化管理工具也是很多的，各有优缺点，portainer功能完善，目前持续维护更新，最终我选择了它，作为Docker管理工具。\n\n提示\n\n对于docker初学者，不推荐使用docker可视化工具，还是先多熟悉熟悉命令行吧，这样便于理解docker。\n\n\n# Portainer介绍\n\nPortainer是Docker的图形化管理工具，提供状态显示面板、应用模板快速部署、容器镜像网络数据卷的基本操作（包括上传下载镜像，创建容器等操作）、事件日志显示、容器控制台操作、Swarm集群和服务等集中管理和操作、登录用户管理和控制等功能。功能十分全面，基本能满足中小型单位对容器管理的全部需求。\n\n\n# 单机版运行\n\n如果仅有一个docker宿主机，则可使用单机版运行，Portainer单机版运行十分简单，使用docker-compose启动容器，来管理该机器上的docker镜像、容器等数据。\n\n\n# 创建 docker-compose.yml\n\nversion: '3.7'\nservices:\n  portainer:\n    image: portainer/portainer\n    restart: always\n    container_name: portainer\n    ports:\n      - 9000:9000\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n\n\n\n# 启动 Portainer\n\ndocker-compose up -d\n\n\n执行完该命令之后，使用该机器IP:PORT即可访问Portainer。\n\n\n# 访问 Portainer\n\n浏览器访问地址：http://ivoov.com:9000\n\n首次登陆需要注册用户，给admin用户设置密码：\n\n\n\n单机版这里选择local即可，选择完毕，点击Connect即可连接到本地docker：\n\n\n\n注意\n\n该页面上有提示需要挂载本地 /var/run/docker.socker与容器内的/var/run/docker.socker连接。因此，在启动时必须指定该挂载文件。\n\n\n# 首页\n\n\n\n\n# 控制台\n\n\n\n\n# 容器列表\n\n\n\n点击容器列表中的容器名Name，即可查看容器详情：\n\n并且在容器详情页可以使用该容器创建镜像：\n\n\n# 镜像列表\n\n在镜像列表可以直接pull一个镜像，可以从远程pull，也可以从私有库中pull。从私有库中pull，需要将私有库的地址提前进行配置，这个在后面会说。\n\n点击镜像ID，即可查看镜像详情信息，在详情信息页面，除了镜像的一些信息外，还可以对该镜像进行打标签tag操作，然后将镜像push到远程仓库或者私有仓库中。\n\n\n# 仓库管理\n\n该界面可以查看配置的镜像仓库列表，同时可以添加仓库，添加成功之后，即可在image镜像页面进行pull、push操作。\n\n添加镜像仓库：\n\nPortainer中还有一些别的操作，比如权限管理、网络管理等等，可以安装上进行了解学习。\n\n\n# 扩展：集群运行\n\n更多的情况下，我们会有一个docker集群，可能有几台机器，也可能有几十台机器，因此，进行集群管理就十分重要了，Portainer也支持集群管理，Portainer可以和Swarm一起来进行集群管理操作。",normalizedContent:"# 部署 docker 可视化管理工具portainer\n\n私服安装成功后就可以使用 docker 命令行工具对 registry 做各种操作了。对于熟悉shell、技术开发人员而言，还是可以接受的，熟练之后，命令行毕竟是很方便的，便于操作及脚本化。但对于命令行过敏、非技术人员，进行docker部署、管理是比较头疼的，学习成本是很高的。而市面上的可视化管理工具也是很多的，各有优缺点，portainer功能完善，目前持续维护更新，最终我选择了它，作为docker管理工具。\n\n提示\n\n对于docker初学者，不推荐使用docker可视化工具，还是先多熟悉熟悉命令行吧，这样便于理解docker。\n\n\n# portainer介绍\n\nportainer是docker的图形化管理工具，提供状态显示面板、应用模板快速部署、容器镜像网络数据卷的基本操作（包括上传下载镜像，创建容器等操作）、事件日志显示、容器控制台操作、swarm集群和服务等集中管理和操作、登录用户管理和控制等功能。功能十分全面，基本能满足中小型单位对容器管理的全部需求。\n\n\n# 单机版运行\n\n如果仅有一个docker宿主机，则可使用单机版运行，portainer单机版运行十分简单，使用docker-compose启动容器，来管理该机器上的docker镜像、容器等数据。\n\n\n# 创建 docker-compose.yml\n\nversion: '3.7'\nservices:\n  portainer:\n    image: portainer/portainer\n    restart: always\n    container_name: portainer\n    ports:\n      - 9000:9000\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n\n\n\n# 启动 portainer\n\ndocker-compose up -d\n\n\n执行完该命令之后，使用该机器ip:port即可访问portainer。\n\n\n# 访问 portainer\n\n浏览器访问地址：http://ivoov.com:9000\n\n首次登陆需要注册用户，给admin用户设置密码：\n\n\n\n单机版这里选择local即可，选择完毕，点击connect即可连接到本地docker：\n\n\n\n注意\n\n该页面上有提示需要挂载本地 /var/run/docker.socker与容器内的/var/run/docker.socker连接。因此，在启动时必须指定该挂载文件。\n\n\n# 首页\n\n\n\n\n# 控制台\n\n\n\n\n# 容器列表\n\n\n\n点击容器列表中的容器名name，即可查看容器详情：\n\n并且在容器详情页可以使用该容器创建镜像：\n\n\n# 镜像列表\n\n在镜像列表可以直接pull一个镜像，可以从远程pull，也可以从私有库中pull。从私有库中pull，需要将私有库的地址提前进行配置，这个在后面会说。\n\n点击镜像id，即可查看镜像详情信息，在详情信息页面，除了镜像的一些信息外，还可以对该镜像进行打标签tag操作，然后将镜像push到远程仓库或者私有仓库中。\n\n\n# 仓库管理\n\n该界面可以查看配置的镜像仓库列表，同时可以添加仓库，添加成功之后，即可在image镜像页面进行pull、push操作。\n\n添加镜像仓库：\n\nportainer中还有一些别的操作，比如权限管理、网络管理等等，可以安装上进行了解学习。\n\n\n# 扩展：集群运行\n\n更多的情况下，我们会有一个docker集群，可能有几台机器，也可能有几十台机器，因此，进行集群管理就十分重要了，portainer也支持集群管理，portainer可以和swarm一起来进行集群管理操作。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Redis 简介",frontmatter:{title:"Redis 简介",date:"2022-02-09T11:23:08.000Z",permalink:"/pages/5b81e8/"},regularPath:"/09.Redis/01.Redis/01.Redis%20%E7%AE%80%E4%BB%8B.html",relativePath:"09.Redis/01.Redis/01.Redis 简介.md",key:"v-818a32e0",path:"/pages/5b81e8/",headers:[{level:2,title:"什么是redis",slug:"什么是redis",normalizedTitle:"什么是redis",charIndex:15},{level:2,title:"redis的应用场景",slug:"redis的应用场景",normalizedTitle:"redis的应用场景",charIndex:166}],headersStr:"什么是redis redis的应用场景",content:"# Redis 简介\n\n\n# 什么是redis\n\nredis是用C语言开发的一个开源的高性能键值对（key-value）数据库。它通过提供多种键值数据类型来适应不同场景下的存储需求，目前为止redis支持的键值数据类型如下字符串、列表（lists）、集合（sets）、有序集合（sorts sets）、哈希表（hashs）\n\n\n# redis的应用场景\n\n * 缓存（数据查询、短连接、新闻内容、商品内容等等）。（最多使用）\n * 分布式集群架构中的session分离。\n * 聊天室的在线好友列表。\n * 任务队列。（秒杀、抢购、12306等等）\n * 应用排行榜。\n * 网站访问统计。\n * 数据过期处理（可以精确到毫秒）",normalizedContent:"# redis 简介\n\n\n# 什么是redis\n\nredis是用c语言开发的一个开源的高性能键值对（key-value）数据库。它通过提供多种键值数据类型来适应不同场景下的存储需求，目前为止redis支持的键值数据类型如下字符串、列表（lists）、集合（sets）、有序集合（sorts sets）、哈希表（hashs）\n\n\n# redis的应用场景\n\n * 缓存（数据查询、短连接、新闻内容、商品内容等等）。（最多使用）\n * 分布式集群架构中的session分离。\n * 聊天室的在线好友列表。\n * 任务队列。（秒杀、抢购、12306等等）\n * 应用排行榜。\n * 网站访问统计。\n * 数据过期处理（可以精确到毫秒）",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"基于源码编译安装 Redis",frontmatter:{title:"基于源码编译安装 Redis",date:"2022-02-09T11:23:08.000Z",permalink:"/pages/3f32a2/"},regularPath:"/09.Redis/01.Redis/02.%E5%9F%BA%E4%BA%8E%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%20Redis.html",relativePath:"09.Redis/01.Redis/02.基于源码编译安装 Redis.md",key:"v-8ba9dd7a",path:"/pages/3f32a2/",headers:[{level:2,title:"依赖",slug:"依赖",normalizedTitle:"依赖",charIndex:21},{level:2,title:"安装Redis",slug:"安装redis",normalizedTitle:"安装redis",charIndex:70},{level:2,title:"启动Redis",slug:"启动redis",normalizedTitle:"启动redis",charIndex:1285},{level:2,title:"关闭Redis",slug:"关闭redis",normalizedTitle:"关闭redis",charIndex:3606},{level:2,title:"设置密码",slug:"设置密码",normalizedTitle:"设置密码",charIndex:3769},{level:2,title:"开机自启",slug:"开机自启",normalizedTitle:"开机自启",charIndex:5189}],headersStr:"依赖 安装Redis 启动Redis 关闭Redis 设置密码 开机自启",content:'# 基于源码编译安装 Redis\n\n\n# 依赖\n\n依赖      版本\n操作系统    centos7\nredis   5.0.0\n\n\n# 安装Redis\n\n下面介绍在CentOS环境下，Redis的安装与部署，Redis从3.0版本以后增加了集群功能。\n\n步骤如下：\n\n由于Redis是用C语言编写，所以编译时需要gcc，\n\nyum install gcc-c++\n\n\n通过官网下载 地址：http://download.redis.io/releases/redis-5.0.0.tar.gz\n\n或 使用linux wget命令：\n\nwget http://download.redis.io/releases/redis-5.0.0.tar.gz\n\n\n把源码包上传到linux服务器，在上传的目录下进行解压\n\ntar -zxvf redis-5.0.0.tar.gz\n\n\n进入解压后的目录进行编译make，指定目录安装make install 如 /usr/local/redis\n\ncd redis-5.0.0/\nmake (这里进redis-5.0.0/目录下直接make编译就好了)\nmake install PREFIX=/usr/local/redis  （指定编译路径）\n\n\n进入安装目录bin下\n\ncd /usr/local/redis/bin\n\n\n[root@localhost bin]# ls -al\ntotal 32628\ndrwxr-xr-x 2 root root     134 Nov 24 12:05 .\ndrwxr-xr-x 3 root root      17 Nov 24 12:05 ..\n-rwxr-xr-x 1 root root 4366600 Nov 24 12:05 redis-benchmark\n-rwxr-xr-x 1 root root 8082744 Nov 24 12:05 redis-check-aof\n-rwxr-xr-x 1 root root 8082744 Nov 24 12:05 redis-check-rdb\n-rwxr-xr-x 1 root root 4783504 Nov 24 12:05 redis-cli\nlrwxrwxrwx 1 root root      12 Nov 24 12:05 redis-sentinel -> redis-server\n-rwxr-xr-x 1 root root 8082744 Nov 24 12:05 redis-server\n\n\n * redis-benchmark redis性能测试工具\n * redis-check-aof AOF文件修复工具\n * redis-check-rdb RDB文件修复工具\n * redis-cli redis命令行客户端\n * redis.conf redis配置文件\n * redis-sentinal redis集群管理工具\n * redis-server redis服务进程\n\n\n# 启动Redis\n\n后端模式启动\n\n1）从redis的源码目录中复制redis.conf到redis的安装目录bin下。\n\ncp /root/redis-5.0.0/redis.conf /usr/local/redis/bin\n\n\n[root@localhost redis-5.0.0]# ls -al\ntotal 240\ndrwxrwxr-x   6 root root   309 Oct 17  2018 .\ndr-xr-x---.  6 root root  4096 Nov 24 12:04 ..\n-rw-rw-r--   1 root root 75104 Oct 17  2018 00-RELEASENOTES\n-rw-rw-r--   1 root root    53 Oct 17  2018 BUGS\n-rw-rw-r--   1 root root  1894 Oct 17  2018 CONTRIBUTING\n-rw-rw-r--   1 root root  1487 Oct 17  2018 COPYING\ndrwxrwxr-x   6 root root   192 Nov 24 12:04 deps\n-rw-rw-r--   1 root root   376 Oct 17  2018 .gitignore\n-rw-rw-r--   1 root root    11 Oct 17  2018 INSTALL\n-rw-rw-r--   1 root root   151 Oct 17  2018 Makefile\n-rw-rw-r--   1 root root  4223 Oct 17  2018 MANIFESTO\n-rw-rw-r--   1 root root 20555 Oct 17  2018 01.Redis 简介.md\n-rw-rw-r--   1 root root 62155 Oct 17  2018 redis.conf\n-rwxrwxr-x   1 root root   275 Oct 17  2018 runtest\n-rwxrwxr-x   1 root root   280 Oct 17  2018 runtest-cluster\n-rwxrwxr-x   1 root root   281 Oct 17  2018 runtest-sentinel\n-rw-rw-r--   1 root root  9710 Oct 17  2018 sentinel.conf\ndrwxrwxr-x   3 root root  8192 Nov 24 12:05 src\ndrwxrwxr-x  10 root root   167 Oct 17  2018 tests\ndrwxrwxr-x   8 root root  4096 Oct 17  2018 utils\n[root@localhost redis-5.0.0]# cd /usr/local/redis/bin/\n[root@localhost bin]# ls -al\ntotal 32692\ndrwxr-xr-x 2 root root     152 Nov 24 12:07 .\ndrwxr-xr-x 3 root root      17 Nov 24 12:05 ..\n-rwxr-xr-x 1 root root 4366600 Nov 24 12:05 redis-benchmark\n-rwxr-xr-x 1 root root 8082744 Nov 24 12:05 redis-check-aof\n-rwxr-xr-x 1 root root 8082744 Nov 24 12:05 redis-check-rdb\n-rwxr-xr-x 1 root root 4783504 Nov 24 12:05 redis-cli\n-rw-r--r-- 1 root root   62155 Nov 24 12:07 redis.conf\nlrwxrwxrwx 1 root root      12 Nov 24 12:05 redis-sentinel -> redis-server\n-rwxr-xr-x 1 root root 8082744 Nov 24 12:05 redis-server\n\n\n2）修改配置文件 （是否后台启动）\n\nvim /usr/local/redis/bin/redis.conf\n\n\n找到 daemonize 按i 进入编辑模式 把no 改为 yes\n\n按ESC + :wq 保存退出\n\n执行如下命令启动redis：\n\ncd /usr/local/redis/bin\n./redis-server ./redis.conf\n\n\n3.查看是否启动成功\n\nps aux|grep redis\n\n\n[root@localhost bin]# ps aux|grep redis\nroot     13853  0.0  0.1 153980  7636 ?        Ssl  12:10   0:00 ./redis-server 127.0.0.1:6379\nroot     13858  0.0  0.0 112808   968 pts/0    S+   12:10   0:00 grep --color=auto redis\n\n\nredis默认端口为 6379，可更改redis.conf文件，修改端口号\n\n\n# 关闭Redis\n\n强行终止redis进程可能会导致redis持久化数据丢失。\n\n正确停止Redis的方式应该是向Redis发送SHUTDOWN命令，\n\n命令为：\n\ncd /usr/local/redis\n./bin/redis-cli shutdown\n\n\n强行终止redis\npkill redis-server\n\n\n\n# 设置密码\n\nvi /usr/local/redis/bin/redis.conf\n\n\n找到# requirepass foobared去掉注释将foobared替换成需要配置的密码即可。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n################################## SECURITY ###################################\n\n# Require clients to issue AUTH <PASSWORD> before processing any other\n# commands.  This might be useful in environments in which you do not trust\n# others with access to the host running redis-server.\n#\n# This should stay commented out for backward compatibility and because most\n# people do not need auth (e.g. they run their own servers).\n#\n# Warning: since Redis is pretty fast an outside user can try up to\n# 150k passwords per second against a good box. This means that you should\n# use a very strong password otherwise it will be very easy to break.\n#\n# requirepass foobared\n\n# Command renaming.\n#\n# It is possible to change the name of dangerous commands in a shared\n# environment. For instance the CONFIG command may be renamed into something\n# hard to guess so that it will still be available for internal-use tools\n# but not available for general clients.\n#\n# Example:\n#\n# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52\n#\n# It is also possible to completely kill a command by renaming it into\n# an empty string:\n#\n# rename-command CONFIG ""\n#\n# Please note that changing the name of commands that are logged into the\n# AOF file or transmitted to replicas may cause problems.\n\n\n\n# 开机自启\n\ncentos 7以上是用Systemd进行系统初始化的，Systemd 是 Linux 系统中最新的初始化系统（init），它主要的设计目标是克服 sysvinit 固有的缺点，提高系统的启动速度。 Systemd服务文件以.service结尾，比如现在要建立redis为开机启动，如果用yum install命令安装的，yum命令会自动创建redis.service文件，直接用命令systemcel enable redis.service设置开机启动即可\n\n在系统服务目录里创建redis.service文件\n\nvim /etc/systemd/system/redis.service\n\n\n写入以下内容：\n\n[Unit]\nDescription=redis-server\nAfter=network.target\n\n[Service]\nType=forking\nExecStart=/usr/local/redis/bin/redis-server /usr/local/redis/bin/redis.conf\nPrivateTmp=true\n\n[Install]\nWantedBy=multi-user.target\n\n\n注意：ExecStart配置成自己的路径\n\n配置描述：\n\n * Description:描述服务\n * After:描述服务类别\n * [Service]服务运行参数的设置\n * Type=forking是后台运行的形式\n * ExecStart为服务的具体运行命令\n * ExecReload为重启命令\n * ExecStop为停止命令\n * PrivateTmp=True表示给服务分配独立的临时空间\n * 注意：[Service]的启动、重启、停止命令全部要求使用绝对路径\n * [Install]运行级别下服务安装的相关设置，可设置为多用户，即系统运行级别为3\n\n重载系统服务：\n\nsystemctl daemon-reload\n\n\n测试并加入开机自启\n\n先关闭redis-server\n\nsystemctl stop redis.service\n\n\n开启redis-server\n\nsystemctl start redis.service\t\n\n\n> 如果服务是开启状态，使用此命令会启动失败。\n\n开启成功，将服务加入开机自启\n\nsystemctl enable redis.service\n\n\n> 注意后面不能跟空格\n\nreboot #重启\n\n查看服务运行状态：\n\nsystemctl status redis.service\n\n\n全部命令\n\nsystemctl start redis.service #启动redis服务\nsystemctl enable redis.service #设置开机自启动 \nsystemctl disable redis.service #停止开机自启动 \nsystemctl status redis.service #查看服务当前状态 \nsystemctl restart redis.service　 #重新启动服务 \nsystemctl list-units --type=service #查看所有已启动的服务\n\n\n至此redis安装配置完毕。\n\ngood luck !',normalizedContent:'# 基于源码编译安装 redis\n\n\n# 依赖\n\n依赖      版本\n操作系统    centos7\nredis   5.0.0\n\n\n# 安装redis\n\n下面介绍在centos环境下，redis的安装与部署，redis从3.0版本以后增加了集群功能。\n\n步骤如下：\n\n由于redis是用c语言编写，所以编译时需要gcc，\n\nyum install gcc-c++\n\n\n通过官网下载 地址：http://download.redis.io/releases/redis-5.0.0.tar.gz\n\n或 使用linux wget命令：\n\nwget http://download.redis.io/releases/redis-5.0.0.tar.gz\n\n\n把源码包上传到linux服务器，在上传的目录下进行解压\n\ntar -zxvf redis-5.0.0.tar.gz\n\n\n进入解压后的目录进行编译make，指定目录安装make install 如 /usr/local/redis\n\ncd redis-5.0.0/\nmake (这里进redis-5.0.0/目录下直接make编译就好了)\nmake install prefix=/usr/local/redis  （指定编译路径）\n\n\n进入安装目录bin下\n\ncd /usr/local/redis/bin\n\n\n[root@localhost bin]# ls -al\ntotal 32628\ndrwxr-xr-x 2 root root     134 nov 24 12:05 .\ndrwxr-xr-x 3 root root      17 nov 24 12:05 ..\n-rwxr-xr-x 1 root root 4366600 nov 24 12:05 redis-benchmark\n-rwxr-xr-x 1 root root 8082744 nov 24 12:05 redis-check-aof\n-rwxr-xr-x 1 root root 8082744 nov 24 12:05 redis-check-rdb\n-rwxr-xr-x 1 root root 4783504 nov 24 12:05 redis-cli\nlrwxrwxrwx 1 root root      12 nov 24 12:05 redis-sentinel -> redis-server\n-rwxr-xr-x 1 root root 8082744 nov 24 12:05 redis-server\n\n\n * redis-benchmark redis性能测试工具\n * redis-check-aof aof文件修复工具\n * redis-check-rdb rdb文件修复工具\n * redis-cli redis命令行客户端\n * redis.conf redis配置文件\n * redis-sentinal redis集群管理工具\n * redis-server redis服务进程\n\n\n# 启动redis\n\n后端模式启动\n\n1）从redis的源码目录中复制redis.conf到redis的安装目录bin下。\n\ncp /root/redis-5.0.0/redis.conf /usr/local/redis/bin\n\n\n[root@localhost redis-5.0.0]# ls -al\ntotal 240\ndrwxrwxr-x   6 root root   309 oct 17  2018 .\ndr-xr-x---.  6 root root  4096 nov 24 12:04 ..\n-rw-rw-r--   1 root root 75104 oct 17  2018 00-releasenotes\n-rw-rw-r--   1 root root    53 oct 17  2018 bugs\n-rw-rw-r--   1 root root  1894 oct 17  2018 contributing\n-rw-rw-r--   1 root root  1487 oct 17  2018 copying\ndrwxrwxr-x   6 root root   192 nov 24 12:04 deps\n-rw-rw-r--   1 root root   376 oct 17  2018 .gitignore\n-rw-rw-r--   1 root root    11 oct 17  2018 install\n-rw-rw-r--   1 root root   151 oct 17  2018 makefile\n-rw-rw-r--   1 root root  4223 oct 17  2018 manifesto\n-rw-rw-r--   1 root root 20555 oct 17  2018 01.redis 简介.md\n-rw-rw-r--   1 root root 62155 oct 17  2018 redis.conf\n-rwxrwxr-x   1 root root   275 oct 17  2018 runtest\n-rwxrwxr-x   1 root root   280 oct 17  2018 runtest-cluster\n-rwxrwxr-x   1 root root   281 oct 17  2018 runtest-sentinel\n-rw-rw-r--   1 root root  9710 oct 17  2018 sentinel.conf\ndrwxrwxr-x   3 root root  8192 nov 24 12:05 src\ndrwxrwxr-x  10 root root   167 oct 17  2018 tests\ndrwxrwxr-x   8 root root  4096 oct 17  2018 utils\n[root@localhost redis-5.0.0]# cd /usr/local/redis/bin/\n[root@localhost bin]# ls -al\ntotal 32692\ndrwxr-xr-x 2 root root     152 nov 24 12:07 .\ndrwxr-xr-x 3 root root      17 nov 24 12:05 ..\n-rwxr-xr-x 1 root root 4366600 nov 24 12:05 redis-benchmark\n-rwxr-xr-x 1 root root 8082744 nov 24 12:05 redis-check-aof\n-rwxr-xr-x 1 root root 8082744 nov 24 12:05 redis-check-rdb\n-rwxr-xr-x 1 root root 4783504 nov 24 12:05 redis-cli\n-rw-r--r-- 1 root root   62155 nov 24 12:07 redis.conf\nlrwxrwxrwx 1 root root      12 nov 24 12:05 redis-sentinel -> redis-server\n-rwxr-xr-x 1 root root 8082744 nov 24 12:05 redis-server\n\n\n2）修改配置文件 （是否后台启动）\n\nvim /usr/local/redis/bin/redis.conf\n\n\n找到 daemonize 按i 进入编辑模式 把no 改为 yes\n\n按esc + :wq 保存退出\n\n执行如下命令启动redis：\n\ncd /usr/local/redis/bin\n./redis-server ./redis.conf\n\n\n3.查看是否启动成功\n\nps aux|grep redis\n\n\n[root@localhost bin]# ps aux|grep redis\nroot     13853  0.0  0.1 153980  7636 ?        ssl  12:10   0:00 ./redis-server 127.0.0.1:6379\nroot     13858  0.0  0.0 112808   968 pts/0    s+   12:10   0:00 grep --color=auto redis\n\n\nredis默认端口为 6379，可更改redis.conf文件，修改端口号\n\n\n# 关闭redis\n\n强行终止redis进程可能会导致redis持久化数据丢失。\n\n正确停止redis的方式应该是向redis发送shutdown命令，\n\n命令为：\n\ncd /usr/local/redis\n./bin/redis-cli shutdown\n\n\n强行终止redis\npkill redis-server\n\n\n\n# 设置密码\n\nvi /usr/local/redis/bin/redis.conf\n\n\n找到# requirepass foobared去掉注释将foobared替换成需要配置的密码即可。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n################################## security ###################################\n\n# require clients to issue auth <password> before processing any other\n# commands.  this might be useful in environments in which you do not trust\n# others with access to the host running redis-server.\n#\n# this should stay commented out for backward compatibility and because most\n# people do not need auth (e.g. they run their own servers).\n#\n# warning: since redis is pretty fast an outside user can try up to\n# 150k passwords per second against a good box. this means that you should\n# use a very strong password otherwise it will be very easy to break.\n#\n# requirepass foobared\n\n# command renaming.\n#\n# it is possible to change the name of dangerous commands in a shared\n# environment. for instance the config command may be renamed into something\n# hard to guess so that it will still be available for internal-use tools\n# but not available for general clients.\n#\n# example:\n#\n# rename-command config b840fc02d524045429941cc15f59e41cb7be6c52\n#\n# it is also possible to completely kill a command by renaming it into\n# an empty string:\n#\n# rename-command config ""\n#\n# please note that changing the name of commands that are logged into the\n# aof file or transmitted to replicas may cause problems.\n\n\n\n# 开机自启\n\ncentos 7以上是用systemd进行系统初始化的，systemd 是 linux 系统中最新的初始化系统（init），它主要的设计目标是克服 sysvinit 固有的缺点，提高系统的启动速度。 systemd服务文件以.service结尾，比如现在要建立redis为开机启动，如果用yum install命令安装的，yum命令会自动创建redis.service文件，直接用命令systemcel enable redis.service设置开机启动即可\n\n在系统服务目录里创建redis.service文件\n\nvim /etc/systemd/system/redis.service\n\n\n写入以下内容：\n\n[unit]\ndescription=redis-server\nafter=network.target\n\n[service]\ntype=forking\nexecstart=/usr/local/redis/bin/redis-server /usr/local/redis/bin/redis.conf\nprivatetmp=true\n\n[install]\nwantedby=multi-user.target\n\n\n注意：execstart配置成自己的路径\n\n配置描述：\n\n * description:描述服务\n * after:描述服务类别\n * [service]服务运行参数的设置\n * type=forking是后台运行的形式\n * execstart为服务的具体运行命令\n * execreload为重启命令\n * execstop为停止命令\n * privatetmp=true表示给服务分配独立的临时空间\n * 注意：[service]的启动、重启、停止命令全部要求使用绝对路径\n * [install]运行级别下服务安装的相关设置，可设置为多用户，即系统运行级别为3\n\n重载系统服务：\n\nsystemctl daemon-reload\n\n\n测试并加入开机自启\n\n先关闭redis-server\n\nsystemctl stop redis.service\n\n\n开启redis-server\n\nsystemctl start redis.service\t\n\n\n> 如果服务是开启状态，使用此命令会启动失败。\n\n开启成功，将服务加入开机自启\n\nsystemctl enable redis.service\n\n\n> 注意后面不能跟空格\n\nreboot #重启\n\n查看服务运行状态：\n\nsystemctl status redis.service\n\n\n全部命令\n\nsystemctl start redis.service #启动redis服务\nsystemctl enable redis.service #设置开机自启动 \nsystemctl disable redis.service #停止开机自启动 \nsystemctl status redis.service #查看服务当前状态 \nsystemctl restart redis.service　 #重新启动服务 \nsystemctl list-units --type=service #查看所有已启动的服务\n\n\n至此redis安装配置完毕。\n\ngood luck !',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"基于 Docker 安装 Redis",frontmatter:{title:"基于 Docker 安装 Redis",date:"2022-02-09T11:23:08.000Z",permalink:"/pages/7a3c10/"},regularPath:"/09.Redis/01.Redis/03.%E5%9F%BA%E4%BA%8E%20Docker%20%E5%AE%89%E8%A3%85%20Redis.html",relativePath:"09.Redis/01.Redis/03.基于 Docker 安装 Redis.md",key:"v-8cb52cfc",path:"/pages/7a3c10/",headers:[{level:2,title:"使用 docker-compose 安装 Redis",slug:"使用-docker-compose-安装-redis",normalizedTitle:"使用 docker-compose 安装 redis",charIndex:25}],headersStr:"使用 docker-compose 安装 Redis",content:"# 基于 Docker 安装 Redis\n\n\n# 使用 docker-compose 安装 Redis\n\n使用 Docker Compose 您可以轻松地配置、安装和升级基于 Docker 的 Redis 安装。\n\n1.安装 Docker Compose\n\n2.创建docker-compose.yml文件。\n\nversion: '3'\n\nservices:\n  redis:\n    image: redis\n    container_name: redis\n    hostname: redis\n    restart: always\n    ports:\n      - 6379:6379\n    networks:\n      - net_db\n    volumes:\n      - ./conf/redis.conf:/etc/redis/redis.conf:rw\n      - ./data:/data:rw\n    command:\n      redis-server /etc/redis/redis.conf --appendonly yes\n\nnetworks:\n  net_db:\n    driver: bridge\n\n\n2.创建redis.conf文件。\n\nrequirepass 123456\nbind 0.0.0.0\n#daemonize yes\nappendonly yes\n",normalizedContent:"# 基于 docker 安装 redis\n\n\n# 使用 docker-compose 安装 redis\n\n使用 docker compose 您可以轻松地配置、安装和升级基于 docker 的 redis 安装。\n\n1.安装 docker compose\n\n2.创建docker-compose.yml文件。\n\nversion: '3'\n\nservices:\n  redis:\n    image: redis\n    container_name: redis\n    hostname: redis\n    restart: always\n    ports:\n      - 6379:6379\n    networks:\n      - net_db\n    volumes:\n      - ./conf/redis.conf:/etc/redis/redis.conf:rw\n      - ./data:/data:rw\n    command:\n      redis-server /etc/redis/redis.conf --appendonly yes\n\nnetworks:\n  net_db:\n    driver: bridge\n\n\n2.创建redis.conf文件。\n\nrequirepass 123456\nbind 0.0.0.0\n#daemonize yes\nappendonly yes\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"什么是 Nginx",frontmatter:{title:"什么是 Nginx",date:"2022-02-09T11:24:38.000Z",permalink:"/pages/8c9288/"},regularPath:"/10.Nginx/01.Nginx/01.%E4%BB%80%E4%B9%88%E6%98%AF%20Nginx.html",relativePath:"10.Nginx/01.Nginx/01.什么是 Nginx.md",key:"v-48788de2",path:"/pages/8c9288/",headersStr:null,content:'# 什么是 Nginx\n\nNginx("engine x")是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。\n\n在高连接并发的情况下，Nginx是Apache服务器不错的替代品。',normalizedContent:'# 什么是 nginx\n\nnginx("engine x")是一款是由俄罗斯的程序设计师igor sysoev所开发高性能的 web和 反向代理 服务器，也是一个 imap/pop3/smtp 代理服务器。\n\n在高连接并发的情况下，nginx是apache服务器不错的替代品。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Nginx 安装SSL证书",frontmatter:{title:"Nginx 安装SSL证书",date:"2022-02-09T11:24:38.000Z",permalink:"/pages/df904c/"},regularPath:"/10.Nginx/01.Nginx/02.Nginx%20%E5%AE%89%E8%A3%85SSL%E8%AF%81%E4%B9%A6.html",relativePath:"10.Nginx/01.Nginx/02.Nginx 安装SSL证书.md",key:"v-4c9ef69c",path:"/pages/df904c/",headers:[{level:2,title:"背景信息",slug:"背景信息",normalizedTitle:"背景信息",charIndex:74},{level:2,title:"操作步骤",slug:"操作步骤",normalizedTitle:"操作步骤",charIndex:426},{level:2,title:"设置HTTP请求自动跳转HTTPS",slug:"设置http请求自动跳转https",normalizedTitle:"设置http请求自动跳转https",charIndex:1711},{level:2,title:"启用HSTS",slug:"启用hsts",normalizedTitle:"启用hsts",charIndex:2102},{level:3,title:"HSTS简介",slug:"hsts简介",normalizedTitle:"hsts简介",charIndex:2289},{level:3,title:"HSTS部署",slug:"hsts部署",normalizedTitle:"hsts部署",charIndex:2722},{level:2,title:"完整示例配置文件",slug:"完整示例配置文件",normalizedTitle:"完整示例配置文件",charIndex:3709},{level:2,title:"完整示例配置文件",slug:"完整示例配置文件-2",normalizedTitle:"完整示例配置文件",charIndex:3709},{level:2,title:"反向代理配置",slug:"反向代理配置",normalizedTitle:"反向代理配置",charIndex:7357}],headersStr:"背景信息 操作步骤 设置HTTP请求自动跳转HTTPS 启用HSTS HSTS简介 HSTS部署 完整示例配置文件 完整示例配置文件 反向代理配置",content:'# Nginx 安装SSL证书\n\n阿里云SSL证书服务支持下载证书安装到Nginx/Tengine服务器上，本文介绍了证书安装的具体操作。\n\n\n# 背景信息\n\n * 本文档以CentOS 7、Nginx 1.16.1为例。\n * 本文档证书名称以www.ivoov.com为示例，如证书文件名称为www.ivoov.com.pem，证书密钥文件名称为www.ivoov.com.key。\n * 下载的Nginx证书压缩文件解压后包含：\n   * .pem：证书文件。PEM文件的扩展名为CRT格式。\n   * .key：证书的密钥文件。申请证书时如果未选择自动创建CRS，则下载的证书文件压缩包中不会包含.key文件，需要您将自己手动创建的密钥文件拷贝到cert目录下。\n\n提示\n\n.pem扩展名的证书文件采用Base64-encoded的PEM格式文本文件，您可根据需要修改成其他扩展名。 证书格式详细内容，请参见主流数字证书都有哪些格式？\n\n\n# 操作步骤\n\n 1. 登录阿里云SSL证书控制台。\n\n 2. 在SSL证书页面，单击已签发标签，定位到需要下载的证书并单击证书卡片右下角的下载。\n\n 3. 在证书下载侧页面中定位到Nginx服务器，并单击右侧操作栏的下载，将Nginx服务器证书压缩包下载到本地。\n\n 4. 解压已下载保存到本地的Nginx证书压缩包文件。\n    \n    解压后的文件夹中有2个文件：\n    \n    * 证书文件：以.pem为后缀或文件类型。\n    * 密钥文件：以.key为后缀或文件类型。\n\n 5. 登录您的Nginx服务器，在Nginx安装目录（默认Nginx安装目录为/usr/local/nginx/conf）下创建cert目录，并将下载的证书文件和密钥文件拷贝到cert目录中。\n\n提示\n\n如果您在申请证书时选择手动创建CSR文件，请将对应的密钥文件放到cert目录中，并命名为www.ivoov.com.key。\n\n 6. 修改Nginx安装目录/conf/nginx.conf文件。\n    \n    按照下文中注释内容修改nginx.conf文件：\n\n# HTTPS server\nserver {\n    listen 443 ssl;   #SSL协议访问端口号为443。此处如未添加ssl，可能会造成Nginx无法启动。\n    server_name ivoov.com www.ivoov.com;  #将localhost修改为您证书绑定的域名，例如：www.ivoov.com。\n\n    root html;\n    index index.html index.htm;\n    \n    ssl_certificate cert/www.ivoov.com.pem;   #将www.ivoov.com.pem替换成您证书的文件名。\n    ssl_certificate_key cert/www.ivoov.com.key;   #将www.ivoov.com.key替换成您证书的密钥文件名。\n    ssl_session_timeout 5m;\n    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;  #使用此加密套件。\n    ssl_protocols TLSv1.1 TLSv1.2 TLSv1.3;   #使用该协议进行配置。\n    ssl_prefer_server_ciphers on;   \n    location / {\n        root /usr/share/nginx/html;   #站点目录。\n        index index.html index.htm;   \n    }\n}\n\n\n 7. 保存nginx.conf文件后退出。\n\n 8. 执行以下命令重启Nginx服务器。\n\nsystemctl restart nginx.service\n\n\n\n# 设置HTTP请求自动跳转HTTPS\n\n在需要跳转的HTTP站点下添加以下rewrite语句，实现HTTP访问自动跳转到HTTPS页面。\n\n# HTTP server\nserver {\n    listen       80;\n    server_name  ivoov.com www.ivoov.com; #将localhost修改为您证书绑定的域名，例如：www.ivoov.com。\n\n    rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n    }\n}\n\n\n\n# 启用HSTS\n\n在网站全站HTTPS后，如果用户手动敲入网站的HTTP地址，或者从其它地方点击了网站的HTTP链接，通常依赖于服务端301/302跳转才能使用HTTPS服务。而第一次的HTTP请求就有可能被劫持，导致请求无法到达服务器，从而构成HTTPS降级劫持。这个问题目前可以通过HSTS(HTTP Strict Transport Security)来解决。\n\n\n# HSTS简介\n\nHSTS(HTTP Strict Transport Security)是国际互联网工程组织IETF发布的一种互联网安全策略机制。采用HSTS策略的网站将保证浏览器始终连接到该网站的HTTPS加密版本，不需要用户手动在URL地址栏中输入加密地址，以减少会话劫持风险。\n\nHSTS响应头格式\n\nStrict-Transport-Security: max-age=expireTime [; includeSubDomains] [; preload]\n\n\n * max-age，单位是秒，用来告诉浏览器在指定时间内，这个网站必须通过HTTPS协议来访问。也就是对于这个网站的HTTP地址，浏览器需要先在本地替换为HTTPS之后再发送请求。\n * includeSubDomains，可选参数，如果指定这个参数，表明这个网站所有子域名也必须通过HTTPS协议来访问。\n * preload，可选参数，一个浏览器内置的使用HTTPS的域名列表。\n\n\n# HSTS部署\n\n服务器开启HSTS的方法是：当客户端通过HTTPS发出请求时，在服务器返回的超文本传输协议响应头中包含Strict-Transport-Security字段。非加密传输时设置的HSTS字段无效。\n\n最佳的部署方案是部署在离用户最近的位置，例如：架构有前端反向代理和后端Web服务器，在前端代理处配置HSTS是最好的，否则就需要在Web服务器层配置HSTS。如果Web服务器不明确支持HSTS，可以通过增加响应头的机制。如果其他方法都失败了，可以在应用程序层增加HSTS。\n\nHSTS启用比较简单，只需在相应头中加上如下信息：\n\nStrict-Transport-Security: max-age=63072000; includeSubdomains;preload;\n\n\nStrict-Transport-Security是Header字段名，max-age代表HSTS在客户端的生效时间。 includeSubdomains表示对所有子域名生效。preload是使用浏览器内置的域名列表。\n\nHSTS策略只能在HTTPS响应中进行设置，网站必须使用默认的443端口；必须使用域名，不能是IP。因此需要把HTTP重定向到HTTPS，如果明文响应中允许设置HSTS头，中间人攻击者就可以通过在普通站点中注入HSTS信息来执行DoS攻击。\n\n编辑nginx配置文件\n\n$ vim /etc/nginx/conf.d/ivoov.conf\n\n\n添加Strict-Transport-Security配置\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nserver {\n   listen 443 ssl;\n   server_name www.ivoov.com;\n   add_header Strict-Transport-Security "max-age=63072000; includeSubdomains; preload";\n...\n}\n \nserver {\n   listen 80;\n   server_name www.ivoov.com;\n   rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;\n...\n}\n\n\n重启Nginx服务\n\nsystemctl nginx restart\n\n\n\n# 完整示例配置文件\n\n最终效果，浏览器直接访问http://ivoov.com,http://www.ivoov.com,https://ivoov.com统一301至https://www.ivoov.com，从而实现网站统一入口，有利于seo优化。\n\n# HTTPS server\nserver {\n    listen 443 ssl;   #SSL协议访问端口号为443。此处如未添加ssl，可能会造成Nginx无法启动。\n    server_name ivoov.com www.ivoov.com;  #将localhost修改为您证书绑定的域名，例如：www.example.com。\n\n    if ($host != \'www.ivoov.com\') {\n        rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n    }\n    \n    root html;\n    index index.html index.htm;\n    \n    ssl_certificate cert/www.ivoov.com.pem;   #将www.ivoov.com.pem替换成您证书的文件名。\n    ssl_certificate_key cert/www.ivoov.com.key;   #将www.ivoov.com.key替换成您证书的密钥文件名。\n    ssl_session_timeout 5m;\n    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;  #使用此加密套件。\n    ssl_protocols TLSv1.1 TLSv1.2 TLSv1.3;   #使用该协议进行配置。\n    ssl_prefer_server_ciphers on;   \n\n    add_header Strict-Transport-Security "max-age=63072000; includeSubdomains; preload";   #启用HSTS\n\n    location / {\n        root /usr/share/nginx/html;   #站点目录。\n        index index.html index.htm;   \n    }\n}\n\n# HTTP server\nserver {\n    listen       80;\n    server_name  ivoov.com www.ivoov.com; #将localhost修改为您证书绑定的域名，例如：www.ivoov.com。\n    rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n}\n\n\n\n# 完整示例配置文件\n\n# HTTPS server\nserver {\n    listen 443 ssl;   #SSL协议访问端口号为443。此处如未添加ssl，可能会造成Nginx无法启动。\n    server_name ivoov.com www.ivoov.com;  #将localhost修改为您证书绑定的域名，例如：www.example.com。\n\n    if ($host != \'www.ivoov.com\') {\n        rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n    }\n    add_header Strict-Transport-Security "max-age=63072000; includeSubdomains; preload";   #启用HSTS\n\n    root html;\n    index index.html index.htm;\n\n    ssl_certificate cert/www.ivoov.com.pem;   #将domain name.pem替换成您证书的文件名。\n    ssl_certificate_key cert/www.ivoov.com.key;   #将domain name.key替换成您证书的密钥文件名。\n\n    ssl_session_timeout 5m;\n    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;  #使用此加密套件。\n    ssl_protocols TLSv1.1 TLSv1.2 TLSv1.3;   #使用该协议进行配置。\n    ssl_prefer_server_ciphers on;\n\n    location / {\n        root /usr/share/nginx/html/ivoov;   #站点目录。\n        index index.html index.htm;\n    }\n}\n\n# HTTPS server\nserver {\n    listen 443 ssl;   #SSL协议访问端口号为443。此处如未添加ssl，可能会造成Nginx无法启动。\n    server_name ieooc.com www.ieooc.com;  #将localhost修改为您证书绑定的域名，例如：www.example.com。\n\n    if ($host != \'www.ieooc.com\') {\n        rewrite ^/(.*)$ https://www.ieooc.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n    }\n    add_header Strict-Transport-Security "max-age=63072000; includeSubdomains; preload";   #启用HSTS\n\n    root html;\n    index index.html index.htm;\n\n    ssl_certificate cert/www.ieooc.com.pem;   #将domain name.pem替换成您证书的文件名。\n    ssl_certificate_key cert/www.ieooc.com.key;   #将domain name.key替换成您证书的密钥文件名。\n\n    ssl_session_timeout 5m;\n    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;  #使用此加密套件。\n    ssl_protocols TLSv1.1 TLSv1.2 TLSv1.3;   #使用该协议进行配置。\n    ssl_prefer_server_ciphers on;\n\n    location / {\n        root /usr/share/nginx/html/ieooc;   #站点目录。\n        index index.html index.htm;\n    }\n}\n\n# HTTP server\nserver {\n    listen       80;\n    server_name ivoov.com www.ivoov.com; #将localhost修改为您证书绑定的域名，例如：www.example.com。\n    rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n}\n\nserver {\n    listen       80;\n    server_name ieooc.com www.ieooc.com; #将localhost修改为您证书绑定的域名，例如：www.example.com。\n    rewrite ^/(.*)$ https://www.ieooc.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n\n\n\n\n# 反向代理配置\n\nlocation / {\n      proxy_pass http://127.0.0.1:8091;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header Host $http_host;\n      proxy_set_header X-NginX-Proxy true;\n      proxy_redirect default;\n}\n',normalizedContent:'# nginx 安装ssl证书\n\n阿里云ssl证书服务支持下载证书安装到nginx/tengine服务器上，本文介绍了证书安装的具体操作。\n\n\n# 背景信息\n\n * 本文档以centos 7、nginx 1.16.1为例。\n * 本文档证书名称以www.ivoov.com为示例，如证书文件名称为www.ivoov.com.pem，证书密钥文件名称为www.ivoov.com.key。\n * 下载的nginx证书压缩文件解压后包含：\n   * .pem：证书文件。pem文件的扩展名为crt格式。\n   * .key：证书的密钥文件。申请证书时如果未选择自动创建crs，则下载的证书文件压缩包中不会包含.key文件，需要您将自己手动创建的密钥文件拷贝到cert目录下。\n\n提示\n\n.pem扩展名的证书文件采用base64-encoded的pem格式文本文件，您可根据需要修改成其他扩展名。 证书格式详细内容，请参见主流数字证书都有哪些格式？\n\n\n# 操作步骤\n\n 1. 登录阿里云ssl证书控制台。\n\n 2. 在ssl证书页面，单击已签发标签，定位到需要下载的证书并单击证书卡片右下角的下载。\n\n 3. 在证书下载侧页面中定位到nginx服务器，并单击右侧操作栏的下载，将nginx服务器证书压缩包下载到本地。\n\n 4. 解压已下载保存到本地的nginx证书压缩包文件。\n    \n    解压后的文件夹中有2个文件：\n    \n    * 证书文件：以.pem为后缀或文件类型。\n    * 密钥文件：以.key为后缀或文件类型。\n\n 5. 登录您的nginx服务器，在nginx安装目录（默认nginx安装目录为/usr/local/nginx/conf）下创建cert目录，并将下载的证书文件和密钥文件拷贝到cert目录中。\n\n提示\n\n如果您在申请证书时选择手动创建csr文件，请将对应的密钥文件放到cert目录中，并命名为www.ivoov.com.key。\n\n 6. 修改nginx安装目录/conf/nginx.conf文件。\n    \n    按照下文中注释内容修改nginx.conf文件：\n\n# https server\nserver {\n    listen 443 ssl;   #ssl协议访问端口号为443。此处如未添加ssl，可能会造成nginx无法启动。\n    server_name ivoov.com www.ivoov.com;  #将localhost修改为您证书绑定的域名，例如：www.ivoov.com。\n\n    root html;\n    index index.html index.htm;\n    \n    ssl_certificate cert/www.ivoov.com.pem;   #将www.ivoov.com.pem替换成您证书的文件名。\n    ssl_certificate_key cert/www.ivoov.com.key;   #将www.ivoov.com.key替换成您证书的密钥文件名。\n    ssl_session_timeout 5m;\n    ssl_ciphers ecdhe-rsa-aes128-gcm-sha256:ecdhe:ecdh:aes:high:!null:!anull:!md5:!adh:!rc4;  #使用此加密套件。\n    ssl_protocols tlsv1.1 tlsv1.2 tlsv1.3;   #使用该协议进行配置。\n    ssl_prefer_server_ciphers on;   \n    location / {\n        root /usr/share/nginx/html;   #站点目录。\n        index index.html index.htm;   \n    }\n}\n\n\n 7. 保存nginx.conf文件后退出。\n\n 8. 执行以下命令重启nginx服务器。\n\nsystemctl restart nginx.service\n\n\n\n# 设置http请求自动跳转https\n\n在需要跳转的http站点下添加以下rewrite语句，实现http访问自动跳转到https页面。\n\n# http server\nserver {\n    listen       80;\n    server_name  ivoov.com www.ivoov.com; #将localhost修改为您证书绑定的域名，例如：www.ivoov.com。\n\n    rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n    }\n}\n\n\n\n# 启用hsts\n\n在网站全站https后，如果用户手动敲入网站的http地址，或者从其它地方点击了网站的http链接，通常依赖于服务端301/302跳转才能使用https服务。而第一次的http请求就有可能被劫持，导致请求无法到达服务器，从而构成https降级劫持。这个问题目前可以通过hsts(http strict transport security)来解决。\n\n\n# hsts简介\n\nhsts(http strict transport security)是国际互联网工程组织ietf发布的一种互联网安全策略机制。采用hsts策略的网站将保证浏览器始终连接到该网站的https加密版本，不需要用户手动在url地址栏中输入加密地址，以减少会话劫持风险。\n\nhsts响应头格式\n\nstrict-transport-security: max-age=expiretime [; includesubdomains] [; preload]\n\n\n * max-age，单位是秒，用来告诉浏览器在指定时间内，这个网站必须通过https协议来访问。也就是对于这个网站的http地址，浏览器需要先在本地替换为https之后再发送请求。\n * includesubdomains，可选参数，如果指定这个参数，表明这个网站所有子域名也必须通过https协议来访问。\n * preload，可选参数，一个浏览器内置的使用https的域名列表。\n\n\n# hsts部署\n\n服务器开启hsts的方法是：当客户端通过https发出请求时，在服务器返回的超文本传输协议响应头中包含strict-transport-security字段。非加密传输时设置的hsts字段无效。\n\n最佳的部署方案是部署在离用户最近的位置，例如：架构有前端反向代理和后端web服务器，在前端代理处配置hsts是最好的，否则就需要在web服务器层配置hsts。如果web服务器不明确支持hsts，可以通过增加响应头的机制。如果其他方法都失败了，可以在应用程序层增加hsts。\n\nhsts启用比较简单，只需在相应头中加上如下信息：\n\nstrict-transport-security: max-age=63072000; includesubdomains;preload;\n\n\nstrict-transport-security是header字段名，max-age代表hsts在客户端的生效时间。 includesubdomains表示对所有子域名生效。preload是使用浏览器内置的域名列表。\n\nhsts策略只能在https响应中进行设置，网站必须使用默认的443端口；必须使用域名，不能是ip。因此需要把http重定向到https，如果明文响应中允许设置hsts头，中间人攻击者就可以通过在普通站点中注入hsts信息来执行dos攻击。\n\n编辑nginx配置文件\n\n$ vim /etc/nginx/conf.d/ivoov.conf\n\n\n添加strict-transport-security配置\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nserver {\n   listen 443 ssl;\n   server_name www.ivoov.com;\n   add_header strict-transport-security "max-age=63072000; includesubdomains; preload";\n...\n}\n \nserver {\n   listen 80;\n   server_name www.ivoov.com;\n   rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;\n...\n}\n\n\n重启nginx服务\n\nsystemctl nginx restart\n\n\n\n# 完整示例配置文件\n\n最终效果，浏览器直接访问http://ivoov.com,http://www.ivoov.com,https://ivoov.com统一301至https://www.ivoov.com，从而实现网站统一入口，有利于seo优化。\n\n# https server\nserver {\n    listen 443 ssl;   #ssl协议访问端口号为443。此处如未添加ssl，可能会造成nginx无法启动。\n    server_name ivoov.com www.ivoov.com;  #将localhost修改为您证书绑定的域名，例如：www.example.com。\n\n    if ($host != \'www.ivoov.com\') {\n        rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n    }\n    \n    root html;\n    index index.html index.htm;\n    \n    ssl_certificate cert/www.ivoov.com.pem;   #将www.ivoov.com.pem替换成您证书的文件名。\n    ssl_certificate_key cert/www.ivoov.com.key;   #将www.ivoov.com.key替换成您证书的密钥文件名。\n    ssl_session_timeout 5m;\n    ssl_ciphers ecdhe-rsa-aes128-gcm-sha256:ecdhe:ecdh:aes:high:!null:!anull:!md5:!adh:!rc4;  #使用此加密套件。\n    ssl_protocols tlsv1.1 tlsv1.2 tlsv1.3;   #使用该协议进行配置。\n    ssl_prefer_server_ciphers on;   \n\n    add_header strict-transport-security "max-age=63072000; includesubdomains; preload";   #启用hsts\n\n    location / {\n        root /usr/share/nginx/html;   #站点目录。\n        index index.html index.htm;   \n    }\n}\n\n# http server\nserver {\n    listen       80;\n    server_name  ivoov.com www.ivoov.com; #将localhost修改为您证书绑定的域名，例如：www.ivoov.com。\n    rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n}\n\n\n\n# 完整示例配置文件\n\n# https server\nserver {\n    listen 443 ssl;   #ssl协议访问端口号为443。此处如未添加ssl，可能会造成nginx无法启动。\n    server_name ivoov.com www.ivoov.com;  #将localhost修改为您证书绑定的域名，例如：www.example.com。\n\n    if ($host != \'www.ivoov.com\') {\n        rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n    }\n    add_header strict-transport-security "max-age=63072000; includesubdomains; preload";   #启用hsts\n\n    root html;\n    index index.html index.htm;\n\n    ssl_certificate cert/www.ivoov.com.pem;   #将domain name.pem替换成您证书的文件名。\n    ssl_certificate_key cert/www.ivoov.com.key;   #将domain name.key替换成您证书的密钥文件名。\n\n    ssl_session_timeout 5m;\n    ssl_ciphers ecdhe-rsa-aes128-gcm-sha256:ecdhe:ecdh:aes:high:!null:!anull:!md5:!adh:!rc4;  #使用此加密套件。\n    ssl_protocols tlsv1.1 tlsv1.2 tlsv1.3;   #使用该协议进行配置。\n    ssl_prefer_server_ciphers on;\n\n    location / {\n        root /usr/share/nginx/html/ivoov;   #站点目录。\n        index index.html index.htm;\n    }\n}\n\n# https server\nserver {\n    listen 443 ssl;   #ssl协议访问端口号为443。此处如未添加ssl，可能会造成nginx无法启动。\n    server_name ieooc.com www.ieooc.com;  #将localhost修改为您证书绑定的域名，例如：www.example.com。\n\n    if ($host != \'www.ieooc.com\') {\n        rewrite ^/(.*)$ https://www.ieooc.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n    }\n    add_header strict-transport-security "max-age=63072000; includesubdomains; preload";   #启用hsts\n\n    root html;\n    index index.html index.htm;\n\n    ssl_certificate cert/www.ieooc.com.pem;   #将domain name.pem替换成您证书的文件名。\n    ssl_certificate_key cert/www.ieooc.com.key;   #将domain name.key替换成您证书的密钥文件名。\n\n    ssl_session_timeout 5m;\n    ssl_ciphers ecdhe-rsa-aes128-gcm-sha256:ecdhe:ecdh:aes:high:!null:!anull:!md5:!adh:!rc4;  #使用此加密套件。\n    ssl_protocols tlsv1.1 tlsv1.2 tlsv1.3;   #使用该协议进行配置。\n    ssl_prefer_server_ciphers on;\n\n    location / {\n        root /usr/share/nginx/html/ieooc;   #站点目录。\n        index index.html index.htm;\n    }\n}\n\n# http server\nserver {\n    listen       80;\n    server_name ivoov.com www.ivoov.com; #将localhost修改为您证书绑定的域名，例如：www.example.com。\n    rewrite ^/(.*)$ https://www.ivoov.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n}\n\nserver {\n    listen       80;\n    server_name ieooc.com www.ieooc.com; #将localhost修改为您证书绑定的域名，例如：www.example.com。\n    rewrite ^/(.*)$ https://www.ieooc.com/$1 permanent;   #将所有http请求通过rewrite重定向到https。\n\n\n\n\n# 反向代理配置\n\nlocation / {\n      proxy_pass http://127.0.0.1:8091;\n      proxy_set_header x-real-ip $remote_addr;\n      proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n      proxy_set_header host $http_host;\n      proxy_set_header x-nginx-proxy true;\n      proxy_redirect default;\n}\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Nginx 开启gzip压缩",frontmatter:{title:"Nginx 开启gzip压缩",date:"2022-02-09T11:24:38.000Z",permalink:"/pages/73e9ae/"},regularPath:"/10.Nginx/01.Nginx/03.Nginx%20%E5%BC%80%E5%90%AFgzip%E5%8E%8B%E7%BC%A9.html",relativePath:"10.Nginx/01.Nginx/03.Nginx 开启gzip压缩.md",key:"v-c80a5a9c",path:"/pages/73e9ae/",headers:[{level:2,title:"使用范例",slug:"使用范例",normalizedTitle:"使用范例",charIndex:21},{level:2,title:"指令",slug:"指令",normalizedTitle:"指令",charIndex:785},{level:2,title:"gzip",slug:"gzip",normalizedTitle:"gzip",charIndex:10},{level:2,title:"gzip_buffers",slug:"gzip-buffers",normalizedTitle:"gzip_buffers",charIndex:659},{level:2,title:"gzipcomplevel",slug:"gzip-comp-level",normalizedTitle:"gzipcomplevel",charIndex:null},{level:2,title:"gzipminlength",slug:"gzip-min-length",normalizedTitle:"gzipminlength",charIndex:null},{level:2,title:"gziphttpversion",slug:"gzip-http-version",normalizedTitle:"gziphttpversion",charIndex:null},{level:2,title:"gzip_proxied",slug:"gzip-proxied",normalizedTitle:"gzip_proxied",charIndex:960},{level:2,title:"gzip_types",slug:"gzip-types",normalizedTitle:"gzip_types",charIndex:268},{level:2,title:"References",slug:"references",normalizedTitle:"references",charIndex:3173}],headersStr:"使用范例 指令 gzip gzip_buffers gzipcomplevel gzipminlength gziphttpversion gzip_proxied gzip_types References",content:'# Nginx 开启gzip压缩\n\n\n# 使用范例\n\n这个模块支持在线实时压缩输出数据流，nginx.conf中关于gzip压缩的主要配置如下：\n\nhttp {\n\t......\n    #开启和关闭gzip模式\n    gzip on;\n    \n    #gizp压缩起点，文件大于1k才进行压缩\n    gzip_min_length 1k;\n    \n    # gzip 压缩级别，1-9，数字越大压缩的越好，也越占用CPU时间\n    gzip_comp_level 6;\n    \n    # 进行压缩的文件类型。\n    gzip_types text/plain application/javascript application/x-javascript text/css application/xml text/xml text/javascript application/json image/png image/gif image/jpeg;\n    \n    #nginx对于静态文件的处理模块，开启后会寻找以.gz结尾的文件，直接返回，不会占用cpu进行压缩，如果找不到则不进行压缩\n    # gzip_static on|off\n    \n    # 是否在http header中添加Vary: Accept-Encoding，建议开启\n    gzip_vary on;\n\n    # 设置压缩所需要的缓冲区大小，以4k为单位，如果文件为7k则申请2*4k的缓冲区 \n    gzip_buffers 4 16k;\n\n    # 设置gzip压缩针对的HTTP协议版本\n    # gzip_http_version 1.1;\n\t......\n}\n\n\n> 内置变量 $gzip_ratio 可以获取到gzip的压缩比率\n\n\n# 指令\n\n * [#gzip gzip]\n * [#gzip_buffers gzip_buffers]\n * [#gzip_comp_level gzip_comp_level]\n * [#gzip_min_length gzip_min_length]\n * [#gzip_http_version gzip_http_version]\n * [#gzip_proxied gzip_proxied]\n * [#gzip_types gzip_types]\n\n\n# gzip\n\n语法: gzip on|off\n\n默认值: gzip off\n\n作用域: http, server, location, if (x) location\n\n开启或者关闭gzip模块\n\n\n# gzip_buffers\n\n语法: gzip_buffers number size\n\n默认值: gzip_buffers 4 4k/8k\n\n作用域: http, server, location\n\n设置系统获取几个单位的缓存用于存储gzip的压缩结果数据流。 例如 4 4k 代表以4k为单位，按照原始数据大小以4k为单位的4倍申请内存。 4 8k 代表以8k为单位，按照原始数据大小以8k为单位的4倍申请内存。\n\n如果没有设置，默认值是申请跟原始数据相同大小的内存空间去存储gzip压缩结果。\n\n\n# gzip_comp_level\n\n语法: gzip_comp_level 1..9\n\n默认值: gzip_comp_level 1\n\n作用域: http, server, location\n\ngzip压缩比，1 压缩比最小处理速度最快，9 压缩比最大但处理最慢（传输快但比较消耗cpu）。\n\n\n# gzip_min_length\n\n语法: gzip_min_length length\n\n默认值: gzip_min_length 0\n\n作用域: http, server, location\n\n设置允许压缩的页面最小字节数，页面字节数从header头中的Content-Length中进行获取。\n\n默认值是0，不管页面多大都压缩。\n\n建议设置成大于1k的字节数，小于1k可能会越压越大。 即: gzip_min_length 1024\n\n\n# gzip_http_version\n\n语法: gzip_http_version 1.0|1.1\n\n默认值: gzip_http_version 1.1\n\n作用域: http, server, location\n\n识别http的协议版本。由于早期的一些浏览器或者http客户端，可能不支持gzip自解压，用户就会看到乱码，所以做一些判断还是有必要的。 注：21世纪都来了，现在除了类似于百度的蜘蛛之类的东西不支持自解压，99.99%的浏览器基本上都支持gzip解压了，所以可以不用设这个值,保持系统默认即可。\n\n\n# gzip_proxied\n\n语法: gzip_proxied [off|expired|no-cache|no-store|private|no_last_modified|no_etag|auth|any] ...\n\n默认值: gzip_proxied off\n\n作用域: http, server, location\n\nNginx作为反向代理的时候启用，开启或者关闭后端服务器返回的结果，匹配的前提是后端服务器必须要返回包含"Via"的 header头。\n\n * off - 关闭所有的代理结果数据的压缩\n * expired - 启用压缩，如果header头中包含 "Expires" 头信息\n * no-cache - 启用压缩，如果header头中包含 "Cache-Control:no-cache" 头信息\n * no-store - 启用压缩，如果header头中包含 "Cache-Control:no-store" 头信息\n * private - 启用压缩，如果header头中包含 "Cache-Control:private" 头信息\n * no_last_modified - 启用压缩,如果header头中不包含 "Last-Modified" 头信息\n * no_etag - 启用压缩 ,如果header头中不包含 "ETag" 头信息\n * auth - 启用压缩 , 如果header头中包含 "Authorization" 头信息\n * any - 无条件启用压缩\n\n\n# gzip_types\n\n语法: gzip_types mime-type [mime-type ...]\n\n默认值: gzip_types text/html\n\n作用域: http, server, location\n\n匹配MIME类型进行压缩，（无论是否指定）"text/html"类型总是会被压缩的。\n\n注意：如果作为http server来使用，主配置文件中要包含文件类型配置文件\n\nhttp\n{\n\tinclude       conf/mime.types;\n\t......\n}\n\n\n如果你希望压缩常规的文件类型，可以写成这个样子\n\nhttp \n{\n: include       conf/mime.types;\n\n: gzip on;\n: gzip_min_length  1000;\n: gzip_buffers     4 8k;   \n: gzip_http_version 1.1; \n: gzip_types       text/plain application/x-javascript text/css text/html application/xml;\n\n: ......\t\n}\n\n\n\n# References\n\n原始文档',normalizedContent:'# nginx 开启gzip压缩\n\n\n# 使用范例\n\n这个模块支持在线实时压缩输出数据流，nginx.conf中关于gzip压缩的主要配置如下：\n\nhttp {\n\t......\n    #开启和关闭gzip模式\n    gzip on;\n    \n    #gizp压缩起点，文件大于1k才进行压缩\n    gzip_min_length 1k;\n    \n    # gzip 压缩级别，1-9，数字越大压缩的越好，也越占用cpu时间\n    gzip_comp_level 6;\n    \n    # 进行压缩的文件类型。\n    gzip_types text/plain application/javascript application/x-javascript text/css application/xml text/xml text/javascript application/json image/png image/gif image/jpeg;\n    \n    #nginx对于静态文件的处理模块，开启后会寻找以.gz结尾的文件，直接返回，不会占用cpu进行压缩，如果找不到则不进行压缩\n    # gzip_static on|off\n    \n    # 是否在http header中添加vary: accept-encoding，建议开启\n    gzip_vary on;\n\n    # 设置压缩所需要的缓冲区大小，以4k为单位，如果文件为7k则申请2*4k的缓冲区 \n    gzip_buffers 4 16k;\n\n    # 设置gzip压缩针对的http协议版本\n    # gzip_http_version 1.1;\n\t......\n}\n\n\n> 内置变量 $gzip_ratio 可以获取到gzip的压缩比率\n\n\n# 指令\n\n * [#gzip gzip]\n * [#gzip_buffers gzip_buffers]\n * [#gzip_comp_level gzip_comp_level]\n * [#gzip_min_length gzip_min_length]\n * [#gzip_http_version gzip_http_version]\n * [#gzip_proxied gzip_proxied]\n * [#gzip_types gzip_types]\n\n\n# gzip\n\n语法: gzip on|off\n\n默认值: gzip off\n\n作用域: http, server, location, if (x) location\n\n开启或者关闭gzip模块\n\n\n# gzip_buffers\n\n语法: gzip_buffers number size\n\n默认值: gzip_buffers 4 4k/8k\n\n作用域: http, server, location\n\n设置系统获取几个单位的缓存用于存储gzip的压缩结果数据流。 例如 4 4k 代表以4k为单位，按照原始数据大小以4k为单位的4倍申请内存。 4 8k 代表以8k为单位，按照原始数据大小以8k为单位的4倍申请内存。\n\n如果没有设置，默认值是申请跟原始数据相同大小的内存空间去存储gzip压缩结果。\n\n\n# gzip_comp_level\n\n语法: gzip_comp_level 1..9\n\n默认值: gzip_comp_level 1\n\n作用域: http, server, location\n\ngzip压缩比，1 压缩比最小处理速度最快，9 压缩比最大但处理最慢（传输快但比较消耗cpu）。\n\n\n# gzip_min_length\n\n语法: gzip_min_length length\n\n默认值: gzip_min_length 0\n\n作用域: http, server, location\n\n设置允许压缩的页面最小字节数，页面字节数从header头中的content-length中进行获取。\n\n默认值是0，不管页面多大都压缩。\n\n建议设置成大于1k的字节数，小于1k可能会越压越大。 即: gzip_min_length 1024\n\n\n# gzip_http_version\n\n语法: gzip_http_version 1.0|1.1\n\n默认值: gzip_http_version 1.1\n\n作用域: http, server, location\n\n识别http的协议版本。由于早期的一些浏览器或者http客户端，可能不支持gzip自解压，用户就会看到乱码，所以做一些判断还是有必要的。 注：21世纪都来了，现在除了类似于百度的蜘蛛之类的东西不支持自解压，99.99%的浏览器基本上都支持gzip解压了，所以可以不用设这个值,保持系统默认即可。\n\n\n# gzip_proxied\n\n语法: gzip_proxied [off|expired|no-cache|no-store|private|no_last_modified|no_etag|auth|any] ...\n\n默认值: gzip_proxied off\n\n作用域: http, server, location\n\nnginx作为反向代理的时候启用，开启或者关闭后端服务器返回的结果，匹配的前提是后端服务器必须要返回包含"via"的 header头。\n\n * off - 关闭所有的代理结果数据的压缩\n * expired - 启用压缩，如果header头中包含 "expires" 头信息\n * no-cache - 启用压缩，如果header头中包含 "cache-control:no-cache" 头信息\n * no-store - 启用压缩，如果header头中包含 "cache-control:no-store" 头信息\n * private - 启用压缩，如果header头中包含 "cache-control:private" 头信息\n * no_last_modified - 启用压缩,如果header头中不包含 "last-modified" 头信息\n * no_etag - 启用压缩 ,如果header头中不包含 "etag" 头信息\n * auth - 启用压缩 , 如果header头中包含 "authorization" 头信息\n * any - 无条件启用压缩\n\n\n# gzip_types\n\n语法: gzip_types mime-type [mime-type ...]\n\n默认值: gzip_types text/html\n\n作用域: http, server, location\n\n匹配mime类型进行压缩，（无论是否指定）"text/html"类型总是会被压缩的。\n\n注意：如果作为http server来使用，主配置文件中要包含文件类型配置文件\n\nhttp\n{\n\tinclude       conf/mime.types;\n\t......\n}\n\n\n如果你希望压缩常规的文件类型，可以写成这个样子\n\nhttp \n{\n: include       conf/mime.types;\n\n: gzip on;\n: gzip_min_length  1000;\n: gzip_buffers     4 8k;   \n: gzip_http_version 1.1; \n: gzip_types       text/plain application/x-javascript text/css text/html application/xml;\n\n: ......\t\n}\n\n\n\n# references\n\n原始文档',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Nginx location正则写法",frontmatter:{title:"Nginx location正则写法",date:"2022-02-09T11:24:38.000Z",permalink:"/pages/9d4443/"},regularPath:"/10.Nginx/01.Nginx/04.Nginx%20location%E6%AD%A3%E5%88%99%E5%86%99%E6%B3%95.html",relativePath:"10.Nginx/01.Nginx/04.Nginx location正则写法.md",key:"v-e4187760",path:"/pages/9d4443/",headers:[{level:2,title:"匹配符号说明",slug:"匹配符号说明",normalizedTitle:"匹配符号说明",charIndex:25},{level:2,title:"正则表达式",slug:"正则表达式",normalizedTitle:"正则表达式",charIndex:75},{level:2,title:"正则表达式补充",slug:"正则表达式补充",normalizedTitle:"正则表达式补充",charIndex:770},{level:3,title:"实例",slug:"实例",normalizedTitle:"实例",charIndex:992},{level:3,title:"实际使用建议",slug:"实际使用建议",normalizedTitle:"实际使用建议",charIndex:2740},{level:2,title:"flag标志位",slug:"flag标志位",normalizedTitle:"flag标志位",charIndex:3940},{level:2,title:"if指令与全局变量",slug:"if指令与全局变量",normalizedTitle:"if指令与全局变量",charIndex:4329},{level:2,title:"rewrite实例",slug:"rewrite实例",normalizedTitle:"rewrite实例",charIndex:5408}],headersStr:"匹配符号说明 正则表达式 正则表达式补充 实例 实际使用建议 flag标志位 if指令与全局变量 rewrite实例",content:'# Nginx location正则写法\n\n\n# 匹配符号说明\n\n表达式   描述\n^~    标识符匹配后面跟一个字符串。匹配字符串后将停止对后续的正则表达式进行匹配。\n=     精准匹配，如location=/，只会匹配url为/的请求。\n~     区分大小写的匹配。\n~*    不区分大小写的匹配。\n!~    对区分大小写的匹配取非。\n!~*   对不区分大小写的匹配取非。\n/     通用匹配，如果没有其它匹配,任何请求都会被匹配到\n\n> 如 location ^~ /images/ ， 在匹配了/images/ 这个字符串后就停止对后续的正则匹配\n\n提示\n\n匹配顺序优先级： （location =）> (location 完整路径）> (location ^~ 路径) > (location ~，~*正则顺序) >(location 部分起始位置) > （/）\n\n\n# 正则表达式\n\n表达式      描述\n*        重复前面的字符0次或多次\n？        重复前面的字符0次或1次\n+        重复前面的字符1次或多次\n.        匹配除换行符以外的任意一个字符\n(a|b)    匹配a或b\n^        以...开头\n$        以...结尾\n{n}      重复前面的字符n次\n{n,}     重复前面的字符n次或更多次\n{n，m}    重复前面的字符n-m次\n*？       重复前面的字符0次或多次，但尽可能少重复\n+？       重复前面的字符1次或多次，但尽可能少重复\n？？       重复前面的字符0次或1次，但尽可能少重复\n{n，m}？   重复前面的字符n-m次，但尽可能少重复\n{n}？     重复前面的字符n次以上，但尽可能少重复\n\n\n# 正则表达式补充\n\n表达式      描述\n\\W       匹配任意不是字母，数字，下划线，汉字的字符（特殊符号）\n\\S       匹配任意不是空白符的字符\n\\D       匹配任意非数字的字符\n\\B       匹配任意不是单词开头或结尾的位置\n[a]      匹配单个字符a\n[a-z]    匹配a-z小写字符的任意一个\n[^a]     匹配除了a以外的任意字符\n[^abc]   匹配除了abc这几个字母以外的任意字符\n\n\n# 实例\n\nlocation  = / {\n  # 精确匹配 / ，主机名后面不能带任何字符串\n  [ configuration A ] \n}\nlocation  / {\n  # 因为所有的地址都以 / 开头，所以这条规则将匹配到所有请求\n  # 但是正则和最长字符串会优先匹配\n  [ configuration B ] \n}\nlocation /documents/ {\n  # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索\n  # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条\n  [ configuration C ] \n}\n\nlocation ~ /documents/Abc {\n  # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索\n  # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条\n  [ configuration CC ] \n}\n\nlocation ^~ /images/ {\n  # 匹配任何以 /images/ 开头的地址，匹配符合以后，停止往下搜索正则，采用这一条。\n  [ configuration D ] \n}\n\nlocation ~* \\.(gif|jpg|jpeg)$ {\n  # 匹配所有以 gif,jpg或jpeg 结尾的请求\n  # 然而，所有请求 /images/ 下的图片会被 config D 处理，因为 ^~ 到达不了这一条正则\n  [ configuration E ] \n}\n\nlocation /images/ {\n  # 字符匹配到 /images/，继续往下，会发现 ^~ 存在\n  [ configuration F ] \n}\n\nlocation /images/abc {\n  # 最长字符匹配到 /images/abc，继续往下，会发现 ^~ 存在\n  # F与G的放置顺序是没有关系的\n  [ configuration G ] \n}\n\nlocation ~ /images/abc/ {\n  # 只有去掉 config D 才有效：先最长匹配 config G 开头的地址，继续往下搜索，匹配到这一条正则，采用\n    [ configuration H ] \n}\n\nlocation ~* /js/.*/\\.js\n\n\n 1. 已=开头表示精确匹配\n 2. 如 A 中只匹配根目录结尾的请求，后面不能带任何字符串。\n 3. ^~ 开头表示uri以某个常规字符串开头，不是正则匹配\n 4. ~ 开头表示区分大小写的正则匹配;\n 5. ~* 开头表示不区分大小写的正则匹配\n 6. / 通用匹配, 如果没有其它匹配,任何请求都会匹配到\n\n提示\n\n顺序 no优先级： (location =) > (location 完整路径) > (location ^~ 路径) > (location ~,~* 正则顺序) > (location 部分起始路径) > (/)\n\n上面的匹配结果 按照上面的location写法，以下的匹配示例成立：\n\n * / -> config A ：精确完全匹配，即使/index.html也匹配不了\n * /downloads/download.html -> config B ：匹配B以后，往下没有任何匹配，采用B\n * /images/1.gif -> configuration D ：匹配到F，往下匹配到D，停止往下\n * /images/abc/def -> config D：最长匹配到G，往下匹配D，停止往下你可以看到 任何以/images/开头的都会匹配到D并停止，FG写在这里是没有任何意义的，H是永远轮不到的，这里只是为了说明匹配顺序\n * /documents/document.html -> config C ：匹配到C，往下没有任何匹配，采用C\n * /documents/1.jpg -> configuration E ：匹配到C，往下正则匹配到E\n * /documents/Abc.jpg -> config CC ：最长匹配到C，往下正则顺序匹配到CC，不会往下到E\n\n\n# 实际使用建议\n\n所以实际使用中，个人觉得至少有三个匹配规则定义，如下：\n#直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。\n#这里是直接转发给后端应用服务器了，也可以是一个静态首页\n# 第一个必选规则\nlocation = / {\n    proxy_pass http://tomcat:8080/index\n}\n# 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项\n# 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用\nlocation ^~ /static/ {\n    root /webroot/static/;\n}\nlocation ~* \\.(gif|jpg|jpeg|png|css|js|ico)$ {\n    root /webroot/res/;\n}\n#第三个规则就是通用规则，用来转发动态请求到后端应用服务器\n#非静态文件请求就默认是动态请求，自己根据实际把握\n#毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了\nlocation / {\n    proxy_pass http://tomcat:8080/\n}\n\n\nhttp://tengine.taobao.org/book/chapter_02.html http://nginx.org/en/docs/http/ngx_http_rewrite_module.html\n\n\n# Rewrite规则\n\nrewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。rewrite只能放在server{},location{},if{}中，并且只能对域名后边的除去传递的参数外的字符串起作用，例如 http://seanlook.com/a/we/index.php?id=1&u=str 只对/a/we/index.php重写。语法rewrite regex replacement [flag];\n\n如果相对域名或参数字符串起作用，可以使用全局变量匹配，也可以使用proxy_pass反向代理。\n\n表面看rewrite和location功能有点像，都能实现跳转，主要区别在于rewrite是在同一域名内更改获取资源的路径，而location是对一类路径做控制访问或反向代理，可以proxy_pass到其他机器。很多情况下rewrite也会写在location里，它们的执行顺序是：\n\n 1. 执行server块的rewrite指令\n 2. 执行location匹配\n 3. 执行选定的location中的rewrite指令\n\n如果其中某步URI被重写，则重新循环执行1-3，直到找到真实存在的文件；循环超过10次，则返回500 Internal Server Error错误。\n\n\n# flag标志位\n\n * last : 相当于Apache的[L]标记，表示完成rewrite\n * break : 停止执行当前虚拟主机的后续rewrite指令集\n * redirect : 返回302临时重定向，地址栏会显示跳转后的地址\n * permanent : 返回301永久重定向，地址栏会显示跳转后的地址\n\n因为301和302不能简单的只返回状态码，还必须有重定向的URL，这就是return指令无法返回301,302的原因了。这里 last 和 break 区别有点难以理解：\n\n 1. last一般写在server和if中，而break一般使用在location中\n 2. last不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后的匹配\n 3. break和last都能组织继续执行后面的rewrite指令\n\n\n# if指令与全局变量\n\nif判断指令 语法为if(condition){...}，对给定的条件condition进行判断。如果为真，大括号内的rewrite指令将被执行，if条件(conditon)可以是如下任何内容：\n\n * 当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false\n * 直接比较变量和内容时，使用=或!=\n * ~正则表达式匹配，~*不区分大小写的匹配，!~区分大小写的不匹配\n\n-f和!-f用来判断是否存在文件 -d和!-d用来判断是否存在目录 -e和!-e用来判断是否存在文件或目录 -x和!-x用来判断文件是否可执行\n\n例如：\n\nif ($http_user_agent ~ MSIE) {\n    rewrite ^(.*)$ /msie/$1 break;\n} //如果UA包含"MSIE"，rewrite请求到/msid/目录下\n\nif ($http_cookie ~* "id=([^;]+)(?:;|$)") {\n    set $id $1;\n } //如果cookie匹配正则，设置变量$id等于正则引用部分\n\nif ($request_method = POST) {\n    return 405;\n} //如果提交方法为POST，则返回状态405（Method not allowed）。return不能返回301,302\n\nif ($slow) {\n    limit_rate 10k;\n} //限速，$slow可以通过 set 指令设置\n\nif (!-f $request_filename){\n    break;\n    proxy_pass  http://127.0.0.1; \n} //如果请求的文件名不存在，则反向代理到localhost 。这里的break也是停止rewrite检查\n\nif ($args ~ post=140){\n    rewrite ^ http://example.com/ permanent;\n} //如果query string中包含"post=140"，永久重定向到example.com\n\nlocation ~* \\.(gif|jpg|png|swf|flv)$ {\n    valid_referers none blocked www.jefflei.com www.leizhenfang.com;\n    if ($invalid_referer) {\n        return 404;\n    } //防盗链\n}\n\n\n\n# rewrite实例\n\n例1：\n\nhttp {\n    # 定义image日志格式\n    log_format imagelog \'[$time_local] \' $image_file \' \' $image_type \' \' $body_bytes_sent \' \' $status;\n    # 开启重写日志\n    rewrite_log on;\n\n    server {\n        root /home/www;\n\n        location / {\n                # 重写规则信息\n                error_log logs/rewrite.log notice; \n                # 注意这里要用‘’单引号引起来，避免{}\n                rewrite \'^/images/([a-z]{2})/([a-z0-9]{5})/(.*)\\.(png|jpg|gif)$\' /data?file=$3.$4;\n                # 注意不能在上面这条规则后面加上“last”参数，否则下面的set指令不会执行\n                set $image_file $3;\n                set $image_type $4;\n        }\n\n        location /data {\n                # 指定针对图片的日志格式，来分析图片类型和大小\n                access_log logs/images.log mian;\n                root /data/images;\n                # 应用前面定义的变量。判断首先文件在不在，不在再判断目录在不在，如果还不在就跳转到最后一个url里\n                try_files /$arg_file /image404.html;\n        }\n        location = /image404.html {\n                # 图片不存在返回特定的信息\n                return 404 "image not found\\n";\n        }\n}\n\n\n对形如/images/ef/uh7b3/test.png的请求，重写到/data?file=test.png，于是匹配到location /data，先看/data/images/test.png文件存不存在，如果存在则正常响应，如果不存在则重写tryfiles到新的image404 location，直接返回404状态码。\n\n例2：\n\nrewrite ^/images/(.*)_(\\d+)x(\\d+)\\.(png|jpg|gif)$ /resizer/$1.$4?width=$2&height=$3? last;\n\n\n对形如/images/bla_500x400.jpg的文件请求，重写到/resizer/bla.jpg?width=500&height=400地址，并会继续尝试匹配location。\n\n这个location\n\nlocation ~ ^/(.+)\\.3gp\\.zip$ {\n  # access_by_lua_file  "/opt/pro/nginx/lua/zip_access.lua";\n  rewrite_by_lua_file "/opt/pro/nginx/lua/zip_access.lua";\n}\n\n\n匹配 https://www.ivoov.com/vod/ts01/TVM/video/3gp/TVM/HuNanHD/2016/04/27/80a4b71a-c000-46fa-916b-70d9e2445635/Q350/Q350.3gp.zip?&end=5',normalizedContent:'# nginx location正则写法\n\n\n# 匹配符号说明\n\n表达式   描述\n^~    标识符匹配后面跟一个字符串。匹配字符串后将停止对后续的正则表达式进行匹配。\n=     精准匹配，如location=/，只会匹配url为/的请求。\n~     区分大小写的匹配。\n~*    不区分大小写的匹配。\n!~    对区分大小写的匹配取非。\n!~*   对不区分大小写的匹配取非。\n/     通用匹配，如果没有其它匹配,任何请求都会被匹配到\n\n> 如 location ^~ /images/ ， 在匹配了/images/ 这个字符串后就停止对后续的正则匹配\n\n提示\n\n匹配顺序优先级： （location =）> (location 完整路径）> (location ^~ 路径) > (location ~，~*正则顺序) >(location 部分起始位置) > （/）\n\n\n# 正则表达式\n\n表达式      描述\n*        重复前面的字符0次或多次\n？        重复前面的字符0次或1次\n+        重复前面的字符1次或多次\n.        匹配除换行符以外的任意一个字符\n(a|b)    匹配a或b\n^        以...开头\n$        以...结尾\n{n}      重复前面的字符n次\n{n,}     重复前面的字符n次或更多次\n{n，m}    重复前面的字符n-m次\n*？       重复前面的字符0次或多次，但尽可能少重复\n+？       重复前面的字符1次或多次，但尽可能少重复\n？？       重复前面的字符0次或1次，但尽可能少重复\n{n，m}？   重复前面的字符n-m次，但尽可能少重复\n{n}？     重复前面的字符n次以上，但尽可能少重复\n\n\n# 正则表达式补充\n\n表达式      描述\n\\w       匹配任意不是字母，数字，下划线，汉字的字符（特殊符号）\n\\s       匹配任意不是空白符的字符\n\\d       匹配任意非数字的字符\n\\b       匹配任意不是单词开头或结尾的位置\n[a]      匹配单个字符a\n[a-z]    匹配a-z小写字符的任意一个\n[^a]     匹配除了a以外的任意字符\n[^abc]   匹配除了abc这几个字母以外的任意字符\n\n\n# 实例\n\nlocation  = / {\n  # 精确匹配 / ，主机名后面不能带任何字符串\n  [ configuration a ] \n}\nlocation  / {\n  # 因为所有的地址都以 / 开头，所以这条规则将匹配到所有请求\n  # 但是正则和最长字符串会优先匹配\n  [ configuration b ] \n}\nlocation /documents/ {\n  # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索\n  # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条\n  [ configuration c ] \n}\n\nlocation ~ /documents/abc {\n  # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索\n  # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条\n  [ configuration cc ] \n}\n\nlocation ^~ /images/ {\n  # 匹配任何以 /images/ 开头的地址，匹配符合以后，停止往下搜索正则，采用这一条。\n  [ configuration d ] \n}\n\nlocation ~* \\.(gif|jpg|jpeg)$ {\n  # 匹配所有以 gif,jpg或jpeg 结尾的请求\n  # 然而，所有请求 /images/ 下的图片会被 config d 处理，因为 ^~ 到达不了这一条正则\n  [ configuration e ] \n}\n\nlocation /images/ {\n  # 字符匹配到 /images/，继续往下，会发现 ^~ 存在\n  [ configuration f ] \n}\n\nlocation /images/abc {\n  # 最长字符匹配到 /images/abc，继续往下，会发现 ^~ 存在\n  # f与g的放置顺序是没有关系的\n  [ configuration g ] \n}\n\nlocation ~ /images/abc/ {\n  # 只有去掉 config d 才有效：先最长匹配 config g 开头的地址，继续往下搜索，匹配到这一条正则，采用\n    [ configuration h ] \n}\n\nlocation ~* /js/.*/\\.js\n\n\n 1. 已=开头表示精确匹配\n 2. 如 a 中只匹配根目录结尾的请求，后面不能带任何字符串。\n 3. ^~ 开头表示uri以某个常规字符串开头，不是正则匹配\n 4. ~ 开头表示区分大小写的正则匹配;\n 5. ~* 开头表示不区分大小写的正则匹配\n 6. / 通用匹配, 如果没有其它匹配,任何请求都会匹配到\n\n提示\n\n顺序 no优先级： (location =) > (location 完整路径) > (location ^~ 路径) > (location ~,~* 正则顺序) > (location 部分起始路径) > (/)\n\n上面的匹配结果 按照上面的location写法，以下的匹配示例成立：\n\n * / -> config a ：精确完全匹配，即使/index.html也匹配不了\n * /downloads/download.html -> config b ：匹配b以后，往下没有任何匹配，采用b\n * /images/1.gif -> configuration d ：匹配到f，往下匹配到d，停止往下\n * /images/abc/def -> config d：最长匹配到g，往下匹配d，停止往下你可以看到 任何以/images/开头的都会匹配到d并停止，fg写在这里是没有任何意义的，h是永远轮不到的，这里只是为了说明匹配顺序\n * /documents/document.html -> config c ：匹配到c，往下没有任何匹配，采用c\n * /documents/1.jpg -> configuration e ：匹配到c，往下正则匹配到e\n * /documents/abc.jpg -> config cc ：最长匹配到c，往下正则顺序匹配到cc，不会往下到e\n\n\n# 实际使用建议\n\n所以实际使用中，个人觉得至少有三个匹配规则定义，如下：\n#直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。\n#这里是直接转发给后端应用服务器了，也可以是一个静态首页\n# 第一个必选规则\nlocation = / {\n    proxy_pass http://tomcat:8080/index\n}\n# 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项\n# 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用\nlocation ^~ /static/ {\n    root /webroot/static/;\n}\nlocation ~* \\.(gif|jpg|jpeg|png|css|js|ico)$ {\n    root /webroot/res/;\n}\n#第三个规则就是通用规则，用来转发动态请求到后端应用服务器\n#非静态文件请求就默认是动态请求，自己根据实际把握\n#毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了\nlocation / {\n    proxy_pass http://tomcat:8080/\n}\n\n\nhttp://tengine.taobao.org/book/chapter_02.html http://nginx.org/en/docs/http/ngx_http_rewrite_module.html\n\n\n# rewrite规则\n\nrewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。rewrite只能放在server{},location{},if{}中，并且只能对域名后边的除去传递的参数外的字符串起作用，例如 http://seanlook.com/a/we/index.php?id=1&u=str 只对/a/we/index.php重写。语法rewrite regex replacement [flag];\n\n如果相对域名或参数字符串起作用，可以使用全局变量匹配，也可以使用proxy_pass反向代理。\n\n表面看rewrite和location功能有点像，都能实现跳转，主要区别在于rewrite是在同一域名内更改获取资源的路径，而location是对一类路径做控制访问或反向代理，可以proxy_pass到其他机器。很多情况下rewrite也会写在location里，它们的执行顺序是：\n\n 1. 执行server块的rewrite指令\n 2. 执行location匹配\n 3. 执行选定的location中的rewrite指令\n\n如果其中某步uri被重写，则重新循环执行1-3，直到找到真实存在的文件；循环超过10次，则返回500 internal server error错误。\n\n\n# flag标志位\n\n * last : 相当于apache的[l]标记，表示完成rewrite\n * break : 停止执行当前虚拟主机的后续rewrite指令集\n * redirect : 返回302临时重定向，地址栏会显示跳转后的地址\n * permanent : 返回301永久重定向，地址栏会显示跳转后的地址\n\n因为301和302不能简单的只返回状态码，还必须有重定向的url，这就是return指令无法返回301,302的原因了。这里 last 和 break 区别有点难以理解：\n\n 1. last一般写在server和if中，而break一般使用在location中\n 2. last不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后的匹配\n 3. break和last都能组织继续执行后面的rewrite指令\n\n\n# if指令与全局变量\n\nif判断指令 语法为if(condition){...}，对给定的条件condition进行判断。如果为真，大括号内的rewrite指令将被执行，if条件(conditon)可以是如下任何内容：\n\n * 当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false\n * 直接比较变量和内容时，使用=或!=\n * ~正则表达式匹配，~*不区分大小写的匹配，!~区分大小写的不匹配\n\n-f和!-f用来判断是否存在文件 -d和!-d用来判断是否存在目录 -e和!-e用来判断是否存在文件或目录 -x和!-x用来判断文件是否可执行\n\n例如：\n\nif ($http_user_agent ~ msie) {\n    rewrite ^(.*)$ /msie/$1 break;\n} //如果ua包含"msie"，rewrite请求到/msid/目录下\n\nif ($http_cookie ~* "id=([^;]+)(?:;|$)") {\n    set $id $1;\n } //如果cookie匹配正则，设置变量$id等于正则引用部分\n\nif ($request_method = post) {\n    return 405;\n} //如果提交方法为post，则返回状态405（method not allowed）。return不能返回301,302\n\nif ($slow) {\n    limit_rate 10k;\n} //限速，$slow可以通过 set 指令设置\n\nif (!-f $request_filename){\n    break;\n    proxy_pass  http://127.0.0.1; \n} //如果请求的文件名不存在，则反向代理到localhost 。这里的break也是停止rewrite检查\n\nif ($args ~ post=140){\n    rewrite ^ http://example.com/ permanent;\n} //如果query string中包含"post=140"，永久重定向到example.com\n\nlocation ~* \\.(gif|jpg|png|swf|flv)$ {\n    valid_referers none blocked www.jefflei.com www.leizhenfang.com;\n    if ($invalid_referer) {\n        return 404;\n    } //防盗链\n}\n\n\n\n# rewrite实例\n\n例1：\n\nhttp {\n    # 定义image日志格式\n    log_format imagelog \'[$time_local] \' $image_file \' \' $image_type \' \' $body_bytes_sent \' \' $status;\n    # 开启重写日志\n    rewrite_log on;\n\n    server {\n        root /home/www;\n\n        location / {\n                # 重写规则信息\n                error_log logs/rewrite.log notice; \n                # 注意这里要用‘’单引号引起来，避免{}\n                rewrite \'^/images/([a-z]{2})/([a-z0-9]{5})/(.*)\\.(png|jpg|gif)$\' /data?file=$3.$4;\n                # 注意不能在上面这条规则后面加上“last”参数，否则下面的set指令不会执行\n                set $image_file $3;\n                set $image_type $4;\n        }\n\n        location /data {\n                # 指定针对图片的日志格式，来分析图片类型和大小\n                access_log logs/images.log mian;\n                root /data/images;\n                # 应用前面定义的变量。判断首先文件在不在，不在再判断目录在不在，如果还不在就跳转到最后一个url里\n                try_files /$arg_file /image404.html;\n        }\n        location = /image404.html {\n                # 图片不存在返回特定的信息\n                return 404 "image not found\\n";\n        }\n}\n\n\n对形如/images/ef/uh7b3/test.png的请求，重写到/data?file=test.png，于是匹配到location /data，先看/data/images/test.png文件存不存在，如果存在则正常响应，如果不存在则重写tryfiles到新的image404 location，直接返回404状态码。\n\n例2：\n\nrewrite ^/images/(.*)_(\\d+)x(\\d+)\\.(png|jpg|gif)$ /resizer/$1.$4?width=$2&height=$3? last;\n\n\n对形如/images/bla_500x400.jpg的文件请求，重写到/resizer/bla.jpg?width=500&height=400地址，并会继续尝试匹配location。\n\n这个location\n\nlocation ~ ^/(.+)\\.3gp\\.zip$ {\n  # access_by_lua_file  "/opt/pro/nginx/lua/zip_access.lua";\n  rewrite_by_lua_file "/opt/pro/nginx/lua/zip_access.lua";\n}\n\n\n匹配 https://www.ivoov.com/vod/ts01/tvm/video/3gp/tvm/hunanhd/2016/04/27/80a4b71a-c000-46fa-916b-70d9e2445635/q350/q350.3gp.zip?&end=5',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"什么是MongoDB",frontmatter:{title:"什么是MongoDB",date:"2022-02-09T11:25:35.000Z",permalink:"/pages/035a1b/"},regularPath:"/11.MongoDB/01.MongoDB/01.%E4%BB%80%E4%B9%88%E6%98%AFMongoDB.html",relativePath:"11.MongoDB/01.MongoDB/01.什么是MongoDB.md",key:"v-29934bff",path:"/pages/035a1b/",headers:[{level:2,title:"主要特点",slug:"主要特点",normalizedTitle:"主要特点",charIndex:212},{level:2,title:"历史",slug:"历史",normalizedTitle:"历史",charIndex:927},{level:2,title:"MongoDB 下载",slug:"mongodb-下载",normalizedTitle:"mongodb 下载",charIndex:1204},{level:2,title:"语言支持",slug:"语言支持",normalizedTitle:"语言支持",charIndex:1440},{level:2,title:"MongoDB 工具",slug:"mongodb-工具",normalizedTitle:"mongodb 工具",charIndex:1597},{level:3,title:"监控",slug:"监控",normalizedTitle:"监控",charIndex:1633},{level:3,title:"GUI",slug:"gui",normalizedTitle:"gui",charIndex:1799},{level:2,title:"MongoDB 应用案例",slug:"mongodb-应用案例",normalizedTitle:"mongodb 应用案例",charIndex:2079}],headersStr:"主要特点 历史 MongoDB 下载 语言支持 MongoDB 工具 监控 GUI MongoDB 应用案例",content:'# 什么是MongoDB\n\nMongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。\n\n在高负载的情况下，添加更多的节点，可以保证服务器性能。\n\nMongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。\n\nMongoDB 将数据存储为一个文档，数据结构由键值(key=>value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。\n\n\n\n\n# 主要特点\n\n * MongoDB 是一个面向文档存储的数据库，操作起来比较简单和容易。\n * 你可以在MongoDB记录中设置任何属性的索引 (如：FirstName="Sameer",Address="8 Gandhi Road")来实现更快的排序。\n * 你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。\n * 如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。\n * Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。\n * MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。\n * Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。\n * Map和Reduce。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。\n * Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。\n * GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。\n * MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。\n * MongoDB支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。\n * MongoDB安装简单。\n\n\n# 历史\n\n * 2007年10月，MongoDB由10gen团队所发展。2009年2月首度推出。\n * 2012年05月23日，MongoDB2.1 开发分支发布了! 该版本采用全新架构，包含诸多增强。\n * 2012年06月06日，MongoDB 2.0.6 发布，分布式文档数据库。\n * 2013年04月23日，MongoDB 2.4.3 发布，此版本包括了一些性能优化，功能增强以及bug修复。\n * 2013年08月20日，MongoDB 2.4.6 发布。\n * 2013年11月01日，MongoDB 2.4.8 发布。\n * ……\n\n\n# MongoDB 下载\n\n你可以在mongodb官网下载该安装包，地址为：https://www.mongodb.com/download-center#community。MonggoDB支持以下平台:\n\n * OS X 32-bit\n * OS X 64-bit\n * Linux 32-bit\n * Linux 64-bit\n * Windows 32-bit\n * Windows 64-bit\n * Solaris i86pc\n * Solaris 64\n\n\n# 语言支持\n\nMongoDB有官方的驱动如下：\n\n * C\n * C++\n * C# / .NET\n * Erlang\n * Haskell\n * Java\n * JavaScript\n * Lisp\n * node.JS\n * Perl\n * PHP\n * Python\n * Ruby\n * Scala\n\n\n# MongoDB 工具\n\n有几种可用于MongoDB的管理工具。\n\n\n# 监控\n\nMongoDB提供了网络和系统监控工具Munin，它作为一个插件应用于MongoDB中。\n\nGangila是MongoDB高性能的系统监视的工具，它作为一个插件应用于MongoDB中。\n\n基于图形界面的开源工具 Cacti, 用于查看CPU负载, 网络带宽利用率,它也提供了一个应用于监控 MongoDB 的插件。\n\n\n# GUI\n\n * Fang of Mongo – 网页式,由Django和jQuery所构成。\n * Futon4Mongo – 一个CouchDB Futon web的mongodb山寨版。\n * Mongo3 – Ruby写成。\n * MongoHub – 适用于OSX的应用程序。\n * Opricot – 一个基于浏览器的MongoDB控制台, 由PHP撰写而成。\n * Database Master — Windows的mongodb管理工具\n * RockMongo — 最好的PHP语言的MongoDB管理工具，轻量级, 支持多国语言.\n\n\n# MongoDB 应用案例\n\n下面列举一些公司MongoDB的实际应用：\n\n * Craiglist上使用MongoDB的存档数十亿条记录。\n * FourSquare，基于位置的社交网站，在Amazon EC2的服务器上使用MongoDB分享数据。\n * Shutterfly，以互联网为基础的社会和个人出版服务，使用MongoDB的各种持久性数据存储的要求。\n * bit.ly, 一个基于Web的网址缩短服务，使用MongoDB的存储自己的数据。\n * spike.com，一个MTV网络的联营公司， spike.com使用MongoDB的。\n * Intuit公司，一个为小企业和个人的软件和服务提供商，为小型企业使用MongoDB的跟踪用户的数据。\n * sourceforge.net，资源网站查找，创建和发布开源软件免费，使用MongoDB的后端存储。\n * etsy.com ，一个购买和出售手工制作物品网站，使用MongoDB。\n * 纽约时报，领先的在线新闻门户网站之一，使用MongoDB。\n * CERN，著名的粒子物理研究所，欧洲核子研究中心大型强子对撞机的数据使用MongoDB。',normalizedContent:'# 什么是mongodb\n\nmongodb 是由c++语言编写的，是一个基于分布式文件存储的开源数据库系统。\n\n在高负载的情况下，添加更多的节点，可以保证服务器性能。\n\nmongodb 旨在为web应用提供可扩展的高性能数据存储解决方案。\n\nmongodb 将数据存储为一个文档，数据结构由键值(key=>value)对组成。mongodb 文档类似于 json 对象。字段值可以包含其他文档，数组及文档数组。\n\n\n\n\n# 主要特点\n\n * mongodb 是一个面向文档存储的数据库，操作起来比较简单和容易。\n * 你可以在mongodb记录中设置任何属性的索引 (如：firstname="sameer",address="8 gandhi road")来实现更快的排序。\n * 你可以通过本地或者网络创建数据镜像，这使得mongodb有更强的扩展性。\n * 如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。\n * mongo支持丰富的查询表达式。查询指令使用json形式的标记，可轻易查询文档中内嵌的对象及数组。\n * mongodb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。\n * mongodb中的map/reduce主要是用来对数据进行批量处理和聚合操作。\n * map和reduce。map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给reduce函数进行处理。\n * map函数和reduce函数是使用javascript编写的，并可以通过db.runcommand或mapreduce命令来执行mapreduce操作。\n * gridfs是mongodb中的一个内置功能，可以用于存放大量小文件。\n * mongodb允许在服务端执行脚本，可以用javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。\n * mongodb支持各种编程语言:ruby，python，java，c++，php，c#等多种语言。\n * mongodb安装简单。\n\n\n# 历史\n\n * 2007年10月，mongodb由10gen团队所发展。2009年2月首度推出。\n * 2012年05月23日，mongodb2.1 开发分支发布了! 该版本采用全新架构，包含诸多增强。\n * 2012年06月06日，mongodb 2.0.6 发布，分布式文档数据库。\n * 2013年04月23日，mongodb 2.4.3 发布，此版本包括了一些性能优化，功能增强以及bug修复。\n * 2013年08月20日，mongodb 2.4.6 发布。\n * 2013年11月01日，mongodb 2.4.8 发布。\n * ……\n\n\n# mongodb 下载\n\n你可以在mongodb官网下载该安装包，地址为：https://www.mongodb.com/download-center#community。monggodb支持以下平台:\n\n * os x 32-bit\n * os x 64-bit\n * linux 32-bit\n * linux 64-bit\n * windows 32-bit\n * windows 64-bit\n * solaris i86pc\n * solaris 64\n\n\n# 语言支持\n\nmongodb有官方的驱动如下：\n\n * c\n * c++\n * c# / .net\n * erlang\n * haskell\n * java\n * javascript\n * lisp\n * node.js\n * perl\n * php\n * python\n * ruby\n * scala\n\n\n# mongodb 工具\n\n有几种可用于mongodb的管理工具。\n\n\n# 监控\n\nmongodb提供了网络和系统监控工具munin，它作为一个插件应用于mongodb中。\n\ngangila是mongodb高性能的系统监视的工具，它作为一个插件应用于mongodb中。\n\n基于图形界面的开源工具 cacti, 用于查看cpu负载, 网络带宽利用率,它也提供了一个应用于监控 mongodb 的插件。\n\n\n# gui\n\n * fang of mongo – 网页式,由django和jquery所构成。\n * futon4mongo – 一个couchdb futon web的mongodb山寨版。\n * mongo3 – ruby写成。\n * mongohub – 适用于osx的应用程序。\n * opricot – 一个基于浏览器的mongodb控制台, 由php撰写而成。\n * database master — windows的mongodb管理工具\n * rockmongo — 最好的php语言的mongodb管理工具，轻量级, 支持多国语言.\n\n\n# mongodb 应用案例\n\n下面列举一些公司mongodb的实际应用：\n\n * craiglist上使用mongodb的存档数十亿条记录。\n * foursquare，基于位置的社交网站，在amazon ec2的服务器上使用mongodb分享数据。\n * shutterfly，以互联网为基础的社会和个人出版服务，使用mongodb的各种持久性数据存储的要求。\n * bit.ly, 一个基于web的网址缩短服务，使用mongodb的存储自己的数据。\n * spike.com，一个mtv网络的联营公司， spike.com使用mongodb的。\n * intuit公司，一个为小企业和个人的软件和服务提供商，为小型企业使用mongodb的跟踪用户的数据。\n * sourceforge.net，资源网站查找，创建和发布开源软件免费，使用mongodb的后端存储。\n * etsy.com ，一个购买和出售手工制作物品网站，使用mongodb。\n * 纽约时报，领先的在线新闻门户网站之一，使用mongodb。\n * cern，著名的粒子物理研究所，欧洲核子研究中心大型强子对撞机的数据使用mongodb。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"CentOS7 离线安装MongoDB",frontmatter:{title:"CentOS7 离线安装MongoDB",date:"2022-02-09T11:25:34.000Z",permalink:"/pages/25c76e/"},regularPath:"/11.MongoDB/01.MongoDB/02.CentOS7%20%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85MongoDB.html",relativePath:"11.MongoDB/01.MongoDB/02.CentOS7 离线安装MongoDB.md",key:"v-6b185955",path:"/pages/25c76e/",headers:[{level:2,title:"软件版本",slug:"软件版本",normalizedTitle:"软件版本",charIndex:74},{level:2,title:"安装依赖",slug:"安装依赖",normalizedTitle:"安装依赖",charIndex:160},{level:2,title:"源码下载",slug:"源码下载",normalizedTitle:"源码下载",charIndex:204},{level:2,title:"准备安装",slug:"准备安装",normalizedTitle:"准备安装",charIndex:377},{level:2,title:"创建数据目录",slug:"创建数据目录",normalizedTitle:"创建数据目录",charIndex:651},{level:2,title:"配置mongodb.conf文件",slug:"配置mongodb-conf文件",normalizedTitle:"配置mongodb.conf文件",charIndex:824},{level:2,title:"配置开机自启动",slug:"配置开机自启动",normalizedTitle:"配置开机自启动",charIndex:1275},{level:2,title:"操作命令",slug:"操作命令",normalizedTitle:"操作命令",charIndex:1876},{level:2,title:"添加防火墙例外",slug:"添加防火墙例外",normalizedTitle:"添加防火墙例外",charIndex:2049},{level:2,title:"创建管理账户",slug:"创建管理账户",normalizedTitle:"创建管理账户",charIndex:2178},{level:2,title:"创建普通账户",slug:"创建普通账户",normalizedTitle:"创建普通账户",charIndex:2448}],headersStr:"软件版本 安装依赖 源码下载 准备安装 创建数据目录 配置mongodb.conf文件 配置开机自启动 操作命令 添加防火墙例外 创建管理账户 创建普通账户",content:'# CentOS7 离线安装MongoDB\n\nMongoDB 提供了 linux 各个发行版本 64 位的安装包，你可以在官网下载安装包。\n\n\n# 软件版本\n\n依赖        版本\n操作系统      CentOS Linux release 7.6.1810 (Core)\nMongoDB   4.4.2\n\n\n# 安装依赖\n\nsudo yum install libcurl openssl\n\n\n\n# 源码下载\n\n前往MongoDB官网下载所需的MongoDB版本，本次以当前最新版本MongoDB 4.4.2为例进行安装。\n\n\n\n点击Download按钮直接下载或使用wget下载\n\nwget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.4.2.tgz\n\n\n\n# 准备安装\n\n解压\n\ntar -zxvf mongodb-linux-x86_64-rhel70-4.4.2.tgz\n\n\n将解压包拷贝到指定目录\n\nmv mongodb-linux-x86_64-rhel70-4.4.2 /usr/local/mongodb\n\n\nMongoDB 的可执行文件位于 bin 目录下，所以可以将其添加到 PATH 路径中：\n\necho \'export PATH=/usr/local/mongodb/bin:$PATH\' >> /etc/profile  \n\n\n更新\n\nsource /etc/profile\n\n\n\n# 创建数据目录\n\n建立data文件夹，用于存放数据文件\n\nmkdir -p /usr/local/mongodb/data/db\n\n\n建立logs文件夹，用于存放日志文件\n\nmkdir /usr/local/mongodb/logs\n\n\n建立conf文件夹，用于存放配置文件\n\nmkdir /usr/local/mongodb/conf\n\n\n\n# 配置mongodb.conf文件\n\n进入conf目录，执行如下命令建立配置文件\n\nvi /usr/local/mongodb/conf/mongodb.conf\n\n\n添加如下内容：\n\nport=27017\ndbpath=/usr/local/mongodb/data/db\nlogpath=/usr/local/mongodb/logs/mongodb.log\nfork=true\nauth=true\nbind_ip = 0.0.0.0\n\n\nmongodb.conf文件描述如下：\n\n名称        描述\nport      监听的端口\ndbpath    数据存放路径\nlogpath   日志存放路径\nfork      是否启用后台运行 1. true启用后台运行 2. false禁止后台运行\nauth      是否需要验证登录权限登录（是否需要用户名密码）\nbind_ip   绑定IP地址，可以写成本机的IP也可以写成0.0.0.0，写成0.0.0.0的话就会监听所有的地址\n\n\n# 配置开机自启动\n\n创建mongodb.service文件\n\nvi /lib/systemd/system/mongodb.service\n\n\n添加如下内容：\n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n[Unit]\n \nDescription=mongodb\nAfter=network.target remote-fs.target nss-lookup.target\n \n[Service]\nType=forking\nExecStart=/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/conf/mongodb.conf\nExecReload=/bin/kill -s HUP $MAINPID\nExecStop=/usr/local/mongodb/bin/mongod --shutdown -f /usr/local/mongodb/conf/mongodb.conf\nPrivateTmp=true\n \n[Install]\nWantedBy=multi-user.target\n\n\n提示\n\n其中ExecStart参数与ExecStop参数中的路径要和安装的路径一致\n\n重新加载服务配置文件\n\nsystemctl daemon-reload\n\n\n添加至开机启动\n\nsystemctl enable mongodb.service\n\n\n\n# 操作命令\n\n启动服务\n\nsystemctl start mongodb.service\n\n\n停止服务\n\nsystemctl stop mongodb.service\n\n\n重启服务\n\nsystemctl restart mongodb.service\n\n\n查看服务状态\n\nsystemctl status mongodb.service\n\n\n\n# 添加防火墙例外\n\n使用root用户执行命令 开启27017 端口\n\nfirewall-cmd --zone=public --add-port=27017/tcp --permanent\n\n\n重新加载\n\nfirewall-cmd --reload\n\n\n\n# 创建管理账户\n\n直接使用mongo命令进行连接，默认端口是27017\n\nmongo\n\n\n创建验证用户, 提示rootpassword替换成你自己的密码\n\n> use admin\n> db.createUser({user:"root",pwd:"rootpassword",roles:[{role:"root",db:"admin"}]})\n\n\n登录验证，提示rootpassword替换成你自己的密码\n\nmongo -u root -p rootpassword --authenticationDatabase admin\n\n\n\n# 创建普通账户\n\n使用用户管理账户登录认证，提示rootpassword替换成你自己的密码\n\nmongo\n> use admin\n> db.auth(\'root\', \'rootpassword\')\n1\n\n\n切换数据库（保持管理账户登录状态）\n\n> use ivoov\n\n\n提示\n\n使用命令：use ivoov切换数据库，如果数据库不存在则自动创建。\n\n建立普通账户（保持管理账户登录状态）\n\n> db.createUser(\n{\n    "user": "ivoov",\n    "pwd": "ivoov.com",\n    "roles": [\n        {\n            "role": "readWrite",\n            "db": "ivoov"\n        }\n    ]\n})\n\n\n提示\n\n该操作创建了一个用户名为ivoov,密码为ivoov.com,所属数据库为ivoovdb 并赋予读写权限。\n\n普通用户登录\n\n> db.auth(\'ivoov\', \'ivoov.com\')\n1\n\n\n完成',normalizedContent:'# centos7 离线安装mongodb\n\nmongodb 提供了 linux 各个发行版本 64 位的安装包，你可以在官网下载安装包。\n\n\n# 软件版本\n\n依赖        版本\n操作系统      centos linux release 7.6.1810 (core)\nmongodb   4.4.2\n\n\n# 安装依赖\n\nsudo yum install libcurl openssl\n\n\n\n# 源码下载\n\n前往mongodb官网下载所需的mongodb版本，本次以当前最新版本mongodb 4.4.2为例进行安装。\n\n\n\n点击download按钮直接下载或使用wget下载\n\nwget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.4.2.tgz\n\n\n\n# 准备安装\n\n解压\n\ntar -zxvf mongodb-linux-x86_64-rhel70-4.4.2.tgz\n\n\n将解压包拷贝到指定目录\n\nmv mongodb-linux-x86_64-rhel70-4.4.2 /usr/local/mongodb\n\n\nmongodb 的可执行文件位于 bin 目录下，所以可以将其添加到 path 路径中：\n\necho \'export path=/usr/local/mongodb/bin:$path\' >> /etc/profile  \n\n\n更新\n\nsource /etc/profile\n\n\n\n# 创建数据目录\n\n建立data文件夹，用于存放数据文件\n\nmkdir -p /usr/local/mongodb/data/db\n\n\n建立logs文件夹，用于存放日志文件\n\nmkdir /usr/local/mongodb/logs\n\n\n建立conf文件夹，用于存放配置文件\n\nmkdir /usr/local/mongodb/conf\n\n\n\n# 配置mongodb.conf文件\n\n进入conf目录，执行如下命令建立配置文件\n\nvi /usr/local/mongodb/conf/mongodb.conf\n\n\n添加如下内容：\n\nport=27017\ndbpath=/usr/local/mongodb/data/db\nlogpath=/usr/local/mongodb/logs/mongodb.log\nfork=true\nauth=true\nbind_ip = 0.0.0.0\n\n\nmongodb.conf文件描述如下：\n\n名称        描述\nport      监听的端口\ndbpath    数据存放路径\nlogpath   日志存放路径\nfork      是否启用后台运行 1. true启用后台运行 2. false禁止后台运行\nauth      是否需要验证登录权限登录（是否需要用户名密码）\nbind_ip   绑定ip地址，可以写成本机的ip也可以写成0.0.0.0，写成0.0.0.0的话就会监听所有的地址\n\n\n# 配置开机自启动\n\n创建mongodb.service文件\n\nvi /lib/systemd/system/mongodb.service\n\n\n添加如下内容：\n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n[unit]\n \ndescription=mongodb\nafter=network.target remote-fs.target nss-lookup.target\n \n[service]\ntype=forking\nexecstart=/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/conf/mongodb.conf\nexecreload=/bin/kill -s hup $mainpid\nexecstop=/usr/local/mongodb/bin/mongod --shutdown -f /usr/local/mongodb/conf/mongodb.conf\nprivatetmp=true\n \n[install]\nwantedby=multi-user.target\n\n\n提示\n\n其中execstart参数与execstop参数中的路径要和安装的路径一致\n\n重新加载服务配置文件\n\nsystemctl daemon-reload\n\n\n添加至开机启动\n\nsystemctl enable mongodb.service\n\n\n\n# 操作命令\n\n启动服务\n\nsystemctl start mongodb.service\n\n\n停止服务\n\nsystemctl stop mongodb.service\n\n\n重启服务\n\nsystemctl restart mongodb.service\n\n\n查看服务状态\n\nsystemctl status mongodb.service\n\n\n\n# 添加防火墙例外\n\n使用root用户执行命令 开启27017 端口\n\nfirewall-cmd --zone=public --add-port=27017/tcp --permanent\n\n\n重新加载\n\nfirewall-cmd --reload\n\n\n\n# 创建管理账户\n\n直接使用mongo命令进行连接，默认端口是27017\n\nmongo\n\n\n创建验证用户, 提示rootpassword替换成你自己的密码\n\n> use admin\n> db.createuser({user:"root",pwd:"rootpassword",roles:[{role:"root",db:"admin"}]})\n\n\n登录验证，提示rootpassword替换成你自己的密码\n\nmongo -u root -p rootpassword --authenticationdatabase admin\n\n\n\n# 创建普通账户\n\n使用用户管理账户登录认证，提示rootpassword替换成你自己的密码\n\nmongo\n> use admin\n> db.auth(\'root\', \'rootpassword\')\n1\n\n\n切换数据库（保持管理账户登录状态）\n\n> use ivoov\n\n\n提示\n\n使用命令：use ivoov切换数据库，如果数据库不存在则自动创建。\n\n建立普通账户（保持管理账户登录状态）\n\n> db.createuser(\n{\n    "user": "ivoov",\n    "pwd": "ivoov.com",\n    "roles": [\n        {\n            "role": "readwrite",\n            "db": "ivoov"\n        }\n    ]\n})\n\n\n提示\n\n该操作创建了一个用户名为ivoov,密码为ivoov.com,所属数据库为ivoovdb 并赋予读写权限。\n\n普通用户登录\n\n> db.auth(\'ivoov\', \'ivoov.com\')\n1\n\n\n完成',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Minio快速入门指南",frontmatter:{title:"Minio快速入门指南",date:"2022-02-09T11:27:28.000Z",permalink:"/pages/76dc6a/"},regularPath:"/12.MinIO/01.Minio%E6%9C%8D%E5%8A%A1%E5%99%A8/01.Minio%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97.html",relativePath:"12.MinIO/01.Minio服务器/01.Minio快速入门指南.md",key:"v-9431b692",path:"/pages/76dc6a/",headers:[{level:2,title:"Docker 容器",slug:"docker-容器",normalizedTitle:"docker 容器",charIndex:210},{level:3,title:"稳定版",slug:"稳定版",normalizedTitle:"稳定版",charIndex:406},{level:2,title:"macOS",slug:"macos",normalizedTitle:"macos",charIndex:752},{level:3,title:"Homebrew",slug:"homebrew",normalizedTitle:"homebrew",charIndex:762},{level:3,title:"下载二进制文件",slug:"下载二进制文件",normalizedTitle:"下载二进制文件",charIndex:1114},{level:2,title:"GNU/Linux",slug:"gnu-linux",normalizedTitle:"gnu/linux",charIndex:1379},{level:2,title:"微软Windows系统",slug:"微软windows系统",normalizedTitle:"微软windows系统",charIndex:2269},{level:2,title:"部署建议",slug:"部署建议",normalizedTitle:"部署建议",charIndex:2827},{level:3,title:"允许防火墙的端口访问",slug:"允许防火墙的端口访问",normalizedTitle:"允许防火墙的端口访问",charIndex:2836},{level:3,title:"ufw",slug:"ufw",normalizedTitle:"ufw",charIndex:2912},{level:3,title:"firewall-cmd",slug:"firewall-cmd",normalizedTitle:"firewall-cmd",charIndex:3068},{level:3,title:"iptables",slug:"iptables",normalizedTitle:"iptables",charIndex:3393},{level:2,title:"已经存在的数据",slug:"已经存在的数据",normalizedTitle:"已经存在的数据",charIndex:3680},{level:2,title:"使用Minio浏览器进行验证",slug:"使用minio浏览器进行验证",normalizedTitle:"使用minio浏览器进行验证",charIndex:3836},{level:2,title:"使用Minio客户端 mc进行验证",slug:"使用minio客户端-mc进行验证",normalizedTitle:"使用minio客户端 mc进行验证",charIndex:3913}],headersStr:"Docker 容器 稳定版 macOS Homebrew 下载二进制文件 GNU/Linux 微软Windows系统 部署建议 允许防火墙的端口访问 ufw firewall-cmd iptables 已经存在的数据 使用Minio浏览器进行验证 使用Minio客户端 mc进行验证",content:'# Minio快速入门指南\n\nMinio 是一个基于Apache License v2.0开源协议的对象存储服务。它兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而一个对象文件可以是任意大小，从几kb到最大5T不等。\n\nMinio是一个非常轻量的服务,可以很简单的和其他应用的结合，类似 NodeJS, Redis 或者 MySQL。\n\n\n# Docker 容器\n\n使用以下命令将独立的 MinIO 服务器作为容器运行。\n\n独立的 MinIO 服务器最适合早期开发和评估。某些功能（例如版本控制、对象锁定和存储桶复制）需要使用擦除编码分布式部署 MinIO。对于扩展的开发和生产，通过编写代码启用擦除部署MinIO -具体而言，用最少的每MinIO服务器4个驱动器。有关更完整的文档，请参阅MinIO 擦除代码快速入门指南 。\n\n\n# 稳定版\n\n运行以下命令以使用临时数据卷将 MinIO 的最新稳定映像作为容器运行：\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  minio/minio server /data --console-address ":9001"\n\n\nMinIO 部署开始使用默认的根凭据minioadmin:minioadmin。您可以使用 MinIO 控制台测试部署，这是一个内置于 MinIO 服务器的嵌入式对象浏览器。将主机上运行的 Web 浏览器指向 http://127.0.0.1:9001 并使用 root 凭据登录。您可以使用浏览器来创建桶、上传对象以及浏览 MinIO 服务器的内容。\n\n更多Docker部署信息请访问 这里\n\n\n# macOS\n\n\n# Homebrew\n\n运行以下命令以使用Homebrew安装最新的稳定 MinIO 包。替换 /data 为您希望 MinIO 存储数据的驱动器或目录的路径。\n\nbrew install minio/stable/minio\nMINIO_ROOT_USER=admin MINIO_ROOT_PASSWORD=password minio server /mnt/data --console-address ":9001"\n\n\n提示\n\n如果你之前使用 brew install minio安装过minio, 可以用 minio/stable/minio 官方仓库重新安装 minio。\n\nbrew uninstall minio\nbrew install minio/stable/minio\n\n\n\n# 下载二进制文件\n\n使用以下命令在 macOS 上下载并运行独立的 MinIO 服务器。替换 /data 为您希望 MinIO 存储数据的驱动器或目录的路径。\n\nwget https://dl.min.io/server/minio/release/darwin-amd64/minio\nchmod +x minio\nMINIO_ROOT_USER=admin MINIO_ROOT_PASSWORD=password ./minio server /mnt/data --console-address ":9001"\n\n\n\n# GNU/Linux\n\n使用以下命令在运行 64 位 Intel/AMD 架构的 Linux 主机上运行独立的 MinIO 服务器。\n\nwget https://dl.min.io/server/minio/release/linux-amd64/minio\nchmod +x minio\nMINIO_ROOT_USER=admin MINIO_ROOT_PASSWORD=password ./minio server /mnt/data --console-address ":9001"\n\n\n替换 /data 为您希望 MinIO 存储数据的驱动器或目录的路径。\n\n下表列出了支持的架构。将wgetURL替换为 Linux 主机的架构。\n\n设置后台启动\n\nnohup /usr/local/bin/minio/minio server  /home/minio/data > /home/minio/data/minio.log 2>&1 &\n\n\n其中：/home/minio/data/minio.log 是启动时日志文件的存放位置（可以是任意目录），minio.log 需要我们自己手动创建。\n\nCPU架构                         URL\n64-bit Intel/AMD              https://dl.min.io/server/minio/release/linux-amd64/minio\n64-bit ARM                    https://dl.min.io/server/minio/release/linux-arm64/minio\n64-bit PowerPC LE (ppc64le)   https://dl.min.io/server/minio/release/linux-ppc64le/minio\nIBM Z-Series (S390X)          https://dl.min.io/server/minio/release/linux-s390x/minio\n\n\n# 微软Windows系统\n\n要在 64 位 Windows 主机上运行 MinIO，请从以下 URL 下载 MinIO 可执行文件：\n\nCPU架构              URL\n64-bit Intel/AMD   https://dl.minio.io/server/minio/release/windows-amd64/minio.exe\n\n使用以下命令在 Windows 主机上运行独立的 MinIO 服务器。替换 F:\\Data 为您希望 MinIO 存储数据的驱动器或目录的路径。您必须将终端或 powershell 目录更改为minio.exe可执行文件的位置，或将该目录的路径添加到系统$PATH：\n\nPS> Invoke-WebRequest -Uri "https://dl.min.io/server/minio/release/windows-amd64/minio.exe" -OutFile "C:\\minio.exe"\nPS> setx MINIO_ROOT_USER admin\nPS> setx MINIO_ROOT_PASSWORD password\nPS> C:\\minio.exe server F:\\Data --console-address ":9001"\n\n\n\n# 部署建议\n\n\n# 允许防火墙的端口访问\n\n默认情况下，MinIO 使用端口 9000 来侦听传入连接。如果您的平台默认阻止该端口，您可能需要启用对该端口的访问。\n\n\n# ufw\n\n对于启用了 ufw 的主机（基于 Debian 的发行版），您可以使用ufw命令来允许特定端口的流量。使用以下命令允许访问端口 9000\n\nufw allow 9000\n\n\n下面的命令启用所有传入端口的流量，范围从 9000 到 9010。\n\nufw allow 9000:9010/tcp\n\n\n\n# firewall-cmd\n\n对于启用了 firewall-cmd (CentOS) 的主机，您可以使用firewall-cmd命令来允许特定端口的流量。使用以下命令允许访问端口 9000\n\nfirewall-cmd --get-active-zones\n\n\n此命令获取活动区域。现在，将端口规则应用于上面返回的相关区域。例如，如果区域是public，请使用\n\nfirewall-cmd --zone=public --add-port=9000/tcp --permanent\n\n\n请注意，permanent确保规则在防火墙启动、重新启动或重新加载时是持久的。最后重新加载防火墙以使更改生效。\n\nfirewall-cmd --reload\n\n\n\n# iptables\n\n对于启用了 iptables 的主机（RHEL、CentOS 等），您可以使用iptables命令来启用进入特定端口的所有流量。使用以下命令允许 访问端口 9000\n\niptables -A INPUT -p tcp --dport 9000 -j ACCEPT\nservice iptables restart\n\n\n下面的命令启用所有传入端口的流量，范围从 9000 到 9010。\n\niptables -A INPUT -p tcp --dport 9000:9010 -j ACCEPT\nservice iptables restart\n\n\n\n# 已经存在的数据\n\n当在单块磁盘上部署Minio server,Minio server允许客户端访问数据目录下已经存在的数据。比如，如果Minio使用minio server /mnt/data启动，那么所有已经在/mnt/data目录下的数据都可以被客户端访问到。\n\n上述描述对所有网关后端同样有效。\n\n\n# 使用Minio浏览器进行验证\n\n安装后使用浏览器访问http://127.0.0.1:9001，如果可以访问，则表示minio已经安装成功。\n\n\n\n\n# 使用Minio客户端 mc进行验证\n\nmc 提供了一些UNIX常用命令的替代品，像ls, cat, cp, mirror, diff这些。 它支持文件系统和亚马逊S3云存储服务。 更多信息请参考 mc快速入门 。',normalizedContent:'# minio快速入门指南\n\nminio 是一个基于apache license v2.0开源协议的对象存储服务。它兼容亚马逊s3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而一个对象文件可以是任意大小，从几kb到最大5t不等。\n\nminio是一个非常轻量的服务,可以很简单的和其他应用的结合，类似 nodejs, redis 或者 mysql。\n\n\n# docker 容器\n\n使用以下命令将独立的 minio 服务器作为容器运行。\n\n独立的 minio 服务器最适合早期开发和评估。某些功能（例如版本控制、对象锁定和存储桶复制）需要使用擦除编码分布式部署 minio。对于扩展的开发和生产，通过编写代码启用擦除部署minio -具体而言，用最少的每minio服务器4个驱动器。有关更完整的文档，请参阅minio 擦除代码快速入门指南 。\n\n\n# 稳定版\n\n运行以下命令以使用临时数据卷将 minio 的最新稳定映像作为容器运行：\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  minio/minio server /data --console-address ":9001"\n\n\nminio 部署开始使用默认的根凭据minioadmin:minioadmin。您可以使用 minio 控制台测试部署，这是一个内置于 minio 服务器的嵌入式对象浏览器。将主机上运行的 web 浏览器指向 http://127.0.0.1:9001 并使用 root 凭据登录。您可以使用浏览器来创建桶、上传对象以及浏览 minio 服务器的内容。\n\n更多docker部署信息请访问 这里\n\n\n# macos\n\n\n# homebrew\n\n运行以下命令以使用homebrew安装最新的稳定 minio 包。替换 /data 为您希望 minio 存储数据的驱动器或目录的路径。\n\nbrew install minio/stable/minio\nminio_root_user=admin minio_root_password=password minio server /mnt/data --console-address ":9001"\n\n\n提示\n\n如果你之前使用 brew install minio安装过minio, 可以用 minio/stable/minio 官方仓库重新安装 minio。\n\nbrew uninstall minio\nbrew install minio/stable/minio\n\n\n\n# 下载二进制文件\n\n使用以下命令在 macos 上下载并运行独立的 minio 服务器。替换 /data 为您希望 minio 存储数据的驱动器或目录的路径。\n\nwget https://dl.min.io/server/minio/release/darwin-amd64/minio\nchmod +x minio\nminio_root_user=admin minio_root_password=password ./minio server /mnt/data --console-address ":9001"\n\n\n\n# gnu/linux\n\n使用以下命令在运行 64 位 intel/amd 架构的 linux 主机上运行独立的 minio 服务器。\n\nwget https://dl.min.io/server/minio/release/linux-amd64/minio\nchmod +x minio\nminio_root_user=admin minio_root_password=password ./minio server /mnt/data --console-address ":9001"\n\n\n替换 /data 为您希望 minio 存储数据的驱动器或目录的路径。\n\n下表列出了支持的架构。将wgeturl替换为 linux 主机的架构。\n\n设置后台启动\n\nnohup /usr/local/bin/minio/minio server  /home/minio/data > /home/minio/data/minio.log 2>&1 &\n\n\n其中：/home/minio/data/minio.log 是启动时日志文件的存放位置（可以是任意目录），minio.log 需要我们自己手动创建。\n\ncpu架构                         url\n64-bit intel/amd              https://dl.min.io/server/minio/release/linux-amd64/minio\n64-bit arm                    https://dl.min.io/server/minio/release/linux-arm64/minio\n64-bit powerpc le (ppc64le)   https://dl.min.io/server/minio/release/linux-ppc64le/minio\nibm z-series (s390x)          https://dl.min.io/server/minio/release/linux-s390x/minio\n\n\n# 微软windows系统\n\n要在 64 位 windows 主机上运行 minio，请从以下 url 下载 minio 可执行文件：\n\ncpu架构              url\n64-bit intel/amd   https://dl.minio.io/server/minio/release/windows-amd64/minio.exe\n\n使用以下命令在 windows 主机上运行独立的 minio 服务器。替换 f:\\data 为您希望 minio 存储数据的驱动器或目录的路径。您必须将终端或 powershell 目录更改为minio.exe可执行文件的位置，或将该目录的路径添加到系统$path：\n\nps> invoke-webrequest -uri "https://dl.min.io/server/minio/release/windows-amd64/minio.exe" -outfile "c:\\minio.exe"\nps> setx minio_root_user admin\nps> setx minio_root_password password\nps> c:\\minio.exe server f:\\data --console-address ":9001"\n\n\n\n# 部署建议\n\n\n# 允许防火墙的端口访问\n\n默认情况下，minio 使用端口 9000 来侦听传入连接。如果您的平台默认阻止该端口，您可能需要启用对该端口的访问。\n\n\n# ufw\n\n对于启用了 ufw 的主机（基于 debian 的发行版），您可以使用ufw命令来允许特定端口的流量。使用以下命令允许访问端口 9000\n\nufw allow 9000\n\n\n下面的命令启用所有传入端口的流量，范围从 9000 到 9010。\n\nufw allow 9000:9010/tcp\n\n\n\n# firewall-cmd\n\n对于启用了 firewall-cmd (centos) 的主机，您可以使用firewall-cmd命令来允许特定端口的流量。使用以下命令允许访问端口 9000\n\nfirewall-cmd --get-active-zones\n\n\n此命令获取活动区域。现在，将端口规则应用于上面返回的相关区域。例如，如果区域是public，请使用\n\nfirewall-cmd --zone=public --add-port=9000/tcp --permanent\n\n\n请注意，permanent确保规则在防火墙启动、重新启动或重新加载时是持久的。最后重新加载防火墙以使更改生效。\n\nfirewall-cmd --reload\n\n\n\n# iptables\n\n对于启用了 iptables 的主机（rhel、centos 等），您可以使用iptables命令来启用进入特定端口的所有流量。使用以下命令允许 访问端口 9000\n\niptables -a input -p tcp --dport 9000 -j accept\nservice iptables restart\n\n\n下面的命令启用所有传入端口的流量，范围从 9000 到 9010。\n\niptables -a input -p tcp --dport 9000:9010 -j accept\nservice iptables restart\n\n\n\n# 已经存在的数据\n\n当在单块磁盘上部署minio server,minio server允许客户端访问数据目录下已经存在的数据。比如，如果minio使用minio server /mnt/data启动，那么所有已经在/mnt/data目录下的数据都可以被客户端访问到。\n\n上述描述对所有网关后端同样有效。\n\n\n# 使用minio浏览器进行验证\n\n安装后使用浏览器访问http://127.0.0.1:9001，如果可以访问，则表示minio已经安装成功。\n\n\n\n\n# 使用minio客户端 mc进行验证\n\nmc 提供了一些unix常用命令的替代品，像ls, cat, cp, mirror, diff这些。 它支持文件系统和亚马逊s3云存储服务。 更多信息请参考 mc快速入门 。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Minio Docker快速入门",frontmatter:{title:"Minio Docker快速入门",date:"2022-02-09T11:27:22.000Z",permalink:"/pages/0cd85f/"},regularPath:"/12.MinIO/01.Minio%E6%9C%8D%E5%8A%A1%E5%99%A8/02.Minio%20Docker%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html",relativePath:"12.MinIO/01.Minio服务器/02.Minio Docker快速入门.md",key:"v-79bebf43",path:"/pages/0cd85f/",headers:[{level:2,title:"前提条件",slug:"前提条件",normalizedTitle:"前提条件",charIndex:23},{level:2,title:"在Docker中运行Minio单点模式。",slug:"在docker中运行minio单点模式。",normalizedTitle:"在docker中运行minio单点模式。",charIndex:61},{level:2,title:"在Docker中运行Minio分布式模式",slug:"在docker中运行minio分布式模式",normalizedTitle:"在docker中运行minio分布式模式",charIndex:1074},{level:2,title:"Minio Docker Tips",slug:"minio-docker-tips",normalizedTitle:"minio docker tips",charIndex:1290},{level:3,title:"Minio自定义Access和Secret密钥",slug:"minio自定义access和secret密钥",normalizedTitle:"minio自定义access和secret密钥",charIndex:1312},{level:3,title:"以普通用户身份运行 MinIO Docker",slug:"以普通用户身份运行-minio-docker",normalizedTitle:"以普通用户身份运行 minio docker",charIndex:1964},{level:3,title:"使用Docker secrets进行Minio Access和Secret密钥自定义",slug:"使用docker-secrets进行minio-access和secret密钥自定义",normalizedTitle:"使用docker secrets进行minio access和secret密钥自定义",charIndex:2427},{level:3,title:"获取容器ID",slug:"获取容器id",normalizedTitle:"获取容器id",charIndex:3523},{level:3,title:"启动和停止容器",slug:"启动和停止容器",normalizedTitle:"启动和停止容器",charIndex:3663},{level:3,title:"Minio容器日志",slug:"minio容器日志",normalizedTitle:"minio容器日志",charIndex:3796},{level:3,title:"监控MinioDocker容器",slug:"监控miniodocker容器",normalizedTitle:"监控miniodocker容器",charIndex:3869}],headersStr:"前提条件 在Docker中运行Minio单点模式。 在Docker中运行Minio分布式模式 Minio Docker Tips Minio自定义Access和Secret密钥 以普通用户身份运行 MinIO Docker 使用Docker secrets进行Minio Access和Secret密钥自定义 获取容器ID 启动和停止容器 Minio容器日志 监控MinioDocker容器",content:'# Minio Docker快速入门\n\n\n# 前提条件\n\n您的机器已经安装docker. 从 这里下载相关软件。\n\n\n# 在Docker中运行Minio单点模式。\n\nMinio 需要一个持久卷来存储配置和应用数据。不过, 如果只是为了测试一下, 您可以通过简单地传递一个目录（在下面的示例中为/ data）启动Minio。这个目录会在容器启动时在容器的文件系统中创建，不过所有的数据都会在容器退出时丢失。\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  -e "MINIO_ROOT_USER=AKIAIOSFODNN7EXAMPLE" \\\n  -e "MINIO_ROOT_PASSWORD=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY" \\\n  minio/minio server /data --console-address ":9001"\n\n\n要创建具有永久存储的Minio容器，您需要将本地持久目录从主机操作系统映射到虚拟配置~/.minio 并导出/data目录。 为此，请运行以下命令\n\n# GNU/Linux 和 macOS\n\n\n\n\n\n\n \n\n\n\n\n\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --name minio1 \\\n  -v /mnt/data:/data \\\n  -e "MINIO_ROOT_USER=AKIAIOSFODNN7EXAMPLE" \\\n  -e "MINIO_ROOT_PASSWORD=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY" \\\n  minio/minio server /data --console-address ":9001"\n\n\n\n# Windows\n\n\n\n\n\n\n \n\n\n\n\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --name minio1 \\\n  -v D:\\data:/data \\\n  -e "MINIO_ROOT_USER=AKIAIOSFODNN7EXAMPLE" \\\n  -e "MINIO_ROOT_PASSWORD=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY" \\\n  minio/minio server /data --console-address ":9001"\n\n\n\n# 在Docker中运行Minio分布式模式\n\n分布式Minio可以通过 Docker Compose 或者 Swarm mode进行部署。这两者之间的主要区别是Docker Compose创建了单个主机，多容器部署，而Swarm模式创建了一个多主机，多容器部署。\n\n这意味着Docker Compose可以让你快速的在你的机器上快速使用分布式Minio-非常适合开发，测试环境；而Swarm模式提供了更健壮，生产级别的部署。\n\n\n# Minio Docker Tips\n\n\n# Minio自定义Access和Secret密钥\n\n要覆盖Minio的自动生成的密钥，您可以将Access和Secret密钥设为环境变量。 Minio允许常规字符串作为Access和Secret密钥。\n\n# GNU/Linux 和 macOS\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --name minio1 \\\n  -e "MINIO_ROOT_USER=AKIAIOSFODNN7EXAMPLE" \\\n  -e "MINIO_ROOT_PASSWORD=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY" \\\n  -v /mnt/data:/data \\\n  minio/minio server /data --console-address ":9001"\n\n\n# Windows\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --name minio1 \\\n  -e "MINIO_ROOT_USER=AKIAIOSFODNN7EXAMPLE" \\\n  -e "MINIO_ROOT_PASSWORD=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY" \\\n  -v D:\\data:/data \\\n  minio/minio server /data --console-address ":9001"\n\n\n\n# 以普通用户身份运行 MinIO Docker\n\nDocker 提供标准化机制来以非 root 用户身份运行 Docker 容器。\n\n# GNU/Linux 和 macOS\n\n在 Linux 和 macOS 上，您可以使用--user普通用户来运行容器。\n\n注意\n\n确保--user在使用之前对 ${HOME}/data 有写权限 --user 。\n\nmkdir -p ${HOME}/data\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --user $(id -u):$(id -g) \\\n  --name minio1 \\\n  -e "MINIO_ROOT_USER=AKIAIOSFODNN7EXAMPLE" \\\n  -e "MINIO_ROOT_PASSWORD=wJalrXUtnFEMIK7MDENGbPxRfiCYEXAMPLEKEY" \\\n  -v ${HOME}/data:/data \\\n  minio/minio server /data\n\n\n\n# 使用Docker secrets进行Minio Access和Secret密钥自定义\n\n要覆盖Minio的自动生成的密钥,你可以把secret和access秘钥创建成Docker secrets. Minio允许常规字符串作为Access和Secret密钥。\n\necho "AKIAIOSFODNN7EXAMPLE" | docker secret create access_key -\necho "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY" | docker secret create secret_key -\n\n\n使用docker service创建Minio服务，并读取Docker secrets。\n\ndocker service create --name="minio-service" --secret="access_key" --secret="secret_key" minio/minio server /data\n\n\n更多 docker service信息，请访问 这里\n\n# MinIO 自定义访问和密钥文件\n\n要使用其他的 secret 名字运行您的服务，按照上面的说明替换access_key和secret_key为您的自定义名称（例如my_secret_key，my_custom_key）。\n\ndocker service create --name="minio-service" \\\n  --secret="my_access_key" \\\n  --secret="my_secret_key" \\\n  --env="MINIO_ROOT_USER_FILE=my_access_key" \\\n  --env="MINIO_ROOT_PASSWORD_FILE=my_secret_key" \\\n  minio/minio server /data\n\n\nMINIO_ROOT_USER_FILE 和 MINIO_ROOT_PASSWORD_FILE还支持自定义绝对路径，以防 Docker secrets 安装到自定义位置或使用其他工具将 secrets 安装到容器中。例如，HashiCorp Vault 将机密注入/vault/secrets. 使用上面的自定义名称，将环境变量设置为\n\nMINIO_ROOT_USER_FILE=/vault/secrets/my_access_key\nMINIO_ROOT_PASSWORD_FILE=/vault/secrets/my_secret_key\n\n\n\n# 获取容器ID\n\n在容器中使用Docker命令, 你需要知道这个容器的 容器ID 。 为了获取 Container ID, 运行\n\ndocker ps -a\n\n\n-a flag 确保你获取所有的容器(创建的，正在运行的，退出的)，然后从输出中识别Container ID。\n\n\n# 启动和停止容器\n\n启动容器,你可以使用 docker start 命令。\n\ndocker start <container_id>\n\n\n停止一下正在运行的容器, 使用 docker stop 命令。\n\ndocker stop <container_id>\n\n\n\n# Minio容器日志\n\n获取Minio日志，使用 docker logs 命令。\n\ndocker logs <container_id>\n\n\n\n# 监控MinioDocker容器\n\n监控Minio容器使用的资源,使用 docker stats 命令.\n\ndocker stats <container_id>\n',normalizedContent:'# minio docker快速入门\n\n\n# 前提条件\n\n您的机器已经安装docker. 从 这里下载相关软件。\n\n\n# 在docker中运行minio单点模式。\n\nminio 需要一个持久卷来存储配置和应用数据。不过, 如果只是为了测试一下, 您可以通过简单地传递一个目录（在下面的示例中为/ data）启动minio。这个目录会在容器启动时在容器的文件系统中创建，不过所有的数据都会在容器退出时丢失。\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  -e "minio_root_user=akiaiosfodnn7example" \\\n  -e "minio_root_password=wjalrxutnfemi/k7mdeng/bpxrficyexamplekey" \\\n  minio/minio server /data --console-address ":9001"\n\n\n要创建具有永久存储的minio容器，您需要将本地持久目录从主机操作系统映射到虚拟配置~/.minio 并导出/data目录。 为此，请运行以下命令\n\n# gnu/linux 和 macos\n\n\n\n\n\n\n \n\n\n\n\n\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --name minio1 \\\n  -v /mnt/data:/data \\\n  -e "minio_root_user=akiaiosfodnn7example" \\\n  -e "minio_root_password=wjalrxutnfemi/k7mdeng/bpxrficyexamplekey" \\\n  minio/minio server /data --console-address ":9001"\n\n\n\n# windows\n\n\n\n\n\n\n \n\n\n\n\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --name minio1 \\\n  -v d:\\data:/data \\\n  -e "minio_root_user=akiaiosfodnn7example" \\\n  -e "minio_root_password=wjalrxutnfemi/k7mdeng/bpxrficyexamplekey" \\\n  minio/minio server /data --console-address ":9001"\n\n\n\n# 在docker中运行minio分布式模式\n\n分布式minio可以通过 docker compose 或者 swarm mode进行部署。这两者之间的主要区别是docker compose创建了单个主机，多容器部署，而swarm模式创建了一个多主机，多容器部署。\n\n这意味着docker compose可以让你快速的在你的机器上快速使用分布式minio-非常适合开发，测试环境；而swarm模式提供了更健壮，生产级别的部署。\n\n\n# minio docker tips\n\n\n# minio自定义access和secret密钥\n\n要覆盖minio的自动生成的密钥，您可以将access和secret密钥设为环境变量。 minio允许常规字符串作为access和secret密钥。\n\n# gnu/linux 和 macos\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --name minio1 \\\n  -e "minio_root_user=akiaiosfodnn7example" \\\n  -e "minio_root_password=wjalrxutnfemi/k7mdeng/bpxrficyexamplekey" \\\n  -v /mnt/data:/data \\\n  minio/minio server /data --console-address ":9001"\n\n\n# windows\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --name minio1 \\\n  -e "minio_root_user=akiaiosfodnn7example" \\\n  -e "minio_root_password=wjalrxutnfemi/k7mdeng/bpxrficyexamplekey" \\\n  -v d:\\data:/data \\\n  minio/minio server /data --console-address ":9001"\n\n\n\n# 以普通用户身份运行 minio docker\n\ndocker 提供标准化机制来以非 root 用户身份运行 docker 容器。\n\n# gnu/linux 和 macos\n\n在 linux 和 macos 上，您可以使用--user普通用户来运行容器。\n\n注意\n\n确保--user在使用之前对 ${home}/data 有写权限 --user 。\n\nmkdir -p ${home}/data\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --user $(id -u):$(id -g) \\\n  --name minio1 \\\n  -e "minio_root_user=akiaiosfodnn7example" \\\n  -e "minio_root_password=wjalrxutnfemik7mdengbpxrficyexamplekey" \\\n  -v ${home}/data:/data \\\n  minio/minio server /data\n\n\n\n# 使用docker secrets进行minio access和secret密钥自定义\n\n要覆盖minio的自动生成的密钥,你可以把secret和access秘钥创建成docker secrets. minio允许常规字符串作为access和secret密钥。\n\necho "akiaiosfodnn7example" | docker secret create access_key -\necho "wjalrxutnfemi/k7mdeng/bpxrficyexamplekey" | docker secret create secret_key -\n\n\n使用docker service创建minio服务，并读取docker secrets。\n\ndocker service create --name="minio-service" --secret="access_key" --secret="secret_key" minio/minio server /data\n\n\n更多 docker service信息，请访问 这里\n\n# minio 自定义访问和密钥文件\n\n要使用其他的 secret 名字运行您的服务，按照上面的说明替换access_key和secret_key为您的自定义名称（例如my_secret_key，my_custom_key）。\n\ndocker service create --name="minio-service" \\\n  --secret="my_access_key" \\\n  --secret="my_secret_key" \\\n  --env="minio_root_user_file=my_access_key" \\\n  --env="minio_root_password_file=my_secret_key" \\\n  minio/minio server /data\n\n\nminio_root_user_file 和 minio_root_password_file还支持自定义绝对路径，以防 docker secrets 安装到自定义位置或使用其他工具将 secrets 安装到容器中。例如，hashicorp vault 将机密注入/vault/secrets. 使用上面的自定义名称，将环境变量设置为\n\nminio_root_user_file=/vault/secrets/my_access_key\nminio_root_password_file=/vault/secrets/my_secret_key\n\n\n\n# 获取容器id\n\n在容器中使用docker命令, 你需要知道这个容器的 容器id 。 为了获取 container id, 运行\n\ndocker ps -a\n\n\n-a flag 确保你获取所有的容器(创建的，正在运行的，退出的)，然后从输出中识别container id。\n\n\n# 启动和停止容器\n\n启动容器,你可以使用 docker start 命令。\n\ndocker start <container_id>\n\n\n停止一下正在运行的容器, 使用 docker stop 命令。\n\ndocker stop <container_id>\n\n\n\n# minio容器日志\n\n获取minio日志，使用 docker logs 命令。\n\ndocker logs <container_id>\n\n\n\n# 监控miniodocker容器\n\n监控minio容器使用的资源,使用 docker stats 命令.\n\ndocker stats <container_id>\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Minio纠删码快速入门",frontmatter:{title:"Minio纠删码快速入门",date:"2022-02-09T11:27:22.000Z",permalink:"/pages/6bd2f8/"},regularPath:"/12.MinIO/01.Minio%E6%9C%8D%E5%8A%A1%E5%99%A8/03.Minio%E7%BA%A0%E5%88%A0%E7%A0%81%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html",relativePath:"12.MinIO/01.Minio服务器/03.Minio纠删码快速入门.md",key:"v-bf7747fa",path:"/pages/6bd2f8/",headers:[{level:2,title:"什么是纠删码erasure code?",slug:"什么是纠删码erasure-code",normalizedTitle:"什么是纠删码erasure code?",charIndex:103},{level:2,title:"为什么纠删码有用?",slug:"为什么纠删码有用",normalizedTitle:"为什么纠删码有用?",charIndex:304},{level:2,title:"什么是位衰减bit rot保护?",slug:"什么是位衰减bit-rot保护",normalizedTitle:"什么是位衰减bit rot保护?",charIndex:524},{level:2,title:"驱动器如何用于擦除代码？",slug:"驱动器如何用于擦除代码",normalizedTitle:"驱动器如何用于擦除代码？",charIndex:721},{level:2,title:"Minio纠删码快速入门",slug:"minio纠删码快速入门-2",normalizedTitle:"minio纠删码快速入门",charIndex:2},{level:3,title:"1.  先决条件",slug:"_1-先决条件",normalizedTitle:"1.  先决条件",charIndex:null},{level:3,title:"2. 以纠删码模式运行Minio",slug:"_2-以纠删码模式运行minio",normalizedTitle:"2. 以纠删码模式运行minio",charIndex:1003},{level:3,title:"3. 验证是否设置成功",slug:"_3-验证是否设置成功",normalizedTitle:"3. 验证是否设置成功",charIndex:1530}],headersStr:"什么是纠删码erasure code? 为什么纠删码有用? 什么是位衰减bit rot保护? 驱动器如何用于擦除代码？ Minio纠删码快速入门 1.  先决条件 2. 以纠删码模式运行Minio 3. 验证是否设置成功",content:'# Minio纠删码快速入门\n\nMinio使用纠删码erasure code和校验和checksum来保护数据免受硬件故障和无声数据损坏。 即便您丢失一半数量（N/2）的硬盘，您仍然可以恢复数据。\n\n\n# 什么是纠删码erasure code?\n\n纠删码是一种恢复丢失和损坏数据的数学算法， Minio采用Reed-Solomon code将对象拆分成N/2数据和N/2 奇偶校验块。 这就意味着如果是12块盘，一个对象会被分成6个数据块、6个奇偶校验块，你可以丢失任意6块盘（不管其是存放的数据块还是奇偶校验块），你仍可以从剩下的盘中的数据进行恢复，是不是很NB，感兴趣的同学请翻墙google。\n\n\n# 为什么纠删码有用?\n\n纠删码的工作原理和RAID或者复制不同，像RAID6可以在损失两块盘的情况下不丢数据，而Minio纠删码可以在丢失一半的盘的情况下，仍可以保证数据安全。 而且Minio纠删码是作用在对象级别，可以一次恢复一个对象，而RAID是作用在卷级别，数据恢复时间很长。 Minio对每个对象单独编码，存储服务一经部署，通常情况下是不需要更换硬盘或者修复。Minio纠删码的设计目标是为了性能和尽可能的使用硬件加速。\n\n\n\n\n# 什么是位衰减bit rot保护?\n\n位衰减又被称为数据腐化Data Rot、无声数据损坏Silent Data Corruption,是目前硬盘数据的一种严重数据丢失问题。硬盘上的数据可能会神不知鬼不觉就损坏了，也没有什么错误日志。正所谓明枪易躲，暗箭难防，这种背地里犯的错比硬盘直接咔咔宕了还危险。 不过不用怕，Minio纠删码采用了高速BLAKE2 基于哈希的校验和来防范位衰减。\n\n\n# 驱动器如何用于擦除代码？\n\nMinIO 将您提供的驱动器分成4 到 16 个驱动器的擦除编码集。因此，您提供的驱动器数量必须是这些数字之一的倍数。每个对象都被写入一个单一的纠删码集。\n\nMinio 使用最大可能的 EC 集大小，该大小分为给定的驱动器数量。例如，18个驱动器配置为2组9个驱动器，24个驱动器配置为2组12个驱动器。这适用于将 MinIO 作为独立纠删码部署运行的场景。然而，在分布式设置中，选择基于节点（亲和性）的擦除条带大小。\n\n\n# Minio纠删码快速入门\n\n\n# 1. 先决条件\n\n安装 MinIO - MinIO 快速入门指南\n\n\n# 2. 以纠删码模式运行Minio\n\n示例: 使用Minio，在12个盘中启动Minio服务。\n\nMINIO_ROOT_USER=admin MINIO_ROOT_PASSWORD=password ./minio server /mnt/data{1...12} --console-address ":9001"\n\n\n示例: 使用Minio Docker镜像，在8块盘中启动Minio服务。\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --name minio \\\n  -v /mnt/data1:/data1 \\\n  -v /mnt/data2:/data2 \\\n  -v /mnt/data3:/data3 \\\n  -v /mnt/data4:/data4 \\\n  -v /mnt/data5:/data5 \\\n  -v /mnt/data6:/data6 \\\n  -v /mnt/data7:/data7 \\\n  -v /mnt/data8:/data8 \\\n  minio/minio server /data{1...8} --console-address ":9001"\n\n\n\n# 3. 验证是否设置成功\n\n你可以随意拔掉硬盘，看Minio是否可以正常读写。\n\n> 原文: https://docs.min.io/docs/minio-erasure-code-quickstart-guide.html',normalizedContent:'# minio纠删码快速入门\n\nminio使用纠删码erasure code和校验和checksum来保护数据免受硬件故障和无声数据损坏。 即便您丢失一半数量（n/2）的硬盘，您仍然可以恢复数据。\n\n\n# 什么是纠删码erasure code?\n\n纠删码是一种恢复丢失和损坏数据的数学算法， minio采用reed-solomon code将对象拆分成n/2数据和n/2 奇偶校验块。 这就意味着如果是12块盘，一个对象会被分成6个数据块、6个奇偶校验块，你可以丢失任意6块盘（不管其是存放的数据块还是奇偶校验块），你仍可以从剩下的盘中的数据进行恢复，是不是很nb，感兴趣的同学请翻墙google。\n\n\n# 为什么纠删码有用?\n\n纠删码的工作原理和raid或者复制不同，像raid6可以在损失两块盘的情况下不丢数据，而minio纠删码可以在丢失一半的盘的情况下，仍可以保证数据安全。 而且minio纠删码是作用在对象级别，可以一次恢复一个对象，而raid是作用在卷级别，数据恢复时间很长。 minio对每个对象单独编码，存储服务一经部署，通常情况下是不需要更换硬盘或者修复。minio纠删码的设计目标是为了性能和尽可能的使用硬件加速。\n\n\n\n\n# 什么是位衰减bit rot保护?\n\n位衰减又被称为数据腐化data rot、无声数据损坏silent data corruption,是目前硬盘数据的一种严重数据丢失问题。硬盘上的数据可能会神不知鬼不觉就损坏了，也没有什么错误日志。正所谓明枪易躲，暗箭难防，这种背地里犯的错比硬盘直接咔咔宕了还危险。 不过不用怕，minio纠删码采用了高速blake2 基于哈希的校验和来防范位衰减。\n\n\n# 驱动器如何用于擦除代码？\n\nminio 将您提供的驱动器分成4 到 16 个驱动器的擦除编码集。因此，您提供的驱动器数量必须是这些数字之一的倍数。每个对象都被写入一个单一的纠删码集。\n\nminio 使用最大可能的 ec 集大小，该大小分为给定的驱动器数量。例如，18个驱动器配置为2组9个驱动器，24个驱动器配置为2组12个驱动器。这适用于将 minio 作为独立纠删码部署运行的场景。然而，在分布式设置中，选择基于节点（亲和性）的擦除条带大小。\n\n\n# minio纠删码快速入门\n\n\n# 1. 先决条件\n\n安装 minio - minio 快速入门指南\n\n\n# 2. 以纠删码模式运行minio\n\n示例: 使用minio，在12个盘中启动minio服务。\n\nminio_root_user=admin minio_root_password=password ./minio server /mnt/data{1...12} --console-address ":9001"\n\n\n示例: 使用minio docker镜像，在8块盘中启动minio服务。\n\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  --name minio \\\n  -v /mnt/data1:/data1 \\\n  -v /mnt/data2:/data2 \\\n  -v /mnt/data3:/data3 \\\n  -v /mnt/data4:/data4 \\\n  -v /mnt/data5:/data5 \\\n  -v /mnt/data6:/data6 \\\n  -v /mnt/data7:/data7 \\\n  -v /mnt/data8:/data8 \\\n  minio/minio server /data{1...8} --console-address ":9001"\n\n\n\n# 3. 验证是否设置成功\n\n你可以随意拔掉硬盘，看minio是否可以正常读写。\n\n> 原文: https://docs.min.io/docs/minio-erasure-code-quickstart-guide.html',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"分布式Minio快速入门",frontmatter:{title:"分布式Minio快速入门",date:"2022-02-09T11:27:22.000Z",permalink:"/pages/8153b1/"},regularPath:"/12.MinIO/01.Minio%E6%9C%8D%E5%8A%A1%E5%99%A8/04.%E5%88%86%E5%B8%83%E5%BC%8FMinio%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html",relativePath:"12.MinIO/01.Minio服务器/04.分布式Minio快速入门.md",key:"v-2a4a0bb7",path:"/pages/8153b1/",headers:[{level:2,title:"分布式Minio有什么好处?",slug:"分布式minio有什么好处",normalizedTitle:"分布式minio有什么好处?",charIndex:90},{level:3,title:"数据保护",slug:"数据保护",normalizedTitle:"数据保护",charIndex:190},{level:3,title:"高可用性",slug:"高可用性",normalizedTitle:"高可用性",charIndex:404},{level:3,title:"一致性保证",slug:"一致性保证",normalizedTitle:"一致性保证",charIndex:667},{level:2,title:"1. 前提条件",slug:"_1-前提条件",normalizedTitle:"1. 前提条件",charIndex:966},{level:2,title:"2. 运行分布式Minio",slug:"_2-运行分布式minio",normalizedTitle:"2. 运行分布式minio",charIndex:1032},{level:2,title:"3. 验证",slug:"_3-验证",normalizedTitle:"3. 验证",charIndex:3070}],headersStr:"分布式Minio有什么好处? 数据保护 高可用性 一致性保证 1. 前提条件 2. 运行分布式Minio 3. 验证",content:'# 分布式Minio快速入门\n\n分布式Minio可以让你将多块硬盘（甚至在不同的机器上）组成一个对象存储服务。由于硬盘分布在不同的节点上，分布式Minio避免了单点故障。\n\n\n# 分布式Minio有什么好处?\n\n在大数据领域，通常的设计理念都是无中心和分布式。Minio分布式模式可以帮助你搭建一个高可用的对象存储服务，你可以使用这些存储设备，而不用考虑其真实物理位置。\n\n\n# 数据保护\n\n分布式 MinIO使用erasure code提供针对多个节点/驱动器故障和bit rot保护。由于分布式 MinIO 所需的最小磁盘为 4（与擦除编码所需的最小磁盘相同），因此在您启动分布式 MinIO 时，擦除代码会自动启动。\n\n如果一个或多个磁盘在 PutObject 或 NewMultipartUpload 操作开始时处于脱机状态，该对象将自动添加额外的数据保护位，以为这些对象提供额外的安全性。\n\n\n# 高可用性\n\n如果托管磁盘的服务器脱机，独立的 MinIO 服务器就会停机。相比之下，只要有m/2 个服务器或m*n/2 个或更多磁盘在线，具有m 个服务器和n 个磁盘的分布式 MinIO 设置将使您的数据安全。\n\n例如，每个节点有 200 个磁盘的 16 台服务器分布式设置将继续提供文件，默认配置下最多 4 个服务器可以离线，即大约 800 个磁盘关闭 MinIO 将继续读取和写入对象。\n\n请参考默认值的更多理解尺寸指南选择取决于你删除的条带大小在这里。可以使用storage classes.更改奇偶校验设置。\n\n\n# 一致性保证\n\n对于分布式和独立模式下的所有 I/O 操作，MinIO 遵循严格的先读后写和写后列表一致性模型。仅当您使用磁盘文件系统（例如 xfs、ext4 或 zfs 等）进行分布式设置时，才能保证此一致性模型。\n\n如果 MinIO 分布式设置在下面使用 NFS 卷，则不能保证 MinIO 将提供这些一致性保证，因为 NFS 不是设计上一致的文件系统（如果您必须使用 NFS，我们建议您至少使用 NFSv4 而不是 NFSv3）。\n\n\n# 开始吧\n\n如果你了解Minio单机模式的搭建的话，分布式搭建的流程基本一样，Minio服务基于命令行传入的参数自动切换成单机模式还是分布式模式。\n\n\n# 1. 前提条件\n\n安装 MinIO - [MinIO 快速入门指南](./02.Minio Docker快速入门.md)。\n\n\n# 2. 运行分布式Minio\n\n要启动分布式 MinIO 实例，您只需将驱动器位置作为参数传递给 minio 服务器命令。然后，您需要在所有参与节点上运行相同的命令。\n\n注意：\n\n * 所有运行分布式 MinIO 的节点都应该共享一个公共根凭证，以便节点相互连接和信任。为了实现这一目标，它建议以root用户和root密码导出为环境变量，MINIO_ROOT_USER和MINIO_ROOT_PASSWORD执行MinIO服务器命令之前，所有节点上。如果未导出，minioadmin/minioadmin则应使用默认凭据。\n * MinIO 创建每组4到16个驱动器的纠删码组。您提供的驱动器总数必须是这些数字之一的倍数。\n * MinIO 选择最大的 EC 集大小，它分为给定的驱动器总数或节点总数 - 确保保持均匀分布，即每个节点参与每组相同数量的驱动器。\n * 每个对象都写入单个 EC 集，因此分布在不超过 16 个驱动器上。\n * 建议所有运行分布式 MinIO 设置的节点是同构的，即相同的操作系统、相同的磁盘数量和相同的网络互连。\n * MinIO 分布式模式需要新目录。如果需要，驱动器可以与其他应用程序共享。您可以通过使用 MinIO 专有的子目录来完成此操作。例如，如果您在/export下安装了您的卷，则可以将/export/data作为参数传递给 MinIO 服务器。\n * 下面的 IP 地址和驱动器路径仅用于演示目的，您需要将它们替换为实际的 IP 地址和驱动器路径或文件夹。\n * 运行分布式 MinIO 实例的服务器之间的间隔应小于 15 分钟。您可以启用NTP服务作为最佳实践，以确保跨服务器的时间相同。\n * MINIO_DOMAIN 应定义和导出环境变量以支持存储桶 DNS 样式。\n * 在Windows操作系统上运行分布式 MinIO被认为是实验性的。请谨慎行事。\n\n示例1：启动分布式MinIO例如在n个节点，其中m个驱动每个安装在/export1到/exportm（如图下文），通过所有n个节点上运行此命令：\n\n\n\n# GNU/Linux 和 macOS\n\nexport MINIO_ROOT_USER=<ACCESS_KEY>\nexport MINIO_ROOT_PASSWORD=<SECRET_KEY>\nminio server http://192.168.1.1{1...n}/export{1...m} --console-address ":9001"\n\n\n注意\n\n在上面的例子中n，m代表正整数，不要复制粘贴，根据本地部署和设置进行更改。\n\n特别注意{1...n}显示有 3 个点！仅使用 2 个{1..n}点将被您的 shell 解释并且不会传递到 MinIO 服务器，从而影响擦除编码顺序，从而影响性能和高可用性。始终使用省略号语法{1...n}（3 个点！）以获得最佳纠删码分布\n\n\n\n示例2: 启动分布式Minio实例，4节点，每节点4块盘，需要在4个节点上都运行下面的命令。\n\n# GNU/Linux 和 macOS\n\nexport MINIO_ACCESS_KEY=<ACCESS_KEY>\nexport MINIO_SECRET_KEY=<SECRET_KEY>\nminio server http://192.168.1.1{1...4}/export{1...4}\n\n\n\n\n# 扩展现有的分布式设置\n\nMinIO 支持通过在命令行上指定新的集群集来扩展分布式纠删码集群，如下所示：\n\nexport MINIO_ROOT_USER=<ACCESS_KEY>\nexport MINIO_ROOT_PASSWORD=<SECRET_KEY>\nminio server http://host{1...n}/export{1...m} http://host{o...z}/export{1...m}\n\n\n例如：\n\nminio server http://host{1...4}/export{1...16} http://host{5...12}/export{1...16}\n\n\n现在服务器已将总存储扩展了(newly_ added_servers*m)个磁盘，使总数达到(existing_servers*m)+(newly_added_servers*m) 个磁盘。新的对象上传请求会自动使用最少使用的集群开始。这种扩展策略无休止地工作，因此您可以根据需要永久扩展集群。当您重新启动时，它会立即且不会中断应用程序。命令行中的每组服务器称为一个池。本示例中有 2 个服务器池。新对象按照每个池中的可用空间量按比例放置在服务器池中。在每个池中，驱动器擦除集的位置是根据确定性散列算法确定的。\n\n注意\n\n您添加的每个池都必须与原始池具有相同的纠删码奇偶校验配置，以便保持相同的数据冗余 SLA。\n\n\n# 3. 验证\n\n验证是否部署成功，使用浏览器访问Minio服务或者使用 mc。多个节点的存储容量和就是分布式Minio的存储容量。',normalizedContent:'# 分布式minio快速入门\n\n分布式minio可以让你将多块硬盘（甚至在不同的机器上）组成一个对象存储服务。由于硬盘分布在不同的节点上，分布式minio避免了单点故障。\n\n\n# 分布式minio有什么好处?\n\n在大数据领域，通常的设计理念都是无中心和分布式。minio分布式模式可以帮助你搭建一个高可用的对象存储服务，你可以使用这些存储设备，而不用考虑其真实物理位置。\n\n\n# 数据保护\n\n分布式 minio使用erasure code提供针对多个节点/驱动器故障和bit rot保护。由于分布式 minio 所需的最小磁盘为 4（与擦除编码所需的最小磁盘相同），因此在您启动分布式 minio 时，擦除代码会自动启动。\n\n如果一个或多个磁盘在 putobject 或 newmultipartupload 操作开始时处于脱机状态，该对象将自动添加额外的数据保护位，以为这些对象提供额外的安全性。\n\n\n# 高可用性\n\n如果托管磁盘的服务器脱机，独立的 minio 服务器就会停机。相比之下，只要有m/2 个服务器或m*n/2 个或更多磁盘在线，具有m 个服务器和n 个磁盘的分布式 minio 设置将使您的数据安全。\n\n例如，每个节点有 200 个磁盘的 16 台服务器分布式设置将继续提供文件，默认配置下最多 4 个服务器可以离线，即大约 800 个磁盘关闭 minio 将继续读取和写入对象。\n\n请参考默认值的更多理解尺寸指南选择取决于你删除的条带大小在这里。可以使用storage classes.更改奇偶校验设置。\n\n\n# 一致性保证\n\n对于分布式和独立模式下的所有 i/o 操作，minio 遵循严格的先读后写和写后列表一致性模型。仅当您使用磁盘文件系统（例如 xfs、ext4 或 zfs 等）进行分布式设置时，才能保证此一致性模型。\n\n如果 minio 分布式设置在下面使用 nfs 卷，则不能保证 minio 将提供这些一致性保证，因为 nfs 不是设计上一致的文件系统（如果您必须使用 nfs，我们建议您至少使用 nfsv4 而不是 nfsv3）。\n\n\n# 开始吧\n\n如果你了解minio单机模式的搭建的话，分布式搭建的流程基本一样，minio服务基于命令行传入的参数自动切换成单机模式还是分布式模式。\n\n\n# 1. 前提条件\n\n安装 minio - [minio 快速入门指南](./02.minio docker快速入门.md)。\n\n\n# 2. 运行分布式minio\n\n要启动分布式 minio 实例，您只需将驱动器位置作为参数传递给 minio 服务器命令。然后，您需要在所有参与节点上运行相同的命令。\n\n注意：\n\n * 所有运行分布式 minio 的节点都应该共享一个公共根凭证，以便节点相互连接和信任。为了实现这一目标，它建议以root用户和root密码导出为环境变量，minio_root_user和minio_root_password执行minio服务器命令之前，所有节点上。如果未导出，minioadmin/minioadmin则应使用默认凭据。\n * minio 创建每组4到16个驱动器的纠删码组。您提供的驱动器总数必须是这些数字之一的倍数。\n * minio 选择最大的 ec 集大小，它分为给定的驱动器总数或节点总数 - 确保保持均匀分布，即每个节点参与每组相同数量的驱动器。\n * 每个对象都写入单个 ec 集，因此分布在不超过 16 个驱动器上。\n * 建议所有运行分布式 minio 设置的节点是同构的，即相同的操作系统、相同的磁盘数量和相同的网络互连。\n * minio 分布式模式需要新目录。如果需要，驱动器可以与其他应用程序共享。您可以通过使用 minio 专有的子目录来完成此操作。例如，如果您在/export下安装了您的卷，则可以将/export/data作为参数传递给 minio 服务器。\n * 下面的 ip 地址和驱动器路径仅用于演示目的，您需要将它们替换为实际的 ip 地址和驱动器路径或文件夹。\n * 运行分布式 minio 实例的服务器之间的间隔应小于 15 分钟。您可以启用ntp服务作为最佳实践，以确保跨服务器的时间相同。\n * minio_domain 应定义和导出环境变量以支持存储桶 dns 样式。\n * 在windows操作系统上运行分布式 minio被认为是实验性的。请谨慎行事。\n\n示例1：启动分布式minio例如在n个节点，其中m个驱动每个安装在/export1到/exportm（如图下文），通过所有n个节点上运行此命令：\n\n\n\n# gnu/linux 和 macos\n\nexport minio_root_user=<access_key>\nexport minio_root_password=<secret_key>\nminio server http://192.168.1.1{1...n}/export{1...m} --console-address ":9001"\n\n\n注意\n\n在上面的例子中n，m代表正整数，不要复制粘贴，根据本地部署和设置进行更改。\n\n特别注意{1...n}显示有 3 个点！仅使用 2 个{1..n}点将被您的 shell 解释并且不会传递到 minio 服务器，从而影响擦除编码顺序，从而影响性能和高可用性。始终使用省略号语法{1...n}（3 个点！）以获得最佳纠删码分布\n\n\n\n示例2: 启动分布式minio实例，4节点，每节点4块盘，需要在4个节点上都运行下面的命令。\n\n# gnu/linux 和 macos\n\nexport minio_access_key=<access_key>\nexport minio_secret_key=<secret_key>\nminio server http://192.168.1.1{1...4}/export{1...4}\n\n\n\n\n# 扩展现有的分布式设置\n\nminio 支持通过在命令行上指定新的集群集来扩展分布式纠删码集群，如下所示：\n\nexport minio_root_user=<access_key>\nexport minio_root_password=<secret_key>\nminio server http://host{1...n}/export{1...m} http://host{o...z}/export{1...m}\n\n\n例如：\n\nminio server http://host{1...4}/export{1...16} http://host{5...12}/export{1...16}\n\n\n现在服务器已将总存储扩展了(newly_ added_servers*m)个磁盘，使总数达到(existing_servers*m)+(newly_added_servers*m) 个磁盘。新的对象上传请求会自动使用最少使用的集群开始。这种扩展策略无休止地工作，因此您可以根据需要永久扩展集群。当您重新启动时，它会立即且不会中断应用程序。命令行中的每组服务器称为一个池。本示例中有 2 个服务器池。新对象按照每个池中的可用空间量按比例放置在服务器池中。在每个池中，驱动器擦除集的位置是根据确定性散列算法确定的。\n\n注意\n\n您添加的每个池都必须与原始池具有相同的纠删码奇偶校验配置，以便保持相同的数据冗余 sla。\n\n\n# 3. 验证\n\n验证是否部署成功，使用浏览器访问minio服务或者使用 mc。多个节点的存储容量和就是分布式minio的存储容量。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"如何使用 TLS 保护对 MinIO 服务器的访问",frontmatter:{title:"如何使用 TLS 保护对 MinIO 服务器的访问",date:"2022-02-09T11:27:22.000Z",permalink:"/pages/98cf59/"},regularPath:"/12.MinIO/01.Minio%E6%9C%8D%E5%8A%A1%E5%99%A8/05.%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%20TLS%20%E4%BF%9D%E6%8A%A4%E5%AF%B9%20MinIO%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E8%AE%BF%E9%97%AE.html",relativePath:"12.MinIO/01.Minio服务器/05.如何使用 TLS 保护对 MinIO 服务器的访问.md",key:"v-7f6889c6",path:"/pages/98cf59/",headers:[{level:2,title:"1.安装MinIO服务器",slug:"_1-安装minio服务器",normalizedTitle:"1.安装minio服务器",charIndex:88},{level:2,title:"2. 在 MinIO 中使用现有的密钥和证书",slug:"_2-在-minio-中使用现有的密钥和证书",normalizedTitle:"2. 在 minio 中使用现有的密钥和证书",charIndex:142},{level:2,title:"3. 使用 MinIO 生成和使用自签名密钥和证书",slug:"_3-使用-minio-生成和使用自签名密钥和证书",normalizedTitle:"3. 使用 minio 生成和使用自签名密钥和证书",charIndex:550},{level:3,title:"3.1certgen用于生成证书",slug:"_3-1certgen用于生成证书",normalizedTitle:"3.1certgen用于生成证书",charIndex:786},{level:3,title:"3.2 使用 OpenSSL 生成证书",slug:"_3-2-使用-openssl-生成证书",normalizedTitle:"3.2 使用 openssl 生成证书",charIndex:623},{level:3,title:"3.3 使用 GnuTLS (for Windows) 生成证书",slug:"_3-3-使用-gnutls-for-windows-生成证书",normalizedTitle:"3.3 使用 gnutls (for windows) 生成证书",charIndex:2535},{level:2,title:"4. 安装来自第三方 CA 的证书",slug:"_4-安装来自第三方-ca-的证书",normalizedTitle:"4. 安装来自第三方 ca 的证书",charIndex:3831}],headersStr:"1.安装MinIO服务器 2. 在 MinIO 中使用现有的密钥和证书 3. 使用 MinIO 生成和使用自签名密钥和证书 3.1certgen用于生成证书 3.2 使用 OpenSSL 生成证书 3.3 使用 GnuTLS (for Windows) 生成证书 4. 安装来自第三方 CA 的证书",content:'# 如何使用 TLS 保护对 MinIO 服务器的访问\n\n本指南解释了如何在 Linux 和 Windows 平台上使用 TLS 证书配置 MinIO Server。\n\n\n# 1.安装MinIO服务器\n\n使用MinIO 快速入门指南中的说明安装 MinIO Server 。\n\n\n# 2. 在 MinIO 中使用现有的密钥和证书\n\n本节介绍如何使用从证书颁发机构 (CA) 获得的私钥和公共证书。如果尚未获取这些文件，请跳至3. 生成自签名证书或使用以下说明使用Let\'s Encrypt生成它们：使用 Certbot for MinIO 生成 Let\'s Encrypt 证书。\n\n将现有的私钥和公共证书复制到certs目录中。默认的 certs 目录是：\n\n * Linux： ${HOME}/.minio/certs\n * 视窗： %%USERPROFILE%%\\.minio\\certs\n\n笔记：\n\n * 可以使用--certs-dir命令行选项指定自定义证书目录的位置。\n * 在certs目录中，私钥必须命名private.key，公钥必须命名public.crt。\n * 由 CA 签署的证书包含有关颁发的身份（例如名称、到期时间、公钥）和任何中间证书的信息。不包括根 CA。\n\n\n# 3. 使用 MinIO 生成和使用自签名密钥和证书\n\n本节介绍如何使用各种工具生成自签名证书：\n\n * 3.1 使用certgen生成证书\n * 3.2 使用 OpenSSL 生成证书\n * 3.3 使用OpenSSL（带IP地址）生成证书\n * 3.4 使用 GnuTLS (for Windows) 生成证书\n\n说明：\n\n * MinIO 在 Linux 和 Windows 上仅支持 PEM 格式的密钥和证书。\n * MinIO 目前不支持 PFX 证书。\n\n\n# 3.1certgen用于生成证书\n\n下载certgen适用于您的特定操作系统和平台。\n\ncertgen是一个简单的Go工具来生成自签名证书，并提供带有 DNS 和 IP 条目的 SAN 证书：\n\n./certgen -ca -host "10.10.0.3,10.10.0.4,10.10.0.5"\n\n\n应显示与此类似的响应：\n\n2018/11/21 10:16:18 wrote public.crt\n2018/11/21 10:16:18 wrote private.key\n\n\n\n# 3.2 使用 OpenSSL 生成证书\n\n使用以下方法之一生成证书openssl：\n\n * 3.2.1用ECDSA生成私钥\n * 3.2.2用RSA生成私钥\n * 3.2.3生成自签名证书\n\n# 3.2.1 使用 ECDSA 生成私钥。\n\n使用以下命令生成带有 ECDSA 的私钥：\n\nopenssl ecparam -genkey -name prime256v1 | openssl ec -out private.key\n\n\n应显示与此类似的响应：\n\nread EC key\nwriting EC key\n\n\n或者，使用以下命令生成受密码保护的私有 ECDSA 密钥：\n\nopenssl ecparam -genkey -name prime256v1 | openssl ec -aes256 -out private.key -passout pass:PASSWORD\n\n\n注意\n\n目前不支持 NIST 曲线 P-384 和 P-521。\n\n# 3.2.2 使用 RSA 生成私钥。\n\n使用以下命令生成带有 RSA 的私钥：\n\nopenssl genrsa -out private.key 2048\n\n\n应显示与此类似的响应：\n\nGenerating RSA private key, 2048 bit long modulus\n............................................+++\n...........+++\ne is 65537 (0x10001)\n\n\n或者，使用以下命令生成受密码保护的私有 RSA 密钥：\n\nopenssl genrsa -aes256 -passout pass:PASSWORD -out private.key 2048\n\n\n注意\n\n使用受密码保护的私钥时，必须MINIO_CERT_PASSWD使用以下命令通过环境变量提供密码：\n\nexport MINIO_CERT_PASSWD=<PASSWORD>\n\n\n私有加密密钥的默认 OpenSSL 格式是 PKCS-8，但 MinIO 仅支持 PKCS-1。可以使用以下命令将已使用 PKCS-8 格式化的 RSA 密钥转换为 PKCS-1：\n\nopenssl rsa -in private-pkcs8-key.key -aes256 -passout pass:PASSWORD -out private.key\n\n\n# 3.2.3 生成自签名证书。\n\n创建一个以openssl.conf以下内容命名的文件。设置IP.1和/或DNS.1指向正确的 IP/DNS 地址：\n\n[req]\ndistinguished_name = req_distinguished_name\nx509_extensions = v3_req\nprompt = no\n\n[req_distinguished_name]\nC = US\nST = VA\nL = Somewhere\nO = MyOrg\nOU = MyOU\nCN = MyServerName\n\n[v3_req]\nsubjectAltName = @alt_names\n\n[alt_names]\nIP.1 = 127.0.0.1\nDNS.1 = localhost\n\n\nopenssl通过指定配置文件运行并在出现提示时输入密码：\n\nopenssl req -new -x509 -nodes -days 730 -key private.key -out public.crt -config openssl.conf\n\n\n\n# 3.3 使用 GnuTLS (for Windows) 生成证书\n\n本节介绍如何在 Windows 上使用 GnuTLS 生成证书。\n\n# 3.3.1 安装和配置 GnuTLS。\n\n从这里下载并解压 Windows 版本的 GnuTLS 。\n\n使用 PowerShell 将提取的 GnuTLS 二进制文件的路径添加到系统路径：\n\nsetx path "%path%;C:\\Users\\MyUser\\Downloads\\gnutls-3.4.9-w64\\bin"\n\n\n注意\n\n可能需要重新启动 PowerShell 才能使此更改生效。\n\n# 3.3.2 生成私钥：\n\n运行以下命令生成私有.key文件：\n\ncerttool.exe --generate-privkey --outfile private.key\n\n\n应显示与此类似的响应：\n\nGenerating a 3072 bit RSA private key...\n\n\n# 3.3.3 生成公共证书：\n\n创建一个cert.cnf使用以下内容调用的文件。此文件包含使用certtool.exe以下命令生成证书所需的所有信息：\n\n# X.509 Certificate options\n#\n# DN options\n\n# The organization of the subject.\norganization = "Example Inc."\n\n# The organizational unit of the subject.\n#unit = "sleeping dept."\n\n# The state of the certificate owner.\nstate = "Example"\n\n# The country of the subject. Two letter code.\ncountry = "EX"\n\n# The common name of the certificate owner.\ncn = "Sally Certowner"\n\n# In how many days, counting from today, this certificate will expire.\nexpiration_days = 365\n\n# X.509 v3 extensions\n\n# DNS name(s) of the server\ndns_name = "localhost"\n\n# (Optional) Server IP address\nip_address = "127.0.0.1"\n\n# Whether this certificate will be used for a TLS server\ntls_www_server\n\n\n运行certtool.exe并指定配置文件生成证书：\n\ncerttool.exe --generate-self-signed --load-privkey private.key --template cert.cnf --outfile public.crt\n\n\n\n# 4. 安装来自第三方 CA 的证书\n\nMinIO 可以连接到其他服务器，包括 MinIO 节点或其他服务器类型，例如 NAT 和 Redis。如果这些服务器使用未在已知 CA 注册的证书，请将这些证书置于以下 MinIO 配置路径之一下，以将这些证书的信任添加到 MinIO 服务器：\n\n * Linux： ~/.minio/certs/CAs/\n * Windods：C:\\Users\\<Username>\\.minio\\certs\\CAs',normalizedContent:'# 如何使用 tls 保护对 minio 服务器的访问\n\n本指南解释了如何在 linux 和 windows 平台上使用 tls 证书配置 minio server。\n\n\n# 1.安装minio服务器\n\n使用minio 快速入门指南中的说明安装 minio server 。\n\n\n# 2. 在 minio 中使用现有的密钥和证书\n\n本节介绍如何使用从证书颁发机构 (ca) 获得的私钥和公共证书。如果尚未获取这些文件，请跳至3. 生成自签名证书或使用以下说明使用let\'s encrypt生成它们：使用 certbot for minio 生成 let\'s encrypt 证书。\n\n将现有的私钥和公共证书复制到certs目录中。默认的 certs 目录是：\n\n * linux： ${home}/.minio/certs\n * 视窗： %%userprofile%%\\.minio\\certs\n\n笔记：\n\n * 可以使用--certs-dir命令行选项指定自定义证书目录的位置。\n * 在certs目录中，私钥必须命名private.key，公钥必须命名public.crt。\n * 由 ca 签署的证书包含有关颁发的身份（例如名称、到期时间、公钥）和任何中间证书的信息。不包括根 ca。\n\n\n# 3. 使用 minio 生成和使用自签名密钥和证书\n\n本节介绍如何使用各种工具生成自签名证书：\n\n * 3.1 使用certgen生成证书\n * 3.2 使用 openssl 生成证书\n * 3.3 使用openssl（带ip地址）生成证书\n * 3.4 使用 gnutls (for windows) 生成证书\n\n说明：\n\n * minio 在 linux 和 windows 上仅支持 pem 格式的密钥和证书。\n * minio 目前不支持 pfx 证书。\n\n\n# 3.1certgen用于生成证书\n\n下载certgen适用于您的特定操作系统和平台。\n\ncertgen是一个简单的go工具来生成自签名证书，并提供带有 dns 和 ip 条目的 san 证书：\n\n./certgen -ca -host "10.10.0.3,10.10.0.4,10.10.0.5"\n\n\n应显示与此类似的响应：\n\n2018/11/21 10:16:18 wrote public.crt\n2018/11/21 10:16:18 wrote private.key\n\n\n\n# 3.2 使用 openssl 生成证书\n\n使用以下方法之一生成证书openssl：\n\n * 3.2.1用ecdsa生成私钥\n * 3.2.2用rsa生成私钥\n * 3.2.3生成自签名证书\n\n# 3.2.1 使用 ecdsa 生成私钥。\n\n使用以下命令生成带有 ecdsa 的私钥：\n\nopenssl ecparam -genkey -name prime256v1 | openssl ec -out private.key\n\n\n应显示与此类似的响应：\n\nread ec key\nwriting ec key\n\n\n或者，使用以下命令生成受密码保护的私有 ecdsa 密钥：\n\nopenssl ecparam -genkey -name prime256v1 | openssl ec -aes256 -out private.key -passout pass:password\n\n\n注意\n\n目前不支持 nist 曲线 p-384 和 p-521。\n\n# 3.2.2 使用 rsa 生成私钥。\n\n使用以下命令生成带有 rsa 的私钥：\n\nopenssl genrsa -out private.key 2048\n\n\n应显示与此类似的响应：\n\ngenerating rsa private key, 2048 bit long modulus\n............................................+++\n...........+++\ne is 65537 (0x10001)\n\n\n或者，使用以下命令生成受密码保护的私有 rsa 密钥：\n\nopenssl genrsa -aes256 -passout pass:password -out private.key 2048\n\n\n注意\n\n使用受密码保护的私钥时，必须minio_cert_passwd使用以下命令通过环境变量提供密码：\n\nexport minio_cert_passwd=<password>\n\n\n私有加密密钥的默认 openssl 格式是 pkcs-8，但 minio 仅支持 pkcs-1。可以使用以下命令将已使用 pkcs-8 格式化的 rsa 密钥转换为 pkcs-1：\n\nopenssl rsa -in private-pkcs8-key.key -aes256 -passout pass:password -out private.key\n\n\n# 3.2.3 生成自签名证书。\n\n创建一个以openssl.conf以下内容命名的文件。设置ip.1和/或dns.1指向正确的 ip/dns 地址：\n\n[req]\ndistinguished_name = req_distinguished_name\nx509_extensions = v3_req\nprompt = no\n\n[req_distinguished_name]\nc = us\nst = va\nl = somewhere\no = myorg\nou = myou\ncn = myservername\n\n[v3_req]\nsubjectaltname = @alt_names\n\n[alt_names]\nip.1 = 127.0.0.1\ndns.1 = localhost\n\n\nopenssl通过指定配置文件运行并在出现提示时输入密码：\n\nopenssl req -new -x509 -nodes -days 730 -key private.key -out public.crt -config openssl.conf\n\n\n\n# 3.3 使用 gnutls (for windows) 生成证书\n\n本节介绍如何在 windows 上使用 gnutls 生成证书。\n\n# 3.3.1 安装和配置 gnutls。\n\n从这里下载并解压 windows 版本的 gnutls 。\n\n使用 powershell 将提取的 gnutls 二进制文件的路径添加到系统路径：\n\nsetx path "%path%;c:\\users\\myuser\\downloads\\gnutls-3.4.9-w64\\bin"\n\n\n注意\n\n可能需要重新启动 powershell 才能使此更改生效。\n\n# 3.3.2 生成私钥：\n\n运行以下命令生成私有.key文件：\n\ncerttool.exe --generate-privkey --outfile private.key\n\n\n应显示与此类似的响应：\n\ngenerating a 3072 bit rsa private key...\n\n\n# 3.3.3 生成公共证书：\n\n创建一个cert.cnf使用以下内容调用的文件。此文件包含使用certtool.exe以下命令生成证书所需的所有信息：\n\n# x.509 certificate options\n#\n# dn options\n\n# the organization of the subject.\norganization = "example inc."\n\n# the organizational unit of the subject.\n#unit = "sleeping dept."\n\n# the state of the certificate owner.\nstate = "example"\n\n# the country of the subject. two letter code.\ncountry = "ex"\n\n# the common name of the certificate owner.\ncn = "sally certowner"\n\n# in how many days, counting from today, this certificate will expire.\nexpiration_days = 365\n\n# x.509 v3 extensions\n\n# dns name(s) of the server\ndns_name = "localhost"\n\n# (optional) server ip address\nip_address = "127.0.0.1"\n\n# whether this certificate will be used for a tls server\ntls_www_server\n\n\n运行certtool.exe并指定配置文件生成证书：\n\ncerttool.exe --generate-self-signed --load-privkey private.key --template cert.cnf --outfile public.crt\n\n\n\n# 4. 安装来自第三方 ca 的证书\n\nminio 可以连接到其他服务器，包括 minio 节点或其他服务器类型，例如 nat 和 redis。如果这些服务器使用未在已知 ca 注册的证书，请将这些证书置于以下 minio 配置路径之一下，以将这些证书的信任添加到 minio 服务器：\n\n * linux： ~/.minio/certs/cas/\n * windods：c:\\users\\<username>\\.minio\\certs\\cas',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Minio存储桶通知指南",frontmatter:{title:"Minio存储桶通知指南",date:"2022-02-09T11:27:22.000Z",permalink:"/pages/50e89b/"},regularPath:"/12.MinIO/01.Minio%E6%9C%8D%E5%8A%A1%E5%99%A8/06.Minio%E5%AD%98%E5%82%A8%E6%A1%B6%E9%80%9A%E7%9F%A5%E6%8C%87%E5%8D%97.html",relativePath:"12.MinIO/01.Minio服务器/06.Minio存储桶通知指南.md",key:"v-459c694a",path:"/pages/50e89b/",headers:[{level:2,title:"前提条件",slug:"前提条件",normalizedTitle:"前提条件",charIndex:177},{level:2,title:"使用AMQP发布Minio事件",slug:"使用amqp发布minio事件",normalizedTitle:"使用amqp发布minio事件",charIndex:237},{level:3,title:"第一步: 将AMQP endpoint添加到Minio",slug:"第一步-将amqp-endpoint添加到minio",normalizedTitle:"第一步: 将amqp endpoint添加到minio",charIndex:275},{level:3,title:"第二步: 使用Minio客户端启用bucket通知",slug:"第二步-使用minio客户端启用bucket通知",normalizedTitle:"第二步: 使用minio客户端启用bucket通知",charIndex:1912},{level:3,title:"第三步:在RabbitMQ上进行验证",slug:"第三步-在rabbitmq上进行验证",normalizedTitle:"第三步:在rabbitmq上进行验证",charIndex:2432},{level:2,title:"使用MQTT发布Minio事件",slug:"使用mqtt发布minio事件",normalizedTitle:"使用mqtt发布minio事件",charIndex:3941},{level:3,title:"第一步: 添加MQTT endpoint到Minio",slug:"第一步-添加mqtt-endpoint到minio",normalizedTitle:"第一步: 添加mqtt endpoint到minio",charIndex:3981},{level:3,title:"第二步: 使用Minio客户端启用bucket通知",slug:"第二步-使用minio客户端启用bucket通知-2",normalizedTitle:"第二步: 使用minio客户端启用bucket通知",charIndex:1912},{level:3,title:"第三步：验证MQTT",slug:"第三步-验证mqtt",normalizedTitle:"第三步：验证mqtt",charIndex:5656},{level:2,title:"使用Elasticsearch发布Minio事件",slug:"使用elasticsearch发布minio事件",normalizedTitle:"使用elasticsearch发布minio事件",charIndex:7344},{level:3,title:"第一步：确保至少满足第低要求",slug:"第一步-确保至少满足第低要求",normalizedTitle:"第一步：确保至少满足第低要求",charIndex:7923},{level:3,title:"第二步：把ES集成到Minio中",slug:"第二步-把es集成到minio中",normalizedTitle:"第二步：把es集成到minio中",charIndex:8002},{level:3,title:"第三步：使用Minio客户端启用bucket通知",slug:"第三步-使用minio客户端启用bucket通知",normalizedTitle:"第三步：使用minio客户端启用bucket通知",charIndex:8844},{level:3,title:"第四步：验证ES",slug:"第四步-验证es",normalizedTitle:"第四步：验证es",charIndex:9372},{level:2,title:"使用Redis发布Minio事件",slug:"使用redis发布minio事件",normalizedTitle:"使用redis发布minio事件",charIndex:11428},{level:3,title:"第一步：集成Redis到Minio",slug:"第一步-集成redis到minio",normalizedTitle:"第一步：集成redis到minio",charIndex:11872},{level:3,title:"第二步: 使用Minio客户端启用bucket通知",slug:"第二步-使用minio客户端启用bucket通知-3",normalizedTitle:"第二步: 使用minio客户端启用bucket通知",charIndex:1912},{level:3,title:"第三步：验证Redis",slug:"第三步-验证redis",normalizedTitle:"第三步：验证redis",charIndex:13352},{level:2,title:"使用NATS发布Minio事件",slug:"使用nats发布minio事件",normalizedTitle:"使用nats发布minio事件",charIndex:14755},{level:3,title:"第一步：集成NATS到Minio",slug:"第一步-集成nats到minio",normalizedTitle:"第一步：集成nats到minio",charIndex:14785},{level:3,title:"第二步: 使用Minio客户端启用bucket通知",slug:"第二步-使用minio客户端启用bucket通知-4",normalizedTitle:"第二步: 使用minio客户端启用bucket通知",charIndex:1912},{level:3,title:"第三步：验证NATS",slug:"第三步-验证nats",normalizedTitle:"第三步：验证nats",charIndex:16596},{level:2,title:"使用PostgreSQL发布Minio事件",slug:"使用postgresql发布minio事件",normalizedTitle:"使用postgresql发布minio事件",charIndex:19987},{level:3,title:"第一步：确保确保至少满足第低要求",slug:"第一步-确保确保至少满足第低要求",normalizedTitle:"第一步：确保确保至少满足第低要求",charIndex:20498},{level:3,title:"第二步：集成PostgreSQL到Minio",slug:"第二步-集成postgresql到minio",normalizedTitle:"第二步：集成postgresql到minio",charIndex:20627},{level:3,title:"第三步：使用Minio客户端启用bucket通知",slug:"第三步-使用minio客户端启用bucket通知-2",normalizedTitle:"第三步：使用minio客户端启用bucket通知",charIndex:8844},{level:3,title:"第四步：验证PostgreSQL",slug:"第四步-验证postgresql",normalizedTitle:"第四步：验证postgresql",charIndex:22760},{level:2,title:"使用MySQL发布Minio事件",slug:"使用mysql发布minio事件",normalizedTitle:"使用mysql发布minio事件",charIndex:24247},{level:3,title:"第一步：确保确保至少满足第低要求",slug:"第一步-确保确保至少满足第低要求-2",normalizedTitle:"第一步：确保确保至少满足第低要求",charIndex:20498},{level:3,title:"第二步：集成MySQL到Minio",slug:"第二步-集成mysql到minio",normalizedTitle:"第二步：集成mysql到minio",charIndex:24831},{level:3,title:"第三步：使用Minio客户端启用bucket通知",slug:"第三步-使用minio客户端启用bucket通知-3",normalizedTitle:"第三步：使用minio客户端启用bucket通知",charIndex:8844},{level:3,title:"第四步：验证MySQL",slug:"第四步-验证mysql",normalizedTitle:"第四步：验证mysql",charIndex:26917},{level:2,title:"使用Kafka发布Minio事件",slug:"使用kafka发布minio事件",normalizedTitle:"使用kafka发布minio事件",charIndex:30649},{level:3,title:"第一步：确保确保至少满足第低要求",slug:"第一步-确保确保至少满足第低要求-3",normalizedTitle:"第一步：确保确保至少满足第低要求",charIndex:20498},{level:3,title:"第二步：集成Kafka到Minio",slug:"第二步-集成kafka到minio",normalizedTitle:"第二步：集成kafka到minio",charIndex:30780},{level:3,title:"第三步：使用Minio客户端启用bucket通知",slug:"第三步-使用minio客户端启用bucket通知-4",normalizedTitle:"第三步：使用minio客户端启用bucket通知",charIndex:8844},{level:3,title:"第四步：验证Kafka",slug:"第四步-验证kafka",normalizedTitle:"第四步：验证kafka",charIndex:31531},{level:2,title:"使用Webhook发布Minio事件",slug:"使用webhook发布minio事件",normalizedTitle:"使用webhook发布minio事件",charIndex:32552},{level:3,title:"第一步：集成MySQL到Minio",slug:"第一步-集成mysql到minio",normalizedTitle:"第一步：集成mysql到minio",charIndex:32606},{level:3,title:"第二步：使用Minio客户端启用bucket通知",slug:"第二步-使用minio客户端启用bucket通知-5",normalizedTitle:"第二步：使用minio客户端启用bucket通知",charIndex:32862},{level:3,title:"第三步：采用Thumbnailer进行验证",slug:"第三步-采用thumbnailer进行验证",normalizedTitle:"第三步：采用thumbnailer进行验证",charIndex:33420}],headersStr:"前提条件 使用AMQP发布Minio事件 第一步: 将AMQP endpoint添加到Minio 第二步: 使用Minio客户端启用bucket通知 第三步:在RabbitMQ上进行验证 使用MQTT发布Minio事件 第一步: 添加MQTT endpoint到Minio 第二步: 使用Minio客户端启用bucket通知 第三步：验证MQTT 使用Elasticsearch发布Minio事件 第一步：确保至少满足第低要求 第二步：把ES集成到Minio中 第三步：使用Minio客户端启用bucket通知 第四步：验证ES 使用Redis发布Minio事件 第一步：集成Redis到Minio 第二步: 使用Minio客户端启用bucket通知 第三步：验证Redis 使用NATS发布Minio事件 第一步：集成NATS到Minio 第二步: 使用Minio客户端启用bucket通知 第三步：验证NATS 使用PostgreSQL发布Minio事件 第一步：确保确保至少满足第低要求 第二步：集成PostgreSQL到Minio 第三步：使用Minio客户端启用bucket通知 第四步：验证PostgreSQL 使用MySQL发布Minio事件 第一步：确保确保至少满足第低要求 第二步：集成MySQL到Minio 第三步：使用Minio客户端启用bucket通知 第四步：验证MySQL 使用Kafka发布Minio事件 第一步：确保确保至少满足第低要求 第二步：集成Kafka到Minio 第三步：使用Minio客户端启用bucket通知 第四步：验证Kafka 使用Webhook发布Minio事件 第一步：集成MySQL到Minio 第二步：使用Minio客户端启用bucket通知 第三步：采用Thumbnailer进行验证",content:'# Minio存储桶通知指南\n\n存储桶（Bucket）如果发生改变,比如上传对象和删除对象，可以使用存储桶事件通知机制进行监控，并通过以下方式发布出去:\n\nNOTIFICATION TARGETS\nAMQP\nMQTT\nElasticsearch\nRedis\nNATS\nPostgreSQL\nMySQL\nApache Kafka\nWebhooks\n\n\n# 前提条件\n\n * 从这里下载并安装Minio Server。\n * 从这里下载并安装Minio Client。\n\n\n# 使用AMQP发布Minio事件\n\n从这里下载安装RabbitMQ。\n\n\n# 第一步: 将AMQP endpoint添加到Minio\n\nMinio Server的配置文件默认路径是 ~/.minio/config.json。AMQP配置信息是在notify这个节点下的amqp节点下，在这里为你的AMQP实例创建配置信息键值对，key是你的AMQP endpoint的名称，value是下面表格中列列的键值对集合。\n\n参数             类型       描述\nenable         bool     (必须) 此AMQP server endpoint是否可用\nurl            string   (必须) AMQP server endpoint, 例如.\n                        amqp://myuser:mypassword@localhost:5672\nexchange       string   exchange名称\nroutingKey     string   发布用的Routing key\nexchangeType   string   exchange类型\ndeliveryMode   uint8    发布方式。 0或1 - 瞬态; 2 - 持久。\nmandatory      bool     Publishing related bool.\nimmediate      bool     Publishing related bool.\ndurable        bool     Exchange declaration related bool.\ninternal       bool     Exchange declaration related bool.\nnoWait         bool     Exchange declaration related bool.\nautoDeleted    bool     Exchange declaration related bool.\n\n下面展示的是RabbitMQ的配置示例:\n\n"amqp": {    "1": {        "enable": true,        "url": "amqp://myuser:mypassword@localhost:5672",        "exchange": "bucketevents",        "routingKey": "bucketlogs",        "exchangeType": "fanout",        "deliveryMode": 0,        "mandatory": false,        "immediate": false,        "durable": false,        "internal": false,        "noWait": false,        "autoDeleted": false    }}\n\n\n更新完配置文件后，重启Minio Server让配置生效。如果一切顺利，Minio Server会在启动时输出一行信息，类似 SQS ARNs: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:amqp。\n\nMinio支持RabbitMQ中所有的交换方式，这次我们采用 fanout 交换。\n\n注意一下，你可以听从你内心的想法，想配几个AMQP服务就配几个，只要每个AMQP服务实例有不同的ID (比如前面示例中的"1") 和配置信息。\n\n\n# 第二步: 使用Minio客户端启用bucket通知\n\n如果一个JPEG图片上传到myminio server里的images 存储桶或者从桶中删除，一个存储桶事件通知就会被触发。 这里ARN值是arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:amqp，想了解更多关于ARN的信息，请参考AWS ARN documentation.\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:amqp --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:amqp s3:ObjectCreated:*,s3:ObjectRemoved:* Filter: suffix=”.jpg”\n\n\n\n# 第三步:在RabbitMQ上进行验证\n\n下面将要出场的python程序会等待队列交换Tbucketevents并在控制台中输出事件通知。我们使用的是Pika Python Client 来实现此功能。\n\n#!/usr/bin/env pythonimport pikaconnection = pika.BlockingConnection(pika.ConnectionParameters(        host=\'localhost\'))channel = connection.channel()channel.exchange_declare(exchange=\'bucketevents\',                         exchange_type=\'fanout\')result = channel.queue_declare(exclusive=False)queue_name = result.method.queuechannel.queue_bind(exchange=\'bucketevents\',                   queue=queue_name)print(\' [*] Waiting for logs. To exit press CTRL+C\')def callback(ch, method, properties, body):    print(" [x] %r" % body)channel.basic_consume(callback,                      queue=queue_name,                      no_ack=False)channel.start_consuming()\n\n\n执行示例中的python程序来观察RabbitMQ事件。\n\npython rabbit.py\n\n\n另开一个terminal终端并上传一张JPEG图片到images存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n一旦上传完毕，你应该会通过RabbitMQ收到下面的事件通知。\n\npython rabbit.py‘{“Records”:[{“eventVersion”:”2.0",”eventSource”:”aws:s3",”awsRegion”:”us-east-1",”eventTime”:”2016–09–08T22:34:38.226Z”,”eventName”:”s3:ObjectCreated:Put”,”userIdentity”:{“principalId”:”minio”},”requestParameters”:{“sourceIPAddress”:”10.1.10.150:44576"},”responseElements”:{},”s3":{“s3SchemaVersion”:”1.0",”configurationId”:”Config”,”bucket”:{“name”:”images”,”ownerIdentity”:{“principalId”:”minio”},”arn”:”arn:aws:s3:::images”},”object”:{“key”:”myphoto.jpg”,”size”:200436,”sequencer”:”147279EAF9F40933"}}}],”level”:”info”,”msg”:””,”time”:”2016–09–08T15:34:38–07:00"}\\n\n\n\n\n# 使用MQTT发布Minio事件\n\n从 这里安装MQTT Broker。\n\n\n# 第一步: 添加MQTT endpoint到Minio\n\nMinio Server的配置文件默认路径是 ~/.minio/config.json。MQTT配置信息是在notify这个节点下的mqtt节点下，在这里为你的MQTT实例创建配置信息键值对，key是你的MQTT endpoint的名称，value是下面表格中列列的键值对集合。\n\n参数         类型       描述\nenable     bool     (必须) 这个 server endpoint是否可用?\nbroker     string   (必须) MQTT server endpoint, 例如. tcp://localhost:1883\ntopic      string   (必须) 要发布的MQTT主题的名称, 例如. minio\nqos        int      设置服务质量级别\nclientId   string   MQTT代理识别Minio的唯一ID\nusername   string   连接MQTT server的用户名 (如果需要的话)\npassword   string   链接MQTT server的密码 (如果需要的话)\n\n以下是一个MQTT的配置示例:\n\n"mqtt": {    "1": {        "enable": true,        "broker": "tcp://localhost:1883",        "topic": "minio",        "qos": 1,        "clientId": "minio",        "username": "",        "password": ""    }}\n\n\n更新完配置文件后，重启Minio Server让配置生效。如果一切顺利，Minio Server会在启动时输出一行信息，类似 SQS ARNs: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:mqtt。\n\nMinio支持任何支持MQTT 3.1或3.1.1的MQTT服务器，并且可以通过TCP，TLS或Websocket连接使用tcp://, tls://, or ws://分别作为代理URL的方案。 更多信息，请参考 Go Client。\n\n注意一下，你还是和之前AMQP一样可以听从你内心的想法，想配几个MQTT服务就配几个，只要每个MQTT服务实例有不同的ID (比如前面示例中的"1") 和配置信息。\n\n\n# 第二步: 使用Minio客户端启用bucket通知\n\n如果一个JPEG图片上传到myminio server里的images 存储桶或者从桶中删除，一个存储桶事件通知就会被触发。 这里ARN值是arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:mqtt。\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:mqtt --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:amqp s3:ObjectCreated:*,s3:ObjectRemoved:* Filter: suffix=”.jpg”\n\n\n\n# 第三步：验证MQTT\n\n下面的python程序等待mqtt主题/ minio，并在控制台上打印事件通知。 我们使用paho-mqtt库来执行此操作。\n\n#!/usr/bin/env pythonfrom __future__ import print_functionimport paho.mqtt.client as mqtt# The callback for when the client receives a CONNACK response from the server.def on_connect(client, userdata, flags, rc):    print("Connected with result code", rc)    # Subscribing in on_connect() means that if we lose the connection and    # reconnect then subscriptions will be renewed.    client.subscribe("/minio")# The callback for when a PUBLISH message is received from the server.def on_message(client, userdata, msg):    print(msg.payload)client = mqtt.Client()client.on_connect = on_connectclient.on_message = on_messageclient.connect("localhost:1883", 1883, 60)# Blocking call that processes network traffic, dispatches callbacks and# handles reconnecting.# Other loop*() functions are available that give a threaded interface and a# manual interface.client.loop_forever()\n\n\n执行这个python示例程序来观察MQTT事件。\n\npython mqtt.py\n\n\n打开一个新的terminal终端并上传一张JPEG图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n一旦上传完毕，你应该会通过MQTT收到下面的事件通知。\n\npython mqtt.py{“Records”:[{“eventVersion”:”2.0",”eventSource”:”aws:s3",”awsRegion”:”us-east-1",”eventTime”:”2016–09–08T22:34:38.226Z”,”eventName”:”s3:ObjectCreated:Put”,”userIdentity”:{“principalId”:”minio”},”requestParameters”:{“sourceIPAddress”:”10.1.10.150:44576"},”responseElements”:{},”s3":{“s3SchemaVersion”:”1.0",”configurationId”:”Config”,”bucket”:{“name”:”images”,”ownerIdentity”:{“principalId”:”minio”},”arn”:”arn:aws:s3:::images”},”object”:{“key”:”myphoto.jpg”,”size”:200436,”sequencer”:”147279EAF9F40933"}}}],”level”:”info”,”msg”:””,”time”:”2016–09–08T15:34:38–07:00"}\n\n\n\n# 使用Elasticsearch发布Minio事件\n\n安装 Elasticsearch 。\n\n这个通知目标支持两种格式: namespace and access。\n\n如果使用的是 namespace 格式, Minio将桶中的对象与索引中的文档进行同步。对于Minio的每一个事件，ES都会创建一个document,这个document的ID就是存储桶以及存储对象的名称。事件的其他细节存储在document的正文中。因此，如果一个已经存在的对象在Minio中被覆盖，在ES中的相对应的document也会被更新。如果一个对象被删除，相对应的document也会从index中删除。\n\n如果使用的是access格式，Minio将事件作为document加到ES的index中。对于每一个事件，ES同样会创建一个document,这个document包含事件的所有细节，document的时间戳设置为事件的时间戳，并将该document加到ES的index中。这个document的ID是由ES随机生成的。在access格式下，没有文档会被删除或者修改，对于一个对象的操作，都会生成新的document附加到index中。\n\n下面的步骤展示的是在namespace格式下，如何使用通知目标。另一种格式和这个很类似，为了不让你们说我墨迹，就不再赘述了。\n\n\n# 第一步：确保至少满足第低要求\n\nMinio要求使用的是ES 5.X系统版本。如果使用的是低版本的ES，也没关系，ES官方支持升级迁移，详情请看这里。\n\n\n# 第二步：把ES集成到Minio中\n\nMinio Server的配置文件默认路径是 ~/.minio/config.json。ES配置信息是在notify这个节点下的elasticsearch节点下，在这里为你的ES实例创建配置信息键值对，key是你的ES的名称，value是下面表格中列列的键值对集合。\n\n参数       类型       描述\nenable   bool     (必须) 是否启用这个配置?\nformat   string   (必须) 是namespace 还是 access\nurl      string   (必须) ES地址，比如: http://localhost:9200\nindex    string   (必须) 给Minio用的index\n\n以下是ES的一个配置示例:\n\n"elasticsearch": {    "1": {        "enable": true,        "format": "namespace",        "url": "http://127.0.0.1:9200",        "index": "minio_events"    }},\n\n\n更新完配置文件后，重启Minio Server让配置生效。如果一切顺利，Minio Server会在启动时输出一行信息，类似 SQS ARNs: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:elasticsearch。\n\n注意一下，你又可以再一次听从你内心的想法，想配几个ES服务就配几个，只要每个ES服务实例有不同的ID (比如前面示例中的"1") 和配置信息。\n\n\n# 第三步：使用Minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知。一旦有文件被创建或者覆盖，一个新的ES的document会被创建或者更新到之前咱配的index里。如果一个已经存在的对象被删除，这个对应的document也会从index中删除。因此，这个ES index里的行，就映射着images存储桶里的对象。\n\n要配置这种存储桶通知，我们需要用到前面步骤Minio输出的ARN信息。更多有关ARN的资料，请参考这里。\n\n有了mc这个工具，这些配置信息很容易就能添加上。假设咱们的Minio服务别名叫myminio,可执行下列脚本：\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:elasticsearch --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:elasticsearch s3:ObjectCreated:*,s3:ObjectRemoved:* Filter: suffix=”.jpg”\n\n\n\n# 第四步：验证ES\n\n上传一张图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n使用curl查到minio_events index中的内容。\n\n$ curl  "http://localhost:9200/minio_events/_search?pretty=true"{  "took" : 40,  "timed_out" : false,  "_shards" : {    "total" : 5,    "successful" : 5,    "failed" : 0  },  "hits" : {    "total" : 1,    "max_score" : 1.0,    "hits" : [      {        "_index" : "minio_events",        "_type" : "event",        "_id" : "images/myphoto.jpg",        "_score" : 1.0,        "_source" : {          "Records" : [            {              "eventVersion" : "2.0",              "eventSource" : "minio:s3",              "awsRegion" : "us-east-1",              "eventTime" : "2017-03-30T08:00:41Z",              "eventName" : "s3:ObjectCreated:Put",              "userIdentity" : {                "principalId" : "minio"              },              "requestParameters" : {                "sourceIPAddress" : "127.0.0.1:38062"              },              "responseElements" : {                "x-amz-request-id" : "14B09A09703FC47B",                "x-minio-origin-endpoint" : "http://192.168.86.115:9000"              },              "s3" : {                "s3SchemaVersion" : "1.0",                "configurationId" : "Config",                "bucket" : {                  "name" : "images",                  "ownerIdentity" : {                    "principalId" : "minio"                  },                  "arn" : "arn:aws:s3:::images"                },                "object" : {                  "key" : "myphoto.jpg",                  "size" : 6474,                  "eTag" : "a3410f4f8788b510d6f19c5067e60a90",                  "sequencer" : "14B09A09703FC47B"                }              },              "source" : {                "host" : "127.0.0.1",                "port" : "38062",                "userAgent" : "Minio (linux; amd64) minio-go/2.0.3 mc/2017-02-15T17:57:25Z"              }            }          ]        }      }    ]  }}\n\n\n这个输出显示在ES中为这个事件创建了一个document。\n\n这里我们可以看到这个document ID就是存储桶和对象的名称。如果用的是access格式，这个document ID就是由ES随机生成的。\n\n\n# 使用Redis发布Minio事件\n\n安装 Redis。为了演示，我们将数据库密码设为"yoursecret"。\n\n这种通知目标支持两种格式: namespace 和 access。\n\n如果用的是namespacee格式，Minio将存储桶里的对象同步成Redis hash中的条目。对于每一个条目，对应一个存储桶里的对象，其key都被设为"存储桶名称/对象名称"，value都是一个有关这个Minio对象的JSON格式的事件数据。如果对象更新或者删除，hash中对象的条目也会相应的更新或者删除。\n\n如果使用的是access,Minio使用RPUSH将事件添加到list中。这个list中每一个元素都是一个JSON格式的list,这个list中又有两个元素，第一个元素是时间戳的字符串，第二个元素是一个含有在这个存储桶上进行操作的事件数据的JSON对象。在这种格式下，list中的元素不会更新或者删除。\n\n下面的步骤展示如何在namespace和access格式下使用通知目标。\n\n\n# 第一步：集成Redis到Minio\n\nMinio Server的配置文件默认路径是 ~/.minio/config.json。Redis配置信息是在notify这个节点下的redis节点下，在这里为你的Redis实例创建配置信息键值对，key是你的Redis端的名称，value是下面表格中的键值对里面值的集合。\n\n参数         类型       描述\nenable     bool     (必须) 这个配置是否可用?\nformat     string   (必须) 是 namespace 还是 access\naddress    string   (必须) Redis服务地址，比如: localhost:6379\npassword   string   (可选) Redis服务密码\nkey        string   (必须) 事件要存储到redis\n                    key的名称。如果用的是namespace格式的话，则是一个hash,如果是access格式的话，则是一个list\n\n下面是一个Redis配置示例:\n\n"redis": {    "1": {        "enable": true,        "address": "127.0.0.1:6379",        "password": "yoursecret",        "key": "bucketevents"    }}\n\n\n更新完配置文件后，重启Minio Server让配置生效。如果一切顺利，Minio Server会在启动时输出一行信息，类似 SQS ARNs: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:redis。\n\n注意一下，你永远都可以听从你内心的想法，想配几个Redis服务就配几个，只要每个Redis服务实例有不同的ID (比如前面示例中的"1") 和配置信息。\n\n\n# 第二步: 使用Minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知。一旦有文件被创建或者覆盖，一个新的key会被创建,或者一个已经存在的key就会被更新到之前配置好的redis hash里。如果一个已经存在的对象被删除，这个对应的key也会从hash中删除。因此，这个Redis hash里的行，就映射着images存储桶里的.jpg对象。\n\n要配置这种存储桶通知，我们需要用到前面步骤Minio输出的ARN信息。更多有关ARN的资料，请参考这里。\n\n有了mc这个工具，这些配置信息很容易就能添加上。假设咱们的Minio服务别名叫myminio,可执行下列脚本：\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:redis --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:redis s3:ObjectCreated:*,s3:ObjectRemoved:* Filter: suffix=”.jpg”\n\n\n\n# 第三步：验证Redis\n\n启动redis-cli这个Redis客户端程序来检查Redis中的内容. 运行monitorRedis命令将会输出在Redis上执行的每个命令的。\n\nredis-cli -a yoursecret127.0.0.1:6379> monitorOK\n\n\n打开一个新的terminal终端并上传一张JPEG图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n在上一个终端中，你将看到Minio在Redis上执行的操作：\n\n127.0.0.1:6379> monitorOK1490686879.650649 [0 172.17.0.1:44710] "PING"1490686879.651061 [0 172.17.0.1:44710] "HSET" "minio_events" "images/myphoto.jpg" "{\\"Records\\":[{\\"eventVersion\\":\\"2.0\\",\\"eventSource\\":\\"minio:s3\\",\\"awsRegion\\":\\"us-east-1\\",\\"eventTime\\":\\"2017-03-28T07:41:19Z\\",\\"eventName\\":\\"s3:ObjectCreated:Put\\",\\"userIdentity\\":{\\"principalId\\":\\"minio\\"},\\"requestParameters\\":{\\"sourceIPAddress\\":\\"127.0.0.1:52234\\"},\\"responseElements\\":{\\"x-amz-request-id\\":\\"14AFFBD1ACE5F632\\",\\"x-minio-origin-endpoint\\":\\"http://192.168.86.115:9000\\"},\\"s3\\":{\\"s3SchemaVersion\\":\\"1.0\\",\\"configurationId\\":\\"Config\\",\\"bucket\\":{\\"name\\":\\"images\\",\\"ownerIdentity\\":{\\"principalId\\":\\"minio\\"},\\"arn\\":\\"arn:aws:s3:::images\\"},\\"object\\":{\\"key\\":\\"myphoto.jpg\\",\\"size\\":2586,\\"eTag\\":\\"5d284463f9da279f060f0ea4d11af098\\",\\"sequencer\\":\\"14AFFBD1ACE5F632\\"}},\\"source\\":{\\"host\\":\\"127.0.0.1\\",\\"port\\":\\"52234\\",\\"userAgent\\":\\"Minio (linux; amd64) minio-go/2.0.3 mc/2017-02-15T17:57:25Z\\"}}]}"\n\n\n在这我们可以看到Minio在minio_events这个key上执行了HSET命令。\n\n如果用的是access格式，那么minio_events就是一个list,Minio就会调用RPUSH添加到list中。这个list的消费者会使用BLPOP从list的最左端删除list元素。\n\n\n# 使用NATS发布Minio事件\n\n安装 NATS.\n\n\n# 第一步：集成NATS到Minio\n\nMinio Server的配置文件默认路径是 ~/.minio/config.json。参考下面的示例修改NATS的配置:\n\n"nats": {    "1": {        "enable": true,        "address": "0.0.0.0:4222",        "subject": "bucketevents",        "username": "yourusername",        "password": "yoursecret",        "token": "",        "secure": false,        "pingInterval": 0        "streaming": {            "enable": false,            "clusterID": "",            "clientID": "",            "async": false,            "maxPubAcksInflight": 0        }    }},\n\n\n更新完配置文件后，重启Minio Server让配置生效。bucketevents是NATS在这个例子中使用的主题。\n\nMinio服务也支持 NATS Streaming mode ，这种模式额外提供了像 Message/event persistence, At-least-once-delivery, 以及 Publisher rate limiting这样的功能。如果想让Minio服务发送通知到NATS Streaming server,参考下面示面进行配置：\n\n"nats": {    "1": {        "enable": true,        "address": "0.0.0.0:4222",        "subject": "bucketevents",        "username": "yourusername",        "password": "yoursecret",        "token": "",        "secure": false,        "pingInterval": 0        "streaming": {            "enable": true,            "clusterID": "test-cluster",            "clientID": "minio-client",            "async": true,            "maxPubAcksInflight": 10        }    }},\n\n\n更多关于 clusterID, clientID 的信息，请看 NATS documentation. 关于 maxPubAcksInflight ，请看 这里.\n\n\n# 第二步: 使用Minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知，一旦myminio server上有文件 从images存储桶里删除或者上传到存储桶中，事件即被触发。在这里，ARN的值是arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:nats。 更多有关ARN的资料，请参考这里。\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:nats --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:nats s3:ObjectCreated:*,s3:ObjectRemoved:* Filter: suffix=”.jpg”\n\n\n\n# 第三步：验证NATS\n\n如果你用的是NATS server，请查看下面的示例程序来记录添加到NATS的存储桶通知。\n\npackage main// Import Go and NATS packagesimport (    "log"    "runtime"    "github.com/nats-io/nats")func main() {    // Create server connection    natsConnection, _ := nats.Connect("nats://yourusername:yoursecret@localhost:4222")    log.Println("Connected")    // Subscribe to subject    log.Printf("Subscribing to subject \'bucketevents\'\\n")    natsConnection.Subscribe("bucketevents", func(msg *nats.Msg) {        // Handle the message        log.Printf("Received message \'%s\\n", string(msg.Data)+"\'")    })    // Keep the connection alive    runtime.Goexit()}\ngo run nats.go2016/10/12 06:39:18 Connected2016/10/12 06:39:18 Subscribing to subject \'bucketevents\'\n\n\n打开一个新的terminal终端并上传一张JPEG图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\nnats.go示例程序将事件通知打印到控制台。\n\ngo run nats.go2016/10/12 06:51:26 Connected2016/10/12 06:51:26 Subscribing to subject \'bucketevents\'2016/10/12 06:51:33 Received message \'{"EventType":"s3:ObjectCreated:Put","Key":"images/myphoto.jpg","Records":[{"eventVersion":"2.0","eventSource":"aws:s3","awsRegion":"us-east-1","eventTime":"2016-10-12T13:51:33Z","eventName":"s3:ObjectCreated:Put","userIdentity":{"principalId":"minio"},"requestParameters":{"sourceIPAddress":"[::1]:57106"},"responseElements":{},"s3":{"s3SchemaVersion":"1.0","configurationId":"Config","bucket":{"name":"images","ownerIdentity":{"principalId":"minio"},"arn":"arn:aws:s3:::images"},"object":{"key":"myphoto.jpg","size":56060,"eTag":"1d97bf45ecb37f7a7b699418070df08f","sequencer":"147CCD1AE054BFD0"}}}],"level":"info","msg":"","time":"2016-10-12T06:51:33-07:00"}\n\n\n如果你用的是NATS Streaming server,请查看下面的示例程序来记录添加到NATS的存储桶通知。\n\npackage main// Import Go and NATS packagesimport (    "fmt"    "runtime"    "github.com/nats-io/go-nats-streaming")func main() {    natsConnection, _ := stan.Connect("test-cluster", "test-client")    log.Println("Connected")    // Subscribe to subject    log.Printf("Subscribing to subject \'bucketevents\'\\n")    natsConnection.Subscribe("bucketevents", func(m *stan.Msg) {        // Handle the message        fmt.Printf("Received a message: %s\\n", string(m.Data))    })    // Keep the connection alive    runtime.Goexit()}\ngo run nats.go2017/07/07 11:47:40 Connected2017/07/07 11:47:40 Subscribing to subject \'bucketevents\'\n\n\n打开一个新的terminal终端并上传一张JPEG图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\nnats.go示例程序将事件通知打印到控制台。\n\nReceived a message: {"EventType":"s3:ObjectCreated:Put","Key":"images/myphoto.jpg","Records":[{"eventVersion":"2.0","eventSource":"minio:s3","awsRegion":"","eventTime":"2017-07-07T18:46:37Z","eventName":"s3:ObjectCreated:Put","userIdentity":{"principalId":"minio"},"requestParameters":{"sourceIPAddress":"192.168.1.80:55328"},"responseElements":{"x-amz-request-id":"14CF20BD1EFD5B93","x-minio-origin-endpoint":"http://127.0.0.1:9000"},"s3":{"s3SchemaVersion":"1.0","configurationId":"Config","bucket":{"name":"images","ownerIdentity":{"principalId":"minio"},"arn":"arn:aws:s3:::images"},"object":{"key":"myphoto.jpg","size":248682,"eTag":"f1671feacb8bbf7b0397c6e9364e8c92","contentType":"image/jpeg","userDefined":{"content-type":"image/jpeg"},"versionId":"1","sequencer":"14CF20BD1EFD5B93"}},"source":{"host":"192.168.1.80","port":"55328","userAgent":"Minio (linux; amd64) minio-go/2.0.4 mc/DEVELOPMENT.GOGET"}}],"level":"info","msg":"","time":"2017-07-07T11:46:37-07:00"}\n\n\n\n# 使用PostgreSQL发布Minio事件\n\n安装 PostgreSQL 数据库。为了演示，我们将"postgres"用户的密码设为password，并且创建了一个minio_events数据库来存储事件信息。\n\n这个通知目标支持两种格式: namespace and access。\n\n如果使用的是namespace格式，Minio将存储桶里的对象同步成数据库表中的行。每一行有两列：key和value。key是这个对象的存储桶名字加上对象名，value都是一个有关这个Minio对象的JSON格式的事件数据。如果对象更新或者删除，表中相应的行也会相应的更新或者删除。\n\n如果使用的是access,Minio将将事件添加到表里，行有两列：event_time 和 event_data。event_time是事件在Minio server里发生的时间，event_data是有关这个Minio对象的JSON格式的事件数据。在这种格式下，不会有行会被删除或者修改。\n\n下面的步骤展示的是如何在namespace格式下使用通知目标，*access*差不多，不再赘述，我相信你可以触类旁通，举一反三，不要让我失望哦。\n\n\n# 第一步：确保确保至少满足第低要求\n\nMinio要求PostgresSQL9.5版本及以上。 Minio用了PostgreSQL9.5引入的INSERT ON CONFLICT (aka UPSERT) 特性,以及9.4引入的 JSONB 数据类型。\n\n\n# 第二步：集成PostgreSQL到Minio\n\nMinio Server的配置文件默认路径是 ~/.minio/config.json。PostgreSQL配置信息是在notify这个节点下的postgresql节点下，在这里为你的PostgreSQL实例创建配置信息键值对，key是你的PostgreSQL的名称，value是下面表格中列列的键值对集合。\n\n参数                 类型       描述\nenable             bool     (必须)此配置是否启用\nformat             string   (必须) 是 namespace 还是 access\nconnectionString   string   (可选) PostgreSQL的连接参数 。比如可以用来设置 sslmode\ntable              string   (必须) 事件对应的表名，如果该表不存在，Mniio server会在启动时创建。\nhost               string   (可选) PostgresSQL的主机名，默认是localhost\nport               string   (可选) PostgreSQL的端口号，默认是5432\nuser               string   (可选)数据库用户名，默认是运行Minio server进程的用户\npassword           string   (可选) 数据库密码\ndatabase           string   (可选)库名\n\n下面是一个PostgreSQL配置示例:\n\n"postgresql": {    "1": {        "enable": true,        "format": "namespace",        "connectionString": "sslmode=disable",        "table": "bucketevents",        "host": "127.0.0.1",        "port": "5432",        "user": "postgres",        "password": "password",        "database": "minio_events"    }}\n\n\n注意一下，为了演示，咱们这把SSL禁掉了，但是为了安全起见，不建议在生产环境这么弄。\n\n更新完配置文件后，重启Minio Server让配置生效。如果一切顺利，Minio Server会在启动时输出一行信息，类似 SQS ARNs: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:postgresql。\n\n和之前描述的一样，你也可以添加多个PostreSQL实例，只要ID不重复就行。\n\n\n# 第三步：使用Minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知，一旦上有文件上传到存储桶中，PostgreSQL中会insert一条新的记录或者一条已经存在的记录会被update，如果一个存在对象被删除，一条对应的记录也会从PostgreSQL表中删除。因此，PostgreSQL表中的行，对应的就是存储桶里的一个对象。\n\n要配置这种存储桶通知，我们需要用到前面步骤中Minio输出的ARN信息。更多有关ARN的资料，请参考这里。\n\n有了mc这个工具，这些配置信息很容易就能添加上。假设Minio服务别名叫myminio,可执行下列脚本：\n\n# Create bucket named `images` in myminiomc mb myminio/images# Add notification configuration on the `images` bucket using the MySQL ARN. The --suffix argument filters events.mc events add myminio/images arn:minio:sqs:us-east-1:1:postgresql --suffix .jpg# Print out the notification configuration on the `images` bucket.mc events list myminio/imagesmc events list myminio/imagesarn:minio:sqs:us-east-1:1:postgresql s3:ObjectCreated:*,s3:ObjectRemoved:* Filter: suffix=”.jpg”\n\n\n\n# 第四步：验证PostgreSQL\n\n打开一个新的terminal终端并上传一张JPEG图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n打开一个PostgreSQL终端列出表 bucketevents 中所有的记录。\n\n$ psql -h 127.0.0.1 -U postgres -d minio_eventsminio_events=# select * from bucketevents;key                 |                      value--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- images/myphoto.jpg | {"Records": [{"s3": {"bucket": {"arn": "arn:aws:s3:::images", "name": "images", "ownerIdentity": {"principalId": "minio"}}, "object": {"key": "myphoto.jpg", "eTag": "1d97bf45ecb37f7a7b699418070df08f", "size": 56060, "sequencer": "147CE57C70B31931"}, "configurationId": "Config", "s3SchemaVersion": "1.0"}, "awsRegion": "us-east-1", "eventName": "s3:ObjectCreated:Put", "eventTime": "2016-10-12T21:18:20Z", "eventSource": "aws:s3", "eventVersion": "2.0", "userIdentity": {"principalId": "minio"}, "responseElements": {}, "requestParameters": {"sourceIPAddress": "[::1]:39706"}}]}(1 row)\n\n\n\n# 使用MySQL发布Minio事件\n\n安装 MySQL. 为了演示，我们将"postgres"用户的密码设为password，并且创建了一个miniodb数据库来存储事件信息。\n\n这个通知目标支持两种格式: namespace and access。\n\n如果使用的是namespace格式，Minio将存储桶里的对象同步成数据库表中的行。每一行有两列：key_name和value。key_name是这个对象的存储桶名字加上对象名，value都是一个有关这个Minio对象的JSON格式的事件数据。如果对象更新或者删除，表中相应的行也会相应的更新或者删除。\n\n如果使用的是access,Minio将将事件添加到表里，行有两列：event_time 和 event_data。event_time是事件在Minio server里发生的时间，event_data是有关这个Minio对象的JSON格式的事件数据。在这种格式下，不会有行会被删除或者修改。\n\n下面的步骤展示的是如何在namespace格式下使用通知目标，*access*差不多，不再赘述。\n\n\n# 第一步：确保确保至少满足第低要求\n\nMinio要求MySQL 版本 5.7.8及以上，Minio使用了MySQL5.7.8版本引入的 JSON 数据类型。我们使用的是MySQL5.7.17进行的测试。\n\n\n# 第二步：集成MySQL到Minio\n\nMinio Server的配置文件默认路径是 ~/.minio/config.json。MySQL配置信息是在notify这个节点下的mysql节点下，在这里为你的MySQL实例创建配置信息键值对，key是你的PostgreSQL的名称，value是下面表格中列列的键值对集合。\n\n参数          类型       描述\nenable      bool     (必须)此配置是否启用？\nformat      string   (必须)是 namespace 还是 access\ndsnString   string   (可选)MySQL的 Data-Source-Name连接串 。如果没设值，连接信息将使用下列参数： host,\n                     port, user, password 以及 database\ntable       string   (必须) 事件对应的表名，如果该表不存在，Mniio server会在启动时创建。\nhost        string   MySQL server主机名 (如果 dsnString 是空才会使用此配置)。\nport        string   MySQL server端口号 (如果 dsnString 是空才会使用此配置)。\nuser        string   数据库用户名 (如果 dsnString 是空才会使用此配置)。\npassword    string   数据库密码(如果 dsnString 是空才会使用此配置)。\ndatabase    string   数据库名(如果 dsnString 是空才会使用此配置)。\n\n下面是一个MySQL配置示例:\n\n"mysql": {        "1": {                "enable": true,                "dsnString": "",                "table": "minio_images",                "host": "172.17.0.1",                "port": "3306",                "user": "root",                "password": "password",                "database": "miniodb"        }}\n\n\n更新完配置文件后，重启Minio Server让配置生效。如果一切顺利，Minio Server会在启动时输出一行信息，类似 SQS ARNs: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:mysql。\n\n和之前描述的一样，你也可以添加多个MySQL实例，只要ID不重复就行。\n\n\n# 第三步：使用Minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知，一旦上有文件上传到存储桶中，MySQL中会insert一条新的记录或者一条已经存在的记录会被update，如果一个存在对象被删除，一条对应的记录也会从MySQL表中删除。因此，MySQL表中的行，对应的就是存储桶里的一个对象。\n\n要配置这种存储桶通知，我们需要用到前面步骤Minio输出的ARN信息。更多有关ARN的资料，请参考这里。\n\n有了mc这个工具，这些配置信息很容易就能添加上。假设咱们的Minio服务别名叫myminio,可执行下列脚本：\n\n# Create bucket named `images` in myminiomc mb myminio/images# Add notification configuration on the `images` bucket using the MySQL ARN. The --suffix argument filters events.mc events add myminio/images arn:minio:sqs:us-east-1:1:postgresql --suffix .jpg# Print out the notification configuration on the `images` bucket.mc events list myminio/imagesarn:minio:sqs:us-east-1:1:postgresql s3:ObjectCreated:*,s3:ObjectRemoved:* Filter: suffix=”.jpg”\n\n\n\n# 第四步：验证MySQL\n\n打开一个新的terminal终端并上传一张JPEG图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n打开一个MySQL终端列出表 minio_images 中所有的记录。\n\n$ mysql -h 172.17.0.1 -P 3306 -u root -p miniodbmysql> select * from minio_images;+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| key_name           | value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| images/myphoto.jpg | {"Records": [{"s3": {"bucket": {"arn": "arn:aws:s3:::images", "name": "images", "ownerIdentity": {"principalId": "minio"}}, "object": {"key": "myphoto.jpg", "eTag": "467886be95c8ecfd71a2900e3f461b4f", "size": 26, "sequencer": "14AC59476F809FD3"}, "configurationId": "Config", "s3SchemaVersion": "1.0"}, "awsRegion": "us-east-1", "eventName": "s3:ObjectCreated:Put", "eventTime": "2017-03-16T11:29:00Z", "eventSource": "aws:s3", "eventVersion": "2.0", "userIdentity": {"principalId": "minio"}, "responseElements": {"x-amz-request-id": "14AC59476F809FD3", "x-minio-origin-endpoint": "http://192.168.86.110:9000"}, "requestParameters": {"sourceIPAddress": "127.0.0.1:38260"}}]} |+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec)\n\n\n\n# 使用Kafka发布Minio事件\n\n安装 Apache Kafka.\n\n\n# 第一步：确保确保至少满足第低要求\n\nMinio要求Kafka版本0.10或者0.9.Minio内部使用了 Shopify/sarama 库，因此需要和该库有同样的版本兼容性。\n\n\n# 第二步：集成Kafka到Minio\n\nMinio Server的配置文件默认路径是 ~/.minio/config.json。参考下面的示例更新Kafka配置：\n\n"kafka": {    "1": {        "enable": true,        "brokers": ["localhost:9092"],        "topic": "bucketevents"    }}\n\n\n重启Minio server让配置生效。bucketevents是本示例用到的Kafka主题（topic）。\n\n\n# 第三步：使用Minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知，一旦上有文件上传到存储桶中，事件将被触发。在这里，ARN的值是arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:kafka。更多有关ARN的资料，请参考这里。\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:kafka --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:kafka s3:ObjectCreated:*,s3:ObjectRemoved:* Filter: suffix=”.jpg”\n\n\n\n# 第四步：验证Kafka\n\n我们使用 kafkacat 将所有的通知输出到控制台。\n\nkafkacat -C -b localhost:9092 -t bucketevents\n\n\n打开一个新的terminal终端并上传一张JPEG图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\nkafkacat 输出事件通知到控制台。\n\nkafkacat -b localhost:9092 -t bucketevents{"EventType":"s3:ObjectCreated:Put","Key":"images/myphoto.jpg","Records":[{"eventVersion":"2.0","eventSource":"aws:s3","awsRegion":"us-east-1","eventTime":"2017-01-31T10:01:51Z","eventName":"s3:ObjectCreated:Put","userIdentity":{"principalId":"88QR09S7IOT4X1IBAQ9B"},"requestParameters":{"sourceIPAddress":"192.173.5.2:57904"},"responseElements":{"x-amz-request-id":"149ED2FD25589220","x-minio-origin-endpoint":"http://192.173.5.2:9000"},"s3":{"s3SchemaVersion":"1.0","configurationId":"Config","bucket":{"name":"images","ownerIdentity":{"principalId":"88QR09S7IOT4X1IBAQ9B"},"arn":"arn:aws:s3:::images"},"object":{"key":"myphoto.jpg","size":541596,"eTag":"04451d05b4faf4d62f3d538156115e2a","sequencer":"149ED2FD25589220"}}}],"level":"info","msg":"","time":"2017-01-31T15:31:51+05:30"}\n\n\n\n# 使用Webhook发布Minio事件\n\nWebhooks 采用推的方式获取数据，而不是一直去拉取。\n\n\n# 第一步：集成MySQL到Minio\n\nMinio Server的配置文件默认路径是 ~/.minio/config.json。参考下面的示例更新Webhook配置：\n\n"webhook": {  "1": {    "enable": true,    "endpoint": "http://localhost:3000/"}\n\n\nendpoint是监听webhook通知的服务。保存配置文件并重启Minio服务让配配置生效。注意一下，在重启Minio时，这个endpoint必须是启动并且可访问到。\n\n\n# 第二步：使用Minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知，一旦上有文件上传到存储桶中，事件将被触发。在这里，ARN的值是arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:webhook。更多有关ARN的资料，请参考这里。\n\nmc mb myminio/imagesmc mb myminio/images-thumbnailmc events add myminio/images arn:minio:sqs:us-east-1:1:webhook --events put --suffix .jpg\n\n\n验证事件通知是否配置正确：\n\nmc events list myminio/images\n\n\n你应该可以收到如下的响应：\n\narn:minio:sqs:us-east-1:1:webhook   s3:ObjectCreated:*   Filter: suffix=".jpg"\n\n\n\n# 第三步：采用Thumbnailer进行验证\n\n我们使用 Thumbnailer 来监听Minio通知。如果有文件上传于是Minio服务，Thumnailer监听到该通知，生成一个缩略图并上传到Minio服务。安装Thumbnailer:\n\ngit clone https://github.com/minio/thumbnailer/npm install\n\n\n然后打开Thumbnailer的config/webhook.json配置文件，添加有关Minio server的配置，使用下面的方式启动Thumbnailer:\n\nNODE_ENV=webhook node thumbnail-webhook.js\n\n\nThumbnailer运行在http://localhost:3000/。下一步，配置Minio server,让其发送消息到这个URL（第一步提到的），并使用 mc 来设置存储桶通知（第二步提到的）。然后上传一张图片到Minio server:\n\nmc cp ~/images.jpg myminio/images.../images.jpg:  8.31 KB / 8.31 KB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 100.00% 59.42 KB/s 0s\n\n\n稍等片刻，然后使用mc ls检查存储桶的内容 -，你将看到有个缩略图出现了。\n\nmc ls myminio/images-thumbnail[2017-02-08 11:39:40 IST]   992B images-thumbnail.jpg\n\n\n注意 如果你用的是 distributed Minio,请修改所有节点的 ~/.minio/config.json。\n\n> 原文: https://docs.min.io/docs/minio-bucket-notification-guide.html',normalizedContent:'# minio存储桶通知指南\n\n存储桶（bucket）如果发生改变,比如上传对象和删除对象，可以使用存储桶事件通知机制进行监控，并通过以下方式发布出去:\n\nnotification targets\namqp\nmqtt\nelasticsearch\nredis\nnats\npostgresql\nmysql\napache kafka\nwebhooks\n\n\n# 前提条件\n\n * 从这里下载并安装minio server。\n * 从这里下载并安装minio client。\n\n\n# 使用amqp发布minio事件\n\n从这里下载安装rabbitmq。\n\n\n# 第一步: 将amqp endpoint添加到minio\n\nminio server的配置文件默认路径是 ~/.minio/config.json。amqp配置信息是在notify这个节点下的amqp节点下，在这里为你的amqp实例创建配置信息键值对，key是你的amqp endpoint的名称，value是下面表格中列列的键值对集合。\n\n参数             类型       描述\nenable         bool     (必须) 此amqp server endpoint是否可用\nurl            string   (必须) amqp server endpoint, 例如.\n                        amqp://myuser:mypassword@localhost:5672\nexchange       string   exchange名称\nroutingkey     string   发布用的routing key\nexchangetype   string   exchange类型\ndeliverymode   uint8    发布方式。 0或1 - 瞬态; 2 - 持久。\nmandatory      bool     publishing related bool.\nimmediate      bool     publishing related bool.\ndurable        bool     exchange declaration related bool.\ninternal       bool     exchange declaration related bool.\nnowait         bool     exchange declaration related bool.\nautodeleted    bool     exchange declaration related bool.\n\n下面展示的是rabbitmq的配置示例:\n\n"amqp": {    "1": {        "enable": true,        "url": "amqp://myuser:mypassword@localhost:5672",        "exchange": "bucketevents",        "routingkey": "bucketlogs",        "exchangetype": "fanout",        "deliverymode": 0,        "mandatory": false,        "immediate": false,        "durable": false,        "internal": false,        "nowait": false,        "autodeleted": false    }}\n\n\n更新完配置文件后，重启minio server让配置生效。如果一切顺利，minio server会在启动时输出一行信息，类似 sqs arns: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:amqp。\n\nminio支持rabbitmq中所有的交换方式，这次我们采用 fanout 交换。\n\n注意一下，你可以听从你内心的想法，想配几个amqp服务就配几个，只要每个amqp服务实例有不同的id (比如前面示例中的"1") 和配置信息。\n\n\n# 第二步: 使用minio客户端启用bucket通知\n\n如果一个jpeg图片上传到myminio server里的images 存储桶或者从桶中删除，一个存储桶事件通知就会被触发。 这里arn值是arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:amqp，想了解更多关于arn的信息，请参考aws arn documentation.\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:amqp --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:amqp s3:objectcreated:*,s3:objectremoved:* filter: suffix=”.jpg”\n\n\n\n# 第三步:在rabbitmq上进行验证\n\n下面将要出场的python程序会等待队列交换tbucketevents并在控制台中输出事件通知。我们使用的是pika python client 来实现此功能。\n\n#!/usr/bin/env pythonimport pikaconnection = pika.blockingconnection(pika.connectionparameters(        host=\'localhost\'))channel = connection.channel()channel.exchange_declare(exchange=\'bucketevents\',                         exchange_type=\'fanout\')result = channel.queue_declare(exclusive=false)queue_name = result.method.queuechannel.queue_bind(exchange=\'bucketevents\',                   queue=queue_name)print(\' [*] waiting for logs. to exit press ctrl+c\')def callback(ch, method, properties, body):    print(" [x] %r" % body)channel.basic_consume(callback,                      queue=queue_name,                      no_ack=false)channel.start_consuming()\n\n\n执行示例中的python程序来观察rabbitmq事件。\n\npython rabbit.py\n\n\n另开一个terminal终端并上传一张jpeg图片到images存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n一旦上传完毕，你应该会通过rabbitmq收到下面的事件通知。\n\npython rabbit.py‘{“records”:[{“eventversion”:”2.0",”eventsource”:”aws:s3",”awsregion”:”us-east-1",”eventtime”:”2016–09–08t22:34:38.226z”,”eventname”:”s3:objectcreated:put”,”useridentity”:{“principalid”:”minio”},”requestparameters”:{“sourceipaddress”:”10.1.10.150:44576"},”responseelements”:{},”s3":{“s3schemaversion”:”1.0",”configurationid”:”config”,”bucket”:{“name”:”images”,”owneridentity”:{“principalid”:”minio”},”arn”:”arn:aws:s3:::images”},”object”:{“key”:”myphoto.jpg”,”size”:200436,”sequencer”:”147279eaf9f40933"}}}],”level”:”info”,”msg”:””,”time”:”2016–09–08t15:34:38–07:00"}\\n\n\n\n\n# 使用mqtt发布minio事件\n\n从 这里安装mqtt broker。\n\n\n# 第一步: 添加mqtt endpoint到minio\n\nminio server的配置文件默认路径是 ~/.minio/config.json。mqtt配置信息是在notify这个节点下的mqtt节点下，在这里为你的mqtt实例创建配置信息键值对，key是你的mqtt endpoint的名称，value是下面表格中列列的键值对集合。\n\n参数         类型       描述\nenable     bool     (必须) 这个 server endpoint是否可用?\nbroker     string   (必须) mqtt server endpoint, 例如. tcp://localhost:1883\ntopic      string   (必须) 要发布的mqtt主题的名称, 例如. minio\nqos        int      设置服务质量级别\nclientid   string   mqtt代理识别minio的唯一id\nusername   string   连接mqtt server的用户名 (如果需要的话)\npassword   string   链接mqtt server的密码 (如果需要的话)\n\n以下是一个mqtt的配置示例:\n\n"mqtt": {    "1": {        "enable": true,        "broker": "tcp://localhost:1883",        "topic": "minio",        "qos": 1,        "clientid": "minio",        "username": "",        "password": ""    }}\n\n\n更新完配置文件后，重启minio server让配置生效。如果一切顺利，minio server会在启动时输出一行信息，类似 sqs arns: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:mqtt。\n\nminio支持任何支持mqtt 3.1或3.1.1的mqtt服务器，并且可以通过tcp，tls或websocket连接使用tcp://, tls://, or ws://分别作为代理url的方案。 更多信息，请参考 go client。\n\n注意一下，你还是和之前amqp一样可以听从你内心的想法，想配几个mqtt服务就配几个，只要每个mqtt服务实例有不同的id (比如前面示例中的"1") 和配置信息。\n\n\n# 第二步: 使用minio客户端启用bucket通知\n\n如果一个jpeg图片上传到myminio server里的images 存储桶或者从桶中删除，一个存储桶事件通知就会被触发。 这里arn值是arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:mqtt。\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:mqtt --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:amqp s3:objectcreated:*,s3:objectremoved:* filter: suffix=”.jpg”\n\n\n\n# 第三步：验证mqtt\n\n下面的python程序等待mqtt主题/ minio，并在控制台上打印事件通知。 我们使用paho-mqtt库来执行此操作。\n\n#!/usr/bin/env pythonfrom __future__ import print_functionimport paho.mqtt.client as mqtt# the callback for when the client receives a connack response from the server.def on_connect(client, userdata, flags, rc):    print("connected with result code", rc)    # subscribing in on_connect() means that if we lose the connection and    # reconnect then subscriptions will be renewed.    client.subscribe("/minio")# the callback for when a publish message is received from the server.def on_message(client, userdata, msg):    print(msg.payload)client = mqtt.client()client.on_connect = on_connectclient.on_message = on_messageclient.connect("localhost:1883", 1883, 60)# blocking call that processes network traffic, dispatches callbacks and# handles reconnecting.# other loop*() functions are available that give a threaded interface and a# manual interface.client.loop_forever()\n\n\n执行这个python示例程序来观察mqtt事件。\n\npython mqtt.py\n\n\n打开一个新的terminal终端并上传一张jpeg图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n一旦上传完毕，你应该会通过mqtt收到下面的事件通知。\n\npython mqtt.py{“records”:[{“eventversion”:”2.0",”eventsource”:”aws:s3",”awsregion”:”us-east-1",”eventtime”:”2016–09–08t22:34:38.226z”,”eventname”:”s3:objectcreated:put”,”useridentity”:{“principalid”:”minio”},”requestparameters”:{“sourceipaddress”:”10.1.10.150:44576"},”responseelements”:{},”s3":{“s3schemaversion”:”1.0",”configurationid”:”config”,”bucket”:{“name”:”images”,”owneridentity”:{“principalid”:”minio”},”arn”:”arn:aws:s3:::images”},”object”:{“key”:”myphoto.jpg”,”size”:200436,”sequencer”:”147279eaf9f40933"}}}],”level”:”info”,”msg”:””,”time”:”2016–09–08t15:34:38–07:00"}\n\n\n\n# 使用elasticsearch发布minio事件\n\n安装 elasticsearch 。\n\n这个通知目标支持两种格式: namespace and access。\n\n如果使用的是 namespace 格式, minio将桶中的对象与索引中的文档进行同步。对于minio的每一个事件，es都会创建一个document,这个document的id就是存储桶以及存储对象的名称。事件的其他细节存储在document的正文中。因此，如果一个已经存在的对象在minio中被覆盖，在es中的相对应的document也会被更新。如果一个对象被删除，相对应的document也会从index中删除。\n\n如果使用的是access格式，minio将事件作为document加到es的index中。对于每一个事件，es同样会创建一个document,这个document包含事件的所有细节，document的时间戳设置为事件的时间戳，并将该document加到es的index中。这个document的id是由es随机生成的。在access格式下，没有文档会被删除或者修改，对于一个对象的操作，都会生成新的document附加到index中。\n\n下面的步骤展示的是在namespace格式下，如何使用通知目标。另一种格式和这个很类似，为了不让你们说我墨迹，就不再赘述了。\n\n\n# 第一步：确保至少满足第低要求\n\nminio要求使用的是es 5.x系统版本。如果使用的是低版本的es，也没关系，es官方支持升级迁移，详情请看这里。\n\n\n# 第二步：把es集成到minio中\n\nminio server的配置文件默认路径是 ~/.minio/config.json。es配置信息是在notify这个节点下的elasticsearch节点下，在这里为你的es实例创建配置信息键值对，key是你的es的名称，value是下面表格中列列的键值对集合。\n\n参数       类型       描述\nenable   bool     (必须) 是否启用这个配置?\nformat   string   (必须) 是namespace 还是 access\nurl      string   (必须) es地址，比如: http://localhost:9200\nindex    string   (必须) 给minio用的index\n\n以下是es的一个配置示例:\n\n"elasticsearch": {    "1": {        "enable": true,        "format": "namespace",        "url": "http://127.0.0.1:9200",        "index": "minio_events"    }},\n\n\n更新完配置文件后，重启minio server让配置生效。如果一切顺利，minio server会在启动时输出一行信息，类似 sqs arns: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:elasticsearch。\n\n注意一下，你又可以再一次听从你内心的想法，想配几个es服务就配几个，只要每个es服务实例有不同的id (比如前面示例中的"1") 和配置信息。\n\n\n# 第三步：使用minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知。一旦有文件被创建或者覆盖，一个新的es的document会被创建或者更新到之前咱配的index里。如果一个已经存在的对象被删除，这个对应的document也会从index中删除。因此，这个es index里的行，就映射着images存储桶里的对象。\n\n要配置这种存储桶通知，我们需要用到前面步骤minio输出的arn信息。更多有关arn的资料，请参考这里。\n\n有了mc这个工具，这些配置信息很容易就能添加上。假设咱们的minio服务别名叫myminio,可执行下列脚本：\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:elasticsearch --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:elasticsearch s3:objectcreated:*,s3:objectremoved:* filter: suffix=”.jpg”\n\n\n\n# 第四步：验证es\n\n上传一张图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n使用curl查到minio_events index中的内容。\n\n$ curl  "http://localhost:9200/minio_events/_search?pretty=true"{  "took" : 40,  "timed_out" : false,  "_shards" : {    "total" : 5,    "successful" : 5,    "failed" : 0  },  "hits" : {    "total" : 1,    "max_score" : 1.0,    "hits" : [      {        "_index" : "minio_events",        "_type" : "event",        "_id" : "images/myphoto.jpg",        "_score" : 1.0,        "_source" : {          "records" : [            {              "eventversion" : "2.0",              "eventsource" : "minio:s3",              "awsregion" : "us-east-1",              "eventtime" : "2017-03-30t08:00:41z",              "eventname" : "s3:objectcreated:put",              "useridentity" : {                "principalid" : "minio"              },              "requestparameters" : {                "sourceipaddress" : "127.0.0.1:38062"              },              "responseelements" : {                "x-amz-request-id" : "14b09a09703fc47b",                "x-minio-origin-endpoint" : "http://192.168.86.115:9000"              },              "s3" : {                "s3schemaversion" : "1.0",                "configurationid" : "config",                "bucket" : {                  "name" : "images",                  "owneridentity" : {                    "principalid" : "minio"                  },                  "arn" : "arn:aws:s3:::images"                },                "object" : {                  "key" : "myphoto.jpg",                  "size" : 6474,                  "etag" : "a3410f4f8788b510d6f19c5067e60a90",                  "sequencer" : "14b09a09703fc47b"                }              },              "source" : {                "host" : "127.0.0.1",                "port" : "38062",                "useragent" : "minio (linux; amd64) minio-go/2.0.3 mc/2017-02-15t17:57:25z"              }            }          ]        }      }    ]  }}\n\n\n这个输出显示在es中为这个事件创建了一个document。\n\n这里我们可以看到这个document id就是存储桶和对象的名称。如果用的是access格式，这个document id就是由es随机生成的。\n\n\n# 使用redis发布minio事件\n\n安装 redis。为了演示，我们将数据库密码设为"yoursecret"。\n\n这种通知目标支持两种格式: namespace 和 access。\n\n如果用的是namespacee格式，minio将存储桶里的对象同步成redis hash中的条目。对于每一个条目，对应一个存储桶里的对象，其key都被设为"存储桶名称/对象名称"，value都是一个有关这个minio对象的json格式的事件数据。如果对象更新或者删除，hash中对象的条目也会相应的更新或者删除。\n\n如果使用的是access,minio使用rpush将事件添加到list中。这个list中每一个元素都是一个json格式的list,这个list中又有两个元素，第一个元素是时间戳的字符串，第二个元素是一个含有在这个存储桶上进行操作的事件数据的json对象。在这种格式下，list中的元素不会更新或者删除。\n\n下面的步骤展示如何在namespace和access格式下使用通知目标。\n\n\n# 第一步：集成redis到minio\n\nminio server的配置文件默认路径是 ~/.minio/config.json。redis配置信息是在notify这个节点下的redis节点下，在这里为你的redis实例创建配置信息键值对，key是你的redis端的名称，value是下面表格中的键值对里面值的集合。\n\n参数         类型       描述\nenable     bool     (必须) 这个配置是否可用?\nformat     string   (必须) 是 namespace 还是 access\naddress    string   (必须) redis服务地址，比如: localhost:6379\npassword   string   (可选) redis服务密码\nkey        string   (必须) 事件要存储到redis\n                    key的名称。如果用的是namespace格式的话，则是一个hash,如果是access格式的话，则是一个list\n\n下面是一个redis配置示例:\n\n"redis": {    "1": {        "enable": true,        "address": "127.0.0.1:6379",        "password": "yoursecret",        "key": "bucketevents"    }}\n\n\n更新完配置文件后，重启minio server让配置生效。如果一切顺利，minio server会在启动时输出一行信息，类似 sqs arns: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:redis。\n\n注意一下，你永远都可以听从你内心的想法，想配几个redis服务就配几个，只要每个redis服务实例有不同的id (比如前面示例中的"1") 和配置信息。\n\n\n# 第二步: 使用minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知。一旦有文件被创建或者覆盖，一个新的key会被创建,或者一个已经存在的key就会被更新到之前配置好的redis hash里。如果一个已经存在的对象被删除，这个对应的key也会从hash中删除。因此，这个redis hash里的行，就映射着images存储桶里的.jpg对象。\n\n要配置这种存储桶通知，我们需要用到前面步骤minio输出的arn信息。更多有关arn的资料，请参考这里。\n\n有了mc这个工具，这些配置信息很容易就能添加上。假设咱们的minio服务别名叫myminio,可执行下列脚本：\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:redis --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:redis s3:objectcreated:*,s3:objectremoved:* filter: suffix=”.jpg”\n\n\n\n# 第三步：验证redis\n\n启动redis-cli这个redis客户端程序来检查redis中的内容. 运行monitorredis命令将会输出在redis上执行的每个命令的。\n\nredis-cli -a yoursecret127.0.0.1:6379> monitorok\n\n\n打开一个新的terminal终端并上传一张jpeg图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n在上一个终端中，你将看到minio在redis上执行的操作：\n\n127.0.0.1:6379> monitorok1490686879.650649 [0 172.17.0.1:44710] "ping"1490686879.651061 [0 172.17.0.1:44710] "hset" "minio_events" "images/myphoto.jpg" "{\\"records\\":[{\\"eventversion\\":\\"2.0\\",\\"eventsource\\":\\"minio:s3\\",\\"awsregion\\":\\"us-east-1\\",\\"eventtime\\":\\"2017-03-28t07:41:19z\\",\\"eventname\\":\\"s3:objectcreated:put\\",\\"useridentity\\":{\\"principalid\\":\\"minio\\"},\\"requestparameters\\":{\\"sourceipaddress\\":\\"127.0.0.1:52234\\"},\\"responseelements\\":{\\"x-amz-request-id\\":\\"14affbd1ace5f632\\",\\"x-minio-origin-endpoint\\":\\"http://192.168.86.115:9000\\"},\\"s3\\":{\\"s3schemaversion\\":\\"1.0\\",\\"configurationid\\":\\"config\\",\\"bucket\\":{\\"name\\":\\"images\\",\\"owneridentity\\":{\\"principalid\\":\\"minio\\"},\\"arn\\":\\"arn:aws:s3:::images\\"},\\"object\\":{\\"key\\":\\"myphoto.jpg\\",\\"size\\":2586,\\"etag\\":\\"5d284463f9da279f060f0ea4d11af098\\",\\"sequencer\\":\\"14affbd1ace5f632\\"}},\\"source\\":{\\"host\\":\\"127.0.0.1\\",\\"port\\":\\"52234\\",\\"useragent\\":\\"minio (linux; amd64) minio-go/2.0.3 mc/2017-02-15t17:57:25z\\"}}]}"\n\n\n在这我们可以看到minio在minio_events这个key上执行了hset命令。\n\n如果用的是access格式，那么minio_events就是一个list,minio就会调用rpush添加到list中。这个list的消费者会使用blpop从list的最左端删除list元素。\n\n\n# 使用nats发布minio事件\n\n安装 nats.\n\n\n# 第一步：集成nats到minio\n\nminio server的配置文件默认路径是 ~/.minio/config.json。参考下面的示例修改nats的配置:\n\n"nats": {    "1": {        "enable": true,        "address": "0.0.0.0:4222",        "subject": "bucketevents",        "username": "yourusername",        "password": "yoursecret",        "token": "",        "secure": false,        "pinginterval": 0        "streaming": {            "enable": false,            "clusterid": "",            "clientid": "",            "async": false,            "maxpubacksinflight": 0        }    }},\n\n\n更新完配置文件后，重启minio server让配置生效。bucketevents是nats在这个例子中使用的主题。\n\nminio服务也支持 nats streaming mode ，这种模式额外提供了像 message/event persistence, at-least-once-delivery, 以及 publisher rate limiting这样的功能。如果想让minio服务发送通知到nats streaming server,参考下面示面进行配置：\n\n"nats": {    "1": {        "enable": true,        "address": "0.0.0.0:4222",        "subject": "bucketevents",        "username": "yourusername",        "password": "yoursecret",        "token": "",        "secure": false,        "pinginterval": 0        "streaming": {            "enable": true,            "clusterid": "test-cluster",            "clientid": "minio-client",            "async": true,            "maxpubacksinflight": 10        }    }},\n\n\n更多关于 clusterid, clientid 的信息，请看 nats documentation. 关于 maxpubacksinflight ，请看 这里.\n\n\n# 第二步: 使用minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知，一旦myminio server上有文件 从images存储桶里删除或者上传到存储桶中，事件即被触发。在这里，arn的值是arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:nats。 更多有关arn的资料，请参考这里。\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:nats --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:nats s3:objectcreated:*,s3:objectremoved:* filter: suffix=”.jpg”\n\n\n\n# 第三步：验证nats\n\n如果你用的是nats server，请查看下面的示例程序来记录添加到nats的存储桶通知。\n\npackage main// import go and nats packagesimport (    "log"    "runtime"    "github.com/nats-io/nats")func main() {    // create server connection    natsconnection, _ := nats.connect("nats://yourusername:yoursecret@localhost:4222")    log.println("connected")    // subscribe to subject    log.printf("subscribing to subject \'bucketevents\'\\n")    natsconnection.subscribe("bucketevents", func(msg *nats.msg) {        // handle the message        log.printf("received message \'%s\\n", string(msg.data)+"\'")    })    // keep the connection alive    runtime.goexit()}\ngo run nats.go2016/10/12 06:39:18 connected2016/10/12 06:39:18 subscribing to subject \'bucketevents\'\n\n\n打开一个新的terminal终端并上传一张jpeg图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\nnats.go示例程序将事件通知打印到控制台。\n\ngo run nats.go2016/10/12 06:51:26 connected2016/10/12 06:51:26 subscribing to subject \'bucketevents\'2016/10/12 06:51:33 received message \'{"eventtype":"s3:objectcreated:put","key":"images/myphoto.jpg","records":[{"eventversion":"2.0","eventsource":"aws:s3","awsregion":"us-east-1","eventtime":"2016-10-12t13:51:33z","eventname":"s3:objectcreated:put","useridentity":{"principalid":"minio"},"requestparameters":{"sourceipaddress":"[::1]:57106"},"responseelements":{},"s3":{"s3schemaversion":"1.0","configurationid":"config","bucket":{"name":"images","owneridentity":{"principalid":"minio"},"arn":"arn:aws:s3:::images"},"object":{"key":"myphoto.jpg","size":56060,"etag":"1d97bf45ecb37f7a7b699418070df08f","sequencer":"147ccd1ae054bfd0"}}}],"level":"info","msg":"","time":"2016-10-12t06:51:33-07:00"}\n\n\n如果你用的是nats streaming server,请查看下面的示例程序来记录添加到nats的存储桶通知。\n\npackage main// import go and nats packagesimport (    "fmt"    "runtime"    "github.com/nats-io/go-nats-streaming")func main() {    natsconnection, _ := stan.connect("test-cluster", "test-client")    log.println("connected")    // subscribe to subject    log.printf("subscribing to subject \'bucketevents\'\\n")    natsconnection.subscribe("bucketevents", func(m *stan.msg) {        // handle the message        fmt.printf("received a message: %s\\n", string(m.data))    })    // keep the connection alive    runtime.goexit()}\ngo run nats.go2017/07/07 11:47:40 connected2017/07/07 11:47:40 subscribing to subject \'bucketevents\'\n\n\n打开一个新的terminal终端并上传一张jpeg图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\nnats.go示例程序将事件通知打印到控制台。\n\nreceived a message: {"eventtype":"s3:objectcreated:put","key":"images/myphoto.jpg","records":[{"eventversion":"2.0","eventsource":"minio:s3","awsregion":"","eventtime":"2017-07-07t18:46:37z","eventname":"s3:objectcreated:put","useridentity":{"principalid":"minio"},"requestparameters":{"sourceipaddress":"192.168.1.80:55328"},"responseelements":{"x-amz-request-id":"14cf20bd1efd5b93","x-minio-origin-endpoint":"http://127.0.0.1:9000"},"s3":{"s3schemaversion":"1.0","configurationid":"config","bucket":{"name":"images","owneridentity":{"principalid":"minio"},"arn":"arn:aws:s3:::images"},"object":{"key":"myphoto.jpg","size":248682,"etag":"f1671feacb8bbf7b0397c6e9364e8c92","contenttype":"image/jpeg","userdefined":{"content-type":"image/jpeg"},"versionid":"1","sequencer":"14cf20bd1efd5b93"}},"source":{"host":"192.168.1.80","port":"55328","useragent":"minio (linux; amd64) minio-go/2.0.4 mc/development.goget"}}],"level":"info","msg":"","time":"2017-07-07t11:46:37-07:00"}\n\n\n\n# 使用postgresql发布minio事件\n\n安装 postgresql 数据库。为了演示，我们将"postgres"用户的密码设为password，并且创建了一个minio_events数据库来存储事件信息。\n\n这个通知目标支持两种格式: namespace and access。\n\n如果使用的是namespace格式，minio将存储桶里的对象同步成数据库表中的行。每一行有两列：key和value。key是这个对象的存储桶名字加上对象名，value都是一个有关这个minio对象的json格式的事件数据。如果对象更新或者删除，表中相应的行也会相应的更新或者删除。\n\n如果使用的是access,minio将将事件添加到表里，行有两列：event_time 和 event_data。event_time是事件在minio server里发生的时间，event_data是有关这个minio对象的json格式的事件数据。在这种格式下，不会有行会被删除或者修改。\n\n下面的步骤展示的是如何在namespace格式下使用通知目标，*access*差不多，不再赘述，我相信你可以触类旁通，举一反三，不要让我失望哦。\n\n\n# 第一步：确保确保至少满足第低要求\n\nminio要求postgressql9.5版本及以上。 minio用了postgresql9.5引入的insert on conflict (aka upsert) 特性,以及9.4引入的 jsonb 数据类型。\n\n\n# 第二步：集成postgresql到minio\n\nminio server的配置文件默认路径是 ~/.minio/config.json。postgresql配置信息是在notify这个节点下的postgresql节点下，在这里为你的postgresql实例创建配置信息键值对，key是你的postgresql的名称，value是下面表格中列列的键值对集合。\n\n参数                 类型       描述\nenable             bool     (必须)此配置是否启用\nformat             string   (必须) 是 namespace 还是 access\nconnectionstring   string   (可选) postgresql的连接参数 。比如可以用来设置 sslmode\ntable              string   (必须) 事件对应的表名，如果该表不存在，mniio server会在启动时创建。\nhost               string   (可选) postgressql的主机名，默认是localhost\nport               string   (可选) postgresql的端口号，默认是5432\nuser               string   (可选)数据库用户名，默认是运行minio server进程的用户\npassword           string   (可选) 数据库密码\ndatabase           string   (可选)库名\n\n下面是一个postgresql配置示例:\n\n"postgresql": {    "1": {        "enable": true,        "format": "namespace",        "connectionstring": "sslmode=disable",        "table": "bucketevents",        "host": "127.0.0.1",        "port": "5432",        "user": "postgres",        "password": "password",        "database": "minio_events"    }}\n\n\n注意一下，为了演示，咱们这把ssl禁掉了，但是为了安全起见，不建议在生产环境这么弄。\n\n更新完配置文件后，重启minio server让配置生效。如果一切顺利，minio server会在启动时输出一行信息，类似 sqs arns: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:postgresql。\n\n和之前描述的一样，你也可以添加多个postresql实例，只要id不重复就行。\n\n\n# 第三步：使用minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知，一旦上有文件上传到存储桶中，postgresql中会insert一条新的记录或者一条已经存在的记录会被update，如果一个存在对象被删除，一条对应的记录也会从postgresql表中删除。因此，postgresql表中的行，对应的就是存储桶里的一个对象。\n\n要配置这种存储桶通知，我们需要用到前面步骤中minio输出的arn信息。更多有关arn的资料，请参考这里。\n\n有了mc这个工具，这些配置信息很容易就能添加上。假设minio服务别名叫myminio,可执行下列脚本：\n\n# create bucket named `images` in myminiomc mb myminio/images# add notification configuration on the `images` bucket using the mysql arn. the --suffix argument filters events.mc events add myminio/images arn:minio:sqs:us-east-1:1:postgresql --suffix .jpg# print out the notification configuration on the `images` bucket.mc events list myminio/imagesmc events list myminio/imagesarn:minio:sqs:us-east-1:1:postgresql s3:objectcreated:*,s3:objectremoved:* filter: suffix=”.jpg”\n\n\n\n# 第四步：验证postgresql\n\n打开一个新的terminal终端并上传一张jpeg图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n打开一个postgresql终端列出表 bucketevents 中所有的记录。\n\n$ psql -h 127.0.0.1 -u postgres -d minio_eventsminio_events=# select * from bucketevents;key                 |                      value--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- images/myphoto.jpg | {"records": [{"s3": {"bucket": {"arn": "arn:aws:s3:::images", "name": "images", "owneridentity": {"principalid": "minio"}}, "object": {"key": "myphoto.jpg", "etag": "1d97bf45ecb37f7a7b699418070df08f", "size": 56060, "sequencer": "147ce57c70b31931"}, "configurationid": "config", "s3schemaversion": "1.0"}, "awsregion": "us-east-1", "eventname": "s3:objectcreated:put", "eventtime": "2016-10-12t21:18:20z", "eventsource": "aws:s3", "eventversion": "2.0", "useridentity": {"principalid": "minio"}, "responseelements": {}, "requestparameters": {"sourceipaddress": "[::1]:39706"}}]}(1 row)\n\n\n\n# 使用mysql发布minio事件\n\n安装 mysql. 为了演示，我们将"postgres"用户的密码设为password，并且创建了一个miniodb数据库来存储事件信息。\n\n这个通知目标支持两种格式: namespace and access。\n\n如果使用的是namespace格式，minio将存储桶里的对象同步成数据库表中的行。每一行有两列：key_name和value。key_name是这个对象的存储桶名字加上对象名，value都是一个有关这个minio对象的json格式的事件数据。如果对象更新或者删除，表中相应的行也会相应的更新或者删除。\n\n如果使用的是access,minio将将事件添加到表里，行有两列：event_time 和 event_data。event_time是事件在minio server里发生的时间，event_data是有关这个minio对象的json格式的事件数据。在这种格式下，不会有行会被删除或者修改。\n\n下面的步骤展示的是如何在namespace格式下使用通知目标，*access*差不多，不再赘述。\n\n\n# 第一步：确保确保至少满足第低要求\n\nminio要求mysql 版本 5.7.8及以上，minio使用了mysql5.7.8版本引入的 json 数据类型。我们使用的是mysql5.7.17进行的测试。\n\n\n# 第二步：集成mysql到minio\n\nminio server的配置文件默认路径是 ~/.minio/config.json。mysql配置信息是在notify这个节点下的mysql节点下，在这里为你的mysql实例创建配置信息键值对，key是你的postgresql的名称，value是下面表格中列列的键值对集合。\n\n参数          类型       描述\nenable      bool     (必须)此配置是否启用？\nformat      string   (必须)是 namespace 还是 access\ndsnstring   string   (可选)mysql的 data-source-name连接串 。如果没设值，连接信息将使用下列参数： host,\n                     port, user, password 以及 database\ntable       string   (必须) 事件对应的表名，如果该表不存在，mniio server会在启动时创建。\nhost        string   mysql server主机名 (如果 dsnstring 是空才会使用此配置)。\nport        string   mysql server端口号 (如果 dsnstring 是空才会使用此配置)。\nuser        string   数据库用户名 (如果 dsnstring 是空才会使用此配置)。\npassword    string   数据库密码(如果 dsnstring 是空才会使用此配置)。\ndatabase    string   数据库名(如果 dsnstring 是空才会使用此配置)。\n\n下面是一个mysql配置示例:\n\n"mysql": {        "1": {                "enable": true,                "dsnstring": "",                "table": "minio_images",                "host": "172.17.0.1",                "port": "3306",                "user": "root",                "password": "password",                "database": "miniodb"        }}\n\n\n更新完配置文件后，重启minio server让配置生效。如果一切顺利，minio server会在启动时输出一行信息，类似 sqs arns: arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:mysql。\n\n和之前描述的一样，你也可以添加多个mysql实例，只要id不重复就行。\n\n\n# 第三步：使用minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知，一旦上有文件上传到存储桶中，mysql中会insert一条新的记录或者一条已经存在的记录会被update，如果一个存在对象被删除，一条对应的记录也会从mysql表中删除。因此，mysql表中的行，对应的就是存储桶里的一个对象。\n\n要配置这种存储桶通知，我们需要用到前面步骤minio输出的arn信息。更多有关arn的资料，请参考这里。\n\n有了mc这个工具，这些配置信息很容易就能添加上。假设咱们的minio服务别名叫myminio,可执行下列脚本：\n\n# create bucket named `images` in myminiomc mb myminio/images# add notification configuration on the `images` bucket using the mysql arn. the --suffix argument filters events.mc events add myminio/images arn:minio:sqs:us-east-1:1:postgresql --suffix .jpg# print out the notification configuration on the `images` bucket.mc events list myminio/imagesarn:minio:sqs:us-east-1:1:postgresql s3:objectcreated:*,s3:objectremoved:* filter: suffix=”.jpg”\n\n\n\n# 第四步：验证mysql\n\n打开一个新的terminal终端并上传一张jpeg图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\n打开一个mysql终端列出表 minio_images 中所有的记录。\n\n$ mysql -h 172.17.0.1 -p 3306 -u root -p miniodbmysql> select * from minio_images;+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| key_name           | value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| images/myphoto.jpg | {"records": [{"s3": {"bucket": {"arn": "arn:aws:s3:::images", "name": "images", "owneridentity": {"principalid": "minio"}}, "object": {"key": "myphoto.jpg", "etag": "467886be95c8ecfd71a2900e3f461b4f", "size": 26, "sequencer": "14ac59476f809fd3"}, "configurationid": "config", "s3schemaversion": "1.0"}, "awsregion": "us-east-1", "eventname": "s3:objectcreated:put", "eventtime": "2017-03-16t11:29:00z", "eventsource": "aws:s3", "eventversion": "2.0", "useridentity": {"principalid": "minio"}, "responseelements": {"x-amz-request-id": "14ac59476f809fd3", "x-minio-origin-endpoint": "http://192.168.86.110:9000"}, "requestparameters": {"sourceipaddress": "127.0.0.1:38260"}}]} |+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec)\n\n\n\n# 使用kafka发布minio事件\n\n安装 apache kafka.\n\n\n# 第一步：确保确保至少满足第低要求\n\nminio要求kafka版本0.10或者0.9.minio内部使用了 shopify/sarama 库，因此需要和该库有同样的版本兼容性。\n\n\n# 第二步：集成kafka到minio\n\nminio server的配置文件默认路径是 ~/.minio/config.json。参考下面的示例更新kafka配置：\n\n"kafka": {    "1": {        "enable": true,        "brokers": ["localhost:9092"],        "topic": "bucketevents"    }}\n\n\n重启minio server让配置生效。bucketevents是本示例用到的kafka主题（topic）。\n\n\n# 第三步：使用minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知，一旦上有文件上传到存储桶中，事件将被触发。在这里，arn的值是arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:kafka。更多有关arn的资料，请参考这里。\n\nmc mb myminio/imagesmc events add  myminio/images arn:minio:sqs:us-east-1:1:kafka --suffix .jpgmc events list myminio/imagesarn:minio:sqs:us-east-1:1:kafka s3:objectcreated:*,s3:objectremoved:* filter: suffix=”.jpg”\n\n\n\n# 第四步：验证kafka\n\n我们使用 kafkacat 将所有的通知输出到控制台。\n\nkafkacat -c -b localhost:9092 -t bucketevents\n\n\n打开一个新的terminal终端并上传一张jpeg图片到images 存储桶。\n\nmc cp myphoto.jpg myminio/images\n\n\nkafkacat 输出事件通知到控制台。\n\nkafkacat -b localhost:9092 -t bucketevents{"eventtype":"s3:objectcreated:put","key":"images/myphoto.jpg","records":[{"eventversion":"2.0","eventsource":"aws:s3","awsregion":"us-east-1","eventtime":"2017-01-31t10:01:51z","eventname":"s3:objectcreated:put","useridentity":{"principalid":"88qr09s7iot4x1ibaq9b"},"requestparameters":{"sourceipaddress":"192.173.5.2:57904"},"responseelements":{"x-amz-request-id":"149ed2fd25589220","x-minio-origin-endpoint":"http://192.173.5.2:9000"},"s3":{"s3schemaversion":"1.0","configurationid":"config","bucket":{"name":"images","owneridentity":{"principalid":"88qr09s7iot4x1ibaq9b"},"arn":"arn:aws:s3:::images"},"object":{"key":"myphoto.jpg","size":541596,"etag":"04451d05b4faf4d62f3d538156115e2a","sequencer":"149ed2fd25589220"}}}],"level":"info","msg":"","time":"2017-01-31t15:31:51+05:30"}\n\n\n\n# 使用webhook发布minio事件\n\nwebhooks 采用推的方式获取数据，而不是一直去拉取。\n\n\n# 第一步：集成mysql到minio\n\nminio server的配置文件默认路径是 ~/.minio/config.json。参考下面的示例更新webhook配置：\n\n"webhook": {  "1": {    "enable": true,    "endpoint": "http://localhost:3000/"}\n\n\nendpoint是监听webhook通知的服务。保存配置文件并重启minio服务让配配置生效。注意一下，在重启minio时，这个endpoint必须是启动并且可访问到。\n\n\n# 第二步：使用minio客户端启用bucket通知\n\n我们现在可以在一个叫images的存储桶上开启事件通知，一旦上有文件上传到存储桶中，事件将被触发。在这里，arn的值是arn![:minio:](https://www.emoji-cheat-sheet.com/graphics/emojis/minio.png)sqs![:us-east-1:](https://www.emoji-cheat-sheet.com/graphics/emojis/us-east-1.png)1:webhook。更多有关arn的资料，请参考这里。\n\nmc mb myminio/imagesmc mb myminio/images-thumbnailmc events add myminio/images arn:minio:sqs:us-east-1:1:webhook --events put --suffix .jpg\n\n\n验证事件通知是否配置正确：\n\nmc events list myminio/images\n\n\n你应该可以收到如下的响应：\n\narn:minio:sqs:us-east-1:1:webhook   s3:objectcreated:*   filter: suffix=".jpg"\n\n\n\n# 第三步：采用thumbnailer进行验证\n\n我们使用 thumbnailer 来监听minio通知。如果有文件上传于是minio服务，thumnailer监听到该通知，生成一个缩略图并上传到minio服务。安装thumbnailer:\n\ngit clone https://github.com/minio/thumbnailer/npm install\n\n\n然后打开thumbnailer的config/webhook.json配置文件，添加有关minio server的配置，使用下面的方式启动thumbnailer:\n\nnode_env=webhook node thumbnail-webhook.js\n\n\nthumbnailer运行在http://localhost:3000/。下一步，配置minio server,让其发送消息到这个url（第一步提到的），并使用 mc 来设置存储桶通知（第二步提到的）。然后上传一张图片到minio server:\n\nmc cp ~/images.jpg myminio/images.../images.jpg:  8.31 kb / 8.31 kb ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 100.00% 59.42 kb/s 0s\n\n\n稍等片刻，然后使用mc ls检查存储桶的内容 -，你将看到有个缩略图出现了。\n\nmc ls myminio/images-thumbnail[2017-02-08 11:39:40 ist]   992b images-thumbnail.jpg\n\n\n注意 如果你用的是 distributed minio,请修改所有节点的 ~/.minio/config.json。\n\n> 原文: https://docs.min.io/docs/minio-bucket-notification-guide.html',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Minio服务限制租户",frontmatter:{title:"Minio服务限制租户",date:"2022-02-09T11:27:22.000Z",permalink:"/pages/f9e3a2/"},regularPath:"/12.MinIO/01.Minio%E6%9C%8D%E5%8A%A1%E5%99%A8/07.Minio%E6%9C%8D%E5%8A%A1%E9%99%90%E5%88%B6%E7%A7%9F%E6%88%B7.html",relativePath:"12.MinIO/01.Minio服务器/07.Minio服务限制租户.md",key:"v-7a72d387",path:"/pages/f9e3a2/",headers:[{level:2,title:"Minio服务限制/租户",slug:"minio服务限制-租户",normalizedTitle:"minio服务限制/租户",charIndex:2},{level:3,title:"纠删码 (多块硬盘 / 服务)",slug:"纠删码-多块硬盘-服务",normalizedTitle:"纠删码 (多块硬盘 / 服务)",charIndex:19},{level:3,title:"浏览器访问",slug:"浏览器访问",normalizedTitle:"浏览器访问",charIndex:112},{level:3,title:"Limits of S3 API",slug:"limits-of-s3-api",normalizedTitle:"limits of s3 api",charIndex:160},{level:3,title:"Minio不支持的Amazon S3 Bucket API",slug:"minio不支持的amazon-s3-bucket-api",normalizedTitle:"minio不支持的amazon s3 bucket api",charIndex:926},{level:3,title:"Minio不支持的Amazon S3 Object API.",slug:"minio不支持的amazon-s3-object-api",normalizedTitle:"minio不支持的amazon s3 object api.",charIndex:1330}],headersStr:"Minio服务限制/租户 纠删码 (多块硬盘 / 服务) 浏览器访问 Limits of S3 API Minio不支持的Amazon S3 Bucket API Minio不支持的Amazon S3 Object API.",content:"# Minio服务限制/租户\n\n\n# 纠删码 (多块硬盘 / 服务)\n\n项目        参数\n最大驱动器数量   16\n最小驱动器数量   4\n读仲裁       N / 2\n写仲裁       N / 2+1\n\n\n# 浏览器访问\n\n项目             参数\nWeb浏览器上传大小限制   5GB\n\n\n# Limits of S3 API\n\n项目                                                    参数\n最大桶数                                                  无限额\n每桶最大对象数                                               无限额\n最大对象大小                                                5 TB\n最小对象大小                                                0 B\n每次PUT操作的最大对象大小                                        5 GB\n每次上传的最大Part数量                                         10,000\nPart大小                                                5MB到5GB. 最后一个part可以从0B到5GB\n每次list parts请求可返回的part最大数量                            1000\n每次list objects请求可返回的object最大数量                        1000\n每次list multipart uploads请求可返回的multipart uploads最大数量   1000\n\n我们认为下列AWS S3的API有些冗余或者说用处不大，因此我们在minio中没有实现这些接口。如果您有不同意见，欢迎在github上提issue。\n\n\n# Minio不支持的Amazon S3 Bucket API\n\n * BucketACL (可以用 bucket policies)\n\n * BucketCORS (所有HTTP方法的所有存储桶都默认启用CORS)\n\n * BucketLifecycle (Minio纠删码不需要)\n\n * BucketReplication (可以用 mc mirror)\n\n * BucketVersions, BucketVersioning (可以用 s3git)\n\n * BucketWebsite (可以用 caddy or nginx)\n\n * BucketAnalytics, BucketMetrics, BucketLogging (可以用 bucket notification APIs)\n\n * BucketRequestPayment\n\n * BucketTagging\n   \n   \n   # Minio不支持的Amazon S3 Object API.\n\n * ObjectACL (可以用 bucket policies)\n\n * ObjectTorrent\n\n> 原文: https://docs.min.io/docs/minio-server-limits-per-tenant.html",normalizedContent:"# minio服务限制/租户\n\n\n# 纠删码 (多块硬盘 / 服务)\n\n项目        参数\n最大驱动器数量   16\n最小驱动器数量   4\n读仲裁       n / 2\n写仲裁       n / 2+1\n\n\n# 浏览器访问\n\n项目             参数\nweb浏览器上传大小限制   5gb\n\n\n# limits of s3 api\n\n项目                                                    参数\n最大桶数                                                  无限额\n每桶最大对象数                                               无限额\n最大对象大小                                                5 tb\n最小对象大小                                                0 b\n每次put操作的最大对象大小                                        5 gb\n每次上传的最大part数量                                         10,000\npart大小                                                5mb到5gb. 最后一个part可以从0b到5gb\n每次list parts请求可返回的part最大数量                            1000\n每次list objects请求可返回的object最大数量                        1000\n每次list multipart uploads请求可返回的multipart uploads最大数量   1000\n\n我们认为下列aws s3的api有些冗余或者说用处不大，因此我们在minio中没有实现这些接口。如果您有不同意见，欢迎在github上提issue。\n\n\n# minio不支持的amazon s3 bucket api\n\n * bucketacl (可以用 bucket policies)\n\n * bucketcors (所有http方法的所有存储桶都默认启用cors)\n\n * bucketlifecycle (minio纠删码不需要)\n\n * bucketreplication (可以用 mc mirror)\n\n * bucketversions, bucketversioning (可以用 s3git)\n\n * bucketwebsite (可以用 caddy or nginx)\n\n * bucketanalytics, bucketmetrics, bucketlogging (可以用 bucket notification apis)\n\n * bucketrequestpayment\n\n * buckettagging\n   \n   \n   # minio不支持的amazon s3 object api.\n\n * objectacl (可以用 bucket policies)\n\n * objecttorrent\n\n> 原文: https://docs.min.io/docs/minio-server-limits-per-tenant.html",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Minio Server配置指南",frontmatter:{title:"Minio Server配置指南",date:"2022-02-09T11:27:22.000Z",permalink:"/pages/625c17/"},regularPath:"/12.MinIO/01.Minio%E6%9C%8D%E5%8A%A1%E5%99%A8/08.Minio%20Server%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97.html",relativePath:"12.MinIO/01.Minio服务器/08.Minio Server配置指南.md",key:"v-e6c9547a",path:"/pages/625c17/",headers:[{level:2,title:"配置目录",slug:"配置目录",normalizedTitle:"配置目录",charIndex:133},{level:3,title:"证书目录",slug:"证书目录",normalizedTitle:"证书目录",charIndex:382},{level:3,title:"配置参数",slug:"配置参数",normalizedTitle:"配置参数",charIndex:642},{level:2,title:"了解更多",slug:"了解更多",normalizedTitle:"了解更多",charIndex:1933}],headersStr:"配置目录 证书目录 配置参数 了解更多",content:'# Minio Server配置指南\n\nMinio server在默认情况下会将所有配置信息存到 ${HOME}/.minio/config.json 文件中。 以下部分提供每个字段的详细说明以及如何自定义它们。一个完整的 config.json 在 这里\n\n\n# 配置目录\n\n默认的配置目录是 ${HOME}/.minio，你可以使用—config-dir命令行选项重写之。 You can override the default configuration directory using —config-dir command-line option. Minio server在首次启动时会生成一个新的config.json，里面带有自动生成的访问凭据。\n\nminio server --config-dir /etc/minio /data\n\n\n\n# 证书目录\n\nTLS证书存在${HOME}/.minio/certs目录下，你需要将证书放在该目录下来启用HTTPS 。如果你是一个乐学上进的好青年，这里有一本免费的秘籍传授一你: 如何使用TLS安全的访问minio.\n\n以下是一个带来TLS证书的Minio server的目录结构。\n\n$ tree ~/.minio\n/home/user1/.minio\n├── certs\n│   ├── CAs\n│   ├── private.key\n│   └── public.crt\n└── config.json\n\n\n\n# 配置参数\n\n# 版本\n\n参数        类型       描述\nversion   string   version决定了配置文件的格式，任何老版本都会在启动时自动迁移到新版本中。 [请勿手动修改]\n\n# 凭据\n\n参数                     类型       描述\ncredential                      对象存储和Web访问的验证凭据。\ncredential.accessKey   string   Access key长度最小是5个字符，你可以通过 MINIO_ACCESS_KEY环境变量进行修改\ncredential.secretKey   string   Secret key长度最小是8个字符，你可以通过MINIO_SECRET_KEY环境变量进行修改\n\n示例:\n\nexport MINIO_ACCESS_KEY=adminexport MINIO_SECRET_KEY=passwordminio server /data\n\n\n# 区域（Region）\n\n参数       类型       描述\nregion   string   region描述的是服务器的物理位置，默认是us-east-1（美国东区1）,这也是亚马逊S3的默认区域。你可以通过MINIO_REGION\n                  环境变量进行修改。如果不了解这块，建议不要随意修改\n\n示例:\n\nexport MINIO_REGION="中国华北一区"minio server /data\n\n\n# 浏览器\n\n参数        类型       描述\nbrowser   string   开启或关闭浏览器访问，默认是开启的，你可以通过MINIO_BROWSER环境变量进行修改\n\n示例:\n\nexport MINIO_BROWSER=offminio server /data\n\n\n# 通知\n\n参数                     类型   描述\nnotify                      通知通过以下方式开启存储桶事件通知，用于lambda计算\nnotify.amqp                 通过AMQP发布Minio事件\nnotify.mqtt                 通过MQTT发布Minio事件\nnotify.elasticsearch        通过Elasticsearch发布Minio事件\nnotify.redis                通过Redis发布Minio事件\nnotify.nats                 通过NATS发布Minio事件\nnotify.postgresql           通过PostgreSQL发布Minio事件\nnotify.kafka                通过Apache Kafka发布Minio事件\nnotify.webhook              通过Webhooks发布Minio事件\n\n\n# 了解更多\n\n * Minio Quickstart Guide\n\n> 原文: https://docs.min.io/docs/minio-server-configuration-guide.html',normalizedContent:'# minio server配置指南\n\nminio server在默认情况下会将所有配置信息存到 ${home}/.minio/config.json 文件中。 以下部分提供每个字段的详细说明以及如何自定义它们。一个完整的 config.json 在 这里\n\n\n# 配置目录\n\n默认的配置目录是 ${home}/.minio，你可以使用—config-dir命令行选项重写之。 you can override the default configuration directory using —config-dir command-line option. minio server在首次启动时会生成一个新的config.json，里面带有自动生成的访问凭据。\n\nminio server --config-dir /etc/minio /data\n\n\n\n# 证书目录\n\ntls证书存在${home}/.minio/certs目录下，你需要将证书放在该目录下来启用https 。如果你是一个乐学上进的好青年，这里有一本免费的秘籍传授一你: 如何使用tls安全的访问minio.\n\n以下是一个带来tls证书的minio server的目录结构。\n\n$ tree ~/.minio\n/home/user1/.minio\n├── certs\n│   ├── cas\n│   ├── private.key\n│   └── public.crt\n└── config.json\n\n\n\n# 配置参数\n\n# 版本\n\n参数        类型       描述\nversion   string   version决定了配置文件的格式，任何老版本都会在启动时自动迁移到新版本中。 [请勿手动修改]\n\n# 凭据\n\n参数                     类型       描述\ncredential                      对象存储和web访问的验证凭据。\ncredential.accesskey   string   access key长度最小是5个字符，你可以通过 minio_access_key环境变量进行修改\ncredential.secretkey   string   secret key长度最小是8个字符，你可以通过minio_secret_key环境变量进行修改\n\n示例:\n\nexport minio_access_key=adminexport minio_secret_key=passwordminio server /data\n\n\n# 区域（region）\n\n参数       类型       描述\nregion   string   region描述的是服务器的物理位置，默认是us-east-1（美国东区1）,这也是亚马逊s3的默认区域。你可以通过minio_region\n                  环境变量进行修改。如果不了解这块，建议不要随意修改\n\n示例:\n\nexport minio_region="中国华北一区"minio server /data\n\n\n# 浏览器\n\n参数        类型       描述\nbrowser   string   开启或关闭浏览器访问，默认是开启的，你可以通过minio_browser环境变量进行修改\n\n示例:\n\nexport minio_browser=offminio server /data\n\n\n# 通知\n\n参数                     类型   描述\nnotify                      通知通过以下方式开启存储桶事件通知，用于lambda计算\nnotify.amqp                 通过amqp发布minio事件\nnotify.mqtt                 通过mqtt发布minio事件\nnotify.elasticsearch        通过elasticsearch发布minio事件\nnotify.redis                通过redis发布minio事件\nnotify.nats                 通过nats发布minio事件\nnotify.postgresql           通过postgresql发布minio事件\nnotify.kafka                通过apache kafka发布minio事件\nnotify.webhook              通过webhooks发布minio事件\n\n\n# 了解更多\n\n * minio quickstart guide\n\n> 原文: https://docs.min.io/docs/minio-server-configuration-guide.html',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Minio多租户（Multi-tenant）部署指南",frontmatter:{title:"Minio多租户（Multi-tenant）部署指南",date:"2022-02-09T11:27:22.000Z",permalink:"/pages/9b5f74/"},regularPath:"/12.MinIO/01.Minio%E6%9C%8D%E5%8A%A1%E5%99%A8/09.Minio%E5%A4%9A%E7%A7%9F%E6%88%B7%EF%BC%88Multi-tenant%EF%BC%89%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97.html",relativePath:"12.MinIO/01.Minio服务器/09.Minio多租户（Multi-tenant）部署指南.md",key:"v-1128e807",path:"/pages/9b5f74/",headers:[{level:2,title:"单机部署",slug:"单机部署",normalizedTitle:"单机部署",charIndex:33},{level:2,title:"分布式部署",slug:"分布式部署",normalizedTitle:"分布式部署",charIndex:783},{level:2,title:"云端可伸缩部署",slug:"云端可伸缩部署",normalizedTitle:"云端可伸缩部署",charIndex:1834}],headersStr:"单机部署 分布式部署 云端可伸缩部署",content:"# Minio多租户（Multi-tenant）部署指南\n\n\n# 单机部署\n\n要在单台机器上托管多个租户，为每个租户运行一个Minio server,使用不同的HTTPS端口、配置和数据目录。\n\n# 示例1：单主机，单磁盘\n\n以下示例在一块磁盘上托管三个租户。\n\nminio --config-dir ~/tenant1 server --address :9001 /data/tenant1minio --config-dir ~/tenant2 server --address :9002 /data/tenant2minio --config-dir ~/tenant3 server --address :9003 /data/tenant3\n\n\n\n\n# 示例2：单主机，多块磁盘 (erasure code)\n\n以下示例在多块磁盘上托管三个租户。\n\nminio --config-dir ~/tenant1 server --address :9001 /disk1/data/tenant1 /disk2/data/tenant1 /disk3/data/tenant1 /disk4/data/tenant1minio --config-dir ~/tenant2 server --address :9002 /disk1/data/tenant2 /disk2/data/tenant2 /disk3/data/tenant2 /disk4/data/tenant2minio --config-dir ~/tenant3 server --address :9003 /disk1/data/tenant3 /disk2/data/tenant3 /disk3/data/tenant3 /disk4/data/tenant3\n\n\n\n\n\n# 分布式部署\n\n要在分布式环境中托管多个租户，同时运行多个分布式Minio实例。To host multiple tenants in a distributed environment, run several distributed Minio instances concurrently.\n\n# 示例3 : 多主机，多块磁盘 (erasure code)\n\n以下示例在一个4节点集群中托管三个租户。在4个节点里都执行下列命令：\n\nexport MINIO_ACCESS_KEY=<TENANT1_ACCESS_KEY>export MINIO_SECRET_KEY=<TENANT1_SECRET_KEY>minio --config-dir ~/tenant1 server --address :9001 http://192.168.10.11/data/tenant1 http://192.168.10.12/data/tenant1 http://192.168.10.13/data/tenant1 http://192.168.10.14/data/tenant1export MINIO_ACCESS_KEY=<TENANT2_ACCESS_KEY>export MINIO_SECRET_KEY=<TENANT2_SECRET_KEY>minio --config-dir ~/tenant2 server --address :9002 http://192.168.10.11/data/tenant2 http://192.168.10.12/data/tenant2 http://192.168.10.13/data/tenant2 http://192.168.10.14/data/tenant2export MINIO_ACCESS_KEY=<TENANT3_ACCESS_KEY>export MINIO_SECRET_KEY=<TENANT3_SECRET_KEY>minio --config-dir ~/tenant3 server --address :9003 http://192.168.10.11/data/tenant3 http://192.168.10.12/data/tenant3 http://192.168.10.13/data/tenant3 http://192.168.10.14/data/tenant3\n\n\n\n\n\n# 云端可伸缩部署\n\n对于大型多租户Minio部署，我们建议使用一个流行的容器编排平台，比如Kubernetes、DC/OS，或者是Docker Swarm.参考 这个文档 ,学习如何在编排平台中使用Minio。\n\n> 原文: https://docs.min.io/docs/multi-tenant-minio-deployment-guide.html",normalizedContent:"# minio多租户（multi-tenant）部署指南\n\n\n# 单机部署\n\n要在单台机器上托管多个租户，为每个租户运行一个minio server,使用不同的https端口、配置和数据目录。\n\n# 示例1：单主机，单磁盘\n\n以下示例在一块磁盘上托管三个租户。\n\nminio --config-dir ~/tenant1 server --address :9001 /data/tenant1minio --config-dir ~/tenant2 server --address :9002 /data/tenant2minio --config-dir ~/tenant3 server --address :9003 /data/tenant3\n\n\n\n\n# 示例2：单主机，多块磁盘 (erasure code)\n\n以下示例在多块磁盘上托管三个租户。\n\nminio --config-dir ~/tenant1 server --address :9001 /disk1/data/tenant1 /disk2/data/tenant1 /disk3/data/tenant1 /disk4/data/tenant1minio --config-dir ~/tenant2 server --address :9002 /disk1/data/tenant2 /disk2/data/tenant2 /disk3/data/tenant2 /disk4/data/tenant2minio --config-dir ~/tenant3 server --address :9003 /disk1/data/tenant3 /disk2/data/tenant3 /disk3/data/tenant3 /disk4/data/tenant3\n\n\n\n\n\n# 分布式部署\n\n要在分布式环境中托管多个租户，同时运行多个分布式minio实例。to host multiple tenants in a distributed environment, run several distributed minio instances concurrently.\n\n# 示例3 : 多主机，多块磁盘 (erasure code)\n\n以下示例在一个4节点集群中托管三个租户。在4个节点里都执行下列命令：\n\nexport minio_access_key=<tenant1_access_key>export minio_secret_key=<tenant1_secret_key>minio --config-dir ~/tenant1 server --address :9001 http://192.168.10.11/data/tenant1 http://192.168.10.12/data/tenant1 http://192.168.10.13/data/tenant1 http://192.168.10.14/data/tenant1export minio_access_key=<tenant2_access_key>export minio_secret_key=<tenant2_secret_key>minio --config-dir ~/tenant2 server --address :9002 http://192.168.10.11/data/tenant2 http://192.168.10.12/data/tenant2 http://192.168.10.13/data/tenant2 http://192.168.10.14/data/tenant2export minio_access_key=<tenant3_access_key>export minio_secret_key=<tenant3_secret_key>minio --config-dir ~/tenant3 server --address :9003 http://192.168.10.11/data/tenant3 http://192.168.10.12/data/tenant3 http://192.168.10.13/data/tenant3 http://192.168.10.14/data/tenant3\n\n\n\n\n\n# 云端可伸缩部署\n\n对于大型多租户minio部署，我们建议使用一个流行的容器编排平台，比如kubernetes、dc/os，或者是docker swarm.参考 这个文档 ,学习如何在编排平台中使用minio。\n\n> 原文: https://docs.min.io/docs/multi-tenant-minio-deployment-guide.html",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Minio客户端快速入门指南",frontmatter:{title:"Minio客户端快速入门指南",date:"2022-02-09T11:29:26.000Z",permalink:"/pages/17c975/"},regularPath:"/12.MinIO/02.Minio%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/01.Minio%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97.html",relativePath:"12.MinIO/02.Minio客户端快速入门指南/01.Minio客户端快速入门指南.md",key:"v-a77e2b92",path:"/pages/17c975/",headers:[{level:2,title:"Docker容器",slug:"docker容器",normalizedTitle:"docker容器",charIndex:490},{level:3,title:"稳定版",slug:"稳定版",normalizedTitle:"稳定版",charIndex:503},{level:3,title:"尝鲜版",slug:"尝鲜版",normalizedTitle:"尝鲜版",charIndex:561},{level:2,title:"macOS",slug:"macos",normalizedTitle:"macos",charIndex:752},{level:3,title:"Homebrew",slug:"homebrew",normalizedTitle:"homebrew",charIndex:762},{level:2,title:"GNU/Linux",slug:"gnu-linux",normalizedTitle:"gnu/linux",charIndex:832},{level:3,title:"下载二进制文件",slug:"下载二进制文件",normalizedTitle:"下载二进制文件",charIndex:846},{level:2,title:"Microsoft Windows",slug:"microsoft-windows",normalizedTitle:"microsoft windows",charIndex:995},{level:3,title:"下载二进制文件",slug:"下载二进制文件-2",normalizedTitle:"下载二进制文件",charIndex:846},{level:2,title:"通过源码安装",slug:"通过源码安装",normalizedTitle:"通过源码安装",charIndex:1179},{level:2,title:"添加一个云存储服务",slug:"添加一个云存储服务",normalizedTitle:"添加一个云存储服务",charIndex:1389},{level:3,title:"示例-Minio云存储",slug:"示例-minio云存储",normalizedTitle:"示例-minio云存储",charIndex:1705},{level:3,title:"示例-Amazon S3云存储",slug:"示例-amazon-s3云存储",normalizedTitle:"示例-amazon s3云存储",charIndex:1873},{level:3,title:"示例-Google云存储",slug:"示例-google云存储",normalizedTitle:"示例-google云存储",charIndex:2062},{level:2,title:"验证",slug:"验证",normalizedTitle:"验证",charIndex:2301},{level:2,title:"日常使用",slug:"日常使用",normalizedTitle:"日常使用",charIndex:1437},{level:3,title:"Shell别名",slug:"shell别名",normalizedTitle:"shell别名",charIndex:2711},{level:3,title:"Shell自动补全",slug:"shell自动补全",normalizedTitle:"shell自动补全",charIndex:2869}],headersStr:"Docker容器 稳定版 尝鲜版 macOS Homebrew GNU/Linux 下载二进制文件 Microsoft Windows 下载二进制文件 通过源码安装 添加一个云存储服务 示例-Minio云存储 示例-Amazon S3云存储 示例-Google云存储 验证 日常使用 Shell别名 Shell自动补全",content:"# Minio客户端快速入门指南\n\nMinio Client (mc)为ls，cat，cp，mirror，diff，find等UNIX命令提供了一种替代方案。它支持文件系统和兼容Amazon S3的云存储服务（AWS Signature v2和v4）。\n\nls       列出文件和文件夹。\nmb       创建一个存储桶或一个文件夹。\ncat      显示文件和对象内容。\npipe     将一个STDIN重定向到一个对象或者文件或者STDOUT。\nshare    生成用于共享的URL。\ncp       拷贝文件和对象。\nmirror   给存储桶和文件夹做镜像。\nfind     基于参数查找文件。\ndiff     对两个文件夹或者存储桶比较差异。\nrm       删除文件和对象。\nevents   管理对象通知。\nwatch    监听文件和对象的事件。\npolicy   管理访问策略。\nsession  为cp命令管理保存的会话。\nconfig   管理mc配置文件。\nupdate   检查软件更新。\nversion  输出版本信息。\n\n\n\n# Docker容器\n\n\n# 稳定版\n\ndocker pull minio/mcdocker run minio/mc ls play\n\n\n\n# 尝鲜版\n\ndocker pull minio/mc:edgedocker run minio/mc:edge ls play\n\n\n注意: 上述示例默认使用Minio演示环境做演示，如果想用mc操作其它S3兼容的服务，采用下面的方式来启动容器：\n\ndocker run -it --entrypoint=/bin/sh minio/mc\n\n\n然后使用mc config命令。\n\n\n# macOS\n\n\n# Homebrew\n\n使用Homebrew安装mc。\n\nbrew install minio/stable/mcmc --help\n\n\n\n# GNU/Linux\n\n\n# 下载二进制文件\n\n平台          CPU架构          URL\nGNU/Linux   64-bit Intel   https://dl.minio.io/client/mc/release/linux-amd64/mc\n\nchmod +x mc./mc --help\n\n\n\n# Microsoft Windows\n\n\n# 下载二进制文件\n\n平台                  CPU架构          URL\nMicrosoft Windows   64-bit Intel   https://dl.minio.io/client/mc/release/windows-amd64/mc.exe\n\nmc.exe --help\n\n\n\n# 通过源码安装\n\n通过源码安装仅适用于开发人员和高级用户。mc update命令不支持基于源码安装的更新通知。请从https://minio.io/downloads/#minio-client下载官方版本。\n\n如果您没有Golang环境，请参照如何安装Golang。\n\ngo get -d github.com/minio/mccd ${GOPATH}/src/github.com/minio/mcmake\n\n\n\n# 添加一个云存储服务\n\n如果你打算仅在POSIX兼容文件系统中使用mc,那你可以直接略过本节，跳到日常使用。\n\n添加一个或多个S3兼容的服务，请参考下面说明。mc将所有的配置信息都存储在~/.mc/config.json文件中。\n\nmc config host add <ALIAS> <YOUR-S3-ENDPOINT> <YOUR-ACCESS-KEY> <YOUR-SECRET-KEY> <API-SIGNATURE>\n\n\n别名就是给你的云存储服务起了一个短点的外号。S3 endpoint,access key和secret key是你的云存储服务提供的。API签名是可选参数，默认情况下，它被设置为\"S3v4\"。\n\n\n# 示例-Minio云存储\n\n从Minio服务获得URL、access key和secret key。\n\nmc config host add minio http://192.168.1.51 BKIKJAA5BMMU2RHO6IBB V7f1CwQqAcwo80UEIJEjc5gVQUSSx5ohQ9GSrr12 S3v4\n\n\n\n# 示例-Amazon S3云存储\n\n参考AWS Credentials指南获取你的AccessKeyID和SecretAccessKey。\n\nmc config host add s3 https://s3.amazonaws.com BKIKJAA5BMMU2RHO6IBB V7f1CwQqAcwo80UEIJEjc5gVQUSSx5ohQ9GSrr12 S3v4\n\n\n\n# 示例-Google云存储\n\n参考Google Credentials Guide获取你的AccessKeyID和SecretAccessKey。\n\nmc config host add gcs  https://storage.googleapis.com BKIKJAA5BMMU2RHO6IBB V8f1CwQqAcwo80UEIJEjc5gVQUSSx5ohQ9GSrr12 S3v2\n\n\n注意：Google云存储只支持旧版签名版本V2，所以你需要选择S3v2。\n\n\n# 验证\n\nmc预先配置了云存储服务URL：https://play.minio.io:9000，别名“play”。它是一个用于研发和测试的Minio服务。如果想测试Amazon S3,你可以将“play”替换为“s3”。\n\n示例:\n\n列出https://play.minio.io:9000上的所有存储桶。\n\nmc ls play\n[2016-03-22 19:47:48 PDT]     0B my-bucketname/\n[2016-03-22 22:01:07 PDT]     0B mytestbucket/\n[2016-03-22 20:04:39 PDT]     0B mybucketname/\n[2016-01-28 17:23:11 PST]     0B newbucket/\n[2016-03-20 09:08:36 PDT]     0B s3git-test/\n\n\n\n# 日常使用\n\n\n# Shell别名\n\n你可以添加shell别名来覆盖默认的Unix工具命令。\n\nalias ls='mc ls'\nalias cp='mc cp'\nalias cat='mc cat'\nalias mkdir='mc mb'\nalias pipe='mc pipe'\nalias find='mc find'\n\n\n\n# Shell自动补全\n\n你也可以下载autocomplete/bash_autocomplete到/etc/bash_completion.d/，然后将其重命名为mc。别忘了在这个文件运行source命令让其在你的当前shell上可用。\n\nsudo wget https://raw.githubusercontent.com/minio/mc/master/autocomplete/bash_autocomplete -O /etc/bash_completion.d/mc\nsource /etc/bash_completion.d/mc\n\n\nmc <TAB>\nadmin    config   diff     ls       mirror   policy   session  update   watch\ncat      cp       events   mb       pipe     rm       share    version\n",normalizedContent:"# minio客户端快速入门指南\n\nminio client (mc)为ls，cat，cp，mirror，diff，find等unix命令提供了一种替代方案。它支持文件系统和兼容amazon s3的云存储服务（aws signature v2和v4）。\n\nls       列出文件和文件夹。\nmb       创建一个存储桶或一个文件夹。\ncat      显示文件和对象内容。\npipe     将一个stdin重定向到一个对象或者文件或者stdout。\nshare    生成用于共享的url。\ncp       拷贝文件和对象。\nmirror   给存储桶和文件夹做镜像。\nfind     基于参数查找文件。\ndiff     对两个文件夹或者存储桶比较差异。\nrm       删除文件和对象。\nevents   管理对象通知。\nwatch    监听文件和对象的事件。\npolicy   管理访问策略。\nsession  为cp命令管理保存的会话。\nconfig   管理mc配置文件。\nupdate   检查软件更新。\nversion  输出版本信息。\n\n\n\n# docker容器\n\n\n# 稳定版\n\ndocker pull minio/mcdocker run minio/mc ls play\n\n\n\n# 尝鲜版\n\ndocker pull minio/mc:edgedocker run minio/mc:edge ls play\n\n\n注意: 上述示例默认使用minio演示环境做演示，如果想用mc操作其它s3兼容的服务，采用下面的方式来启动容器：\n\ndocker run -it --entrypoint=/bin/sh minio/mc\n\n\n然后使用mc config命令。\n\n\n# macos\n\n\n# homebrew\n\n使用homebrew安装mc。\n\nbrew install minio/stable/mcmc --help\n\n\n\n# gnu/linux\n\n\n# 下载二进制文件\n\n平台          cpu架构          url\ngnu/linux   64-bit intel   https://dl.minio.io/client/mc/release/linux-amd64/mc\n\nchmod +x mc./mc --help\n\n\n\n# microsoft windows\n\n\n# 下载二进制文件\n\n平台                  cpu架构          url\nmicrosoft windows   64-bit intel   https://dl.minio.io/client/mc/release/windows-amd64/mc.exe\n\nmc.exe --help\n\n\n\n# 通过源码安装\n\n通过源码安装仅适用于开发人员和高级用户。mc update命令不支持基于源码安装的更新通知。请从https://minio.io/downloads/#minio-client下载官方版本。\n\n如果您没有golang环境，请参照如何安装golang。\n\ngo get -d github.com/minio/mccd ${gopath}/src/github.com/minio/mcmake\n\n\n\n# 添加一个云存储服务\n\n如果你打算仅在posix兼容文件系统中使用mc,那你可以直接略过本节，跳到日常使用。\n\n添加一个或多个s3兼容的服务，请参考下面说明。mc将所有的配置信息都存储在~/.mc/config.json文件中。\n\nmc config host add <alias> <your-s3-endpoint> <your-access-key> <your-secret-key> <api-signature>\n\n\n别名就是给你的云存储服务起了一个短点的外号。s3 endpoint,access key和secret key是你的云存储服务提供的。api签名是可选参数，默认情况下，它被设置为\"s3v4\"。\n\n\n# 示例-minio云存储\n\n从minio服务获得url、access key和secret key。\n\nmc config host add minio http://192.168.1.51 bkikjaa5bmmu2rho6ibb v7f1cwqqacwo80ueijejc5gvqussx5ohq9gsrr12 s3v4\n\n\n\n# 示例-amazon s3云存储\n\n参考aws credentials指南获取你的accesskeyid和secretaccesskey。\n\nmc config host add s3 https://s3.amazonaws.com bkikjaa5bmmu2rho6ibb v7f1cwqqacwo80ueijejc5gvqussx5ohq9gsrr12 s3v4\n\n\n\n# 示例-google云存储\n\n参考google credentials guide获取你的accesskeyid和secretaccesskey。\n\nmc config host add gcs  https://storage.googleapis.com bkikjaa5bmmu2rho6ibb v8f1cwqqacwo80ueijejc5gvqussx5ohq9gsrr12 s3v2\n\n\n注意：google云存储只支持旧版签名版本v2，所以你需要选择s3v2。\n\n\n# 验证\n\nmc预先配置了云存储服务url：https://play.minio.io:9000，别名“play”。它是一个用于研发和测试的minio服务。如果想测试amazon s3,你可以将“play”替换为“s3”。\n\n示例:\n\n列出https://play.minio.io:9000上的所有存储桶。\n\nmc ls play\n[2016-03-22 19:47:48 pdt]     0b my-bucketname/\n[2016-03-22 22:01:07 pdt]     0b mytestbucket/\n[2016-03-22 20:04:39 pdt]     0b mybucketname/\n[2016-01-28 17:23:11 pst]     0b newbucket/\n[2016-03-20 09:08:36 pdt]     0b s3git-test/\n\n\n\n# 日常使用\n\n\n# shell别名\n\n你可以添加shell别名来覆盖默认的unix工具命令。\n\nalias ls='mc ls'\nalias cp='mc cp'\nalias cat='mc cat'\nalias mkdir='mc mb'\nalias pipe='mc pipe'\nalias find='mc find'\n\n\n\n# shell自动补全\n\n你也可以下载autocomplete/bash_autocomplete到/etc/bash_completion.d/，然后将其重命名为mc。别忘了在这个文件运行source命令让其在你的当前shell上可用。\n\nsudo wget https://raw.githubusercontent.com/minio/mc/master/autocomplete/bash_autocomplete -o /etc/bash_completion.d/mc\nsource /etc/bash_completion.d/mc\n\n\nmc <tab>\nadmin    config   diff     ls       mirror   policy   session  update   watch\ncat      cp       events   mb       pipe     rm       share    version\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Ubuntu 开发环境变量配置",frontmatter:{title:"Ubuntu 开发环境变量配置",date:"2022-02-09T10:56:45.000Z",permalink:"/pages/65555a/"},regularPath:"/04.Ubuntu/02.Ubuntu%20%E4%BD%BF%E7%94%A8/03.Ubuntu%20%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE.html",relativePath:"04.Ubuntu/02.Ubuntu 使用/03.Ubuntu 开发环境变量配置.md",key:"v-40000776",path:"/pages/65555a/",headers:[{level:2,title:"JDK",slug:"jdk",normalizedTitle:"jdk",charIndex:28},{level:3,title:"下载安装包",slug:"下载安装包",normalizedTitle:"下载安装包",charIndex:36},{level:3,title:"上传并解压",slug:"上传并解压",normalizedTitle:"上传并解压",charIndex:191},{level:3,title:"配置环境变量",slug:"配置环境变量",normalizedTitle:"配置环境变量",charIndex:295},{level:3,title:"执行source /etc/profile",slug:"执行source-etc-profile",normalizedTitle:"执行source /etc/profile",charIndex:518},{level:3,title:"检查jdk是否生效java -version",slug:"检查jdk是否生效java-version",normalizedTitle:"检查jdk是否生效java -version",charIndex:568},{level:2,title:"Maven",slug:"maven",normalizedTitle:"maven",charIndex:806},{level:3,title:"下载安装包",slug:"下载安装包-2",normalizedTitle:"下载安装包",charIndex:36},{level:3,title:"上传并解压",slug:"上传并解压-2",normalizedTitle:"上传并解压",charIndex:191},{level:3,title:"配置环境变量",slug:"配置环境变量-2",normalizedTitle:"配置环境变量",charIndex:295},{level:3,title:"执行source /etc/profile",slug:"执行source-etc-profile-2",normalizedTitle:"执行source /etc/profile",charIndex:518},{level:3,title:"检查maven是否生效mvn -v",slug:"检查maven是否生效mvn-v",normalizedTitle:"检查maven是否生效mvn -v",charIndex:1220},{level:2,title:"NodeJs",slug:"nodejs",normalizedTitle:"nodejs",charIndex:1640},{level:3,title:"离线安装",slug:"离线安装",normalizedTitle:"离线安装",charIndex:1651},{level:3,title:"下载安装包",slug:"下载安装包-3",normalizedTitle:"下载安装包",charIndex:36},{level:3,title:"上传并解压",slug:"上传并解压-3",normalizedTitle:"上传并解压",charIndex:191},{level:3,title:"配置环境变量",slug:"配置环境变量-3",normalizedTitle:"配置环境变量",charIndex:295},{level:3,title:"执行source /etc/profile",slug:"执行source-etc-profile-3",normalizedTitle:"执行source /etc/profile",charIndex:518},{level:3,title:"执行node -v和npm -v命令检查软件版本",slug:"执行node-v和npm-v命令检查软件版本",normalizedTitle:"执行node -v和npm -v命令检查软件版本",charIndex:2211},{level:3,title:"在线安装",slug:"在线安装",normalizedTitle:"在线安装",charIndex:2278},{level:3,title:"使用 apt 安装 node",slug:"使用-apt-安装-node",normalizedTitle:"使用 apt 安装 node",charIndex:2336},{level:3,title:"使用 apt 安装 npm",slug:"使用-apt-安装-npm",normalizedTitle:"使用 apt 安装 npm",charIndex:2381},{level:3,title:"npm 更新",slug:"npm-更新",normalizedTitle:"npm 更新",charIndex:2459},{level:3,title:"node 更新",slug:"node-更新",normalizedTitle:"node 更新",charIndex:2535},{level:2,title:"淘宝 NPM 镜像",slug:"淘宝-npm-镜像",normalizedTitle:"淘宝 npm 镜像",charIndex:2978},{level:2,title:"npm 安装yarn",slug:"npm-安装yarn",normalizedTitle:"npm 安装yarn",charIndex:2998},{level:3,title:"一句命令搞定",slug:"一句命令搞定",normalizedTitle:"一句命令搞定",charIndex:3013},{level:3,title:"再配置下源",slug:"再配置下源",normalizedTitle:"再配置下源",charIndex:3089}],headersStr:"JDK 下载安装包 上传并解压 配置环境变量 执行source /etc/profile 检查jdk是否生效java -version Maven 下载安装包 上传并解压 配置环境变量 执行source /etc/profile 检查maven是否生效mvn -v NodeJs 离线安装 下载安装包 上传并解压 配置环境变量 执行source /etc/profile 执行node -v和npm -v命令检查软件版本 在线安装 使用 apt 安装 node 使用 apt 安装 npm npm 更新 node 更新 淘宝 NPM 镜像 npm 安装yarn 一句命令搞定 再配置下源",content:'# Ubuntu 18.04 开发环境变量配置\n\n\n# JDK\n\n\n# 下载安装包\n\n访问JDK官方下载页面 ，下载Linux x64 Compressed Archive版本安装包，本教程使用的版本为：jdk-8u261-linux-x64.tar.gz。\n\n勾选同意协议，点击下载\n\n首次使用登录Oracle账号，无果没有账号免费注册一个即可。登录成功会自动开始下载。\n\n\n# 上传并解压\n\n$ sudo mkdir /usr/local/java\n$ sudo tar -zxvf jdk-8u261-linux-x64.tar.gz -C /usr/local/java\n\n\n\n# 配置环境变量\n\n$ sudo vi /etc/profile\n\n\n在文件末尾插入如下内容\n\n# jdk\nexport JAVA_HOME=/usr/local/java/jdk1.8.0_261\nexport JRE_HOME=${JAVA_HOME}/jre\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\nexport PATH=${JAVA_HOME}/bin:$PATH\n\n\n\n# 执行source /etc/profile\n\n$ source /etc/profile\n\n\n\n# 检查jdk是否生效java -version\n\nheyuqiang@heyuqiang:/usr/local/java/jdk1.8.0_261$ java -version \njava version "1.8.0_261"\nJava(TM) SE Runtime Environment (build 1.8.0_261-b12)\nJava HotSpot(TM) 64-Bit Server VM (build 25.261-b12, mixed mode)\n\n\n\n# Maven\n\n\n# 下载安装包\n\n访问Maven官方下载页面 ，下载Binary tar.gz archive版本安装包，本教程使用的版本为：apache-maven-3.6.3-bin.tar.gz。\n\n\n# 上传并解压\n\n$ sudo mkdir /usr/local/maven\n$ sudo tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /usr/local/maven\n\n\n\n# 配置环境变量\n\n$ sudo vi /etc/profile\n\n\n在文件末尾插入如下内容\n\n# maven\nexport NODEJS_HOME=/usr/local/maven/apache-maven-3.6.3\nexport PATH=$NODEJS_HOME/bin:$PATH\n\n\n\n# 执行source /etc/profile\n\n$ source /etc/profile\n\n\n\n# 检查maven是否生效mvn -v\n\nheyuqiang@heyuqiang:/usr/local/maven/apache-maven-3.6.3$ mvn -v \nApache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)\nMaven home: /usr/local/maven/apache-maven-3.6.3\nJava version: 1.8.0_261, vendor: Oracle Corporation, runtime: /usr/local/java/jdk1.8.0_261/jre\nDefault locale: en_US, platform encoding: UTF-8\nOS name: "linux", version: "4.15.0-76-generic", arch: "amd64", family: "unix"\n\n\n\n# NodeJs\n\n\n# 离线安装\n\n推荐大家使用离线安装方式，首先根据自己的需要先从官网下载需要的releases版本安装包，我这里下载安装的node版本为v10.22.0，解压配置好环境变量即可。\n\n\n\n\n# 下载安装包\n\n使用wget命令下载二进制安装包node-v10.22.0-linux-x64.tar.gz\n\n$ wget https://nodejs.org/dist/latest-v10.x/node-v10.22.0-linux-x64.tar.gz\n\n\n\n# 上传并解压\n\n$ sudo mkdir /usr/local/lib/nodejs\n$ sudo tar -zxvf node-v10.22.0-linux-x64.tar.gz -C /usr/local/lib/nodejs/\n\n\n\n# 配置环境变量\n\n$ sudo vi /etc/profile\n\n\n在文档末尾追加如下内容\n\n# nodejs\nexport NODEJS_HOME=/usr/local/lib/nodejs/node-v10.22.0-linux-x64\nexport PATH=$NODEJS_HOME/bin:$PATH\n\n\n\n# 执行source /etc/profile\n\n$ source /etc/profile\n\n\n\n# 执行node -v和npm -v命令检查软件版本\n\n$ node -v\nv10.22.0\n$ npm -v \n6.14.6\n\n\n\n# 在线安装\n\n提示\n\n默认使用 apt 安装的 node 和 npm 版本可能比较老，需要更新一下对应的版本\n\n\n# 使用 apt 安装 node\n\nsudo apt install nodejs\n\n\n\n# 使用 apt 安装 npm\n\nsudo apt install npm\n\n\n$ nodejs -v\nv8.10.0\n$ npm -v\n3.5.2\n\n\n\n# npm 更新\n\nsudo npm i -g npm\n\n\n重新打开终端，查看 npm 版本，如下版本已经是最新\n\n$ npm -v\n6.7.0\n\n\n\n# node 更新\n\n先安装 n 模块\n\nsudo npm install -g n\n\n\n$ sudo npm install -g n\n/usr/local/bin/n -> /usr/local/lib/node_modules/n/bin/n\n+ n@2.1.12\nadded 1 package from 4 contributors in 0.649s\n\n\n执行 sudo n latest 命令更新 node\n\n$ sudo n latest\n\n     install : node-v11.8.0\n       mkdir : /usr/local/n/versions/node/11.8.0\n       fetch : https://nodejs.org/dist/v11.8.0/node-v11.8.0-linux-x64.tar.gz\n   installed : v11.8.0\n\n\n重新打开一个终端查看版本\n\n$ node -v\nv11.8.0\n\n\n\n# 淘宝 NPM 镜像\n\n使用说明\n\n\n# npm 安装yarn\n\n\n# 一句命令搞定\n\nnpm install -g yarn --registry=https://registry.npm.taobao.org\n\n\n\n# 再配置下源\n\nyarn config set registry https://registry.npm.taobao.org -g\nyarn config set sass_binary_site http://cdn.npm.taobao.org/dist/node-sass -g\n',normalizedContent:'# ubuntu 18.04 开发环境变量配置\n\n\n# jdk\n\n\n# 下载安装包\n\n访问jdk官方下载页面 ，下载linux x64 compressed archive版本安装包，本教程使用的版本为：jdk-8u261-linux-x64.tar.gz。\n\n勾选同意协议，点击下载\n\n首次使用登录oracle账号，无果没有账号免费注册一个即可。登录成功会自动开始下载。\n\n\n# 上传并解压\n\n$ sudo mkdir /usr/local/java\n$ sudo tar -zxvf jdk-8u261-linux-x64.tar.gz -c /usr/local/java\n\n\n\n# 配置环境变量\n\n$ sudo vi /etc/profile\n\n\n在文件末尾插入如下内容\n\n# jdk\nexport java_home=/usr/local/java/jdk1.8.0_261\nexport jre_home=${java_home}/jre\nexport classpath=.:${java_home}/lib:${jre_home}/lib\nexport path=${java_home}/bin:$path\n\n\n\n# 执行source /etc/profile\n\n$ source /etc/profile\n\n\n\n# 检查jdk是否生效java -version\n\nheyuqiang@heyuqiang:/usr/local/java/jdk1.8.0_261$ java -version \njava version "1.8.0_261"\njava(tm) se runtime environment (build 1.8.0_261-b12)\njava hotspot(tm) 64-bit server vm (build 25.261-b12, mixed mode)\n\n\n\n# maven\n\n\n# 下载安装包\n\n访问maven官方下载页面 ，下载binary tar.gz archive版本安装包，本教程使用的版本为：apache-maven-3.6.3-bin.tar.gz。\n\n\n# 上传并解压\n\n$ sudo mkdir /usr/local/maven\n$ sudo tar -zxvf apache-maven-3.6.3-bin.tar.gz -c /usr/local/maven\n\n\n\n# 配置环境变量\n\n$ sudo vi /etc/profile\n\n\n在文件末尾插入如下内容\n\n# maven\nexport nodejs_home=/usr/local/maven/apache-maven-3.6.3\nexport path=$nodejs_home/bin:$path\n\n\n\n# 执行source /etc/profile\n\n$ source /etc/profile\n\n\n\n# 检查maven是否生效mvn -v\n\nheyuqiang@heyuqiang:/usr/local/maven/apache-maven-3.6.3$ mvn -v \napache maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)\nmaven home: /usr/local/maven/apache-maven-3.6.3\njava version: 1.8.0_261, vendor: oracle corporation, runtime: /usr/local/java/jdk1.8.0_261/jre\ndefault locale: en_us, platform encoding: utf-8\nos name: "linux", version: "4.15.0-76-generic", arch: "amd64", family: "unix"\n\n\n\n# nodejs\n\n\n# 离线安装\n\n推荐大家使用离线安装方式，首先根据自己的需要先从官网下载需要的releases版本安装包，我这里下载安装的node版本为v10.22.0，解压配置好环境变量即可。\n\n\n\n\n# 下载安装包\n\n使用wget命令下载二进制安装包node-v10.22.0-linux-x64.tar.gz\n\n$ wget https://nodejs.org/dist/latest-v10.x/node-v10.22.0-linux-x64.tar.gz\n\n\n\n# 上传并解压\n\n$ sudo mkdir /usr/local/lib/nodejs\n$ sudo tar -zxvf node-v10.22.0-linux-x64.tar.gz -c /usr/local/lib/nodejs/\n\n\n\n# 配置环境变量\n\n$ sudo vi /etc/profile\n\n\n在文档末尾追加如下内容\n\n# nodejs\nexport nodejs_home=/usr/local/lib/nodejs/node-v10.22.0-linux-x64\nexport path=$nodejs_home/bin:$path\n\n\n\n# 执行source /etc/profile\n\n$ source /etc/profile\n\n\n\n# 执行node -v和npm -v命令检查软件版本\n\n$ node -v\nv10.22.0\n$ npm -v \n6.14.6\n\n\n\n# 在线安装\n\n提示\n\n默认使用 apt 安装的 node 和 npm 版本可能比较老，需要更新一下对应的版本\n\n\n# 使用 apt 安装 node\n\nsudo apt install nodejs\n\n\n\n# 使用 apt 安装 npm\n\nsudo apt install npm\n\n\n$ nodejs -v\nv8.10.0\n$ npm -v\n3.5.2\n\n\n\n# npm 更新\n\nsudo npm i -g npm\n\n\n重新打开终端，查看 npm 版本，如下版本已经是最新\n\n$ npm -v\n6.7.0\n\n\n\n# node 更新\n\n先安装 n 模块\n\nsudo npm install -g n\n\n\n$ sudo npm install -g n\n/usr/local/bin/n -> /usr/local/lib/node_modules/n/bin/n\n+ n@2.1.12\nadded 1 package from 4 contributors in 0.649s\n\n\n执行 sudo n latest 命令更新 node\n\n$ sudo n latest\n\n     install : node-v11.8.0\n       mkdir : /usr/local/n/versions/node/11.8.0\n       fetch : https://nodejs.org/dist/v11.8.0/node-v11.8.0-linux-x64.tar.gz\n   installed : v11.8.0\n\n\n重新打开一个终端查看版本\n\n$ node -v\nv11.8.0\n\n\n\n# 淘宝 npm 镜像\n\n使用说明\n\n\n# npm 安装yarn\n\n\n# 一句命令搞定\n\nnpm install -g yarn --registry=https://registry.npm.taobao.org\n\n\n\n# 再配置下源\n\nyarn config set registry https://registry.npm.taobao.org -g\nyarn config set sass_binary_site http://cdn.npm.taobao.org/dist/node-sass -g\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Minio Client完全指南",frontmatter:{title:"Minio Client完全指南",date:"2022-02-09T11:29:26.000Z",permalink:"/pages/538f88/"},regularPath:"/12.MinIO/02.Minio%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/02.Minio%20Client%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97.html",relativePath:"12.MinIO/02.Minio客户端快速入门指南/02.Minio Client完全指南.md",key:"v-2ac12114",path:"/pages/538f88/",headers:[{level:2,title:"1. 下载Minio Client",slug:"_1-下载minio-client",normalizedTitle:"1. 下载minio client",charIndex:492},{level:3,title:"Docker稳定版",slug:"docker稳定版",normalizedTitle:"docker稳定版",charIndex:514},{level:3,title:"Docker尝鲜版",slug:"docker尝鲜版",normalizedTitle:"docker尝鲜版",charIndex:578},{level:3,title:"Homebrew (macOS)",slug:"homebrew-macos",normalizedTitle:"homebrew (macos)",charIndex:775},{level:3,title:"下载二进制文件(GNU/Linux)",slug:"下载二进制文件-gnu-linux",normalizedTitle:"下载二进制文件(gnu/linux)",charIndex:853},{level:3,title:"下载二进制文件(Microsoft Windows)",slug:"下载二进制文件-microsoft-windows",normalizedTitle:"下载二进制文件(microsoft windows)",charIndex:1013},{level:3,title:"通过源码安装",slug:"通过源码安装",normalizedTitle:"通过源码安装",charIndex:1194},{level:2,title:"2. 运行Minio Client",slug:"_2-运行minio-client",normalizedTitle:"2. 运行minio client",charIndex:1377},{level:3,title:"GNU/Linux",slug:"gnu-linux",normalizedTitle:"gnu/linux",charIndex:861},{level:3,title:"macOS",slug:"macos",normalizedTitle:"macos",charIndex:785},{level:3,title:"Microsoft Windows",slug:"microsoft-windows",normalizedTitle:"microsoft windows",charIndex:1021},{level:2,title:"3. 添加一个云存储服务",slug:"_3-添加一个云存储服务",normalizedTitle:"3. 添加一个云存储服务",charIndex:1512},{level:3,title:"示例-Minio云存储",slug:"示例-minio云存储",normalizedTitle:"示例-minio云存储",charIndex:1839},{level:3,title:"示例-Amazon S3云存储",slug:"示例-amazon-s3云存储",normalizedTitle:"示例-amazon s3云存储",charIndex:2007},{level:3,title:"示例-Google云存储",slug:"示例-google云存储",normalizedTitle:"示例-google云存储",charIndex:2196},{level:2,title:"4. 验证",slug:"_4-验证",normalizedTitle:"4. 验证",charIndex:2435},{level:2,title:"5. 日常使用",slug:"_5-日常使用",normalizedTitle:"5. 日常使用",charIndex:2839},{level:2,title:"6. 全局参数",slug:"_6-全局参数",normalizedTitle:"6. 全局参数",charIndex:2997},{level:3,title:"参数 [—debug]",slug:"参数-debug",normalizedTitle:"参数 [—debug]",charIndex:3009},{level:3,title:"参数 [—json]",slug:"参数-json",normalizedTitle:"参数 [—json]",charIndex:4029},{level:3,title:"参数 [—no-color]",slug:"参数-no-color",normalizedTitle:"参数 [—no-color]",charIndex:4554},{level:3,title:"参数 [—quiet]",slug:"参数-quiet",normalizedTitle:"参数 [—quiet]",charIndex:4599},{level:3,title:"参数 [—config-folder]",slug:"参数-config-folder",normalizedTitle:"参数 [—config-folder]",charIndex:4631},{level:3,title:"参数 [ —insecure]",slug:"参数-insecure",normalizedTitle:"参数 [ —insecure]",charIndex:4674},{level:2,title:"7. 命令",slug:"_7-命令",normalizedTitle:"7. 命令",charIndex:4706},{level:3,title:"ls命令 - 列出对象",slug:"ls命令-列出对象",normalizedTitle:"ls命令 - 列出对象",charIndex:5098},{level:3,title:"mb命令 - 创建存储桶",slug:"mb命令-创建存储桶",normalizedTitle:"mb命令 - 创建存储桶",charIndex:5593},{level:3,title:"cat命令 - 合并对象",slug:"cat命令-合并对象",normalizedTitle:"cat命令 - 合并对象",charIndex:6029},{level:3,title:"pipe命令 - Pipe到对象",slug:"pipe命令-pipe到对象",normalizedTitle:"pipe命令 - pipe到对象",charIndex:6261},{level:3,title:"cp命令 - 拷贝对象",slug:"cp命令-拷贝对象",normalizedTitle:"cp命令 - 拷贝对象",charIndex:6542},{level:3,title:"rm命令 - 删除存储桶和对象。",slug:"rm命令-删除存储桶和对象。",normalizedTitle:"rm命令 - 删除存储桶和对象。",charIndex:6913},{level:3,title:"share命令 - 共享",slug:"share命令-共享",normalizedTitle:"share命令 - 共享",charIndex:7797},{level:3,title:"子命令share download - 共享下载",slug:"子命令share-download-共享下载",normalizedTitle:"子命令share download - 共享下载",charIndex:8144},{level:3,title:"mirror命令 - 存储桶镜像",slug:"mirror命令-存储桶镜像",normalizedTitle:"mirror命令 - 存储桶镜像",charIndex:10374},{level:3,title:"find命令 - 查找文件和对象",slug:"find命令-查找文件和对象",normalizedTitle:"find命令 - 查找文件和对象",charIndex:11075},{level:3,title:"diff命令 - 显示差异",slug:"diff命令-显示差异",normalizedTitle:"diff命令 - 显示差异",charIndex:11452},{level:3,title:"watch命令 - 监听文件和对象存储事件。",slug:"watch命令-监听文件和对象存储事件。",normalizedTitle:"watch命令 - 监听文件和对象存储事件。",charIndex:11808},{level:3,title:"events命令 - 管理存储桶事件通知。",slug:"events命令-管理存储桶事件通知。",normalizedTitle:"events命令 - 管理存储桶事件通知。",charIndex:12856},{level:3,title:"policy命令 - 管理存储桶策略",slug:"policy命令-管理存储桶策略",normalizedTitle:"policy命令 - 管理存储桶策略",charIndex:13719},{level:3,title:"session命令 - 管理session",slug:"session命令-管理session",normalizedTitle:"session命令 - 管理session",charIndex:14642},{level:3,title:"config命令 - 管理配置文件",slug:"config命令-管理配置文件",normalizedTitle:"config命令 - 管理配置文件",charIndex:15277},{level:3,title:"update命令 - 软件更新",slug:"update命令-软件更新",normalizedTitle:"update命令 - 软件更新",charIndex:15854},{level:3,title:"version命令 - 显示版本信息",slug:"version命令-显示版本信息",normalizedTitle:"version命令 - 显示版本信息",charIndex:16128}],headersStr:"1. 下载Minio Client Docker稳定版 Docker尝鲜版 Homebrew (macOS) 下载二进制文件(GNU/Linux) 下载二进制文件(Microsoft Windows) 通过源码安装 2. 运行Minio Client GNU/Linux macOS Microsoft Windows 3. 添加一个云存储服务 示例-Minio云存储 示例-Amazon S3云存储 示例-Google云存储 4. 验证 5. 日常使用 6. 全局参数 参数 [—debug] 参数 [—json] 参数 [—no-color] 参数 [—quiet] 参数 [—config-folder] 参数 [ —insecure] 7. 命令 ls命令 - 列出对象 mb命令 - 创建存储桶 cat命令 - 合并对象 pipe命令 - Pipe到对象 cp命令 - 拷贝对象 rm命令 - 删除存储桶和对象。 share命令 - 共享 子命令share download - 共享下载 mirror命令 - 存储桶镜像 find命令 - 查找文件和对象 diff命令 - 显示差异 watch命令 - 监听文件和对象存储事件。 events命令 - 管理存储桶事件通知。 policy命令 - 管理存储桶策略 session命令 - 管理session config命令 - 管理配置文件 update命令 - 软件更新 version命令 - 显示版本信息",content:'# Minio Client完全指南\n\nMinio Client (mc)为ls，cat，cp，mirror，diff，find等UNIX命令提供了一种替代方案。它支持文件系统和兼容Amazon S3的云存储服务（AWS Signature v2和v4）。\n\nls       列出文件和文件夹。\nmb       创建一个存储桶或一个文件夹。\ncat      显示文件和对象内容。\npipe     将一个STDIN重定向到一个对象或者文件或者STDOUT。\nshare    生成用于共享的URL。\ncp       拷贝文件和对象。\nmirror   给存储桶和文件夹做镜像。\nfind     基于参数查找文件。\ndiff     对两个文件夹或者存储桶比较差异。\nrm       删除文件和对象。\nevents   管理对象通知。\nwatch    监视文件和对象的事件。\npolicy   管理访问策略。\nsession  为cp命令管理保存的会话。\nconfig   管理mc配置文件。\nupdate   检查软件更新。\nversion  输出版本信息。\n\n\n\n# 1. 下载Minio Client\n\n\n# Docker稳定版\n\ndocker pull minio/mcdocker run minio/mc ls play\n\n\n\n# Docker尝鲜版\n\ndocker pull minio/mc:edgedocker run minio/mc:edge ls play\n\n\n注意: 上述示例默认使用Minio演示环境做演示，如果想用mc操作其它S3兼容的服务，采用下面的方式来启动容器：\n\ndocker run -it --entrypoint=/bin/sh minio/mc\n\n\n然后使用mc config命令。\n\n\n# Homebrew (macOS)\n\n使用Homebrew安装mc。\n\nbrew install minio/stable/mcmc --help\n\n\n\n# 下载二进制文件(GNU/Linux)\n\n平台          CPU架构          URL\nGNU/Linux   64-bit Intel   https://dl.minio.io/client/mc/release/linux-amd64/mc\n\nchmod +x mc./mc --help\n\n\n\n# 下载二进制文件(Microsoft Windows)\n\n平台                  CPU架构          URL\nMicrosoft Windows   64-bit Intel   https://dl.minio.io/client/mc/release/windows-amd64/mc.exe\n\nmc.exe --help\n\n\n\n# 通过源码安装\n\n通过源码安装仅适用于开发人员和高级用户。mc update命令不支持基于源码安装的更新通知。请从minio-client下载官方版本。\n\n如果您没有Golang环境，请按照 如何安装Golang。\n\ngo get -d github.com/minio/mccd ${GOPATH}/src/github.com/minio/mcmake\n\n\n\n# 2. 运行Minio Client\n\n\n# GNU/Linux\n\nchmod +x mc./mc --help\n\n\n\n# macOS\n\nchmod 755 mc./mc --help\n\n\n\n# Microsoft Windows\n\nmc.exe --help\n\n\n\n# 3. 添加一个云存储服务\n\n如果你打算仅在POSIX兼容文件系统中使用mc,那你可以直接略过本节，跳到Step 4。\n\n添加一个或多个S3兼容的服务，请参考下面说明。mc将所有的配置信息都存储在~/.mc/config.json文件中。\n\n# 使用\n\nmc config host add <ALIAS> <YOUR-S3-ENDPOINT> <YOUR-ACCESS-KEY> <YOUR-SECRET-KEY> <API-SIGNATURE>\n\n\n别名就是给你的云存储服务起了一个短点的外号。S3 endpoint,access key和secret key是你的云存储服务提供的。API签名是可选参数，默认情况下，它被设置为"S3v4"。\n\n\n# 示例-Minio云存储\n\n从Minio服务获得URL、access key和secret key。\n\nmc config host add minio http://192.168.1.51 BKIKJAA5BMMU2RHO6IBB V7f1CwQqAcwo80UEIJEjc5gVQUSSx5ohQ9GSrr12 S3v4\n\n\n\n# 示例-Amazon S3云存储\n\n参考AWS Credentials指南获取你的AccessKeyID和SecretAccessKey。\n\nmc config host add s3 https://s3.amazonaws.com BKIKJAA5BMMU2RHO6IBB V7f1CwQqAcwo80UEIJEjc5gVQUSSx5ohQ9GSrr12 S3v4\n\n\n\n# 示例-Google云存储\n\n参考Google Credentials Guide获取你的AccessKeyID和SecretAccessKey。\n\nmc config host add gcs  https://storage.googleapis.com BKIKJAA5BMMU2RHO6IBB V8f1CwQqAcwo80UEIJEjc5gVQUSSx5ohQ9GSrr12 S3v2\n\n\n注意：Google云存储只支持旧版签名版本V2，所以你需要选择S3v2。\n\n\n# 4. 验证\n\nmc预先配置了云存储服务URL：https://play.minio.io:9000，别名“play”。它是一个用于研发和测试的Minio服务。如果想测试Amazon S3,你可以将“play”替换为“s3”。\n\n示例:\n\n列出https://play.minio.io:9000上的所有存储桶。\n\nmc ls play\n[2016-03-22 19:47:48 PDT]     0B my-bucketname/\n[2016-03-22 22:01:07 PDT]     0B mytestbucket/\n[2016-03-22 20:04:39 PDT]     0B mybucketname/\n[2016-01-28 17:23:11 PST]     0B newbucket/\n[2016-03-20 09:08:36 PDT]     0B s3git-test/\n\n\n\n# 5. 日常使用\n\n你可以添加shell别名来覆盖默认的Unix工具命令。\n\nalias ls=\'mc ls\'\nalias cp=\'mc cp\'\nalias cat=\'mc cat\'\nalias mkdir=\'mc mb\'\nalias pipe=\'mc pipe\'\nalias find=\'mc find\'\n\n\n\n# 6. 全局参数\n\n\n# 参数 [—debug]\n\nDebug参数开启控制台输出debug信息。\n\n示例：输出ls命令的详细debug信息。\n\nmc --debug ls play\nmc: <DEBUG> GET / HTTP/1.1\nHost: play.minio.io:9000\nUser-Agent: Minio (darwin; amd64) minio-go/1.0.1 mc/2016-04-01T00:22:11Z\nAuthorization: AWS4-HMAC-SHA256 Credential=**REDACTED**/20160408/us-east-1/s3/aws4_request, SignedHeaders=expect;host;x-amz-content-sha256;x-amz-date, Signature=**REDACTED**\nExpect: 100-continue\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nX-Amz-Date: 20160408T145236Z\nAccept-Encoding: gzip\nmc: <DEBUG> HTTP/1.1 200 OK\nTransfer-Encoding: chunked\nAccept-Ranges: bytes\nContent-Type: text/xml; charset=utf-8\nDate: Fri, 08 Apr 2016 14:54:55 GMT\nServer: Minio/DEVELOPMENT.2016-04-07T18-53-27Z (linux; amd64)\nVary: Origin\nX-Amz-Request-Id: HP30I0W2U49BDBIO\nmc: <DEBUG> Response Time:  1.220112837s\n[...]\n[2016-04-08 03:56:14 IST]     0B albums/\n[2016-04-04 16:11:45 IST]     0B backup/\n[2016-04-01 20:10:53 IST]     0B deebucket/\n[2016-03-28 21:53:49 IST]     0B guestbucket/\n\n\n\n# 参数 [—json]\n\nJSON参数启用JSON格式的输出。\n\n示例：列出Minio play服务的所有存储桶。\n\nmc --json ls play\n{"status":"success","type":"folder","lastModified":"2016-04-08T03:56:14.577+05:30","size":0,"key":"albums/"}\n{"status":"success","type":"folder","lastModified":"2016-04-04T16:11:45.349+05:30","size":0,"key":"backup/"}\n{"status":"success","type":"folder","lastModified":"2016-04-01T20:10:53.941+05:30","size":0,"key":"deebucket/"}\n{"status":"success","type":"folder","lastModified":"2016-03-28T21:53:49.217+05:30","size":0,"key":"guestbucket/"}\n\n\n\n# 参数 [—no-color]\n\n这个参数禁用颜色主题。对于一些比较老的终端有用。\n\n\n# 参数 [—quiet]\n\n这个参数关闭控制台日志输出。\n\n\n# 参数 [—config-folder]\n\n这个参数参数自定义的配置文件路径。\n\n\n# 参数 [ —insecure]\n\n跳过SSL证书验证。\n\n\n# 7. 命令\n\n                                           \nls - 列出存储桶和对象     mb - 创建存储桶               cat - 合并对象\ncp - 拷贝对象         rm - 删除对象                pipe - Pipe到一个对象\nshare - 共享        mirror - 存储桶镜像           find - 查找文件和对象\ndiff - 比较存储桶差异    policy - 给存储桶或前缀设置访问策略   session - 管理保存的会话\nconfig - 管理配置文件   watch - 事件监听             events - 管理存储桶事件\nupdate - 管理软件更新   version - 显示版本信息         \n\n\n# ls命令 - 列出对象\n\nls命令列出文件、对象和存储桶。使用—incomplete flag可列出未完整拷贝的内容。\n\n用法：\n   mc ls [FLAGS] TARGET [TARGET ...]\nFLAGS:\n  --help, -h                       显示帮助。\n  --recursive, -r          递归。\n  --incomplete, -I         列出未完整上传的对象。\n\n\n示例： 列出所有https://play.minio.io:9000上的存储桶。\n\nmc ls play\n[2016-04-08 03:56:14 IST]     0B albums/\n[2016-04-04 16:11:45 IST]     0B backup/\n[2016-04-01 20:10:53 IST]     0B deebucket/\n[2016-03-28 21:53:49 IST]     0B guestbucket/\n[2016-04-08 20:58:18 IST]     0B mybucket/\n\n\n\n# mb命令 - 创建存储桶\n\nmb命令在对象存储上创建一个新的存储桶。在文件系统，它就和mkdir -p命令是一样的。存储桶相当于文件系统中的磁盘或挂载点，不应视为文件夹。Minio对每个用户创建的存储桶数量没有限制。在Amazon S3上，每个帐户被限制为100个存储桶。有关更多信息，请参阅S3上的存储桶限制和限制 。\n\n用法：\n   mc mb [FLAGS] TARGET [TARGET...]\nFLAGS:\n  --help, -h                       显示帮助。\n  --region "us-east-1"         指定存储桶的region，默认是‘us-east-1’.\n\n\n示例：在https://play.minio.io:9000上创建一个名叫"mybucket"的存储桶。\n\nmc mb play/mybucket\nBucket created successfully ‘play/mybucket’.\n\n\n\n# cat命令 - 合并对象\n\ncat命令将一个文件或者对象的内容合并到另一个上。你也可以用它将对象的内容输出到stdout。\n\n用法：\n   mc cat [FLAGS] SOURCE [SOURCE...]\nFLAGS:\n  --help, -h                       显示帮助。\n\n\n示例： 显示myobject.txt文件的内容\n\nmc cat play/mybucket/myobject.txt\nHello Minio!!\n\n\n\n# pipe命令 - Pipe到对象\n\npipe命令拷贝stdin里的内容到目标输出，如果没有指定目标输出，则输出到stdout。\n\n用法：\n   mc pipe [FLAGS] [TARGET]\nFLAGS:\n  --help, -h                    显示帮助。\n\n\n示例： 将MySQL数据库dump文件输出到Amazon S3。\n\nmysqldump -u root -p ******* accountsdb | mc pipe s3/ferenginar/backups/accountsdb-oct-9-2015.sql\n\n\n\n# cp命令 - 拷贝对象\n\ncp命令拷贝一个或多个源文件目标输出。所有到对象存储的拷贝操作都进行了MD4SUM checkSUM校验。可以从故障点恢复中断或失败的复制操作。\n\n用法：\n   mc cp [FLAGS] SOURCE [SOURCE...] TARGET\nFLAGS:\n  --help, -h                       显示帮助。\n  --recursive, -r          递归拷贝。\n\n\n示例： 拷贝一个文本文件到对象存储。\n\nmc cp myobject.txt play/mybucket\nmyobject.txt:    14 B / 14 B  ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓  100.00 % 41 B/s 0\n\n\n\n# rm命令 - 删除存储桶和对象。\n\n使用rm命令删除文件对象或者存储桶。\n\n用法：\n   mc rm [FLAGS] TARGET [TARGET ...]\nFLAGS:\n  --help, -h                       显示帮助。\n  --recursive, -r              递归删除。\n  --force              强制执行删除操作。\n  --prefix             删除批配这个前缀的对象。\n  --incomplete, -I      删除未完整上传的对象。\n  --fake               模拟一个假的删除操作。\n  --stdin              从STDIN中读对象列表。\n  --older-than value               删除N天前的对象（默认是0天）。\n\n\n示例： 删除一个对象。\n\nmc rm play/mybucket/myobject.txt\nRemoved ‘play/mybucket/myobject.txt’.\n\n\n示例：删除一个存储桶并递归删除里面所有的内容。由于这个操作太危险了，你必须传—force参数指定强制删除。\n\nmc rm --recursive --force play/myobject\nRemoved ‘play/myobject/newfile.txt’.\nRemoved \'play/myobject/otherobject.txt’.\n\n\n示例： 从mybucket里删除所有未完整上传的对象。\n\nmc rm  --incomplete --recursive --force play/mybucket\nRemoved ‘play/mybucket/mydvd.iso’.\nRemoved \'play/mybucket/backup.tgz’.\n\n\n示例： 删除一天前的对象。\n\nmc rm --force --older-than=1 play/mybucket/oldsongs\n\n\n\n# share命令 - 共享\n\nshare命令安全地授予上传或下载的权限。此访问只是临时的，与远程用户和应用程序共享也是安全的。如果你想授予永久访问权限，你可以看看mc policy命令。\n\n生成的网址中含有编码后的访问认证信息，任何企图篡改URL的行为都会使访问无效。想了解这种机制是如何工作的，请参考Pre-Signed URL技术。\n\n用法：\n   mc share [FLAGS] COMMAND\nFLAGS:\n  --help, -h                       显示帮助。\nCOMMANDS:\n   download   生成有下载权限的URL。\n   upload     生成有上传权限的URL。\n   list       列出先前共享的对象和文件夹。\n\n\n\n# 子命令share download - 共享下载\n\nshare download命令生成不需要access key和secret key即可下载的URL，过期参数设置成最大有效期（不大于7天），过期之后权限自动回收。\n\n用法：\n   mc share download [FLAGS] TARGET [TARGET...]\nFLAGS:\n  --help, -h                       显示帮助。\n  --recursive, -r          递归共享所有对象。\n  --expire, -E "168h"          设置过期时限，NN[h|m|s]。\n\n\n示例： 生成一个对一个对象有4小时访问权限的URL。\n\nmc share download --expire 4h play/mybucket/myobject.txt\nURL: https://play.minio.io:9000/mybucket/myobject.txt\nExpire: 0 days 4 hours 0 minutes 0 seconds\nShare: https://play.minio.io:9000/mybucket/myobject.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=Q3AM3UQ867SPQQA43P2F%2F20160408%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20160408T182008Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=1527fc8f21a3a7e39ce3c456907a10b389125047adc552bcd86630b9d459b634\n\n\n# 子命令share upload - 共享上传\n\nshare upload命令生成不需要access key和secret key即可上传的URL。过期参数设置成最大有效期（不大于7天），过期之后权限自动回收。Content-type参数限制只允许上传指定类型的文件。\n\n用法：\n   mc share upload [FLAGS] TARGET [TARGET...]\nFLAGS:\n  --help, -h                       显示帮助。\n  --recursive, -r              递归共享所有对象。\n  --expire, -E "168h"          设置过期时限，NN[h|m|s].\n\n\n示例： 生成一个curl命令，赋予上传到play/mybucket/myotherobject.txt的权限。\n\nmc share upload play/mybucket/myotherobject.txt\nURL: https://play.minio.io:9000/mybucket/myotherobject.txt\nExpire: 7 days 0 hours 0 minutes 0 seconds\nShare: curl https://play.minio.io:9000/mybucket -F x-amz-date=20160408T182356Z -F x-amz-signature=de343934bd0ba38bda0903813b5738f23dde67b4065ea2ec2e4e52f6389e51e1 -F bucket=mybucket -F policy=eyJleHBpcmF0aW9uIjoiMjAxNi0wNC0xNVQxODoyMzo1NS4wMDdaIiwiY29uZGl0aW9ucyI6W1siZXEiLCIkYnVja2V0IiwibXlidWNrZXQiXSxbImVxIiwiJGtleSIsIm15b3RoZXJvYmplY3QudHh0Il0sWyJlcSIsIiR4LWFtei1kYXRlIiwiMjAxNjA0MDhUMTgyMzU2WiJdLFsiZXEiLCIkeC1hbXotYWxnb3JpdGhtIiwiQVdTNC1ITUFDLVNIQTI1NiJdLFsiZXEiLCIkeC1hbXotY3JlZGVudGlhbCIsIlEzQU0zVVE4NjdTUFFRQTQzUDJGLzIwMTYwNDA4L3VzLWVhc3QtMS9zMy9hd3M0X3JlcXVlc3QiXV19 -F x-amz-algorithm=AWS4-HMAC-SHA256 -F x-amz-credential=Q3AM3UQ867SPQQA43P2F/20160408/us-east-1/s3/aws4_request -F key=myotherobject.txt -F file=@<FILE>\n\n\n# 子命令share list - 列出之前的共享\n\nshare list列出没未过期的共享URL。\n\n用法：\n   mc share list COMMAND\nCOMMAND:\n   upload:   列出先前共享的有上传权限的URL。\n   download: 列出先前共享的有下载权限的URL。\n\n\n\n# mirror命令 - 存储桶镜像\n\nmirror命令和rsync类似，只不过它是在文件系统和对象存储之间做同步。\n\n用法：\n   mc mirror [FLAGS] SOURCE TARGET\nFLAGS:\n  --help, -h                       显示帮助。\n  --force              强制覆盖已经存在的目标。\n  --fake               模拟一个假的操作。\n  --watch, -w                      监听改变并执行镜像操作。\n  --remove             删除目标上的外部的文件。\n\n\n示例： 将一个本地文件夹镜像到https://play.minio.io:9000上的\'mybucket\'存储桶。\n\nmc mirror localdir/ play/mybucket\nlocaldir/b.txt:  40 B / 40 B  ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃  100.00 % 73 B/s 0\n\n\n示例： 持续监听本地文件夹修改并镜像到https://play.minio.io:9000上的\'mybucket\'存储桶。\n\nmc mirror -w localdir play/mybucket\nlocaldir/new.txt:  10 MB / 10 MB  ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃  100.00 % 1 MB/s 15s\n\n\n\n# find命令 - 查找文件和对象\n\nfind命令通过指定参数查找文件，它只列出满足条件的数据。\n\n用法：\n  mc find PATH [FLAGS]\nFLAGS:\n  --help, -h                       显示帮助。\n  --exec value                     为每个匹配对象生成一个外部进程（请参阅FORMAT）\n  --name value                     查找匹配通配符模式的对象。\n  ...\n  ...\n\n\n示例： 持续从s3存储桶中查找所有jpeg图像，并复制到minio "play/bucket"存储桶\n\nmc find s3/bucket --name "*.jpg" --watch --exec "mc cp {} play/bucket"\n\n\n\n# diff命令 - 显示差异\n\ndiff命令计算两个目录之间的差异。它只列出缺少的或者大小不同的内容。\n\n它不比较内容，所以可能的是，名称相同，大小相同但内容不同的对象没有被检测到。这样，它可以在不同站点或者大量数据的情况下快速比较。\n\n用法：\n  mc diff [FLAGS] FIRST SECOND\nFLAGS:\n  --help, -h                       显示帮助。\n\n\n示例： 比较一个本地文件夹和一个远程对象存储服务\n\n mc diff localdir play/mybucket\n‘localdir/notes.txt’ and ‘https://play.minio.io:9000/mybucket/notes.txt’ - only in first.\n\n\n\n# watch命令 - 监听文件和对象存储事件。\n\nwatch命令提供了一种方便监听对象存储和文件系统上不同类型事件的方式。\n\n用法：\n  mc watch [FLAGS] PATH\nFLAGS:\n  --events value                   过滤不同类型的事件，默认是所有类型的事件 (默认： "put,delete,get")\n  --prefix value                   基于前缀过滤事件。\n  --suffix value                   基于后缀过滤事件。\n  --recursive                      递归方式监听事件。\n  --help, -h                       显示帮助。\n\n\n示例： 监听对象存储的所有事件\n\nmc watch play/testbucket\n[2016-08-18T00:51:29.735Z] 2.7KiB ObjectCreated https://play.minio.io:9000/testbucket/CONTRIBUTING.md\n[2016-08-18T00:51:29.780Z]  1009B ObjectCreated https://play.minio.io:9000/testbucket/MAINTAINERS.md\n[2016-08-18T00:51:29.839Z] 6.9KiB ObjectCreated https://play.minio.io:9000/testbucket/README.md\n\n\n示例： 监听本地文件夹的所有事件\n\nmc watch ~/Photos\n[2016-08-17T17:54:19.565Z] 3.7MiB ObjectCreated /home/minio/Downloads/tmp/5467026530_a8611b53f9_o.jpg\n[2016-08-17T17:54:19.565Z] 3.7MiB ObjectCreated /home/minio/Downloads/tmp/5467026530_a8611b53f9_o.jpg\n...\n[2016-08-17T17:54:19.565Z] 7.5MiB ObjectCreated /home/minio/Downloads/tmp/8771468997_89b762d104_o.jpg\n\n\n\n# events命令 - 管理存储桶事件通知。\n\nevents提供了一种方便的配置存储桶的各种类型事件通知的方式。Minio事件通知可以配置成使用 AMQP，Redis，ElasticSearch，NATS和PostgreSQL服务。Minio configuration提供了如何配置的更多细节。\n\n用法：\n  mc events COMMAND [COMMAND FLAGS | -h] [ARGUMENTS...]\nCOMMANDS:\n  add     添加一个新的存储桶通知。\n  remove  删除一个存储桶通知。使用\'--force\'可以删除所有存储桶通知。\n  list    列出存储桶通知。\nFLAGS:\n  --help, -h                       显示帮助。\n\n\n示例： 列出所有存储桶通知。\n\nmc events list play/andoria\nMyTopic        arn:minio:sns:us-east-1:1:TestTopic    s3:ObjectCreated:*,s3:ObjectRemoved:*   suffix:.jpg\n\n\n示例： 添加一个新的\'sqs\'通知，仅接收ObjectCreated事件。\n\nmc events add play/andoria arn:minio:sqs:us-east-1:1:your-queue --events put\n\n\n示例： 添加一个带有过滤器的\'sqs\'通知。\n\n给sqs通知添加prefix和suffix过滤规则。\n\nmc events add play/andoria arn:minio:sqs:us-east-1:1:your-queue --prefix photos/ --suffix .jpg\n\n\n示例： 删除一个\'sqs\'通知\n\nmc events remove play/andoria arn:minio:sqs:us-east-1:1:your-queue\n\n\n\n# policy命令 - 管理存储桶策略\n\n管理匿名访问存储桶和其内部内容的策略。\n\n用法：\n  mc policy [FLAGS] PERMISSION TARGET\n  mc policy [FLAGS] TARGET\n  mc policy list [FLAGS] TARGET\nPERMISSION:\n  Allowed policies are: [none, download, upload, public].\nFLAGS:\n  --help, -h                       显示帮助。\n\n\n示例： 显示当前匿名存储桶策略\n\n显示当前mybucket/myphotos/2020/子文件夹的匿名策略。\n\nmc policy play/mybucket/myphotos/2020/\nAccess permission for ‘play/mybucket/myphotos/2020/’ is ‘none’\n\n\n示例：设置可下载的匿名存储桶策略。\n\n设置mybucket/myphotos/2020/子文件夹可匿名下载的策略。现在，这个文件夹下的对象可被公开访问。比如：mybucket/myphotos/2020/yourobjectname可通过这个URL https://play.minio.io:9000/mybucket/myphotos/2020/yourobjectname访问。\n\nmc policy download play/mybucket/myphotos/2020/\nAccess permission for ‘play/mybucket/myphotos/2020/’ is set to \'download\'\n\n\n示例：删除当前的匿名存储桶策略\n\n删除所有*mybucket/myphotos/2020/*这个子文件夹下的匿名存储桶策略。\n\nmc policy none play/mybucket/myphotos/2020/\nAccess permission for ‘play/mybucket/myphotos/2020/’ is set to \'none\'\n\n\n\n# session命令 - 管理session\n\nsession命令管理之前保存的cp和mirror操作的会话。\n\n用法：\n  mc session COMMAND [COMMAND FLAGS | -h] [ARGUMENTS...]\nCOMMANDS:\n  list    列出所有之前保存的会话。\n  clear   清除某个之前保存的会话。\n  resume  恢复某个之前保存的会话。\nFLAGS:\n  --help, -h                       显示帮助。\n\n\n示例： 列出所有之前保存的会话\n\nmc session list\nIXWKjpQM -> [2016-04-08 19:11:14 IST] cp assets.go play/mybucket\nApwAxSwa -> [2016-04-08 01:49:19 IST] mirror miniodoc/ play/mybucket\n\n\n示例： 恢复一个之前保存的会话\n\nmc session resume IXWKjpQM\n...assets.go: 1.68 KB / 1.68 KB  ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓  100.00 % 784 B/s 2s\n\n\n示例： 清除一个之前保存的会话。\n\nmc session clear ApwAxSwa\nSession ‘ApwAxSwa’ cleared successfully.\n\n\n\n# config命令 - 管理配置文件\n\nconfig host命令提供了一个方便地管理~/.mc/config.json配置文件中的主机信息的方式，你也可以用文本编辑器手动修改这个配置文件。\n\n用法：\n  mc config host COMMAND [COMMAND FLAGS | -h] [ARGUMENTS...]\nCOMMANDS:\n  add, a      添加一个新的主机到配置文件。\n  remove, rm  从配置文件中删除一个主机。\n  list, ls    列出配置文件中的主机。\nFLAGS:\n  --help, -h                       显示帮助。\n\n\n示例： 管理配置文件\n\n添加Minio服务的access和secret key到配置文件，注意，shell的history特性可能会记录这些信息，从而带来安全隐患。在bash shell,使用set -o和set +o来关闭和开启history特性。\n\nset +o history\nmc config host add myminio http://localhost:9000 OMQAGGOL63D7UNVQFY8X GcY5RHNmnEWvD/1QxD3spEIGj+Vt9L7eHaAaBTkJ\nset -o history\n\n\n\n# update命令 - 软件更新\n\n从https://dl.minio.io检查软件更新。Experimental标志会检查unstable实验性的版本，通常用作测试用途。\n\n用法：\n  mc update [FLAGS]\nFLAGS:\n  --quiet, -q  关闭控制台输出。\n  --json       使用JSON格式输出。\n  --help, -h   显示帮助。\n\n\n示例： 检查更新\n\nmc update\nYou are already running the most recent version of ‘mc’.\n\n\n\n# version命令 - 显示版本信息\n\n显示当前安装的mc版本。\n\n用法：\n  mc version [FLAGS]\nFLAGS:\n  --quiet, -q  关闭控制台输出。\n  --json       使用JSON格式输出。\n  --help, -h   显示帮助。\n\n\n示例： 输出mc版本。\n\nmc version\nVersion: 2016-04-01T00:22:11Z\nRelease-tag: RELEASE.2016-04-01T00-22-11Z\nCommit-id: 12adf3be326f5b6610cdd1438f72dfd861597fce\n\n\n> 原文: https://docs.min.io/docs/minio-client-complete-guide.html',normalizedContent:'# minio client完全指南\n\nminio client (mc)为ls，cat，cp，mirror，diff，find等unix命令提供了一种替代方案。它支持文件系统和兼容amazon s3的云存储服务（aws signature v2和v4）。\n\nls       列出文件和文件夹。\nmb       创建一个存储桶或一个文件夹。\ncat      显示文件和对象内容。\npipe     将一个stdin重定向到一个对象或者文件或者stdout。\nshare    生成用于共享的url。\ncp       拷贝文件和对象。\nmirror   给存储桶和文件夹做镜像。\nfind     基于参数查找文件。\ndiff     对两个文件夹或者存储桶比较差异。\nrm       删除文件和对象。\nevents   管理对象通知。\nwatch    监视文件和对象的事件。\npolicy   管理访问策略。\nsession  为cp命令管理保存的会话。\nconfig   管理mc配置文件。\nupdate   检查软件更新。\nversion  输出版本信息。\n\n\n\n# 1. 下载minio client\n\n\n# docker稳定版\n\ndocker pull minio/mcdocker run minio/mc ls play\n\n\n\n# docker尝鲜版\n\ndocker pull minio/mc:edgedocker run minio/mc:edge ls play\n\n\n注意: 上述示例默认使用minio演示环境做演示，如果想用mc操作其它s3兼容的服务，采用下面的方式来启动容器：\n\ndocker run -it --entrypoint=/bin/sh minio/mc\n\n\n然后使用mc config命令。\n\n\n# homebrew (macos)\n\n使用homebrew安装mc。\n\nbrew install minio/stable/mcmc --help\n\n\n\n# 下载二进制文件(gnu/linux)\n\n平台          cpu架构          url\ngnu/linux   64-bit intel   https://dl.minio.io/client/mc/release/linux-amd64/mc\n\nchmod +x mc./mc --help\n\n\n\n# 下载二进制文件(microsoft windows)\n\n平台                  cpu架构          url\nmicrosoft windows   64-bit intel   https://dl.minio.io/client/mc/release/windows-amd64/mc.exe\n\nmc.exe --help\n\n\n\n# 通过源码安装\n\n通过源码安装仅适用于开发人员和高级用户。mc update命令不支持基于源码安装的更新通知。请从minio-client下载官方版本。\n\n如果您没有golang环境，请按照 如何安装golang。\n\ngo get -d github.com/minio/mccd ${gopath}/src/github.com/minio/mcmake\n\n\n\n# 2. 运行minio client\n\n\n# gnu/linux\n\nchmod +x mc./mc --help\n\n\n\n# macos\n\nchmod 755 mc./mc --help\n\n\n\n# microsoft windows\n\nmc.exe --help\n\n\n\n# 3. 添加一个云存储服务\n\n如果你打算仅在posix兼容文件系统中使用mc,那你可以直接略过本节，跳到step 4。\n\n添加一个或多个s3兼容的服务，请参考下面说明。mc将所有的配置信息都存储在~/.mc/config.json文件中。\n\n# 使用\n\nmc config host add <alias> <your-s3-endpoint> <your-access-key> <your-secret-key> <api-signature>\n\n\n别名就是给你的云存储服务起了一个短点的外号。s3 endpoint,access key和secret key是你的云存储服务提供的。api签名是可选参数，默认情况下，它被设置为"s3v4"。\n\n\n# 示例-minio云存储\n\n从minio服务获得url、access key和secret key。\n\nmc config host add minio http://192.168.1.51 bkikjaa5bmmu2rho6ibb v7f1cwqqacwo80ueijejc5gvqussx5ohq9gsrr12 s3v4\n\n\n\n# 示例-amazon s3云存储\n\n参考aws credentials指南获取你的accesskeyid和secretaccesskey。\n\nmc config host add s3 https://s3.amazonaws.com bkikjaa5bmmu2rho6ibb v7f1cwqqacwo80ueijejc5gvqussx5ohq9gsrr12 s3v4\n\n\n\n# 示例-google云存储\n\n参考google credentials guide获取你的accesskeyid和secretaccesskey。\n\nmc config host add gcs  https://storage.googleapis.com bkikjaa5bmmu2rho6ibb v8f1cwqqacwo80ueijejc5gvqussx5ohq9gsrr12 s3v2\n\n\n注意：google云存储只支持旧版签名版本v2，所以你需要选择s3v2。\n\n\n# 4. 验证\n\nmc预先配置了云存储服务url：https://play.minio.io:9000，别名“play”。它是一个用于研发和测试的minio服务。如果想测试amazon s3,你可以将“play”替换为“s3”。\n\n示例:\n\n列出https://play.minio.io:9000上的所有存储桶。\n\nmc ls play\n[2016-03-22 19:47:48 pdt]     0b my-bucketname/\n[2016-03-22 22:01:07 pdt]     0b mytestbucket/\n[2016-03-22 20:04:39 pdt]     0b mybucketname/\n[2016-01-28 17:23:11 pst]     0b newbucket/\n[2016-03-20 09:08:36 pdt]     0b s3git-test/\n\n\n\n# 5. 日常使用\n\n你可以添加shell别名来覆盖默认的unix工具命令。\n\nalias ls=\'mc ls\'\nalias cp=\'mc cp\'\nalias cat=\'mc cat\'\nalias mkdir=\'mc mb\'\nalias pipe=\'mc pipe\'\nalias find=\'mc find\'\n\n\n\n# 6. 全局参数\n\n\n# 参数 [—debug]\n\ndebug参数开启控制台输出debug信息。\n\n示例：输出ls命令的详细debug信息。\n\nmc --debug ls play\nmc: <debug> get / http/1.1\nhost: play.minio.io:9000\nuser-agent: minio (darwin; amd64) minio-go/1.0.1 mc/2016-04-01t00:22:11z\nauthorization: aws4-hmac-sha256 credential=**redacted**/20160408/us-east-1/s3/aws4_request, signedheaders=expect;host;x-amz-content-sha256;x-amz-date, signature=**redacted**\nexpect: 100-continue\nx-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date: 20160408t145236z\naccept-encoding: gzip\nmc: <debug> http/1.1 200 ok\ntransfer-encoding: chunked\naccept-ranges: bytes\ncontent-type: text/xml; charset=utf-8\ndate: fri, 08 apr 2016 14:54:55 gmt\nserver: minio/development.2016-04-07t18-53-27z (linux; amd64)\nvary: origin\nx-amz-request-id: hp30i0w2u49bdbio\nmc: <debug> response time:  1.220112837s\n[...]\n[2016-04-08 03:56:14 ist]     0b albums/\n[2016-04-04 16:11:45 ist]     0b backup/\n[2016-04-01 20:10:53 ist]     0b deebucket/\n[2016-03-28 21:53:49 ist]     0b guestbucket/\n\n\n\n# 参数 [—json]\n\njson参数启用json格式的输出。\n\n示例：列出minio play服务的所有存储桶。\n\nmc --json ls play\n{"status":"success","type":"folder","lastmodified":"2016-04-08t03:56:14.577+05:30","size":0,"key":"albums/"}\n{"status":"success","type":"folder","lastmodified":"2016-04-04t16:11:45.349+05:30","size":0,"key":"backup/"}\n{"status":"success","type":"folder","lastmodified":"2016-04-01t20:10:53.941+05:30","size":0,"key":"deebucket/"}\n{"status":"success","type":"folder","lastmodified":"2016-03-28t21:53:49.217+05:30","size":0,"key":"guestbucket/"}\n\n\n\n# 参数 [—no-color]\n\n这个参数禁用颜色主题。对于一些比较老的终端有用。\n\n\n# 参数 [—quiet]\n\n这个参数关闭控制台日志输出。\n\n\n# 参数 [—config-folder]\n\n这个参数参数自定义的配置文件路径。\n\n\n# 参数 [ —insecure]\n\n跳过ssl证书验证。\n\n\n# 7. 命令\n\n                                           \nls - 列出存储桶和对象     mb - 创建存储桶               cat - 合并对象\ncp - 拷贝对象         rm - 删除对象                pipe - pipe到一个对象\nshare - 共享        mirror - 存储桶镜像           find - 查找文件和对象\ndiff - 比较存储桶差异    policy - 给存储桶或前缀设置访问策略   session - 管理保存的会话\nconfig - 管理配置文件   watch - 事件监听             events - 管理存储桶事件\nupdate - 管理软件更新   version - 显示版本信息         \n\n\n# ls命令 - 列出对象\n\nls命令列出文件、对象和存储桶。使用—incomplete flag可列出未完整拷贝的内容。\n\n用法：\n   mc ls [flags] target [target ...]\nflags:\n  --help, -h                       显示帮助。\n  --recursive, -r          递归。\n  --incomplete, -i         列出未完整上传的对象。\n\n\n示例： 列出所有https://play.minio.io:9000上的存储桶。\n\nmc ls play\n[2016-04-08 03:56:14 ist]     0b albums/\n[2016-04-04 16:11:45 ist]     0b backup/\n[2016-04-01 20:10:53 ist]     0b deebucket/\n[2016-03-28 21:53:49 ist]     0b guestbucket/\n[2016-04-08 20:58:18 ist]     0b mybucket/\n\n\n\n# mb命令 - 创建存储桶\n\nmb命令在对象存储上创建一个新的存储桶。在文件系统，它就和mkdir -p命令是一样的。存储桶相当于文件系统中的磁盘或挂载点，不应视为文件夹。minio对每个用户创建的存储桶数量没有限制。在amazon s3上，每个帐户被限制为100个存储桶。有关更多信息，请参阅s3上的存储桶限制和限制 。\n\n用法：\n   mc mb [flags] target [target...]\nflags:\n  --help, -h                       显示帮助。\n  --region "us-east-1"         指定存储桶的region，默认是‘us-east-1’.\n\n\n示例：在https://play.minio.io:9000上创建一个名叫"mybucket"的存储桶。\n\nmc mb play/mybucket\nbucket created successfully ‘play/mybucket’.\n\n\n\n# cat命令 - 合并对象\n\ncat命令将一个文件或者对象的内容合并到另一个上。你也可以用它将对象的内容输出到stdout。\n\n用法：\n   mc cat [flags] source [source...]\nflags:\n  --help, -h                       显示帮助。\n\n\n示例： 显示myobject.txt文件的内容\n\nmc cat play/mybucket/myobject.txt\nhello minio!!\n\n\n\n# pipe命令 - pipe到对象\n\npipe命令拷贝stdin里的内容到目标输出，如果没有指定目标输出，则输出到stdout。\n\n用法：\n   mc pipe [flags] [target]\nflags:\n  --help, -h                    显示帮助。\n\n\n示例： 将mysql数据库dump文件输出到amazon s3。\n\nmysqldump -u root -p ******* accountsdb | mc pipe s3/ferenginar/backups/accountsdb-oct-9-2015.sql\n\n\n\n# cp命令 - 拷贝对象\n\ncp命令拷贝一个或多个源文件目标输出。所有到对象存储的拷贝操作都进行了md4sum checksum校验。可以从故障点恢复中断或失败的复制操作。\n\n用法：\n   mc cp [flags] source [source...] target\nflags:\n  --help, -h                       显示帮助。\n  --recursive, -r          递归拷贝。\n\n\n示例： 拷贝一个文本文件到对象存储。\n\nmc cp myobject.txt play/mybucket\nmyobject.txt:    14 b / 14 b  ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓  100.00 % 41 b/s 0\n\n\n\n# rm命令 - 删除存储桶和对象。\n\n使用rm命令删除文件对象或者存储桶。\n\n用法：\n   mc rm [flags] target [target ...]\nflags:\n  --help, -h                       显示帮助。\n  --recursive, -r              递归删除。\n  --force              强制执行删除操作。\n  --prefix             删除批配这个前缀的对象。\n  --incomplete, -i      删除未完整上传的对象。\n  --fake               模拟一个假的删除操作。\n  --stdin              从stdin中读对象列表。\n  --older-than value               删除n天前的对象（默认是0天）。\n\n\n示例： 删除一个对象。\n\nmc rm play/mybucket/myobject.txt\nremoved ‘play/mybucket/myobject.txt’.\n\n\n示例：删除一个存储桶并递归删除里面所有的内容。由于这个操作太危险了，你必须传—force参数指定强制删除。\n\nmc rm --recursive --force play/myobject\nremoved ‘play/myobject/newfile.txt’.\nremoved \'play/myobject/otherobject.txt’.\n\n\n示例： 从mybucket里删除所有未完整上传的对象。\n\nmc rm  --incomplete --recursive --force play/mybucket\nremoved ‘play/mybucket/mydvd.iso’.\nremoved \'play/mybucket/backup.tgz’.\n\n\n示例： 删除一天前的对象。\n\nmc rm --force --older-than=1 play/mybucket/oldsongs\n\n\n\n# share命令 - 共享\n\nshare命令安全地授予上传或下载的权限。此访问只是临时的，与远程用户和应用程序共享也是安全的。如果你想授予永久访问权限，你可以看看mc policy命令。\n\n生成的网址中含有编码后的访问认证信息，任何企图篡改url的行为都会使访问无效。想了解这种机制是如何工作的，请参考pre-signed url技术。\n\n用法：\n   mc share [flags] command\nflags:\n  --help, -h                       显示帮助。\ncommands:\n   download   生成有下载权限的url。\n   upload     生成有上传权限的url。\n   list       列出先前共享的对象和文件夹。\n\n\n\n# 子命令share download - 共享下载\n\nshare download命令生成不需要access key和secret key即可下载的url，过期参数设置成最大有效期（不大于7天），过期之后权限自动回收。\n\n用法：\n   mc share download [flags] target [target...]\nflags:\n  --help, -h                       显示帮助。\n  --recursive, -r          递归共享所有对象。\n  --expire, -e "168h"          设置过期时限，nn[h|m|s]。\n\n\n示例： 生成一个对一个对象有4小时访问权限的url。\n\nmc share download --expire 4h play/mybucket/myobject.txt\nurl: https://play.minio.io:9000/mybucket/myobject.txt\nexpire: 0 days 4 hours 0 minutes 0 seconds\nshare: https://play.minio.io:9000/mybucket/myobject.txt?x-amz-algorithm=aws4-hmac-sha256&x-amz-credential=q3am3uq867spqqa43p2f%2f20160408%2fus-east-1%2fs3%2faws4_request&x-amz-date=20160408t182008z&x-amz-expires=604800&x-amz-signedheaders=host&x-amz-signature=1527fc8f21a3a7e39ce3c456907a10b389125047adc552bcd86630b9d459b634\n\n\n# 子命令share upload - 共享上传\n\nshare upload命令生成不需要access key和secret key即可上传的url。过期参数设置成最大有效期（不大于7天），过期之后权限自动回收。content-type参数限制只允许上传指定类型的文件。\n\n用法：\n   mc share upload [flags] target [target...]\nflags:\n  --help, -h                       显示帮助。\n  --recursive, -r              递归共享所有对象。\n  --expire, -e "168h"          设置过期时限，nn[h|m|s].\n\n\n示例： 生成一个curl命令，赋予上传到play/mybucket/myotherobject.txt的权限。\n\nmc share upload play/mybucket/myotherobject.txt\nurl: https://play.minio.io:9000/mybucket/myotherobject.txt\nexpire: 7 days 0 hours 0 minutes 0 seconds\nshare: curl https://play.minio.io:9000/mybucket -f x-amz-date=20160408t182356z -f x-amz-signature=de343934bd0ba38bda0903813b5738f23dde67b4065ea2ec2e4e52f6389e51e1 -f bucket=mybucket -f policy=eyjlehbpcmf0aw9uijoimjaxni0wnc0xnvqxodoymzo1ns4wmddaiiwiy29uzgl0aw9ucyi6w1sizxeilcikynvja2v0iiwibxlidwnrzxqixsxbimvxiiwijgtlesisim15b3rozxjvymply3qudhh0il0swyjlcsisiir4lwftei1kyxrliiwimjaxnja0mdhumtgymzu2wijdlfsizxeilcikec1hbxotywxnb3jpdghtiiwiqvdtnc1itufdlvniqti1nijdlfsizxeilcikec1hbxoty3jlzgvudglhbcisilezqu0zvve4njdtuffrqtqzudjglziwmtywnda4l3vzlwvhc3qtms9zmy9hd3m0x3jlcxvlc3qixv19 -f x-amz-algorithm=aws4-hmac-sha256 -f x-amz-credential=q3am3uq867spqqa43p2f/20160408/us-east-1/s3/aws4_request -f key=myotherobject.txt -f file=@<file>\n\n\n# 子命令share list - 列出之前的共享\n\nshare list列出没未过期的共享url。\n\n用法：\n   mc share list command\ncommand:\n   upload:   列出先前共享的有上传权限的url。\n   download: 列出先前共享的有下载权限的url。\n\n\n\n# mirror命令 - 存储桶镜像\n\nmirror命令和rsync类似，只不过它是在文件系统和对象存储之间做同步。\n\n用法：\n   mc mirror [flags] source target\nflags:\n  --help, -h                       显示帮助。\n  --force              强制覆盖已经存在的目标。\n  --fake               模拟一个假的操作。\n  --watch, -w                      监听改变并执行镜像操作。\n  --remove             删除目标上的外部的文件。\n\n\n示例： 将一个本地文件夹镜像到https://play.minio.io:9000上的\'mybucket\'存储桶。\n\nmc mirror localdir/ play/mybucket\nlocaldir/b.txt:  40 b / 40 b  ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃  100.00 % 73 b/s 0\n\n\n示例： 持续监听本地文件夹修改并镜像到https://play.minio.io:9000上的\'mybucket\'存储桶。\n\nmc mirror -w localdir play/mybucket\nlocaldir/new.txt:  10 mb / 10 mb  ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃  100.00 % 1 mb/s 15s\n\n\n\n# find命令 - 查找文件和对象\n\nfind命令通过指定参数查找文件，它只列出满足条件的数据。\n\n用法：\n  mc find path [flags]\nflags:\n  --help, -h                       显示帮助。\n  --exec value                     为每个匹配对象生成一个外部进程（请参阅format）\n  --name value                     查找匹配通配符模式的对象。\n  ...\n  ...\n\n\n示例： 持续从s3存储桶中查找所有jpeg图像，并复制到minio "play/bucket"存储桶\n\nmc find s3/bucket --name "*.jpg" --watch --exec "mc cp {} play/bucket"\n\n\n\n# diff命令 - 显示差异\n\ndiff命令计算两个目录之间的差异。它只列出缺少的或者大小不同的内容。\n\n它不比较内容，所以可能的是，名称相同，大小相同但内容不同的对象没有被检测到。这样，它可以在不同站点或者大量数据的情况下快速比较。\n\n用法：\n  mc diff [flags] first second\nflags:\n  --help, -h                       显示帮助。\n\n\n示例： 比较一个本地文件夹和一个远程对象存储服务\n\n mc diff localdir play/mybucket\n‘localdir/notes.txt’ and ‘https://play.minio.io:9000/mybucket/notes.txt’ - only in first.\n\n\n\n# watch命令 - 监听文件和对象存储事件。\n\nwatch命令提供了一种方便监听对象存储和文件系统上不同类型事件的方式。\n\n用法：\n  mc watch [flags] path\nflags:\n  --events value                   过滤不同类型的事件，默认是所有类型的事件 (默认： "put,delete,get")\n  --prefix value                   基于前缀过滤事件。\n  --suffix value                   基于后缀过滤事件。\n  --recursive                      递归方式监听事件。\n  --help, -h                       显示帮助。\n\n\n示例： 监听对象存储的所有事件\n\nmc watch play/testbucket\n[2016-08-18t00:51:29.735z] 2.7kib objectcreated https://play.minio.io:9000/testbucket/contributing.md\n[2016-08-18t00:51:29.780z]  1009b objectcreated https://play.minio.io:9000/testbucket/maintainers.md\n[2016-08-18t00:51:29.839z] 6.9kib objectcreated https://play.minio.io:9000/testbucket/readme.md\n\n\n示例： 监听本地文件夹的所有事件\n\nmc watch ~/photos\n[2016-08-17t17:54:19.565z] 3.7mib objectcreated /home/minio/downloads/tmp/5467026530_a8611b53f9_o.jpg\n[2016-08-17t17:54:19.565z] 3.7mib objectcreated /home/minio/downloads/tmp/5467026530_a8611b53f9_o.jpg\n...\n[2016-08-17t17:54:19.565z] 7.5mib objectcreated /home/minio/downloads/tmp/8771468997_89b762d104_o.jpg\n\n\n\n# events命令 - 管理存储桶事件通知。\n\nevents提供了一种方便的配置存储桶的各种类型事件通知的方式。minio事件通知可以配置成使用 amqp，redis，elasticsearch，nats和postgresql服务。minio configuration提供了如何配置的更多细节。\n\n用法：\n  mc events command [command flags | -h] [arguments...]\ncommands:\n  add     添加一个新的存储桶通知。\n  remove  删除一个存储桶通知。使用\'--force\'可以删除所有存储桶通知。\n  list    列出存储桶通知。\nflags:\n  --help, -h                       显示帮助。\n\n\n示例： 列出所有存储桶通知。\n\nmc events list play/andoria\nmytopic        arn:minio:sns:us-east-1:1:testtopic    s3:objectcreated:*,s3:objectremoved:*   suffix:.jpg\n\n\n示例： 添加一个新的\'sqs\'通知，仅接收objectcreated事件。\n\nmc events add play/andoria arn:minio:sqs:us-east-1:1:your-queue --events put\n\n\n示例： 添加一个带有过滤器的\'sqs\'通知。\n\n给sqs通知添加prefix和suffix过滤规则。\n\nmc events add play/andoria arn:minio:sqs:us-east-1:1:your-queue --prefix photos/ --suffix .jpg\n\n\n示例： 删除一个\'sqs\'通知\n\nmc events remove play/andoria arn:minio:sqs:us-east-1:1:your-queue\n\n\n\n# policy命令 - 管理存储桶策略\n\n管理匿名访问存储桶和其内部内容的策略。\n\n用法：\n  mc policy [flags] permission target\n  mc policy [flags] target\n  mc policy list [flags] target\npermission:\n  allowed policies are: [none, download, upload, public].\nflags:\n  --help, -h                       显示帮助。\n\n\n示例： 显示当前匿名存储桶策略\n\n显示当前mybucket/myphotos/2020/子文件夹的匿名策略。\n\nmc policy play/mybucket/myphotos/2020/\naccess permission for ‘play/mybucket/myphotos/2020/’ is ‘none’\n\n\n示例：设置可下载的匿名存储桶策略。\n\n设置mybucket/myphotos/2020/子文件夹可匿名下载的策略。现在，这个文件夹下的对象可被公开访问。比如：mybucket/myphotos/2020/yourobjectname可通过这个url https://play.minio.io:9000/mybucket/myphotos/2020/yourobjectname访问。\n\nmc policy download play/mybucket/myphotos/2020/\naccess permission for ‘play/mybucket/myphotos/2020/’ is set to \'download\'\n\n\n示例：删除当前的匿名存储桶策略\n\n删除所有*mybucket/myphotos/2020/*这个子文件夹下的匿名存储桶策略。\n\nmc policy none play/mybucket/myphotos/2020/\naccess permission for ‘play/mybucket/myphotos/2020/’ is set to \'none\'\n\n\n\n# session命令 - 管理session\n\nsession命令管理之前保存的cp和mirror操作的会话。\n\n用法：\n  mc session command [command flags | -h] [arguments...]\ncommands:\n  list    列出所有之前保存的会话。\n  clear   清除某个之前保存的会话。\n  resume  恢复某个之前保存的会话。\nflags:\n  --help, -h                       显示帮助。\n\n\n示例： 列出所有之前保存的会话\n\nmc session list\nixwkjpqm -> [2016-04-08 19:11:14 ist] cp assets.go play/mybucket\napwaxswa -> [2016-04-08 01:49:19 ist] mirror miniodoc/ play/mybucket\n\n\n示例： 恢复一个之前保存的会话\n\nmc session resume ixwkjpqm\n...assets.go: 1.68 kb / 1.68 kb  ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓  100.00 % 784 b/s 2s\n\n\n示例： 清除一个之前保存的会话。\n\nmc session clear apwaxswa\nsession ‘apwaxswa’ cleared successfully.\n\n\n\n# config命令 - 管理配置文件\n\nconfig host命令提供了一个方便地管理~/.mc/config.json配置文件中的主机信息的方式，你也可以用文本编辑器手动修改这个配置文件。\n\n用法：\n  mc config host command [command flags | -h] [arguments...]\ncommands:\n  add, a      添加一个新的主机到配置文件。\n  remove, rm  从配置文件中删除一个主机。\n  list, ls    列出配置文件中的主机。\nflags:\n  --help, -h                       显示帮助。\n\n\n示例： 管理配置文件\n\n添加minio服务的access和secret key到配置文件，注意，shell的history特性可能会记录这些信息，从而带来安全隐患。在bash shell,使用set -o和set +o来关闭和开启history特性。\n\nset +o history\nmc config host add myminio http://localhost:9000 omqaggol63d7unvqfy8x gcy5rhnmnewvd/1qxd3speigj+vt9l7ehaaabtkj\nset -o history\n\n\n\n# update命令 - 软件更新\n\n从https://dl.minio.io检查软件更新。experimental标志会检查unstable实验性的版本，通常用作测试用途。\n\n用法：\n  mc update [flags]\nflags:\n  --quiet, -q  关闭控制台输出。\n  --json       使用json格式输出。\n  --help, -h   显示帮助。\n\n\n示例： 检查更新\n\nmc update\nyou are already running the most recent version of ‘mc’.\n\n\n\n# version命令 - 显示版本信息\n\n显示当前安装的mc版本。\n\n用法：\n  mc version [flags]\nflags:\n  --quiet, -q  关闭控制台输出。\n  --json       使用json格式输出。\n  --help, -h   显示帮助。\n\n\n示例： 输出mc版本。\n\nmc version\nversion: 2016-04-01t00:22:11z\nrelease-tag: release.2016-04-01t00-22-11z\ncommit-id: 12adf3be326f5b6610cdd1438f72dfd861597fce\n\n\n> 原文: https://docs.min.io/docs/minio-client-complete-guide.html',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Java Client快速入门指南",frontmatter:{title:"Java Client快速入门指南",date:"2022-02-09T11:29:53.000Z",permalink:"/pages/53a165/"},regularPath:"/12.MinIO/03.Minio%20SDKs/01.Java%20Client%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97.html",relativePath:"12.MinIO/03.Minio SDKs/01.Java Client快速入门指南.md",key:"v-d416917a",path:"/pages/53a165/",headers:[{level:2,title:"最低需求",slug:"最低需求",normalizedTitle:"最低需求",charIndex:200},{level:2,title:"使用maven",slug:"使用maven",normalizedTitle:"使用maven",charIndex:267},{level:2,title:"使用gradle",slug:"使用gradle",normalizedTitle:"使用gradle",charIndex:401},{level:2,title:"直接下载JAR",slug:"直接下载jar",normalizedTitle:"直接下载jar",charIndex:467},{level:2,title:"快速入门示例－文件上传",slug:"快速入门示例-文件上传",normalizedTitle:"快速入门示例－文件上传",charIndex:504},{level:2,title:"API文档",slug:"api文档",normalizedTitle:"api文档",charIndex:2320},{level:3,title:"API文档: 操作存储桶",slug:"api文档-操作存储桶",normalizedTitle:"api文档: 操作存储桶",charIndex:2366},{level:3,title:"API文档: 操作文件对象",slug:"api文档-操作文件对象",normalizedTitle:"api文档: 操作文件对象",charIndex:2499},{level:3,title:"API文档: Presigned操作",slug:"api文档-presigned操作",normalizedTitle:"api文档: presigned操作",charIndex:2628},{level:3,title:"API文档: 操作存储桶策略",slug:"api文档-操作存储桶策略",normalizedTitle:"api文档: 操作存储桶策略",charIndex:2730},{level:2,title:"完整示例",slug:"完整示例",normalizedTitle:"完整示例",charIndex:2798},{level:2,title:"了解更多",slug:"了解更多",normalizedTitle:"了解更多",charIndex:3409}],headersStr:"最低需求 使用maven 使用gradle 直接下载JAR 快速入门示例－文件上传 API文档 API文档: 操作存储桶 API文档: 操作文件对象 API文档: Presigned操作 API文档: 操作存储桶策略 完整示例 了解更多",content:'# Java Client快速入门指南\n\n\n# 适用于与Amazon S3兼容的云存储的Minio Java SDK\n\nMinio Java Client SDK提供简单的API来访问任何与Amazon S3兼容的对象存储服务。\n\n本快速入门指南将向你展示如何安装客户端SDK并执行示例java程序。有关API和示例的完整列表，请查看Java Client API Reference文档。\n\n\n# 最低需求\n\nJava 1.8或更高版本:\n\n * OracleJDK 8.0\n\n * OpenJDK8.0\n   \n   \n   # 使用maven\n\n<dependency>    <groupId>io.minio</groupId>    <artifactId>minio</artifactId>    <version>3.0.10</version></dependency>\n\n\n\n# 使用gradle\n\ndependencies {    compile \'io.minio:minio:3.0.10\'}\n\n\n\n# 直接下载JAR\n\n你可以到maven仓库直接下载最新版的JAR。\n\n\n# 快速入门示例－文件上传\n\n本示例程序连接到一个对象存储服务，创建一个存储桶并上传一个文件到该桶中。\n\n你需要有存储服务的三个参数才能连接到该服务。\n\n参数           说明\nEndpoint     对象存储服务的URL\nAccess Key   Access key就像用户ID，可以唯一标识你的账户。\nSecret Key   Secret key是你账户的密码。\n\n在下面的例子的中，我们将使用一个运行在https://play.minio.io:9000的免费托管的Minio服务。你可以随意使用此服务进行测试和开发。此示例中显示的访问凭据是公开的。\n\n# FileUploader.java\n\nimport java.io.IOException;import java.security.NoSuchAlgorithmException;import java.security.InvalidKeyException;import org.xmlpull.v1.XmlPullParserException;import io.minio.MinioClient;import io.minio.errors.MinioException;public class FileUploader {  public static void main(String[] args) throws NoSuchAlgorithmException, IOException, InvalidKeyException, XmlPullParserException {    try {      // 使用Minio服务的URL，端口，Access key和Secret key创建一个MinioClient对象      MinioClient minioClient = new MinioClient("https://play.minio.io:9000", "Q3AM3UQ867SPQQA43P2F", "zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG");      // 检查存储桶是否已经存在      boolean isExist = minioClient.bucketExists("asiatrip");      if(isExist) {        System.out.println("Bucket already exists.");      } else {        // 创建一个名为asiatrip的存储桶，用于存储照片的zip文件。        minioClient.makeBucket("asiatrip");      }      // 使用putObject上传一个文件到存储桶中。      minioClient.putObject("asiatrip","asiaphotos.zip", "/home/user/Photos/asiaphotos.zip");      System.out.println("/home/user/Photos/asiaphotos.zip is successfully uploaded as asiaphotos.zip to `asiatrip` bucket.");    } catch(MinioException e) {      System.out.println("Error occurred: " + e);    }  }}\n\n\n# 编译FileUploader\n\njavac -cp "minio-3.0.9-all.jar"  FileUploader.java\n\n\n# 运行FileUploader\n\njava -cp "minio-3.0.9-all.jar:." FileUploader/home/user/Photos/asiaphotos.zip is successfully uploaded as asiaphotos.zip to `asiatrip` bucket.mc ls play/asiatrip/[2016-06-02 18:10:29 PDT]  82KiB asiaphotos.zip\n\n\n\n# API文档\n\n下面链接是完整的API文档\n\n * API完整文档\n   \n   \n   # API文档: 操作存储桶\n\n * makeBucket\n\n * listBuckets\n\n * bucketExists\n\n * removeBucket\n\n * listObjects\n\n * listIncompleteUploads\n   \n   \n   # API文档: 操作文件对象\n\n * getObject\n\n * putObject\n\n * copyObject\n\n * statObject\n\n * removeObject\n\n * removeIncompleteUpload\n   \n   \n   # API文档: Presigned操作\n\n * presignedGetObject\n\n * presignedPutObject\n\n * presignedPostPolicy\n   \n   \n   # API文档: 操作存储桶策略\n\n * getBucketPolicy\n\n * setBucketPolicy\n   \n   \n   # 完整示例\n\n# 完整示例: Bucket Operations\n\n * ListBuckets.java\n\n * ListObjects.java\n\n * BucketExists.java\n\n * MakeBucket.java\n\n * RemoveBucket.java\n\n * ListIncompleteUploads.java\n   \n   # 完整示例: Object Operations\n\n * PutObject.java\n\n * PutObjectEncrypted.java\n\n * GetObject.Java\n\n * GetObjectEncrypted.Java\n\n * GetPartialObject.java\n\n * RemoveObject.java\n\n * RemoveObjects.java\n\n * StatObject.java\n   \n   # 完整示例: Presigned Operations\n\n * PresignedGetObject.java\n\n * PresignedPutObject.java\n\n * PresignedPostPolicy.java\n   \n   # 完整示例: Bucket Policy Operations\n\n * SetBucketPolicy.java\n\n * GetBucketPolicy.Java\n   \n   \n   # 了解更多\n\n * Minio官方文档\n\n * Minio Java Client SDK API文档\n\n * [创建属于你的照片API服务-完整示例](',normalizedContent:'# java client快速入门指南\n\n\n# 适用于与amazon s3兼容的云存储的minio java sdk\n\nminio java client sdk提供简单的api来访问任何与amazon s3兼容的对象存储服务。\n\n本快速入门指南将向你展示如何安装客户端sdk并执行示例java程序。有关api和示例的完整列表，请查看java client api reference文档。\n\n\n# 最低需求\n\njava 1.8或更高版本:\n\n * oraclejdk 8.0\n\n * openjdk8.0\n   \n   \n   # 使用maven\n\n<dependency>    <groupid>io.minio</groupid>    <artifactid>minio</artifactid>    <version>3.0.10</version></dependency>\n\n\n\n# 使用gradle\n\ndependencies {    compile \'io.minio:minio:3.0.10\'}\n\n\n\n# 直接下载jar\n\n你可以到maven仓库直接下载最新版的jar。\n\n\n# 快速入门示例－文件上传\n\n本示例程序连接到一个对象存储服务，创建一个存储桶并上传一个文件到该桶中。\n\n你需要有存储服务的三个参数才能连接到该服务。\n\n参数           说明\nendpoint     对象存储服务的url\naccess key   access key就像用户id，可以唯一标识你的账户。\nsecret key   secret key是你账户的密码。\n\n在下面的例子的中，我们将使用一个运行在https://play.minio.io:9000的免费托管的minio服务。你可以随意使用此服务进行测试和开发。此示例中显示的访问凭据是公开的。\n\n# fileuploader.java\n\nimport java.io.ioexception;import java.security.nosuchalgorithmexception;import java.security.invalidkeyexception;import org.xmlpull.v1.xmlpullparserexception;import io.minio.minioclient;import io.minio.errors.minioexception;public class fileuploader {  public static void main(string[] args) throws nosuchalgorithmexception, ioexception, invalidkeyexception, xmlpullparserexception {    try {      // 使用minio服务的url，端口，access key和secret key创建一个minioclient对象      minioclient minioclient = new minioclient("https://play.minio.io:9000", "q3am3uq867spqqa43p2f", "zuf+tfteslswru7bj86wekitnifilbzam1kyy3tg");      // 检查存储桶是否已经存在      boolean isexist = minioclient.bucketexists("asiatrip");      if(isexist) {        system.out.println("bucket already exists.");      } else {        // 创建一个名为asiatrip的存储桶，用于存储照片的zip文件。        minioclient.makebucket("asiatrip");      }      // 使用putobject上传一个文件到存储桶中。      minioclient.putobject("asiatrip","asiaphotos.zip", "/home/user/photos/asiaphotos.zip");      system.out.println("/home/user/photos/asiaphotos.zip is successfully uploaded as asiaphotos.zip to `asiatrip` bucket.");    } catch(minioexception e) {      system.out.println("error occurred: " + e);    }  }}\n\n\n# 编译fileuploader\n\njavac -cp "minio-3.0.9-all.jar"  fileuploader.java\n\n\n# 运行fileuploader\n\njava -cp "minio-3.0.9-all.jar:." fileuploader/home/user/photos/asiaphotos.zip is successfully uploaded as asiaphotos.zip to `asiatrip` bucket.mc ls play/asiatrip/[2016-06-02 18:10:29 pdt]  82kib asiaphotos.zip\n\n\n\n# api文档\n\n下面链接是完整的api文档\n\n * api完整文档\n   \n   \n   # api文档: 操作存储桶\n\n * makebucket\n\n * listbuckets\n\n * bucketexists\n\n * removebucket\n\n * listobjects\n\n * listincompleteuploads\n   \n   \n   # api文档: 操作文件对象\n\n * getobject\n\n * putobject\n\n * copyobject\n\n * statobject\n\n * removeobject\n\n * removeincompleteupload\n   \n   \n   # api文档: presigned操作\n\n * presignedgetobject\n\n * presignedputobject\n\n * presignedpostpolicy\n   \n   \n   # api文档: 操作存储桶策略\n\n * getbucketpolicy\n\n * setbucketpolicy\n   \n   \n   # 完整示例\n\n# 完整示例: bucket operations\n\n * listbuckets.java\n\n * listobjects.java\n\n * bucketexists.java\n\n * makebucket.java\n\n * removebucket.java\n\n * listincompleteuploads.java\n   \n   # 完整示例: object operations\n\n * putobject.java\n\n * putobjectencrypted.java\n\n * getobject.java\n\n * getobjectencrypted.java\n\n * getpartialobject.java\n\n * removeobject.java\n\n * removeobjects.java\n\n * statobject.java\n   \n   # 完整示例: presigned operations\n\n * presignedgetobject.java\n\n * presignedputobject.java\n\n * presignedpostpolicy.java\n   \n   # 完整示例: bucket policy operations\n\n * setbucketpolicy.java\n\n * getbucketpolicy.java\n   \n   \n   # 了解更多\n\n * minio官方文档\n\n * minio java client sdk api文档\n\n * [创建属于你的照片api服务-完整示例](',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Java Client API参考文档",frontmatter:{title:"Java Client API参考文档",date:"2022-02-09T11:29:54.000Z",permalink:"/pages/b8b311/"},regularPath:"/12.MinIO/03.Minio%20SDKs/02.Java%20Client%20API%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3.html",relativePath:"12.MinIO/03.Minio SDKs/02.Java Client API参考文档.md",key:"v-3785d681",path:"/pages/b8b311/",headers:[{level:2,title:"初始化Minio Client object。",slug:"初始化minio-client-object。",normalizedTitle:"初始化minio client object。",charIndex:26},{level:2,title:"Minio",slug:"minio",normalizedTitle:"minio",charIndex:29},{level:2,title:"AWS S3",slug:"aws-s3",normalizedTitle:"aws s3",charIndex:207},{level:2,title:"1. 构造函数",slug:"_1-构造函数",normalizedTitle:"1. 构造函数",charIndex:872},{level:3,title:"Minio",slug:"minio-2",normalizedTitle:"minio",charIndex:29},{level:3,title:"AWS S3",slug:"aws-s3-2",normalizedTitle:"aws s3",charIndex:207},{level:2,title:"2. 存储桶操作",slug:"_2-存储桶操作",normalizedTitle:"2. 存储桶操作",charIndex:6758},{level:3,title:"makeBucket(String bucketName)",slug:"makebucket-string-bucketname",normalizedTitle:"makebucket(string bucketname)",charIndex:6771},{level:3,title:"listBuckets()",slug:"listbuckets",normalizedTitle:"listbuckets()",charIndex:7573},{level:3,title:"bucketExists(String bucketName)",slug:"bucketexists-string-bucketname",normalizedTitle:"bucketexists(string bucketname)",charIndex:8340},{level:3,title:"removeBucket(String bucketName)",slug:"removebucket-string-bucketname",normalizedTitle:"removebucket(string bucketname)",charIndex:9311},{level:3,title:"listObjects(String bucketName, String prefix, boolean recursive, boolean useVersion1)",slug:"listobjects-string-bucketname-string-prefix-boolean-recursive-boolean-useversion1",normalizedTitle:"listobjects(string bucketname, string prefix, boolean recursive, boolean useversion1)",charIndex:10211},{level:3,title:"listIncompleteUploads(String bucketName, String prefix, boolean recursive)",slug:"listincompleteuploads-string-bucketname-string-prefix-boolean-recursive",normalizedTitle:"listincompleteuploads(string bucketname, string prefix, boolean recursive)",charIndex:11289},{level:3,title:"getBucketPolicy(String bucketName, String objectPrefix)",slug:"getbucketpolicy-string-bucketname-string-objectprefix",normalizedTitle:"getbucketpolicy(string bucketname, string objectprefix)",charIndex:12313},{level:3,title:"setBucketPolicy(String bucketName, String objectPrefix, PolicyType policy)",slug:"setbucketpolicy-string-bucketname-string-objectprefix-policytype-policy",normalizedTitle:"setbucketpolicy(string bucketname, string objectprefix, policytype policy)",charIndex:13861},{level:2,title:"3. Object operations",slug:"_3-object-operations",normalizedTitle:"3. object operations",charIndex:14969},{level:3,title:"getObject(String bucketName, String objectName)",slug:"getobject-string-bucketname-string-objectname",normalizedTitle:"getobject(string bucketname, string objectname)",charIndex:14994},{level:3,title:"getObject(String bucketName, String objectName, long offset, Long length)",slug:"getobject-string-bucketname-string-objectname-long-offset-long-length",normalizedTitle:"getobject(string bucketname, string objectname, long offset, long length)",charIndex:16450},{level:3,title:"getObject(String bucketName, String objectName, String fileName)",slug:"getobject-string-bucketname-string-objectname-string-filename",normalizedTitle:"getobject(string bucketname, string objectname, string filename)",charIndex:18082},{level:3,title:"getObject(String bucketName, String objectName, SecretKey key)",slug:"getobject-string-bucketname-string-objectname-secretkey-key",normalizedTitle:"getobject(string bucketname, string objectname, secretkey key)",charIndex:18968},{level:3,title:"getObject(String bucketName, String objectName, KeyPair key)",slug:"getobject-string-bucketname-string-objectname-keypair-key",normalizedTitle:"getobject(string bucketname, string objectname, keypair key)",charIndex:20781},{level:3,title:"putObject(String bucketName, String objectName, InputStream stream, long size, String contentType)",slug:"putobject-string-bucketname-string-objectname-inputstream-stream-long-size-string-contenttype",normalizedTitle:"putobject(string bucketname, string objectname, inputstream stream, long size, string contenttype)",charIndex:22593},{level:3,title:"putObject(String bucketName, String objectName, String fileName)",slug:"putobject-string-bucketname-string-objectname-string-filename",normalizedTitle:"putobject(string bucketname, string objectname, string filename)",charIndex:25141},{level:3,title:"putObject(String bucketName, String objectName, InputStream stream, long size, String contentType, SecretKey key)",slug:"putobject-string-bucketname-string-objectname-inputstream-stream-long-size-string-contenttype-secretkey-key",normalizedTitle:"putobject(string bucketname, string objectname, inputstream stream, long size, string contenttype, secretkey key)",charIndex:25938},{level:3,title:"putObject(String bucketName, String objectName, InputStream stream, long size, String contentType, KeyPair key)",slug:"putobject-string-bucketname-string-objectname-inputstream-stream-long-size-string-contenttype-keypair-key",normalizedTitle:"putobject(string bucketname, string objectname, inputstream stream, long size, string contenttype, keypair key)",charIndex:29072},{level:3,title:"statObject(String bucketName, String objectName)",slug:"statobject-string-bucketname-string-objectname",normalizedTitle:"statobject(string bucketname, string objectname)",charIndex:32219},{level:3,title:"copyObject(String bucketName, String objectName, String destBucketName, String destObjectName, CopyConditions cpConds, Map metadata)",slug:"copyobject-string-bucketname-string-objectname-string-destbucketname-string-destobjectname-copyconditions-cpconds-map-metadata",normalizedTitle:"copyobject(string bucketname, string objectname, string destbucketname, string destobjectname, copyconditions cpconds, map metadata)",charIndex:33213},{level:3,title:"removeObject(String bucketName, String objectName)",slug:"removeobject-string-bucketname-string-objectname",normalizedTitle:"removeobject(string bucketname, string objectname)",charIndex:34548},{level:3,title:"removeObject(String bucketName, Iterable objectNames)",slug:"removeobject-string-bucketname-iterable-objectnames",normalizedTitle:"removeobject(string bucketname, iterable objectnames)",charIndex:35295},{level:3,title:"removeIncompleteUpload(String bucketName, String objectName)",slug:"removeincompleteupload-string-bucketname-string-objectname",normalizedTitle:"removeincompleteupload(string bucketname, string objectname)",charIndex:36252},{level:2,title:"4. Presigned操作",slug:"_4-presigned操作",normalizedTitle:"4. presigned操作",charIndex:36990},{level:3,title:"presignedGetObject(String bucketName, String objectName, Integer expires)",slug:"presignedgetobject-string-bucketname-string-objectname-integer-expires",normalizedTitle:"presignedgetobject(string bucketname, string objectname, integer expires)",charIndex:37009},{level:3,title:"presignedPutObject(String bucketName, String objectName, Integer expires)",slug:"presignedputobject-string-bucketname-string-objectname-integer-expires",normalizedTitle:"presignedputobject(string bucketname, string objectname, integer expires)",charIndex:38236},{level:3,title:"presignedPostPolicy(PostPolicy policy)",slug:"presignedpostpolicy-postpolicy-policy",normalizedTitle:"presignedpostpolicy(postpolicy policy)",charIndex:39463},{level:2,title:"5. 了解更多",slug:"_5-了解更多",normalizedTitle:"5. 了解更多",charIndex:40711}],headersStr:"初始化Minio Client object。 Minio AWS S3 1. 构造函数 Minio AWS S3 2. 存储桶操作 makeBucket(String bucketName) listBuckets() bucketExists(String bucketName) removeBucket(String bucketName) listObjects(String bucketName, String prefix, boolean recursive, boolean useVersion1) listIncompleteUploads(String bucketName, String prefix, boolean recursive) getBucketPolicy(String bucketName, String objectPrefix) setBucketPolicy(String bucketName, String objectPrefix, PolicyType policy) 3. Object operations getObject(String bucketName, String objectName) getObject(String bucketName, String objectName, long offset, Long length) getObject(String bucketName, String objectName, String fileName) getObject(String bucketName, String objectName, SecretKey key) getObject(String bucketName, String objectName, KeyPair key) putObject(String bucketName, String objectName, InputStream stream, long size, String contentType) putObject(String bucketName, String objectName, String fileName) putObject(String bucketName, String objectName, InputStream stream, long size, String contentType, SecretKey key) putObject(String bucketName, String objectName, InputStream stream, long size, String contentType, KeyPair key) statObject(String bucketName, String objectName) copyObject(String bucketName, String objectName, String destBucketName, String destObjectName, CopyConditions cpConds, Map metadata) removeObject(String bucketName, String objectName) removeObject(String bucketName, Iterable objectNames) removeIncompleteUpload(String bucketName, String objectName) 4. Presigned操作 presignedGetObject(String bucketName, String objectName, Integer expires) presignedPutObject(String bucketName, String objectName, Integer expires) presignedPostPolicy(PostPolicy policy) 5. 了解更多",content:'# Java Client API参考文档\n\n\n# 初始化Minio Client object。\n\n\n# Minio\n\nMinioClient minioClient = new MinioClient("https://play.minio.io:9000", "Q3AM3UQ867SPQQA43P2F", "zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG");\n\n\n\n# AWS S3\n\nMinioClient s3Client = new MinioClient("https://s3.amazonaws.com", "YOUR-ACCESSKEYID", "YOUR-SECRETACCESSKEY");\n\n\n存储桶操作                   文件对象操作                   PRESIGNED操作           存储桶策略\nmakeBucket              getObject                presignedGetObject    getBucketPolicy\nlistBuckets             putObject                presignedPutObject    setBucketPolicy\nbucketExists            copyObject               presignedPostPolicy   \nremoveBucket            statObject                                     \nlistObjects             removeObject                                   \nlistIncompleteUploads   removeIncompleteUpload                         \n\n\n# 1. 构造函数\n\n\npublic MinioClient(String endpoint) throws\nNullPointerException, InvalidEndpointException,\nInvalidPortException\n使用给定的endpoint以及匿名方式创建一个Minio client对象。\n查看 Javadoc\n\n\npublic MinioClient(URL url) throws NullPointerException,\nInvalidEndpointException, InvalidPortException\n使用给定的url以及匿名方式创建一个Minio client对象。\n查看 Javadoc\n\n\npublic MinioClient(com.squareup.okhttp.HttpUrl url) throws\nNullPointerException, InvalidEndpointException,\nInvalidPortException\n使用给定的HttpUrl以及匿名方式创建一个Minio client对象。\n查看 Javadoc\n\n\npublic MinioClient(String endpoint, String accessKey, String\nsecretKey) throws NullPointerException,\nInvalidEndpointException, InvalidPortException\n使用给定的endpoint、access key和secret key创建一个Minio client对象。\n查看 Javadoc\n\n\npublic MinioClient(String endpoint, int port, String\naccessKey, String secretKey) throws NullPointerException,\nInvalidEndpointException, InvalidPortException\n使用给定的endpoint、port、access key和secret key创建一个Minio client对象。\n查看 Javadoc\n\n\npublic MinioClient(String endpoint, String accessKey, String\nsecretKey, boolean secure) throws NullPointerException,\nInvalidEndpointException, InvalidPortException\n使用给定的endpoint、access key、secret\nkey和一个secure选项（是否使用https）创建一个Minio client对象。\n查看 Javadoc\n\n\npublic MinioClient(String endpoint, int port, String\naccessKey, String secretKey, boolean secure) throws\nNullPointerException, InvalidEndpointException,\nInvalidPortException\n使用给定的endpoint、port、access key、secret\nkey和一个secure选项（是否使用https）创建一个Minio client对象。\n查看 Javadoc\n\n\npublic MinioClient(com.squareup.okhttp.HttpUrl url, String\naccessKey, String secretKey) throws NullPointerException,\nInvalidEndpointException, InvalidPortException\n使用给定的HttpUrl对象、access key、secret key创建一个Minio client对象。\n查看 Javadoc\n\n\npublic MinioClient(URL url, String accessKey, String\nsecretKey) throws NullPointerException,\nInvalidEndpointException, InvalidPortException\n使用给定的URL对象、access key、secret key创建一个Minio client对象。\n查看 Javadoc\n\n参数\n\n参数          类型        描述\nendpoint    string    endPoint是一个URL，域名，IPv4或者IPv6地址。以下是合法的endpoints:\n                      https://s3.amazonaws.com\n                      https://play.minio.io:9000\n                      localhost\n                      play.minio.io\nport        int       TCP/IP端口号。可选，默认值是，如果是http,则默认80端口，如果是https,则默认是443端口。\naccessKey   string    accessKey类似于用户ID，用于唯一标识你的账户。\nsecretKey   string    secretKey是你账户的密码。\nsecure      boolean   如果是true，则用的是https而不是http,默认值是true。\nurl         URL       Endpoint URL对象。\nurl         HttpURL   Endpoint HttpUrl对象。\n\n示例\n\n\n# Minio\n\n// 1. public MinioClient(String endpoint)\nMinioClient minioClient = new MinioClient("https://play.minio.io:9000");\n// 2. public MinioClient(URL url)\nMinioClient minioClient = new MinioClient(new URL("https://play.minio.io:9000"));\n// 3. public MinioClient(com.squareup.okhttp.HttpUrl url)\n MinioClient minioClient = new MinioClient(new HttpUrl.parse("https://play.minio.io:9000"));\n// 4. public MinioClient(String endpoint, String accessKey, String secretKey)\nMinioClient minioClient = new MinioClient("https://play.minio.io:9000", "Q3AM3UQ867SPQQA43P2F", "zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG");\n// 5. public MinioClient(String endpoint, int port,  String accessKey, String secretKey)\nMinioClient minioClient = new MinioClient("https://play.minio.io", 9000, "Q3AM3UQ867SPQQA43P2F", "zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG");\n// 6. public MinioClient(String endpoint, String accessKey, String secretKey, boolean insecure)\nMinioClient minioClient = new MinioClient("https://play.minio.io:9000", "Q3AM3UQ867SPQQA43P2F", "zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG", true);\n// 7. public MinioClient(String endpoint, int port,  String accessKey, String secretKey, boolean insecure)\nMinioClient minioClient = new MinioClient("https://play.minio.io", 9000, "Q3AM3UQ867SPQQA43P2F", "zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG", true);\n// 8. public MinioClient(com.squareup.okhttp.HttpUrl url, String accessKey, String secretKey)\n MinioClient minioClient = new MinioClient(new URL("https://play.minio.io:9000"), "Q3AM3UQ867SPQQA43P2F", "zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG");\n// 9. public MinioClient(URL url, String accessKey, String secretKey)\nMinioClient minioClient = new MinioClient(HttpUrl.parse("https://play.minio.io:9000"), "Q3AM3UQ867SPQQA43P2F", "zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG");\n\n\n\n# AWS S3\n\n// 1. public MinioClient(String endpoint)\nMinioClient s3Client = new MinioClient("https://s3.amazonaws.com");\n// 2. public MinioClient(URL url)\nMinioClient minioClient = new MinioClient(new URL("https://s3.amazonaws.com"));\n// 3. public MinioClient(com.squareup.okhttp.HttpUrl url)\n MinioClient s3Client = new MinioClient(new HttpUrl.parse("https://s3.amazonaws.com"));\n// 4. public MinioClient(String endpoint, String accessKey, String secretKey)\nMinioClient s3Client = new MinioClient("s3.amazonaws.com", "YOUR-ACCESSKEYID", "YOUR-SECRETACCESSKEY");\n// 5. public MinioClient(String endpoint, int port,  String accessKey, String secretKey)\nMinioClient s3Client = new MinioClient("s3.amazonaws.com", 80, "YOUR-ACCESSKEYID", "YOUR-SECRETACCESSKEY");\n// 6. public MinioClient(String endpoint, String accessKey, String secretKey, boolean insecure)\nMinioClient s3Client = new MinioClient("s3.amazonaws.com", "YOUR-ACCESSKEYID", "YOUR-SECRETACCESSKEY", false);\n// 7. public MinioClient(String endpoint, int port,  String accessKey, String secretKey, boolean insecure)\nMinioClient s3Client = new MinioClient("s3.amazonaws.com", 80, "YOUR-ACCESSKEYID", "YOUR-SECRETACCESSKEY",false);\n// 8. public MinioClient(com.squareup.okhttp.HttpUrl url, String accessKey, String secretKey)\n MinioClient s3Client = new MinioClient(new URL("s3.amazonaws.com"), "YOUR-ACCESSKEYID", "YOUR-SECRETACCESSKEY");\n// 9. public MinioClient(URL url, String accessKey, String secretKey)\nMinioClient s3Client = new MinioClient(HttpUrl.parse("s3.amazonaws.com"), "YOUR-ACCESSKEYID", "YOUR-SECRETACCESSKEY");\n\n\n\n# 2. 存储桶操作\n\n\n# makeBucket(String bucketName)\n\npublic void makeBucket(String bucketName)\n\n\n创建一个新的存储桶\n\n查看Javadoc\n\n参数\n\n参数           类型       描述\nbucketName   String   存储桶名称\n\n返回值类型   异常\nNone    异常列表:\n        InvalidBucketNameException : 非法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常\n        ErrorResponseException : 执行失败\n        InternalException : 内部异常\n\n示例\n\ntry {\n  // 如存储桶不存在，创建之。\n  boolean found = minioClient.bucketExists("mybucket");\n  if (found) {\n    System.out.println("mybucket already exists");\n  } else {\n    // 创建名为\'my-bucketname\'的存储桶。\n    minioClient.makeBucket("mybucket");\n    System.out.println("mybucket is created successfully");\n  }\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# listBuckets()\n\npublic List<Bucket> listBuckets()\n\n\n列出所有存储桶。\n\n查看Javadoc\n\n返回值类型                                异常\nList Bucket : List of bucket type.   异常列表：\n                                     NoResponseException : 服务端无响应\n                                     IOException : 连接异常\n                                     org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常\n                                     ErrorResponseException :执行失败异溃\n                                     InternalException : 内部错误\n\n示例\n\ntry {\n  // 列出所有存储桶\n  List<Bucket> bucketList = minioClient.listBuckets();\n  for (Bucket bucket : bucketList) {\n    System.out.println(bucket.creationDate() + ", " + bucket.name());\n  }\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# bucketExists(String bucketName)\n\npublic boolean bucketExists(String bucketName)\n\n\n检查存储桶是否存在。\n\n查看 Javadoc\n\n参数\n\n参数           类型       描述\nbucketName   String   存储桶名称\n\n返回值值类型                               异常\nboolean: true if the bucket exists   异常列表：\n                                     InvalidBucketNameException : 不合法的存储桶名称。\n                                     NoResponseException : 服务器无响应。\n                                     IOException : 连接异常。\n                                     org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n                                     ErrorResponseException : 执行失败异常。\n                                     InternalException : 内部错误。\n\n示例\n\ntry {\n  // 检查\'my-bucketname\'是否存在。\n  boolean found = minioClient.bucketExists("mybucket");\n  if (found) {\n    System.out.println("mybucket exists");\n  } else {\n    System.out.println("mybucket does not exist");\n  }\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# removeBucket(String bucketName)\n\npublic void removeBucket(String bucketName)\n\n\n删除一个存储桶。\n\n查看 Javadoc\n\n注意: - removeBucket不会删除存储桶里的对象，你需要通过removeObject API来删除它们。\n\n参数\n\n参数           类型       描述\nbucketName   String   存储桶名称。\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n\n示例\n\ntry {\n  // 删除之前先检查`my-bucket`是否存在。\n  boolean found = minioClient.bucketExists("mybucket");\n  if (found) {\n    // 删除`my-bucketname`存储桶，注意，只有存储桶为空时才能删除成功。\n    minioClient.removeBucket("mybucket");\n    System.out.println("mybucket is removed successfully");\n  } else {\n    System.out.println("mybucket does not exist");\n  }\n} catch(MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# listObjects(String bucketName, String prefix, boolean recursive, boolean useVersion1)\n\npublic Iterable<Result<Item>> listObjects(String bucketName, String prefix, boolean recursive, boolean useVersion1)\n\n\n列出某个存储桶中的所有对象。\n\n查看Javadoc\n\n参数\n\n参数            类型        描述\nbucketName    String    存储桶名称。\nprefix        String    对象名称的前缀\nrecursive     boolean   是否递归查找，如果是false,就模拟文件夹结构查找。\nuseVersion1   boolean   如果是true, 使用版本1 REST API\n\n返回值类型                                                 异常\nIterable<Result<Item>>:an iterator of Result Items.   None\n\n示例\n\ntry {\n  // 检查\'mybucket\'是否存在。\n  boolean found = minioClient.bucketExists("mybucket");\n  if (found) {\n    // 列出\'my-bucketname\'里的对象\n    Iterable<Result<Item>> myObjects = minioClient.listObjects("mybucket");\n    for (Result<Item> result : myObjects) {\n      Item item = result.get();\n      System.out.println(item.lastModified() + ", " + item.size() + ", " + item.objectName());\n    }\n  } else {\n    System.out.println("mybucket does not exist");\n  }\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# listIncompleteUploads(String bucketName, String prefix, boolean recursive)\n\npublic Iterable<Result<Upload>> listIncompleteUploads(String bucketName, String prefix, boolean recursive)\n\n\n列出存储桶中被部分上传的对象。\n\n查看 Javadoc\n\n参数\n\n参数           类型        描述\nbucketName   String    存储桶名称。\nprefix       String    对象名称的前缀，列出有该前缀的对象\nrecursive    boolean   是否递归查找，如果是false,就模拟文件夹结构查找。\n\n返回值类型                                              异常\nIterable<Result<Upload>>: an iterator of Upload.   None\n\n示例\n\ntry {\n  // 检查\'mybucket\'是否存在。\n  boolean found = minioClient.bucketExists("mybucket");\n  if (found) {\n    // 列出\'mybucket\'中所有未完成的multipart上传的的对象。 \n    Iterable<Result<Upload>> myObjects = minioClient.listIncompleteUploads("mybucket");\n    for (Result<Upload> result : myObjects) {\n      Upload upload = result.get();\n      System.out.println(upload.uploadId() + ", " + upload.objectName());\n    }\n  } else {\n    System.out.println("mybucket does not exist");\n  }\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# getBucketPolicy(String bucketName, String objectPrefix)\n\npublic PolicyType getBucketPolicy(String bucketName, String objectPrefix)\n\n\n获得指定对象前缀的存储桶策略。\n\n查看 Javadoc\n\n参数\n\n参数             类型       描述\nbucketName     String   存储桶名称。\nobjectPrefix   String   策略适用的对象的前缀\n\n返回值类型                                                    异常\nPolicyType: The current bucket policy type for a given   异常列表：\nbucket and objectPrefix.\n                                                         InvalidBucketNameException : 不合法的存储桶名称。\n                                                         NoResponseException : 服务器无响应。\n                                                         IOException : 连接异常。\n                                                         org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n                                                         ErrorResponseException : 执行失败异常。\n                                                         InternalException : 内部错误。\n                                                         InvalidBucketNameException : 不合法的存储桶名称。\n                                                         InvalidObjectPrefixException : 不合法的对象前缀\n                                                         NoSuchAlgorithmException : 找不到相应的签名算法。\n                                                         InsufficientDataException : 在读到相应length之前就得到一个EOFException。\n\n示例\n\ntry {\n  System.out.println("Current policy: " + minioClient.getBucketPolicy("myBucket", "downloads"));\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# setBucketPolicy(String bucketName, String objectPrefix, PolicyType policy)\n\npublic void setBucketPolicy(String bucketName, String objectPrefix, PolicyType policy)\n\n\n给一个存储桶+对象前缀设置策略。\n\n查看 Javadoc\n\n参数\n\n参数             类型           描述\nbucketName     String       存储桶名称。\nobjectPrefix   String       对象前缀。\npolicy         PolicyType   要赋予的策略，可选值有[PolicyType.NONE, PolicyType.READ_ONLY,\n                            PolicyType.READ_WRITE, PolicyType.WRITE_ONLY].\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n        InvalidBucketNameException : 不合法的存储桶名称。\n        InvalidObjectPrefixException : 不合法的对象前缀\n        NoSuchAlgorithmException : 找不到相应的签名算法。\n        InsufficientDataException : 在读到相应length之前就得到一个EOFException。\n\n示例\n\ntry {\n  minioClient.setBucketPolicy("myBucket", "uploads", PolicyType.READ_ONLY);\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# 3. Object operations\n\n\n# getObject(String bucketName, String objectName)\n\npublic InputStream getObject(String bucketName, String objectName, long offset)\n\n\n以流的形式下载一个对象。\n\n查看 Javadoc\n\n参数\n\n参数           类型       描述\nbucketName   String   存储桶名称。\nobjectName   String   存储桶里的对象名称。\n\n返回值类型                                                  异常\nInputStream: InputStream containing the object data.   异常列表：\n                                                       InvalidBucketNameException : 不合法的存储桶名称。\n                                                       NoResponseException : 服务器无响应。\n                                                       IOException : 连接异常。\n                                                       org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n                                                       ErrorResponseException : 执行失败异常。\n                                                       InternalException : 内部错误。\n\n示例\n\ntry {\n  // 调用statObject()来判断对象是否存在。\n  // 如果不存在, statObject()抛出异常,\n  // 否则则代表对象存在。\n  minioClient.statObject("mybucket", "myobject");\n  // 获取"myobject"的输入流。\n  InputStream stream = minioClient.getObject("mybucket", "myobject");\n  // 读取输入流直到EOF并打印到控制台。\n  byte[] buf = new byte[16384];\n  int bytesRead;\n  while ((bytesRead = stream.read(buf, 0, buf.length)) >= 0) {\n    System.out.println(new String(buf, 0, bytesRead));\n  }\n  // 关闭流，此处为示例，流关闭最好放在finally块。\n  stream.close();\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# getObject(String bucketName, String objectName, long offset, Long length)\n\npublic InputStream getObject(String bucketName, String objectName, long offset, Long length)\n\n\n下载对象指定区域的字节数组做为流。（断点下载）\n\n查看 Javadoc\n\n参数\n\n参数           类型       描述\nbucketName   String   存储桶名称。\nobjectName   String   存储桶里的对象名称。\noffset       Long     offset 是起始字节的位置\nlength       Long     length是要读取的长度 (可选，如果无值则代表读到文件结尾)。\n\n返回值类型                                                     异常\nInputStream : InputStream containing the object\'s data.   异常列表：\n                                                          InvalidBucketNameException : 不合法的存储桶名称。\n                                                          NoResponseException : 服务器无响应。\n                                                          IOException : 连接异常。\n                                                          org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n                                                          ErrorResponseException : 执行失败异常。\n                                                          InternalException : 内部错误。\n\n示例\n\ntry {\n  // 调用statObject()来判断对象是否存在。\n  // 如果不存在, statObject()抛出异常,\n  // 否则则代表对象存在。\n  minioClient.statObject("mybucket", "myobject");\n  // 获取指定offset和length的"myobject"的输入流。\n  InputStream stream = minioClient.getObject("mybucket", "myobject", 1024L, 4096L);\n  // 读取输入流直到EOF并打印到控制台。\n  byte[] buf = new byte[16384];\n  int bytesRead;\n  while ((bytesRead = stream.read(buf, 0, buf.length)) >= 0) {\n    System.out.println(new String(buf, 0, bytesRead));\n  }\n  // 关闭流。\n  stream.close();\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# getObject(String bucketName, String objectName, String fileName)\n\npublic void getObject(String bucketName, String objectName, String fileName)\n\n\n下载并将文件保存到本地。\n\n查看 Javadoc\n\n参数\n\n参数           类型       描述\nbucketName   String   存储桶名称。\nobjectName   String   存储桶里的对象名称。\nfileName     String   File name.\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n\n示例\n\ntry {\n  // 调用statObject()来判断对象是否存在。\n  // 如果不存在, statObject()抛出异常,\n  // 否则则代表对象存在。\n  minioClient.statObject("mybucket", "myobject");\n  // 获取myobject的流并保存到photo.jpg文件中。\n  minioClient.getObject("mybucket", "myobject", "photo.jpg");\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# getObject(String bucketName, String objectName, SecretKey key)\n\npublic CipherInputStream getObject(String bucketName, String objectName, SecretKey key)\n\n\n在给定的存储桶中获取整个加密对象的数据作为InputStream，然后用传入的master key解密和加密对象关联的content key。然后创建一个含有InputStream和Cipher的CipherInputStream。这个Cipher被初始为用于使用content key进行解密，所以CipherInputStream会在返回数据前，尝试读取数据并进行解密。所以read()方法返回的是处理过的原始对象数据。\n\nCipherInputStream必须用完关闭，否则连接不会被释放。\n\n查看 Javadoc\n\n参数\n\n参数           类型          描述\nbucketName   String      存储桶名称。\nobjectName   String      存储桶里的对象名称。\nkey          SecretKey   SecretKey类型的数据。\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n        InvalidEncryptionMetadataException : 加密秘钥错误。\n        BadPaddingException : 错误的padding\n        IllegalBlockSizeException : 不正确的block size\n        NoSuchPaddingException : 错误的pading类型\n        InvalidAlgorithmParameterException : 该算法不存在\n\n示例\n\ntry {\n  // 调用statObject()来判断对象是否存在。\n  // 如果不存在, statObject()抛出异常,\n  // 否则则代表对象存在。\n  minioClient.statObject("mybucket", "myobject");\n  //生成256位AES key。\n  KeyGenerator symKeyGenerator = KeyGenerator.getInstance("AES");\n  symKeyGenerator.init(256);\n  SecretKey symKey = symKeyGenerator.generateKey();\n  // 获取对象数据并保存到photo.jpg\n  InputStream stream = minioClient.getObject("testbucket", "my-objectname", symKey);\n  // 读流到EOF，并输出到控制台。\n  byte[] buf = new byte[16384];\n  int bytesRead;\n  while ((bytesRead = stream.read(buf, 0, buf.length)) >= 0) {\n    System.out.println(new String(buf, 0, bytesRead, StandardCharsets.UTF_8));\n  }\n  // 关闭流。\n  stream.close();\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# getObject(String bucketName, String objectName, KeyPair key)\n\npublic InputStream getObject(String bucketName, String objectName, KeyPair key)\n\n\n在给定的存储桶中获取整个加密对象的数据作为InputStream，然后用传入的master keyPair解密和加密对象关联的content key。然后创建一个含有InputStream和Cipher的CipherInputStream。这个Cipher被初始为用于使用content key进行解密，所以CipherInputStream会在返回数据前，尝试读取数据并进行解密。所以read()方法返回的是处理过的原始对象数据。\n\nCipherInputStream必须用完关闭，否则连接不会被释放。\n\n查看 Javadoc\n\n参数\n\n参数           类型        描述\nbucketName   String    存储桶名称。\nobjectName   String    存储桶里的对象名称。\nkey          KeyPair   RSA KeyPair类型的对象。\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n        InvalidEncryptionMetadataException : 加密秘钥错误。\n        BadPaddingException : 错误的padding\n        IllegalBlockSizeException : 不正确的block size\n        NoSuchPaddingException : 错误的pading类型\n        InvalidAlgorithmParameterException : 该算法不存在\n\n示例\n\ntry {\n  // 调用statObject()来判断对象是否存在。\n  // 如果不存在, statObject()抛出异常,\n  // 否则则代表对象存在。\n  minioClient.statObject("mybucket", "myobject");\n  KeyPairGenerator keyGenerator = KeyPairGenerator.getInstance("RSA");\n  keyGenerator.initialize(1024, new SecureRandom());\n  KeyPair keypair = keyGenerator.generateKeyPair();\n  // 获取对象数据并保存到photo.jpg\n  InputStream stream = minioClient.getObject("testbucket", "my-objectname", keypair);\n  // 读流到EOF，并输出到控制台。\n  byte[] buf = new byte[16384];\n  int bytesRead;\n  while ((bytesRead = stream.read(buf, 0, buf.length)) >= 0) {\n    System.out.println(new String(buf, 0, bytesRead, StandardCharsets.UTF_8));\n  }\n  // 关闭流。\n  stream.close();\n} catch (MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# putObject(String bucketName, String objectName, InputStream stream, long size, String contentType)\n\npublic void putObject(String bucketName, String objectName, InputStream stream, long size, String contentType)\n\n\n通过InputStream上传对象。\n\n查看 Javadoc\n\n参数\n\n参数            类型            描述\nbucketName    String        存储桶名称。\nobjectName    String        存储桶里的对象名称。\nstream        InputStream   要上传的流。\nsize          long          要上传的stream的size\ncontentType   String        Content type。\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n\n示例\n\n单个对象的最大大小限制在5TB。putObject在对象大于5MiB时，自动使用multiple parts方式上传。这样，当上传失败时，客户端只需要上传未成功的部分即可（类似断点上传）。上传的对象使用MD5SUM签名进行完整性验证。\n\ntry {\n  StringBuilder builder = new StringBuilder();\n  for (int i = 0; i < 1000; i++) {\n    builder.append("Sphinx of black quartz, judge my vow: Used by Adobe InDesign to display font samples. ");\n    builder.append("(29 letters)\\n");\n    builder.append("Jackdaws love my big sphinx of quartz: Similarly, used by Windows XP for some fonts. ");\n    builder.append("(31 letters)\\n");\n    builder.append("Pack my box with five dozen liquor jugs: According to Wikipedia, this one is used on ");\n    builder.append("NASAs Space Shuttle. (32 letters)\\n");\n    builder.append("The quick onyx goblin jumps over the lazy dwarf: Flavor text from an Unhinged Magic Card. ");\n    builder.append("(39 letters)\\n");\n    builder.append("How razorback-jumping frogs can level six piqued gymnasts!: Not going to win any brevity ");\n    builder.append("awards at 49 letters long, but old-time Mac users may recognize it.\\n");\n    builder.append("Cozy lummox gives smart squid who asks for job pen: A 41-letter tester sentence for Mac ");\n    builder.append("computers after System 7.\\n");\n    builder.append("A few others we like: Amazingly few discotheques provide jukeboxes; Now fax quiz Jack! my ");\n    builder.append("brave ghost pled; Watch Jeopardy!, Alex Trebeks fun TV quiz game.\\n");\n    builder.append("- --\\n");\n  }\n  ByteArrayInputStream bais = new\n  ByteArrayInputStream(builder.toString().getBytes("UTF-8"));\n  // 创建对象\n  minioClient.putObject("mybucket", "myobject", bais, bais.available(), "application/octet-stream");\n  bais.close();\n  System.out.println("myobject is uploaded successfully");\n} catch(MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# putObject(String bucketName, String objectName, String fileName)\n\npublic void putObject(String bucketName, String objectName, String fileName)\n\n\n通过文件上传到对象中。查看 Javadoc\n\n参数\n\n参数           类型       描述\nbucketName   String   存储桶名称。\nobjectName   String   存储桶里的对象名称。\nfileName     String   File name.\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n\n示例\n\ntry {\n  minioClient.putObject("mybucket",  "island.jpg", "/mnt/photos/island.jpg")\n  System.out.println("island.jpg is uploaded successfully");\n} catch(MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# putObject(String bucketName, String objectName, InputStream stream, long size, String contentType, SecretKey key)\n\npublic void putObject(String bucketName, String objectName, InputStream stream, long size, String contentType,SecretKey key)\n\n\n拿到流的数据，使用随机生成的content key进行加密，并上传到指定存储桶中。同时将加密后的content key和iv做为加密对象有header也上传到存储桶中。content key使用传入到该方法的master key进行加密。\n\n如果对象大于5MB,客户端会自动进行multi part上传。\n\n查看 Javadoc\n\n参数\n\n参数            类型            描述\nbucketName    String        存储桶名称。\nobjectName    String        存储桶里的对象名称。\nstream        InputStream   要上传的流。\nsize          long          要上传的流的大小。\ncontentType   String        Content type。\nkey           SecretKey     用AES初使化的对象SecretKey。\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n        InvalidAlgorithmParameterException : 错误的加密算法。\n        BadPaddingException : 不正确的padding.\n        IllegalBlockSizeException : 不正确的block。\n        NoSuchPaddingException : 错误的padding类型。\n\n示例\n\n对象使用随机生成的key进行加密，然后这个用于加密数据的key又被由仅被client知道的master key(封装在encryptionMaterials对象里)进行加密。这个被加密后的key和IV做为对象的header和加密后的对象一起被上传到存储服务上。\n\ntry {\n  StringBuilder builder = new StringBuilder();\n  for (int i = 0; i < 1000; i++) {\n    builder.append("Sphinx of black quartz, judge my vow: Used by Adobe InDesign to display font samples. ");\n    builder.append("(29 letters)\\n");\n    builder.append("Jackdaws love my big sphinx of quartz: Similarly, used by Windows XP for some fonts. ");\n    builder.append("(31 letters)\\n");\n    builder.append("Pack my box with five dozen liquor jugs: According to Wikipedia, this one is used on ");\n    builder.append("NASAs Space Shuttle. (32 letters)\\n");\n    builder.append("The quick onyx goblin jumps over the lazy dwarf: Flavor text from an Unhinged Magic Card. ");\n    builder.append("(39 letters)\\n");\n    builder.append("How razorback-jumping frogs can level six piqued gymnasts!: Not going to win any brevity ");\n    builder.append("awards at 49 letters long, but old-time Mac users may recognize it.\\n");\n    builder.append("Cozy lummox gives smart squid who asks for job pen: A 41-letter tester sentence for Mac ");\n    builder.append("computers after System 7.\\n");\n    builder.append("A few others we like: Amazingly few discotheques provide jukeboxes; Now fax quiz Jack! my ");\n    builder.append("brave ghost pled; Watch Jeopardy!, Alex Trebeks fun TV quiz game.\\n");\n    builder.append("- --\\n");\n  }\n  ByteArrayInputStream bais = new\n  ByteArrayInputStream(builder.toString().getBytes("UTF-8"));\n  //生成256位AES key.\n  KeyGenerator symKeyGenerator = KeyGenerator.getInstance("AES");\n  symKeyGenerator.init(256);\n  SecretKey symKey = symKeyGenerator.generateKey();\n  // 创建一个对象\n  minioClient.putObject("mybucket", "myobject", bais, bais.available(), "application/octet-stream", symKey);\n  bais.close();\n  System.out.println("myobject is uploaded successfully");\n} catch(MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# putObject(String bucketName, String objectName, InputStream stream, long size, String contentType, KeyPair key)\n\npublic void putObject(String bucketName, String objectName, InputStream stream, long size, String contentType,KeyPair key)\n\n\n拿到流的数据，使用随机生成的content key进行加密，并上传到指定存储桶中。同时将加密后的content key和iv做为加密对象有header也上传到存储桶中。content key使用传入到该方法的master key进行加密。\n\n如果对象大于5MB,客户端会自动进行multi part上传。\n\n查看 Javadoc\n\n参数\n\n参数            类型            描述\nbucketName    String        存储桶名称。\nobjectName    String        存储桶里的对象名称。\nstream        InputStream   要上传的流。\nsize          long          要上传的流的大小。\ncontentType   String        Content type。\nkey           KeyPair       一个RSA KeyPair的对象。\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n        InvalidAlgorithmParameterException : 错误的加密算法。\n        BadPaddingException : 不正确的padding。\n        IllegalBlockSizeException : 不正确的block。\n        NoSuchPaddingException : 错误的pading类型。\n\n示例\n\n对象使用随机生成的key进行加密，然后这个用于加密数据的key又被由仅被client知道的master key(封装在encryptionMaterials对象里)进行加密。这个被加密后的key和IV做为对象的header和加密后的对象一起被上传到存储服务上。\n\ntry {\n  StringBuilder builder = new StringBuilder();\n  for (int i = 0; i < 1000; i++) {\n    builder.append("Sphinx of black quartz, judge my vow: Used by Adobe InDesign to display font samples. ");\n    builder.append("(29 letters)\\n");\n    builder.append("Jackdaws love my big sphinx of quartz: Similarly, used by Windows XP for some fonts. ");\n    builder.append("(31 letters)\\n");\n    builder.append("Pack my box with five dozen liquor jugs: According to Wikipedia, this one is used on ");\n    builder.append("NASAs Space Shuttle. (32 letters)\\n");\n    builder.append("The quick onyx goblin jumps over the lazy dwarf: Flavor text from an Unhinged Magic Card. ");\n    builder.append("(39 letters)\\n");\n    builder.append("How razorback-jumping frogs can level six piqued gymnasts!: Not going to win any brevity ");\n    builder.append("awards at 49 letters long, but old-time Mac users may recognize it.\\n");\n    builder.append("Cozy lummox gives smart squid who asks for job pen: A 41-letter tester sentence for Mac ");\n    builder.append("computers after System 7.\\n");\n    builder.append("A few others we like: Amazingly few discotheques provide jukeboxes; Now fax quiz Jack! my ");\n    builder.append("brave ghost pled; Watch Jeopardy!, Alex Trebeks fun TV quiz game.\\n");\n    builder.append("- --\\n");\n  }\n  ByteArrayInputStream bais = new\n  ByteArrayInputStream(builder.toString().getBytes("UTF-8"));\n  KeyPairGenerator keyGenerator = KeyPairGenerator.getInstance("RSA");\n  keyGenerator.initialize(1024, new SecureRandom());\n  KeyPair keypair = keyGenerator.generateKeyPair();\n  // Create an object\n  minioClient.putObject("mybucket", "myobject", bais, bais.available(), "application/octet-stream", keypair);\n  bais.close();\n  System.out.println("myobject is uploaded successfully");\n} catch(MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# statObject(String bucketName, String objectName)\n\npublic ObjectStat statObject(String bucketName, String objectName)\n\n获取对象的元数据。\n\n查看 Javadoc\n\n参数\n\n参数           类型       描述\nbucketName   String   存储桶名称。\nobjectName   String   存储桶里的对象名称。\n\n返回值类型                                     异常\nObjectStat: Populated object meta data.   异常列表：\n                                          InvalidBucketNameException : 不合法的存储桶名称。\n                                          NoResponseException : 服务器无响应。\n                                          IOException : 连接异常。\n                                          org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n                                          ErrorResponseException : 执行失败异常。\n                                          InternalException : 内部错误。\n\n示例\n\ntry {\n  // 获得对象的元数据。\n  ObjectStat objectStat = minioClient.statObject("mybucket", "myobject");\n  System.out.println(objectStat);\n} catch(MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# copyObject(String bucketName, String objectName, String destBucketName, String destObjectName, CopyConditions cpConds, Map metadata)\n\npublic void copyObject(String bucketName, String objectName, String destBucketName, String destObjectName, CopyConditions cpConds, Map<String, String> metadata)\n\n从objectName指定的对象中将数据拷贝到destObjectName指定的对象。\n\n查看 Javadoc\n\n参数\n\n参数               类型               描述\nbucketName       String           源存储桶名称。\nobjectName       String           源存储桶中的源对象名称。\ndestBucketName   String           目标存储桶名称。\ndestObjectName   String           要创建的目标对象名称,如果为空，默认为源对象名称。\ncopyConditions   CopyConditions   拷贝操作的一些条件Map。\nmetadata         Map              给目标对象的元数据Map。\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n\n示例\n\n本API执行了一个服务端的拷贝操作。\n\ntry {\n  CopyConditions copyConditions = new CopyConditions();\n  copyConditions.setMatchETagNone("TestETag");\n  minioClient.copyObject("mybucket",  "island.jpg", "mydestbucket", "processed.png", copyConditions);\n  System.out.println("island.jpg is uploaded successfully");\n} catch(MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# removeObject(String bucketName, String objectName)\n\npublic void removeObject(String bucketName, String objectName)\n\n\n删除一个对象。\n\n查看 Javadoc\n\n参数\n\n参数           类型       描述\nbucketName   String   存储桶名称。\nobjectName   String   存储桶里的对象名称。\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n\n示例\n\ntry {\n      // 从mybucket中删除myobject。\n      minioClient.removeObject("mybucket", "myobject");\n      System.out.println("successfully removed mybucket/myobject");\n} catch (MinioException e) {\n      System.out.println("Error: " + e);\n}\n\n\n\n# removeObject(String bucketName, Iterable objectNames)\n\npublic Iterable<Result<DeleteError>> removeObject(String bucketName, Iterable<String> objectNames)\n\n\n删除多个对象。\n\n查看 Javadoc\n\n参数\n\n参数            类型         描述\nbucketName    String     存储桶名称。\nobjectNames   Iterable   含有要删除的多个object名称的迭代器对象。\n\n返回值类型                                                 异常\nIterable<Result<DeleteError>>:an iterator of Result   None\nDeleteError.\n\n示例\n\nList<String> objectNames = new LinkedList<String>();\nobjectNames.add("my-objectname1");\nobjectNames.add("my-objectname2");\nobjectNames.add("my-objectname3");\ntry {\n      // 删除my-bucketname里的多个对象\n      for (Result<DeleteError> errorResult: minioClient.removeObject("my-bucketname", objectNames)) {\n        DeleteError error = errorResult.get();\n        System.out.println("Failed to remove \'" + error.objectName() + "\'. Error:" + error.message());\n      }\n} catch (MinioException e) {\n      System.out.println("Error: " + e);\n}\n\n\n\n# removeIncompleteUpload(String bucketName, String objectName)\n\npublic void removeIncompleteUpload(String bucketName, String objectName)\n\n\n删除一个未完整上传的对象。\n\n查看 Javadoc\n\n参数\n\n参数           类型       描述\nbucketName   String   存储桶名称。\nobjectName   String   存储桶里的对象名称。\n\n返回值类型   异常\nNone    异常列表：\n        InvalidBucketNameException : 不合法的存储桶名称。\n        NoResponseException : 服务器无响应。\n        IOException : 连接异常。\n        org.xmlpull.v1.XmlPullParserException : 解析返回的XML异常。\n        ErrorResponseException : 执行失败异常。\n        InternalException : 内部错误。\n\n示例\n\ntry {\n    String url = minioClient.presignedGetObject("mybucket", "myobject", 60 * 60 * 24);\n    System.out.println(url);\n} catch(MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# 4. Presigned操作\n\n\n# presignedGetObject(String bucketName, String objectName, Integer expires)\n\npublic String presignedGetObject(String bucketName, String objectName, Integer expires)\n\n\n生成一个给HTTP GET请求用的presigned URL。浏览器/移动端的客户端可以用这个URL进行下载，即使其所在的存储桶是私有的。这个presigned URL可以设置一个失效时间，默认值是7天。\n\n查看 Javadoc\n\n参数\n\n参数           类型        描述\nbucketName   String    存储桶名称。\nobjectName   String    存储桶里的对象名称。\nexpiry       Integer   失效时间（以秒为单位），默认是7天，不得大于七天。\n\n返回值类型                                                  异常\nString : string contains URL to download the object.   异常列表：\n                                                       InvalidBucketNameException : 不合法的存储桶名称。\n                                                       InvalidKeyException : 不合法的access key或者secret key。\n                                                       IOException : 连接异常。\n                                                       NoSuchAlgorithmException : 找不到相应的签名算法。\n                                                       InvalidExpiresRangeException : presigned URL已经过期了。\n\n示例\n\ntry {\n    String url = minioClient.presignedPutObject("mybucket", "myobject", 60 * 60 * 24);\n    System.out.println(url);\n} catch(MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# presignedPutObject(String bucketName, String objectName, Integer expires)\n\npublic String presignedPutObject(String bucketName, String objectName, Integer expires)\n\n\n生成一个给HTTP PUT请求用的presigned URL。浏览器/移动端的客户端可以用这个URL进行上传，即使其所在的存储桶是私有的。这个presigned URL可以设置一个失效时间，默认值是7天。\n\n查看 Javadoc\n\n参数\n\n参数           类型        描述\nbucketName   String    存储桶名称。\nobjectName   String    存储桶里的对象名称。\nexpiry       Integer   失效时间（以秒为单位），默认是7天，不得大于七天。\n\n返回值类型                                                  异常\nString : string contains URL to download the object.   异常列表：\n                                                       InvalidBucketNameException : 不合法的存储桶名称。\n                                                       InvalidKeyException : 不合法的access key或者secret key。\n                                                       IOException : 连接异常。\n                                                       NoSuchAlgorithmException : 找不到相应的签名算法。\n                                                       InvalidExpiresRangeException : presigned URL已经过期了。\n\n示例\n\ntry {\n    String url = minioClient.presignedPutObject("mybucket", "myobject", 60 * 60 * 24);\n    System.out.println(url);\n} catch(MinioException e) {\n  System.out.println("Error occurred: " + e);\n}\n\n\n\n# presignedPostPolicy(PostPolicy policy)\n\npublic Map<String,String> presignedPostPolicy(PostPolicy policy)\n\n\n允许给POST请求的presigned URL设置策略，比如接收对象上传的存储桶名称的策略，key名称前缀，过期策略。\n\n查看 Javadoc\n\n参数\n\n参数       类型           描述\npolicy   PostPolicy   对象的post策略\n\n返回值类型                                         异常\nMap: Map of strings to construct form-data.   异常列表：\n                                              InvalidBucketNameException : 不合法的存储桶名称。\n                                              InvalidKeyException : 不合法的access key或者secret key。\n                                              IOException : 连接异常。\n                                              NoSuchAlgorithmException : 找不到相应的签名算法。\n\n示例\n\ntry {\n    PostPolicy policy = new PostPolicy("mybucket", "myobject",\n  DateTime.now().plusDays(7));\n    policy.setContentType("image/png");\n    Map<String,String> formData = minioClient.presignedPostPolicy(policy);\n    System.out.print("curl -X POST ");\n    for (Map.Entry<String,String> entry : formData.entrySet()) {\n    System.out.print(" -F " + entry.getKey() + "=" + entry.getValue());\n    }\n    System.out.println(" -F file=@/tmp/userpic.png  https://play.minio.io:9000/mybucket");\n} catch(MinioException e) {\n  System.out.println("Error occurred: " + e);\n\n\n\n# 5. 了解更多\n\n * 创建属于你的照片API服务示例\n * [完整的JavaDoc](',normalizedContent:'# java client api参考文档\n\n\n# 初始化minio client object。\n\n\n# minio\n\nminioclient minioclient = new minioclient("https://play.minio.io:9000", "q3am3uq867spqqa43p2f", "zuf+tfteslswru7bj86wekitnifilbzam1kyy3tg");\n\n\n\n# aws s3\n\nminioclient s3client = new minioclient("https://s3.amazonaws.com", "your-accesskeyid", "your-secretaccesskey");\n\n\n存储桶操作                   文件对象操作                   presigned操作           存储桶策略\nmakebucket              getobject                presignedgetobject    getbucketpolicy\nlistbuckets             putobject                presignedputobject    setbucketpolicy\nbucketexists            copyobject               presignedpostpolicy   \nremovebucket            statobject                                     \nlistobjects             removeobject                                   \nlistincompleteuploads   removeincompleteupload                         \n\n\n# 1. 构造函数\n\n\npublic minioclient(string endpoint) throws\nnullpointerexception, invalidendpointexception,\ninvalidportexception\n使用给定的endpoint以及匿名方式创建一个minio client对象。\n查看 javadoc\n\n\npublic minioclient(url url) throws nullpointerexception,\ninvalidendpointexception, invalidportexception\n使用给定的url以及匿名方式创建一个minio client对象。\n查看 javadoc\n\n\npublic minioclient(com.squareup.okhttp.httpurl url) throws\nnullpointerexception, invalidendpointexception,\ninvalidportexception\n使用给定的httpurl以及匿名方式创建一个minio client对象。\n查看 javadoc\n\n\npublic minioclient(string endpoint, string accesskey, string\nsecretkey) throws nullpointerexception,\ninvalidendpointexception, invalidportexception\n使用给定的endpoint、access key和secret key创建一个minio client对象。\n查看 javadoc\n\n\npublic minioclient(string endpoint, int port, string\naccesskey, string secretkey) throws nullpointerexception,\ninvalidendpointexception, invalidportexception\n使用给定的endpoint、port、access key和secret key创建一个minio client对象。\n查看 javadoc\n\n\npublic minioclient(string endpoint, string accesskey, string\nsecretkey, boolean secure) throws nullpointerexception,\ninvalidendpointexception, invalidportexception\n使用给定的endpoint、access key、secret\nkey和一个secure选项（是否使用https）创建一个minio client对象。\n查看 javadoc\n\n\npublic minioclient(string endpoint, int port, string\naccesskey, string secretkey, boolean secure) throws\nnullpointerexception, invalidendpointexception,\ninvalidportexception\n使用给定的endpoint、port、access key、secret\nkey和一个secure选项（是否使用https）创建一个minio client对象。\n查看 javadoc\n\n\npublic minioclient(com.squareup.okhttp.httpurl url, string\naccesskey, string secretkey) throws nullpointerexception,\ninvalidendpointexception, invalidportexception\n使用给定的httpurl对象、access key、secret key创建一个minio client对象。\n查看 javadoc\n\n\npublic minioclient(url url, string accesskey, string\nsecretkey) throws nullpointerexception,\ninvalidendpointexception, invalidportexception\n使用给定的url对象、access key、secret key创建一个minio client对象。\n查看 javadoc\n\n参数\n\n参数          类型        描述\nendpoint    string    endpoint是一个url，域名，ipv4或者ipv6地址。以下是合法的endpoints:\n                      https://s3.amazonaws.com\n                      https://play.minio.io:9000\n                      localhost\n                      play.minio.io\nport        int       tcp/ip端口号。可选，默认值是，如果是http,则默认80端口，如果是https,则默认是443端口。\naccesskey   string    accesskey类似于用户id，用于唯一标识你的账户。\nsecretkey   string    secretkey是你账户的密码。\nsecure      boolean   如果是true，则用的是https而不是http,默认值是true。\nurl         url       endpoint url对象。\nurl         httpurl   endpoint httpurl对象。\n\n示例\n\n\n# minio\n\n// 1. public minioclient(string endpoint)\nminioclient minioclient = new minioclient("https://play.minio.io:9000");\n// 2. public minioclient(url url)\nminioclient minioclient = new minioclient(new url("https://play.minio.io:9000"));\n// 3. public minioclient(com.squareup.okhttp.httpurl url)\n minioclient minioclient = new minioclient(new httpurl.parse("https://play.minio.io:9000"));\n// 4. public minioclient(string endpoint, string accesskey, string secretkey)\nminioclient minioclient = new minioclient("https://play.minio.io:9000", "q3am3uq867spqqa43p2f", "zuf+tfteslswru7bj86wekitnifilbzam1kyy3tg");\n// 5. public minioclient(string endpoint, int port,  string accesskey, string secretkey)\nminioclient minioclient = new minioclient("https://play.minio.io", 9000, "q3am3uq867spqqa43p2f", "zuf+tfteslswru7bj86wekitnifilbzam1kyy3tg");\n// 6. public minioclient(string endpoint, string accesskey, string secretkey, boolean insecure)\nminioclient minioclient = new minioclient("https://play.minio.io:9000", "q3am3uq867spqqa43p2f", "zuf+tfteslswru7bj86wekitnifilbzam1kyy3tg", true);\n// 7. public minioclient(string endpoint, int port,  string accesskey, string secretkey, boolean insecure)\nminioclient minioclient = new minioclient("https://play.minio.io", 9000, "q3am3uq867spqqa43p2f", "zuf+tfteslswru7bj86wekitnifilbzam1kyy3tg", true);\n// 8. public minioclient(com.squareup.okhttp.httpurl url, string accesskey, string secretkey)\n minioclient minioclient = new minioclient(new url("https://play.minio.io:9000"), "q3am3uq867spqqa43p2f", "zuf+tfteslswru7bj86wekitnifilbzam1kyy3tg");\n// 9. public minioclient(url url, string accesskey, string secretkey)\nminioclient minioclient = new minioclient(httpurl.parse("https://play.minio.io:9000"), "q3am3uq867spqqa43p2f", "zuf+tfteslswru7bj86wekitnifilbzam1kyy3tg");\n\n\n\n# aws s3\n\n// 1. public minioclient(string endpoint)\nminioclient s3client = new minioclient("https://s3.amazonaws.com");\n// 2. public minioclient(url url)\nminioclient minioclient = new minioclient(new url("https://s3.amazonaws.com"));\n// 3. public minioclient(com.squareup.okhttp.httpurl url)\n minioclient s3client = new minioclient(new httpurl.parse("https://s3.amazonaws.com"));\n// 4. public minioclient(string endpoint, string accesskey, string secretkey)\nminioclient s3client = new minioclient("s3.amazonaws.com", "your-accesskeyid", "your-secretaccesskey");\n// 5. public minioclient(string endpoint, int port,  string accesskey, string secretkey)\nminioclient s3client = new minioclient("s3.amazonaws.com", 80, "your-accesskeyid", "your-secretaccesskey");\n// 6. public minioclient(string endpoint, string accesskey, string secretkey, boolean insecure)\nminioclient s3client = new minioclient("s3.amazonaws.com", "your-accesskeyid", "your-secretaccesskey", false);\n// 7. public minioclient(string endpoint, int port,  string accesskey, string secretkey, boolean insecure)\nminioclient s3client = new minioclient("s3.amazonaws.com", 80, "your-accesskeyid", "your-secretaccesskey",false);\n// 8. public minioclient(com.squareup.okhttp.httpurl url, string accesskey, string secretkey)\n minioclient s3client = new minioclient(new url("s3.amazonaws.com"), "your-accesskeyid", "your-secretaccesskey");\n// 9. public minioclient(url url, string accesskey, string secretkey)\nminioclient s3client = new minioclient(httpurl.parse("s3.amazonaws.com"), "your-accesskeyid", "your-secretaccesskey");\n\n\n\n# 2. 存储桶操作\n\n\n# makebucket(string bucketname)\n\npublic void makebucket(string bucketname)\n\n\n创建一个新的存储桶\n\n查看javadoc\n\n参数\n\n参数           类型       描述\nbucketname   string   存储桶名称\n\n返回值类型   异常\nnone    异常列表:\n        invalidbucketnameexception : 非法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常\n        errorresponseexception : 执行失败\n        internalexception : 内部异常\n\n示例\n\ntry {\n  // 如存储桶不存在，创建之。\n  boolean found = minioclient.bucketexists("mybucket");\n  if (found) {\n    system.out.println("mybucket already exists");\n  } else {\n    // 创建名为\'my-bucketname\'的存储桶。\n    minioclient.makebucket("mybucket");\n    system.out.println("mybucket is created successfully");\n  }\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# listbuckets()\n\npublic list<bucket> listbuckets()\n\n\n列出所有存储桶。\n\n查看javadoc\n\n返回值类型                                异常\nlist bucket : list of bucket type.   异常列表：\n                                     noresponseexception : 服务端无响应\n                                     ioexception : 连接异常\n                                     org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常\n                                     errorresponseexception :执行失败异溃\n                                     internalexception : 内部错误\n\n示例\n\ntry {\n  // 列出所有存储桶\n  list<bucket> bucketlist = minioclient.listbuckets();\n  for (bucket bucket : bucketlist) {\n    system.out.println(bucket.creationdate() + ", " + bucket.name());\n  }\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# bucketexists(string bucketname)\n\npublic boolean bucketexists(string bucketname)\n\n\n检查存储桶是否存在。\n\n查看 javadoc\n\n参数\n\n参数           类型       描述\nbucketname   string   存储桶名称\n\n返回值值类型                               异常\nboolean: true if the bucket exists   异常列表：\n                                     invalidbucketnameexception : 不合法的存储桶名称。\n                                     noresponseexception : 服务器无响应。\n                                     ioexception : 连接异常。\n                                     org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n                                     errorresponseexception : 执行失败异常。\n                                     internalexception : 内部错误。\n\n示例\n\ntry {\n  // 检查\'my-bucketname\'是否存在。\n  boolean found = minioclient.bucketexists("mybucket");\n  if (found) {\n    system.out.println("mybucket exists");\n  } else {\n    system.out.println("mybucket does not exist");\n  }\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# removebucket(string bucketname)\n\npublic void removebucket(string bucketname)\n\n\n删除一个存储桶。\n\n查看 javadoc\n\n注意: - removebucket不会删除存储桶里的对象，你需要通过removeobject api来删除它们。\n\n参数\n\n参数           类型       描述\nbucketname   string   存储桶名称。\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n\n示例\n\ntry {\n  // 删除之前先检查`my-bucket`是否存在。\n  boolean found = minioclient.bucketexists("mybucket");\n  if (found) {\n    // 删除`my-bucketname`存储桶，注意，只有存储桶为空时才能删除成功。\n    minioclient.removebucket("mybucket");\n    system.out.println("mybucket is removed successfully");\n  } else {\n    system.out.println("mybucket does not exist");\n  }\n} catch(minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# listobjects(string bucketname, string prefix, boolean recursive, boolean useversion1)\n\npublic iterable<result<item>> listobjects(string bucketname, string prefix, boolean recursive, boolean useversion1)\n\n\n列出某个存储桶中的所有对象。\n\n查看javadoc\n\n参数\n\n参数            类型        描述\nbucketname    string    存储桶名称。\nprefix        string    对象名称的前缀\nrecursive     boolean   是否递归查找，如果是false,就模拟文件夹结构查找。\nuseversion1   boolean   如果是true, 使用版本1 rest api\n\n返回值类型                                                 异常\niterable<result<item>>:an iterator of result items.   none\n\n示例\n\ntry {\n  // 检查\'mybucket\'是否存在。\n  boolean found = minioclient.bucketexists("mybucket");\n  if (found) {\n    // 列出\'my-bucketname\'里的对象\n    iterable<result<item>> myobjects = minioclient.listobjects("mybucket");\n    for (result<item> result : myobjects) {\n      item item = result.get();\n      system.out.println(item.lastmodified() + ", " + item.size() + ", " + item.objectname());\n    }\n  } else {\n    system.out.println("mybucket does not exist");\n  }\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# listincompleteuploads(string bucketname, string prefix, boolean recursive)\n\npublic iterable<result<upload>> listincompleteuploads(string bucketname, string prefix, boolean recursive)\n\n\n列出存储桶中被部分上传的对象。\n\n查看 javadoc\n\n参数\n\n参数           类型        描述\nbucketname   string    存储桶名称。\nprefix       string    对象名称的前缀，列出有该前缀的对象\nrecursive    boolean   是否递归查找，如果是false,就模拟文件夹结构查找。\n\n返回值类型                                              异常\niterable<result<upload>>: an iterator of upload.   none\n\n示例\n\ntry {\n  // 检查\'mybucket\'是否存在。\n  boolean found = minioclient.bucketexists("mybucket");\n  if (found) {\n    // 列出\'mybucket\'中所有未完成的multipart上传的的对象。 \n    iterable<result<upload>> myobjects = minioclient.listincompleteuploads("mybucket");\n    for (result<upload> result : myobjects) {\n      upload upload = result.get();\n      system.out.println(upload.uploadid() + ", " + upload.objectname());\n    }\n  } else {\n    system.out.println("mybucket does not exist");\n  }\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# getbucketpolicy(string bucketname, string objectprefix)\n\npublic policytype getbucketpolicy(string bucketname, string objectprefix)\n\n\n获得指定对象前缀的存储桶策略。\n\n查看 javadoc\n\n参数\n\n参数             类型       描述\nbucketname     string   存储桶名称。\nobjectprefix   string   策略适用的对象的前缀\n\n返回值类型                                                    异常\npolicytype: the current bucket policy type for a given   异常列表：\nbucket and objectprefix.\n                                                         invalidbucketnameexception : 不合法的存储桶名称。\n                                                         noresponseexception : 服务器无响应。\n                                                         ioexception : 连接异常。\n                                                         org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n                                                         errorresponseexception : 执行失败异常。\n                                                         internalexception : 内部错误。\n                                                         invalidbucketnameexception : 不合法的存储桶名称。\n                                                         invalidobjectprefixexception : 不合法的对象前缀\n                                                         nosuchalgorithmexception : 找不到相应的签名算法。\n                                                         insufficientdataexception : 在读到相应length之前就得到一个eofexception。\n\n示例\n\ntry {\n  system.out.println("current policy: " + minioclient.getbucketpolicy("mybucket", "downloads"));\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# setbucketpolicy(string bucketname, string objectprefix, policytype policy)\n\npublic void setbucketpolicy(string bucketname, string objectprefix, policytype policy)\n\n\n给一个存储桶+对象前缀设置策略。\n\n查看 javadoc\n\n参数\n\n参数             类型           描述\nbucketname     string       存储桶名称。\nobjectprefix   string       对象前缀。\npolicy         policytype   要赋予的策略，可选值有[policytype.none, policytype.read_only,\n                            policytype.read_write, policytype.write_only].\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n        invalidbucketnameexception : 不合法的存储桶名称。\n        invalidobjectprefixexception : 不合法的对象前缀\n        nosuchalgorithmexception : 找不到相应的签名算法。\n        insufficientdataexception : 在读到相应length之前就得到一个eofexception。\n\n示例\n\ntry {\n  minioclient.setbucketpolicy("mybucket", "uploads", policytype.read_only);\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# 3. object operations\n\n\n# getobject(string bucketname, string objectname)\n\npublic inputstream getobject(string bucketname, string objectname, long offset)\n\n\n以流的形式下载一个对象。\n\n查看 javadoc\n\n参数\n\n参数           类型       描述\nbucketname   string   存储桶名称。\nobjectname   string   存储桶里的对象名称。\n\n返回值类型                                                  异常\ninputstream: inputstream containing the object data.   异常列表：\n                                                       invalidbucketnameexception : 不合法的存储桶名称。\n                                                       noresponseexception : 服务器无响应。\n                                                       ioexception : 连接异常。\n                                                       org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n                                                       errorresponseexception : 执行失败异常。\n                                                       internalexception : 内部错误。\n\n示例\n\ntry {\n  // 调用statobject()来判断对象是否存在。\n  // 如果不存在, statobject()抛出异常,\n  // 否则则代表对象存在。\n  minioclient.statobject("mybucket", "myobject");\n  // 获取"myobject"的输入流。\n  inputstream stream = minioclient.getobject("mybucket", "myobject");\n  // 读取输入流直到eof并打印到控制台。\n  byte[] buf = new byte[16384];\n  int bytesread;\n  while ((bytesread = stream.read(buf, 0, buf.length)) >= 0) {\n    system.out.println(new string(buf, 0, bytesread));\n  }\n  // 关闭流，此处为示例，流关闭最好放在finally块。\n  stream.close();\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# getobject(string bucketname, string objectname, long offset, long length)\n\npublic inputstream getobject(string bucketname, string objectname, long offset, long length)\n\n\n下载对象指定区域的字节数组做为流。（断点下载）\n\n查看 javadoc\n\n参数\n\n参数           类型       描述\nbucketname   string   存储桶名称。\nobjectname   string   存储桶里的对象名称。\noffset       long     offset 是起始字节的位置\nlength       long     length是要读取的长度 (可选，如果无值则代表读到文件结尾)。\n\n返回值类型                                                     异常\ninputstream : inputstream containing the object\'s data.   异常列表：\n                                                          invalidbucketnameexception : 不合法的存储桶名称。\n                                                          noresponseexception : 服务器无响应。\n                                                          ioexception : 连接异常。\n                                                          org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n                                                          errorresponseexception : 执行失败异常。\n                                                          internalexception : 内部错误。\n\n示例\n\ntry {\n  // 调用statobject()来判断对象是否存在。\n  // 如果不存在, statobject()抛出异常,\n  // 否则则代表对象存在。\n  minioclient.statobject("mybucket", "myobject");\n  // 获取指定offset和length的"myobject"的输入流。\n  inputstream stream = minioclient.getobject("mybucket", "myobject", 1024l, 4096l);\n  // 读取输入流直到eof并打印到控制台。\n  byte[] buf = new byte[16384];\n  int bytesread;\n  while ((bytesread = stream.read(buf, 0, buf.length)) >= 0) {\n    system.out.println(new string(buf, 0, bytesread));\n  }\n  // 关闭流。\n  stream.close();\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# getobject(string bucketname, string objectname, string filename)\n\npublic void getobject(string bucketname, string objectname, string filename)\n\n\n下载并将文件保存到本地。\n\n查看 javadoc\n\n参数\n\n参数           类型       描述\nbucketname   string   存储桶名称。\nobjectname   string   存储桶里的对象名称。\nfilename     string   file name.\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n\n示例\n\ntry {\n  // 调用statobject()来判断对象是否存在。\n  // 如果不存在, statobject()抛出异常,\n  // 否则则代表对象存在。\n  minioclient.statobject("mybucket", "myobject");\n  // 获取myobject的流并保存到photo.jpg文件中。\n  minioclient.getobject("mybucket", "myobject", "photo.jpg");\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# getobject(string bucketname, string objectname, secretkey key)\n\npublic cipherinputstream getobject(string bucketname, string objectname, secretkey key)\n\n\n在给定的存储桶中获取整个加密对象的数据作为inputstream，然后用传入的master key解密和加密对象关联的content key。然后创建一个含有inputstream和cipher的cipherinputstream。这个cipher被初始为用于使用content key进行解密，所以cipherinputstream会在返回数据前，尝试读取数据并进行解密。所以read()方法返回的是处理过的原始对象数据。\n\ncipherinputstream必须用完关闭，否则连接不会被释放。\n\n查看 javadoc\n\n参数\n\n参数           类型          描述\nbucketname   string      存储桶名称。\nobjectname   string      存储桶里的对象名称。\nkey          secretkey   secretkey类型的数据。\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n        invalidencryptionmetadataexception : 加密秘钥错误。\n        badpaddingexception : 错误的padding\n        illegalblocksizeexception : 不正确的block size\n        nosuchpaddingexception : 错误的pading类型\n        invalidalgorithmparameterexception : 该算法不存在\n\n示例\n\ntry {\n  // 调用statobject()来判断对象是否存在。\n  // 如果不存在, statobject()抛出异常,\n  // 否则则代表对象存在。\n  minioclient.statobject("mybucket", "myobject");\n  //生成256位aes key。\n  keygenerator symkeygenerator = keygenerator.getinstance("aes");\n  symkeygenerator.init(256);\n  secretkey symkey = symkeygenerator.generatekey();\n  // 获取对象数据并保存到photo.jpg\n  inputstream stream = minioclient.getobject("testbucket", "my-objectname", symkey);\n  // 读流到eof，并输出到控制台。\n  byte[] buf = new byte[16384];\n  int bytesread;\n  while ((bytesread = stream.read(buf, 0, buf.length)) >= 0) {\n    system.out.println(new string(buf, 0, bytesread, standardcharsets.utf_8));\n  }\n  // 关闭流。\n  stream.close();\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# getobject(string bucketname, string objectname, keypair key)\n\npublic inputstream getobject(string bucketname, string objectname, keypair key)\n\n\n在给定的存储桶中获取整个加密对象的数据作为inputstream，然后用传入的master keypair解密和加密对象关联的content key。然后创建一个含有inputstream和cipher的cipherinputstream。这个cipher被初始为用于使用content key进行解密，所以cipherinputstream会在返回数据前，尝试读取数据并进行解密。所以read()方法返回的是处理过的原始对象数据。\n\ncipherinputstream必须用完关闭，否则连接不会被释放。\n\n查看 javadoc\n\n参数\n\n参数           类型        描述\nbucketname   string    存储桶名称。\nobjectname   string    存储桶里的对象名称。\nkey          keypair   rsa keypair类型的对象。\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n        invalidencryptionmetadataexception : 加密秘钥错误。\n        badpaddingexception : 错误的padding\n        illegalblocksizeexception : 不正确的block size\n        nosuchpaddingexception : 错误的pading类型\n        invalidalgorithmparameterexception : 该算法不存在\n\n示例\n\ntry {\n  // 调用statobject()来判断对象是否存在。\n  // 如果不存在, statobject()抛出异常,\n  // 否则则代表对象存在。\n  minioclient.statobject("mybucket", "myobject");\n  keypairgenerator keygenerator = keypairgenerator.getinstance("rsa");\n  keygenerator.initialize(1024, new securerandom());\n  keypair keypair = keygenerator.generatekeypair();\n  // 获取对象数据并保存到photo.jpg\n  inputstream stream = minioclient.getobject("testbucket", "my-objectname", keypair);\n  // 读流到eof，并输出到控制台。\n  byte[] buf = new byte[16384];\n  int bytesread;\n  while ((bytesread = stream.read(buf, 0, buf.length)) >= 0) {\n    system.out.println(new string(buf, 0, bytesread, standardcharsets.utf_8));\n  }\n  // 关闭流。\n  stream.close();\n} catch (minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# putobject(string bucketname, string objectname, inputstream stream, long size, string contenttype)\n\npublic void putobject(string bucketname, string objectname, inputstream stream, long size, string contenttype)\n\n\n通过inputstream上传对象。\n\n查看 javadoc\n\n参数\n\n参数            类型            描述\nbucketname    string        存储桶名称。\nobjectname    string        存储桶里的对象名称。\nstream        inputstream   要上传的流。\nsize          long          要上传的stream的size\ncontenttype   string        content type。\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n\n示例\n\n单个对象的最大大小限制在5tb。putobject在对象大于5mib时，自动使用multiple parts方式上传。这样，当上传失败时，客户端只需要上传未成功的部分即可（类似断点上传）。上传的对象使用md5sum签名进行完整性验证。\n\ntry {\n  stringbuilder builder = new stringbuilder();\n  for (int i = 0; i < 1000; i++) {\n    builder.append("sphinx of black quartz, judge my vow: used by adobe indesign to display font samples. ");\n    builder.append("(29 letters)\\n");\n    builder.append("jackdaws love my big sphinx of quartz: similarly, used by windows xp for some fonts. ");\n    builder.append("(31 letters)\\n");\n    builder.append("pack my box with five dozen liquor jugs: according to wikipedia, this one is used on ");\n    builder.append("nasas space shuttle. (32 letters)\\n");\n    builder.append("the quick onyx goblin jumps over the lazy dwarf: flavor text from an unhinged magic card. ");\n    builder.append("(39 letters)\\n");\n    builder.append("how razorback-jumping frogs can level six piqued gymnasts!: not going to win any brevity ");\n    builder.append("awards at 49 letters long, but old-time mac users may recognize it.\\n");\n    builder.append("cozy lummox gives smart squid who asks for job pen: a 41-letter tester sentence for mac ");\n    builder.append("computers after system 7.\\n");\n    builder.append("a few others we like: amazingly few discotheques provide jukeboxes; now fax quiz jack! my ");\n    builder.append("brave ghost pled; watch jeopardy!, alex trebeks fun tv quiz game.\\n");\n    builder.append("- --\\n");\n  }\n  bytearrayinputstream bais = new\n  bytearrayinputstream(builder.tostring().getbytes("utf-8"));\n  // 创建对象\n  minioclient.putobject("mybucket", "myobject", bais, bais.available(), "application/octet-stream");\n  bais.close();\n  system.out.println("myobject is uploaded successfully");\n} catch(minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# putobject(string bucketname, string objectname, string filename)\n\npublic void putobject(string bucketname, string objectname, string filename)\n\n\n通过文件上传到对象中。查看 javadoc\n\n参数\n\n参数           类型       描述\nbucketname   string   存储桶名称。\nobjectname   string   存储桶里的对象名称。\nfilename     string   file name.\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n\n示例\n\ntry {\n  minioclient.putobject("mybucket",  "island.jpg", "/mnt/photos/island.jpg")\n  system.out.println("island.jpg is uploaded successfully");\n} catch(minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# putobject(string bucketname, string objectname, inputstream stream, long size, string contenttype, secretkey key)\n\npublic void putobject(string bucketname, string objectname, inputstream stream, long size, string contenttype,secretkey key)\n\n\n拿到流的数据，使用随机生成的content key进行加密，并上传到指定存储桶中。同时将加密后的content key和iv做为加密对象有header也上传到存储桶中。content key使用传入到该方法的master key进行加密。\n\n如果对象大于5mb,客户端会自动进行multi part上传。\n\n查看 javadoc\n\n参数\n\n参数            类型            描述\nbucketname    string        存储桶名称。\nobjectname    string        存储桶里的对象名称。\nstream        inputstream   要上传的流。\nsize          long          要上传的流的大小。\ncontenttype   string        content type。\nkey           secretkey     用aes初使化的对象secretkey。\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n        invalidalgorithmparameterexception : 错误的加密算法。\n        badpaddingexception : 不正确的padding.\n        illegalblocksizeexception : 不正确的block。\n        nosuchpaddingexception : 错误的padding类型。\n\n示例\n\n对象使用随机生成的key进行加密，然后这个用于加密数据的key又被由仅被client知道的master key(封装在encryptionmaterials对象里)进行加密。这个被加密后的key和iv做为对象的header和加密后的对象一起被上传到存储服务上。\n\ntry {\n  stringbuilder builder = new stringbuilder();\n  for (int i = 0; i < 1000; i++) {\n    builder.append("sphinx of black quartz, judge my vow: used by adobe indesign to display font samples. ");\n    builder.append("(29 letters)\\n");\n    builder.append("jackdaws love my big sphinx of quartz: similarly, used by windows xp for some fonts. ");\n    builder.append("(31 letters)\\n");\n    builder.append("pack my box with five dozen liquor jugs: according to wikipedia, this one is used on ");\n    builder.append("nasas space shuttle. (32 letters)\\n");\n    builder.append("the quick onyx goblin jumps over the lazy dwarf: flavor text from an unhinged magic card. ");\n    builder.append("(39 letters)\\n");\n    builder.append("how razorback-jumping frogs can level six piqued gymnasts!: not going to win any brevity ");\n    builder.append("awards at 49 letters long, but old-time mac users may recognize it.\\n");\n    builder.append("cozy lummox gives smart squid who asks for job pen: a 41-letter tester sentence for mac ");\n    builder.append("computers after system 7.\\n");\n    builder.append("a few others we like: amazingly few discotheques provide jukeboxes; now fax quiz jack! my ");\n    builder.append("brave ghost pled; watch jeopardy!, alex trebeks fun tv quiz game.\\n");\n    builder.append("- --\\n");\n  }\n  bytearrayinputstream bais = new\n  bytearrayinputstream(builder.tostring().getbytes("utf-8"));\n  //生成256位aes key.\n  keygenerator symkeygenerator = keygenerator.getinstance("aes");\n  symkeygenerator.init(256);\n  secretkey symkey = symkeygenerator.generatekey();\n  // 创建一个对象\n  minioclient.putobject("mybucket", "myobject", bais, bais.available(), "application/octet-stream", symkey);\n  bais.close();\n  system.out.println("myobject is uploaded successfully");\n} catch(minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# putobject(string bucketname, string objectname, inputstream stream, long size, string contenttype, keypair key)\n\npublic void putobject(string bucketname, string objectname, inputstream stream, long size, string contenttype,keypair key)\n\n\n拿到流的数据，使用随机生成的content key进行加密，并上传到指定存储桶中。同时将加密后的content key和iv做为加密对象有header也上传到存储桶中。content key使用传入到该方法的master key进行加密。\n\n如果对象大于5mb,客户端会自动进行multi part上传。\n\n查看 javadoc\n\n参数\n\n参数            类型            描述\nbucketname    string        存储桶名称。\nobjectname    string        存储桶里的对象名称。\nstream        inputstream   要上传的流。\nsize          long          要上传的流的大小。\ncontenttype   string        content type。\nkey           keypair       一个rsa keypair的对象。\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n        invalidalgorithmparameterexception : 错误的加密算法。\n        badpaddingexception : 不正确的padding。\n        illegalblocksizeexception : 不正确的block。\n        nosuchpaddingexception : 错误的pading类型。\n\n示例\n\n对象使用随机生成的key进行加密，然后这个用于加密数据的key又被由仅被client知道的master key(封装在encryptionmaterials对象里)进行加密。这个被加密后的key和iv做为对象的header和加密后的对象一起被上传到存储服务上。\n\ntry {\n  stringbuilder builder = new stringbuilder();\n  for (int i = 0; i < 1000; i++) {\n    builder.append("sphinx of black quartz, judge my vow: used by adobe indesign to display font samples. ");\n    builder.append("(29 letters)\\n");\n    builder.append("jackdaws love my big sphinx of quartz: similarly, used by windows xp for some fonts. ");\n    builder.append("(31 letters)\\n");\n    builder.append("pack my box with five dozen liquor jugs: according to wikipedia, this one is used on ");\n    builder.append("nasas space shuttle. (32 letters)\\n");\n    builder.append("the quick onyx goblin jumps over the lazy dwarf: flavor text from an unhinged magic card. ");\n    builder.append("(39 letters)\\n");\n    builder.append("how razorback-jumping frogs can level six piqued gymnasts!: not going to win any brevity ");\n    builder.append("awards at 49 letters long, but old-time mac users may recognize it.\\n");\n    builder.append("cozy lummox gives smart squid who asks for job pen: a 41-letter tester sentence for mac ");\n    builder.append("computers after system 7.\\n");\n    builder.append("a few others we like: amazingly few discotheques provide jukeboxes; now fax quiz jack! my ");\n    builder.append("brave ghost pled; watch jeopardy!, alex trebeks fun tv quiz game.\\n");\n    builder.append("- --\\n");\n  }\n  bytearrayinputstream bais = new\n  bytearrayinputstream(builder.tostring().getbytes("utf-8"));\n  keypairgenerator keygenerator = keypairgenerator.getinstance("rsa");\n  keygenerator.initialize(1024, new securerandom());\n  keypair keypair = keygenerator.generatekeypair();\n  // create an object\n  minioclient.putobject("mybucket", "myobject", bais, bais.available(), "application/octet-stream", keypair);\n  bais.close();\n  system.out.println("myobject is uploaded successfully");\n} catch(minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# statobject(string bucketname, string objectname)\n\npublic objectstat statobject(string bucketname, string objectname)\n\n获取对象的元数据。\n\n查看 javadoc\n\n参数\n\n参数           类型       描述\nbucketname   string   存储桶名称。\nobjectname   string   存储桶里的对象名称。\n\n返回值类型                                     异常\nobjectstat: populated object meta data.   异常列表：\n                                          invalidbucketnameexception : 不合法的存储桶名称。\n                                          noresponseexception : 服务器无响应。\n                                          ioexception : 连接异常。\n                                          org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n                                          errorresponseexception : 执行失败异常。\n                                          internalexception : 内部错误。\n\n示例\n\ntry {\n  // 获得对象的元数据。\n  objectstat objectstat = minioclient.statobject("mybucket", "myobject");\n  system.out.println(objectstat);\n} catch(minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# copyobject(string bucketname, string objectname, string destbucketname, string destobjectname, copyconditions cpconds, map metadata)\n\npublic void copyobject(string bucketname, string objectname, string destbucketname, string destobjectname, copyconditions cpconds, map<string, string> metadata)\n\n从objectname指定的对象中将数据拷贝到destobjectname指定的对象。\n\n查看 javadoc\n\n参数\n\n参数               类型               描述\nbucketname       string           源存储桶名称。\nobjectname       string           源存储桶中的源对象名称。\ndestbucketname   string           目标存储桶名称。\ndestobjectname   string           要创建的目标对象名称,如果为空，默认为源对象名称。\ncopyconditions   copyconditions   拷贝操作的一些条件map。\nmetadata         map              给目标对象的元数据map。\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n\n示例\n\n本api执行了一个服务端的拷贝操作。\n\ntry {\n  copyconditions copyconditions = new copyconditions();\n  copyconditions.setmatchetagnone("testetag");\n  minioclient.copyobject("mybucket",  "island.jpg", "mydestbucket", "processed.png", copyconditions);\n  system.out.println("island.jpg is uploaded successfully");\n} catch(minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# removeobject(string bucketname, string objectname)\n\npublic void removeobject(string bucketname, string objectname)\n\n\n删除一个对象。\n\n查看 javadoc\n\n参数\n\n参数           类型       描述\nbucketname   string   存储桶名称。\nobjectname   string   存储桶里的对象名称。\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n\n示例\n\ntry {\n      // 从mybucket中删除myobject。\n      minioclient.removeobject("mybucket", "myobject");\n      system.out.println("successfully removed mybucket/myobject");\n} catch (minioexception e) {\n      system.out.println("error: " + e);\n}\n\n\n\n# removeobject(string bucketname, iterable objectnames)\n\npublic iterable<result<deleteerror>> removeobject(string bucketname, iterable<string> objectnames)\n\n\n删除多个对象。\n\n查看 javadoc\n\n参数\n\n参数            类型         描述\nbucketname    string     存储桶名称。\nobjectnames   iterable   含有要删除的多个object名称的迭代器对象。\n\n返回值类型                                                 异常\niterable<result<deleteerror>>:an iterator of result   none\ndeleteerror.\n\n示例\n\nlist<string> objectnames = new linkedlist<string>();\nobjectnames.add("my-objectname1");\nobjectnames.add("my-objectname2");\nobjectnames.add("my-objectname3");\ntry {\n      // 删除my-bucketname里的多个对象\n      for (result<deleteerror> errorresult: minioclient.removeobject("my-bucketname", objectnames)) {\n        deleteerror error = errorresult.get();\n        system.out.println("failed to remove \'" + error.objectname() + "\'. error:" + error.message());\n      }\n} catch (minioexception e) {\n      system.out.println("error: " + e);\n}\n\n\n\n# removeincompleteupload(string bucketname, string objectname)\n\npublic void removeincompleteupload(string bucketname, string objectname)\n\n\n删除一个未完整上传的对象。\n\n查看 javadoc\n\n参数\n\n参数           类型       描述\nbucketname   string   存储桶名称。\nobjectname   string   存储桶里的对象名称。\n\n返回值类型   异常\nnone    异常列表：\n        invalidbucketnameexception : 不合法的存储桶名称。\n        noresponseexception : 服务器无响应。\n        ioexception : 连接异常。\n        org.xmlpull.v1.xmlpullparserexception : 解析返回的xml异常。\n        errorresponseexception : 执行失败异常。\n        internalexception : 内部错误。\n\n示例\n\ntry {\n    string url = minioclient.presignedgetobject("mybucket", "myobject", 60 * 60 * 24);\n    system.out.println(url);\n} catch(minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# 4. presigned操作\n\n\n# presignedgetobject(string bucketname, string objectname, integer expires)\n\npublic string presignedgetobject(string bucketname, string objectname, integer expires)\n\n\n生成一个给http get请求用的presigned url。浏览器/移动端的客户端可以用这个url进行下载，即使其所在的存储桶是私有的。这个presigned url可以设置一个失效时间，默认值是7天。\n\n查看 javadoc\n\n参数\n\n参数           类型        描述\nbucketname   string    存储桶名称。\nobjectname   string    存储桶里的对象名称。\nexpiry       integer   失效时间（以秒为单位），默认是7天，不得大于七天。\n\n返回值类型                                                  异常\nstring : string contains url to download the object.   异常列表：\n                                                       invalidbucketnameexception : 不合法的存储桶名称。\n                                                       invalidkeyexception : 不合法的access key或者secret key。\n                                                       ioexception : 连接异常。\n                                                       nosuchalgorithmexception : 找不到相应的签名算法。\n                                                       invalidexpiresrangeexception : presigned url已经过期了。\n\n示例\n\ntry {\n    string url = minioclient.presignedputobject("mybucket", "myobject", 60 * 60 * 24);\n    system.out.println(url);\n} catch(minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# presignedputobject(string bucketname, string objectname, integer expires)\n\npublic string presignedputobject(string bucketname, string objectname, integer expires)\n\n\n生成一个给http put请求用的presigned url。浏览器/移动端的客户端可以用这个url进行上传，即使其所在的存储桶是私有的。这个presigned url可以设置一个失效时间，默认值是7天。\n\n查看 javadoc\n\n参数\n\n参数           类型        描述\nbucketname   string    存储桶名称。\nobjectname   string    存储桶里的对象名称。\nexpiry       integer   失效时间（以秒为单位），默认是7天，不得大于七天。\n\n返回值类型                                                  异常\nstring : string contains url to download the object.   异常列表：\n                                                       invalidbucketnameexception : 不合法的存储桶名称。\n                                                       invalidkeyexception : 不合法的access key或者secret key。\n                                                       ioexception : 连接异常。\n                                                       nosuchalgorithmexception : 找不到相应的签名算法。\n                                                       invalidexpiresrangeexception : presigned url已经过期了。\n\n示例\n\ntry {\n    string url = minioclient.presignedputobject("mybucket", "myobject", 60 * 60 * 24);\n    system.out.println(url);\n} catch(minioexception e) {\n  system.out.println("error occurred: " + e);\n}\n\n\n\n# presignedpostpolicy(postpolicy policy)\n\npublic map<string,string> presignedpostpolicy(postpolicy policy)\n\n\n允许给post请求的presigned url设置策略，比如接收对象上传的存储桶名称的策略，key名称前缀，过期策略。\n\n查看 javadoc\n\n参数\n\n参数       类型           描述\npolicy   postpolicy   对象的post策略\n\n返回值类型                                         异常\nmap: map of strings to construct form-data.   异常列表：\n                                              invalidbucketnameexception : 不合法的存储桶名称。\n                                              invalidkeyexception : 不合法的access key或者secret key。\n                                              ioexception : 连接异常。\n                                              nosuchalgorithmexception : 找不到相应的签名算法。\n\n示例\n\ntry {\n    postpolicy policy = new postpolicy("mybucket", "myobject",\n  datetime.now().plusdays(7));\n    policy.setcontenttype("image/png");\n    map<string,string> formdata = minioclient.presignedpostpolicy(policy);\n    system.out.print("curl -x post ");\n    for (map.entry<string,string> entry : formdata.entryset()) {\n    system.out.print(" -f " + entry.getkey() + "=" + entry.getvalue());\n    }\n    system.out.println(" -f file=@/tmp/userpic.png  https://play.minio.io:9000/mybucket");\n} catch(minioexception e) {\n  system.out.println("error occurred: " + e);\n\n\n\n# 5. 了解更多\n\n * 创建属于你的照片api服务示例\n * [完整的javadoc](',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"使用Certbot生成Let's Encrypt证书",frontmatter:{title:"使用Certbot生成Let's Encrypt证书",date:"2022-02-09T11:30:18.000Z",permalink:"/pages/4a395a/"},regularPath:"/12.MinIO/04.%E5%AE%9E%E6%88%98%E7%A7%98%E7%B1%8D/01.%E4%BD%BF%E7%94%A8Certbot%E7%94%9F%E6%88%90Let's%20Encrypt%E8%AF%81%E4%B9%A6.html",relativePath:"12.MinIO/04.实战秘籍/01.使用Certbot生成Let's Encrypt证书.md",key:"v-117cc90e",path:"/pages/4a395a/",headers:[{level:2,title:"1. 前提条件",slug:"_1-前提条件",normalizedTitle:"1. 前提条件",charIndex:161},{level:2,title:"2. 依赖",slug:"_2-依赖",normalizedTitle:"2. 依赖",charIndex:229},{level:2,title:"3. 步骤",slug:"_3-步骤",normalizedTitle:"3. 步骤",charIndex:368},{level:3,title:"步骤1: 安装Certbot",slug:"步骤1-安装certbot",normalizedTitle:"步骤1: 安装certbot",charIndex:378},{level:3,title:"步骤2: 生成Let's Encrypt证书",slug:"步骤2-生成let-s-encrypt证书",normalizedTitle:"步骤2: 生成let's encrypt证书",charIndex:413},{level:3,title:"步骤3: 验证证书",slug:"步骤3-验证证书",normalizedTitle:"步骤3: 验证证书",charIndex:537},{level:3,title:"步骤4: 使用证书给Minio Server设置SSL。",slug:"步骤4-使用证书给minio-server设置ssl。",normalizedTitle:"步骤4: 使用证书给minio server设置ssl。",charIndex:1056},{level:3,title:"步骤5: 修改证书的ownership。",slug:"步骤5-修改证书的ownership。",normalizedTitle:"步骤5: 修改证书的ownership。",charIndex:1299},{level:3,title:"步骤6: 使用HTTPS启动Minio Server。",slug:"步骤6-使用https启动minio-server。",normalizedTitle:"步骤6: 使用https启动minio server。",charIndex:1442},{level:3,title:"步骤7: 通过浏览器访问https://myminio.com。",slug:"步骤7-通过浏览器访问https-myminio-com。",normalizedTitle:"步骤7: 通过浏览器访问https://myminio.com。",charIndex:1709}],headersStr:"1. 前提条件 2. 依赖 3. 步骤 步骤1: 安装Certbot 步骤2: 生成Let's Encrypt证书 步骤3: 验证证书 步骤4: 使用证书给Minio Server设置SSL。 步骤5: 修改证书的ownership。 步骤6: 使用HTTPS启动Minio Server。 步骤7: 通过浏览器访问https://myminio.com。",content:"# 使用Certbot生成Let's Encrypt证书\n\nLet's Encrypt 是一个新的免费的，自动的，开源的认证中心。\n\nCertbot是Let's Encrypt的基于控制台的证书生成工具。\n\n本文我们将使用Certbot生成Let's Encrypt证书。该证书将被部署在Minio服务器中使用。\n\n\n# 1. 前提条件\n\n * 从这里下载并安装Minio Server。\n\n * 从这里下载并安装Certbot。\n   \n   \n   # 2. 依赖\n\n * 执行certbot时，需要打开443端口并确保可以访问。\n\n * Certbot需要有root权限，因为只有root才允许绑定1024以下的端口。\n\n * 本文我们将使用myminio.com这个域名，请在设置时改成你自己的域名。\n   \n   \n   # 3. 步骤\n\n\n# 步骤1: 安装Certbot\n\n参考这里安装Certbot。\n\n\n# 步骤2: 生成Let's Encrypt证书\n\n# certbot certonly --standalone -d myminio.com --staple-ocsp -m test@yourdomain.io --agree-tos\n\n\n\n# 步骤3: 验证证书\n\n列出/etc/letsencrypt/live/myminio.com里的证书。\n\n$ ls -l /etc/letsencrypt/live/myminio.comtotal 4lrwxrwxrwx 1 root root  37 Aug  2 09:58 cert.pem -> ../../archive/myminio.com/cert4.pemlrwxrwxrwx 1 root root  38 Aug  2 09:58 chain.pem -> ../../archive/myminio.com/chain4.pemlrwxrwxrwx 1 root root  42 Aug  2 09:58 fullchain.pem -> ../../archive/myminio.com/fullchain4.pemlrwxrwxrwx 1 root root  40 Aug  2 09:58 privkey.pem -> ../../archive/myminio.com/privkey4.pem-rw-r--r-- 1 root root 543 May 10 22:07 README\n\n\n\n# 步骤4: 使用证书给Minio Server设置SSL。\n\nCertbot生成的证书和key需要放到用户的home文件夹里。\n\n$ cp /etc/letsencrypt/live/myminio.com/fullchain.pem /home/user/.minio/certs/public.crt$ cp /etc/letsencrypt/live/myminio.com/privkey.pem /home/user/.minio/certs/private.key\n\n\n\n# 步骤5: 修改证书的ownership。\n\n$ sudo chown user:user /home/user/.minio/certs/private.key$ sudo chown user:user /home/user/.minio/certs/public.crt\n\n\n\n# 步骤6: 使用HTTPS启动Minio Server。\n\n启动Minio Server,使用443端口。\n\n$ sudo ./minio server --address \":443\" /mnt/data\n\n\n如果你用的是Minio Docker版，则你需要\n\n$ sudo docker run -p 443:443 -v /home/user/.minio:/root/.minio/ -v /home/user/data:/data minio/minio server --address \":443\" /data\n\n\n\n# 步骤7: 通过浏览器访问https://myminio.com。\n\n\n\n> 原文: https://docs.min.io/docs/generate-let-s-encypt-certificate-using-concert-for-minio.html",normalizedContent:"# 使用certbot生成let's encrypt证书\n\nlet's encrypt 是一个新的免费的，自动的，开源的认证中心。\n\ncertbot是let's encrypt的基于控制台的证书生成工具。\n\n本文我们将使用certbot生成let's encrypt证书。该证书将被部署在minio服务器中使用。\n\n\n# 1. 前提条件\n\n * 从这里下载并安装minio server。\n\n * 从这里下载并安装certbot。\n   \n   \n   # 2. 依赖\n\n * 执行certbot时，需要打开443端口并确保可以访问。\n\n * certbot需要有root权限，因为只有root才允许绑定1024以下的端口。\n\n * 本文我们将使用myminio.com这个域名，请在设置时改成你自己的域名。\n   \n   \n   # 3. 步骤\n\n\n# 步骤1: 安装certbot\n\n参考这里安装certbot。\n\n\n# 步骤2: 生成let's encrypt证书\n\n# certbot certonly --standalone -d myminio.com --staple-ocsp -m test@yourdomain.io --agree-tos\n\n\n\n# 步骤3: 验证证书\n\n列出/etc/letsencrypt/live/myminio.com里的证书。\n\n$ ls -l /etc/letsencrypt/live/myminio.comtotal 4lrwxrwxrwx 1 root root  37 aug  2 09:58 cert.pem -> ../../archive/myminio.com/cert4.pemlrwxrwxrwx 1 root root  38 aug  2 09:58 chain.pem -> ../../archive/myminio.com/chain4.pemlrwxrwxrwx 1 root root  42 aug  2 09:58 fullchain.pem -> ../../archive/myminio.com/fullchain4.pemlrwxrwxrwx 1 root root  40 aug  2 09:58 privkey.pem -> ../../archive/myminio.com/privkey4.pem-rw-r--r-- 1 root root 543 may 10 22:07 readme\n\n\n\n# 步骤4: 使用证书给minio server设置ssl。\n\ncertbot生成的证书和key需要放到用户的home文件夹里。\n\n$ cp /etc/letsencrypt/live/myminio.com/fullchain.pem /home/user/.minio/certs/public.crt$ cp /etc/letsencrypt/live/myminio.com/privkey.pem /home/user/.minio/certs/private.key\n\n\n\n# 步骤5: 修改证书的ownership。\n\n$ sudo chown user:user /home/user/.minio/certs/private.key$ sudo chown user:user /home/user/.minio/certs/public.crt\n\n\n\n# 步骤6: 使用https启动minio server。\n\n启动minio server,使用443端口。\n\n$ sudo ./minio server --address \":443\" /mnt/data\n\n\n如果你用的是minio docker版，则你需要\n\n$ sudo docker run -p 443:443 -v /home/user/.minio:/root/.minio/ -v /home/user/data:/data minio/minio server --address \":443\" /data\n\n\n\n# 步骤7: 通过浏览器访问https://myminio.com。\n\n\n\n> 原文: https://docs.min.io/docs/generate-let-s-encypt-certificate-using-concert-for-minio.html",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"使用 MinIO 服务器设置 Nginx 代理",frontmatter:{title:"使用 MinIO 服务器设置 Nginx 代理",date:"2022-02-09T11:30:40.000Z",permalink:"/pages/7acf5a/"},regularPath:"/12.MinIO/04.%E5%AE%9E%E6%88%98%E7%A7%98%E7%B1%8D/02.%E4%BD%BF%E7%94%A8%20MinIO%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%BE%E7%BD%AE%20Nginx%20%E4%BB%A3%E7%90%86.html",relativePath:"12.MinIO/04.实战秘籍/02.使用 MinIO 服务器设置 Nginx 代理.md",key:"v-84e3ebde",path:"/pages/7acf5a/",headers:[{level:2,title:"1. 先决条件",slug:"_1-先决条件",normalizedTitle:"1. 先决条件",charIndex:107},{level:2,title:"2. 安装",slug:"_2-安装",normalizedTitle:"2. 安装",charIndex:137},{level:2,title:"3. 配置",slug:"_3-配置",normalizedTitle:"3. 配置",charIndex:162},{level:3,title:"代理所有请求",slug:"代理所有请求",normalizedTitle:"代理所有请求",charIndex:172},{level:3,title:"基于桶的代理请求",slug:"基于桶的代理请求",normalizedTitle:"基于桶的代理请求",charIndex:1568},{level:2,title:"4. 步骤",slug:"_4-步骤",normalizedTitle:"4. 步骤",charIndex:2735},{level:3,title:"第一步：启动MinIO服务器。",slug:"第一步-启动minio服务器。",normalizedTitle:"第一步：启动minio服务器。",charIndex:2745},{level:3,title:"第 2 步：重启 Nginx 服务器。",slug:"第-2-步-重启-nginx-服务器。",normalizedTitle:"第 2 步：重启 nginx 服务器。",charIndex:2791},{level:2,title:"进一步探索",slug:"进一步探索",normalizedTitle:"进一步探索",charIndex:2844}],headersStr:"1. 先决条件 2. 安装 3. 配置 代理所有请求 基于桶的代理请求 4. 步骤 第一步：启动MinIO服务器。 第 2 步：重启 Nginx 服务器。 进一步探索",content:'# 使用 MinIO 服务器设置 Nginx 代理\n\nNginx 是一个开源的 Web 服务器和一个反向代理服务器。\n\n在这个秘籍中，我们将学习如何使用 MinIO Server 设置 Nginx 代理。\n\n\n# 1. 先决条件\n\n从这里安装 MinIO 服务器。\n\n\n# 2. 安装\n\n从这里安装 Nginx 。\n\n\n# 3. 配置\n\n\n# 代理所有请求\n\n将以下内容添加为文件/etc/nginx/sites-enabled，例如/etc/nginx/sites-enables/minio ，并删除default同一目录中的现有文件。\n\nserver {\n listen 80;\n server_name example.com;\n\n # To allow special characters in headers\n ignore_invalid_headers off;\n # Allow any size file to be uploaded.\n # Set to a value such as 1000m; to restrict file size to a specific value\n client_max_body_size 0;\n # To disable buffering\n proxy_buffering off;\n\n location / {\n   proxy_set_header X-Real-IP $remote_addr;\n   proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n   proxy_set_header X-Forwarded-Proto $scheme;\n   proxy_set_header Host $http_host;\n\n   proxy_connect_timeout 300;\n   # Default is HTTP/1, keepalive is only enabled in HTTP/1.1\n   proxy_http_version 1.1;\n   proxy_set_header Connection "";\n   chunked_transfer_encoding off;\n\n   proxy_pass http://localhost:9000; # If you are using docker-compose this would be the hostname i.e. minio\n   # Health Check endpoint might go here. See https://www.nginx.com/resources/wiki/modules/healthcheck/\n   # /minio/health/live;\n }\n}\n\n\n笔记：\n\n * 将 example.com 替换为您自己的主机名。\n * 替换http://localhost:9000 为您自己的服务器名称。\n * 添加client_max_body_size 1000m;的http情况下，为了能够上传大文件-只需相应地调整值。默认值1m对于大多数场景来说太低了。要禁用对客户端请求正文大小的检查，请设置client_max_body_size为0。\n * Nginx 默认缓冲响应。要禁止 Nginx 将 MinIO 响应缓冲到临时文件，请设置proxy_buffering off;. 这将改善客户端请求的第一个字节的时间。\n * Nginx 默认不允许特殊字符。设置ignore_invalid_headers off;为允许带有特殊字符的标题。\n\n\n# 基于桶的代理请求\n\n如果您想从同一个 nginx 端口为 Web 应用程序和 MinIO 提供服务，那么您可以使用基于路径的路由基于存储桶名称代理 MinIO 请求。对于 nginx，它使用该location指令，该指令还支持基于对象键模式匹配的代理拆分。\n\n # Proxy requests to the bucket "photos" to MinIO server running on port 9000\n location /photos/ {\n   proxy_set_header X-Real-IP $remote_addr;\n   proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n   proxy_set_header X-Forwarded-Proto $scheme;\n   proxy_set_header Host $http_host;\n\n   proxy_connect_timeout 300;\n   # Default is HTTP/1, keepalive is only enabled in HTTP/1.1\n   proxy_http_version 1.1;\n   proxy_set_header Connection "";\n   chunked_transfer_encoding off;\n\n   proxy_pass http://localhost:9000;\n }\n\n # Proxy any other request to the application server running on port 9001\n location / {\n   proxy_set_header X-Real-IP $remote_addr;\n   proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n   proxy_set_header X-Forwarded-Proto $scheme;\n   proxy_set_header Host $http_host;\n\n   proxy_connect_timeout 300;\n   # Default is HTTP/1, keepalive is only enabled in HTTP/1.1\n   proxy_http_version 1.1;\n   proxy_set_header Connection "";\n   chunked_transfer_encoding off;\n\n   proxy_pass http://localhost:9001;\n }\n\n\n\n# 4. 步骤\n\n\n# 第一步：启动MinIO服务器。\n\nminio server /mydatadir\n\n\n\n# 第 2 步：重启 Nginx 服务器。\n\nsudo service nginx restart\n\n\n\n# 进一步探索\n\n有关各种 MinIO 和 Nginx 配置选项，请参阅此博客文章。',normalizedContent:'# 使用 minio 服务器设置 nginx 代理\n\nnginx 是一个开源的 web 服务器和一个反向代理服务器。\n\n在这个秘籍中，我们将学习如何使用 minio server 设置 nginx 代理。\n\n\n# 1. 先决条件\n\n从这里安装 minio 服务器。\n\n\n# 2. 安装\n\n从这里安装 nginx 。\n\n\n# 3. 配置\n\n\n# 代理所有请求\n\n将以下内容添加为文件/etc/nginx/sites-enabled，例如/etc/nginx/sites-enables/minio ，并删除default同一目录中的现有文件。\n\nserver {\n listen 80;\n server_name example.com;\n\n # to allow special characters in headers\n ignore_invalid_headers off;\n # allow any size file to be uploaded.\n # set to a value such as 1000m; to restrict file size to a specific value\n client_max_body_size 0;\n # to disable buffering\n proxy_buffering off;\n\n location / {\n   proxy_set_header x-real-ip $remote_addr;\n   proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n   proxy_set_header x-forwarded-proto $scheme;\n   proxy_set_header host $http_host;\n\n   proxy_connect_timeout 300;\n   # default is http/1, keepalive is only enabled in http/1.1\n   proxy_http_version 1.1;\n   proxy_set_header connection "";\n   chunked_transfer_encoding off;\n\n   proxy_pass http://localhost:9000; # if you are using docker-compose this would be the hostname i.e. minio\n   # health check endpoint might go here. see https://www.nginx.com/resources/wiki/modules/healthcheck/\n   # /minio/health/live;\n }\n}\n\n\n笔记：\n\n * 将 example.com 替换为您自己的主机名。\n * 替换http://localhost:9000 为您自己的服务器名称。\n * 添加client_max_body_size 1000m;的http情况下，为了能够上传大文件-只需相应地调整值。默认值1m对于大多数场景来说太低了。要禁用对客户端请求正文大小的检查，请设置client_max_body_size为0。\n * nginx 默认缓冲响应。要禁止 nginx 将 minio 响应缓冲到临时文件，请设置proxy_buffering off;. 这将改善客户端请求的第一个字节的时间。\n * nginx 默认不允许特殊字符。设置ignore_invalid_headers off;为允许带有特殊字符的标题。\n\n\n# 基于桶的代理请求\n\n如果您想从同一个 nginx 端口为 web 应用程序和 minio 提供服务，那么您可以使用基于路径的路由基于存储桶名称代理 minio 请求。对于 nginx，它使用该location指令，该指令还支持基于对象键模式匹配的代理拆分。\n\n # proxy requests to the bucket "photos" to minio server running on port 9000\n location /photos/ {\n   proxy_set_header x-real-ip $remote_addr;\n   proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n   proxy_set_header x-forwarded-proto $scheme;\n   proxy_set_header host $http_host;\n\n   proxy_connect_timeout 300;\n   # default is http/1, keepalive is only enabled in http/1.1\n   proxy_http_version 1.1;\n   proxy_set_header connection "";\n   chunked_transfer_encoding off;\n\n   proxy_pass http://localhost:9000;\n }\n\n # proxy any other request to the application server running on port 9001\n location / {\n   proxy_set_header x-real-ip $remote_addr;\n   proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n   proxy_set_header x-forwarded-proto $scheme;\n   proxy_set_header host $http_host;\n\n   proxy_connect_timeout 300;\n   # default is http/1, keepalive is only enabled in http/1.1\n   proxy_http_version 1.1;\n   proxy_set_header connection "";\n   chunked_transfer_encoding off;\n\n   proxy_pass http://localhost:9001;\n }\n\n\n\n# 4. 步骤\n\n\n# 第一步：启动minio服务器。\n\nminio server /mydatadir\n\n\n\n# 第 2 步：重启 nginx 服务器。\n\nsudo service nginx restart\n\n\n\n# 进一步探索\n\n有关各种 minio 和 nginx 配置选项，请参阅此博客文章。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Minio部署快速入门",frontmatter:{title:"Minio部署快速入门",date:"2022-02-09T11:30:18.000Z",permalink:"/pages/542516/"},regularPath:"/12.MinIO/05.Minio%E9%83%A8%E7%BD%B2/01.Minio%E9%83%A8%E7%BD%B2%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html",relativePath:"12.MinIO/05.Minio部署/01.Minio部署快速入门.md",key:"v-427cb080",path:"/pages/542516/",headers:[{level:2,title:"为什么说Minio是云原生的（cloud-native）?",slug:"为什么说minio是云原生的-cloud-native",normalizedTitle:"为什么说minio是云原生的（cloud-native）?",charIndex:511}],headersStr:"为什么说Minio是云原生的（cloud-native）?",content:"# Minio部署快速入门\n\nMinio是一个[云原生](https://baike.baidu.com/item/Cloud Native/19865304?fr=aladdin)的应用程序，旨在在多租户环境中以可持续的方式进行扩展。Orchestration平台为Minio的扩展提供了非常好的支撑。以下是各种orchestration平台的Minio部署文档Minio is a cloud-native application designed to scale in a sustainable manner in multi-tenant environments. Orchestration platforms provide perfect launchpad for Minio to scale. Below is the list of Minio deployment documents for various orchestration platforms:\n\nORCHESTRATION平台\nDocker Swarm\nDocker Compose\nKubernetes\nDC/OS\n\n\n# 为什么说Minio是云原生的（cloud-native）?\n\n云原生这个词代表的是一些思想的集合，比如微服务部署，可伸缩，而不是说把一个单体应用改造成容器部署。一个云原生的应用在设计时就考虑了移植性和可伸缩性，而且可以通过简单的复制即可实现水平扩展。现在兴起的编排平台，想Swarm,Kubernetes,以及DC/OS，让大规模集群的复制和管理变得前所未有的简单，哪里不会点哪里。\n\n容器提供了隔离的应用执行环境，编排（orchestration）平台通过容器管理以及复制功能提供了无缝的扩展。Minio继承了这些，针对每个租户提供了存储环境的隔离。\n\nMinio是建立在云原生的基础上，有纠删码、分布式和共享存储这些特性。Minio专注于并且只专注于存储，而且做的还不错。它可以通过编排平台复制一个Minio实例就实现了水平扩展。\n\n> 在一个云原生环境中，伸缩性不是应用的一个功能而是编排平台的功能。\n\n现在的应用、数据库，key-store这些，很多都已经部署在容器中，并且通过编排平台进行管理。Minio提供了一个健壮的、可伸缩、AWS S3兼容的对象存储，这是Minio的立身之本，凭此在云原生应用中占据一席之地。\n\n\n\n> 原文: https://docs.min.io/docs/minio-deployment-quickstart-guide.html",normalizedContent:"# minio部署快速入门\n\nminio是一个[云原生](https://baike.baidu.com/item/cloud native/19865304?fr=aladdin)的应用程序，旨在在多租户环境中以可持续的方式进行扩展。orchestration平台为minio的扩展提供了非常好的支撑。以下是各种orchestration平台的minio部署文档minio is a cloud-native application designed to scale in a sustainable manner in multi-tenant environments. orchestration platforms provide perfect launchpad for minio to scale. below is the list of minio deployment documents for various orchestration platforms:\n\norchestration平台\ndocker swarm\ndocker compose\nkubernetes\ndc/os\n\n\n# 为什么说minio是云原生的（cloud-native）?\n\n云原生这个词代表的是一些思想的集合，比如微服务部署，可伸缩，而不是说把一个单体应用改造成容器部署。一个云原生的应用在设计时就考虑了移植性和可伸缩性，而且可以通过简单的复制即可实现水平扩展。现在兴起的编排平台，想swarm,kubernetes,以及dc/os，让大规模集群的复制和管理变得前所未有的简单，哪里不会点哪里。\n\n容器提供了隔离的应用执行环境，编排（orchestration）平台通过容器管理以及复制功能提供了无缝的扩展。minio继承了这些，针对每个租户提供了存储环境的隔离。\n\nminio是建立在云原生的基础上，有纠删码、分布式和共享存储这些特性。minio专注于并且只专注于存储，而且做的还不错。它可以通过编排平台复制一个minio实例就实现了水平扩展。\n\n> 在一个云原生环境中，伸缩性不是应用的一个功能而是编排平台的功能。\n\n现在的应用、数据库，key-store这些，很多都已经部署在容器中，并且通过编排平台进行管理。minio提供了一个健壮的、可伸缩、aws s3兼容的对象存储，这是minio的立身之本，凭此在云原生应用中占据一席之地。\n\n\n\n> 原文: https://docs.min.io/docs/minio-deployment-quickstart-guide.html",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"使用Docker Swarm部署Minio",frontmatter:{title:"使用Docker Swarm部署Minio",date:"2022-02-09T11:30:40.000Z",permalink:"/pages/b1ea49/"},regularPath:"/12.MinIO/05.Minio%E9%83%A8%E7%BD%B2/02.%E4%BD%BF%E7%94%A8Docker%20Swarm%E9%83%A8%E7%BD%B2Minio.html",relativePath:"12.MinIO/05.Minio部署/02.使用Docker Swarm部署Minio.md",key:"v-03ae78c4",path:"/pages/b1ea49/",headers:[{level:2,title:"1. 前提条件",slug:"_1-前提条件",normalizedTitle:"1. 前提条件",charIndex:275},{level:2,title:"2. 创建Swarm",slug:"_2-创建swarm",normalizedTitle:"2. 创建swarm",charIndex:486},{level:2,title:"3. 为Minio创建Docker secret",slug:"_3-为minio创建docker-secret",normalizedTitle:"3. 为minio创建docker secret",charIndex:820},{level:2,title:"4. 部署分布式minio服务",slug:"_4-部署分布式minio服务",normalizedTitle:"4. 部署分布式minio服务",charIndex:998},{level:2,title:"4. 删除分布式Minio services",slug:"_4-删除分布式minio-services",normalizedTitle:"4. 删除分布式minio services",charIndex:1331},{level:3,title:"注意事项",slug:"注意事项",normalizedTitle:"注意事项",charIndex:1627},{level:3,title:"了解更多",slug:"了解更多",normalizedTitle:"了解更多",charIndex:2163}],headersStr:"1. 前提条件 2. 创建Swarm 3. 为Minio创建Docker secret 4. 部署分布式minio服务 4. 删除分布式Minio services 注意事项 了解更多",content:'# 使用Docker Swarm部署Minio\n\nDocker Engine在Swarm模式下提供集群管理和编排功能。 Minio服务器可以在Swarm的分布式模式下轻松部署，创建一个多租户，高可用性和可扩展的对象存储。\n\n从Docker Engine v1.13.0 (Docker Compose v3.0)开始, Docker Swarm和Compose 二者cross-compatible。这允许将Compose file用作在Swarm上部署服务的模板。 我们使用Docker Compose file创建分布式Minio设置。\n\n\n# 1. 前提条件\n\n * 熟悉Swarm mode key concepts.\n\n * Docker engine v1.13.0运行在[networked host machines]集群上(\n   \n   https://docs.docker.com/engine/swarm/swarm-tutorial/#/three-networked-host-machines\n   \n   ).\n   \n   \n   # 2. 创建Swarm\n\n在管理节点上创建一个swarm,请运行下面的命令\n\ndocker swarm init --advertise-addr <MANAGER-IP>\n\n\n一旦swarm初使化了，你可以看到下面的响应信息\n\ndocker swarm join \\  --token  SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\  192.168.99.100:2377\n\n\n你现在可以运行上述命令添加worker节点到swarm。更多关于创建swarm的细节步骤，请访问Docker documentation site.\n\n\n# 3. 为Minio创建Docker secret\n\necho "AKIAIOSFODNN7EXAMPLE" | docker secret create access_key -echo "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY" | docker secret create secret_key -\n\n\n\n# 4. 部署分布式minio服务\n\n在你的Swarm master上下载Docker Compose file ，然后运行下面的命令\n\ndocker stack deploy --compose-file=docker-compose-secrets.yaml minio_stack\n\n\n这将把Compose file里描述的服务部署为Docker stackminio_stack。 更多 docker stack 命令参考。\n\n在stack成功部署之后，你可以通过Minio Client mc 或者浏览器访问http://[Node_Public_IP_Address]:[Expose_Port_on_Host]来访问你的Minio server。\n\n\n# 4. 删除分布式Minio services\n\n删除分布式Minio services以及相关的网络，请运行下面的命令\n\ndocker stack rm minio_stack\n\n\nSwarm不会自动删除为Minio服务创建的host volumes,如果下次新的Minio服务不熟到swarm上，可能会导致损坏。因此，我们建议手动删除所有Minio使用的volumes。为此，到每一个swarm的节点上，列出所有的volumes\n\ndocker volume ls\n\n\n然后删除minio_stack volumes\n\ndocker volume rm volume_name \n\n\n\n# 注意事项\n\n * 默认情况下Docker Compose file使用的是最新版的Minio server的Docker镜像，你可以修改image tag来拉取指定版本的Minio Docker image.\n * 默认情况下会创建4个minio实例，你可以添加更多的Minio服务（最多总共16个）到你的Minio Comose deployment。添加一个服务\n   * 复制服务定义并适当地更改新服务的名称。\n   * 更新每个服务中的命令部分。\n   * 更新要为新服务公开的端口号。 另外，请确保分配给新服务的端口尚未使用。 关于分布式Minio的更多资料，请访问这里.\n * 默认情况下，Minio服务使用的是local volume driver. 更多配置选项，请访问Docker documentation 。\n * Docker compose file中的Minio服务使用的端口是9001到9004，这允许多个服务在主机上运行。更多配置选项，请访问Docker documentation.\n * Docker Swarm默认使用的是ingress做负载均衡，你可以跟据需要配置external load balancer based。\n\n\n# 了解更多\n\n * Docker Swarm mode概述\n * Minio Docker快速入门\n * 使用Docker Compose部署Minio\n * Minio纠删码快速入门\n\n> 原文: https://docs.min.io/docs/deploy-minio-on-docker-swarm.html',normalizedContent:'# 使用docker swarm部署minio\n\ndocker engine在swarm模式下提供集群管理和编排功能。 minio服务器可以在swarm的分布式模式下轻松部署，创建一个多租户，高可用性和可扩展的对象存储。\n\n从docker engine v1.13.0 (docker compose v3.0)开始, docker swarm和compose 二者cross-compatible。这允许将compose file用作在swarm上部署服务的模板。 我们使用docker compose file创建分布式minio设置。\n\n\n# 1. 前提条件\n\n * 熟悉swarm mode key concepts.\n\n * docker engine v1.13.0运行在[networked host machines]集群上(\n   \n   https://docs.docker.com/engine/swarm/swarm-tutorial/#/three-networked-host-machines\n   \n   ).\n   \n   \n   # 2. 创建swarm\n\n在管理节点上创建一个swarm,请运行下面的命令\n\ndocker swarm init --advertise-addr <manager-ip>\n\n\n一旦swarm初使化了，你可以看到下面的响应信息\n\ndocker swarm join \\  --token  swmtkn-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\  192.168.99.100:2377\n\n\n你现在可以运行上述命令添加worker节点到swarm。更多关于创建swarm的细节步骤，请访问docker documentation site.\n\n\n# 3. 为minio创建docker secret\n\necho "akiaiosfodnn7example" | docker secret create access_key -echo "wjalrxutnfemi/k7mdeng/bpxrficyexamplekey" | docker secret create secret_key -\n\n\n\n# 4. 部署分布式minio服务\n\n在你的swarm master上下载docker compose file ，然后运行下面的命令\n\ndocker stack deploy --compose-file=docker-compose-secrets.yaml minio_stack\n\n\n这将把compose file里描述的服务部署为docker stackminio_stack。 更多 docker stack 命令参考。\n\n在stack成功部署之后，你可以通过minio client mc 或者浏览器访问http://[node_public_ip_address]:[expose_port_on_host]来访问你的minio server。\n\n\n# 4. 删除分布式minio services\n\n删除分布式minio services以及相关的网络，请运行下面的命令\n\ndocker stack rm minio_stack\n\n\nswarm不会自动删除为minio服务创建的host volumes,如果下次新的minio服务不熟到swarm上，可能会导致损坏。因此，我们建议手动删除所有minio使用的volumes。为此，到每一个swarm的节点上，列出所有的volumes\n\ndocker volume ls\n\n\n然后删除minio_stack volumes\n\ndocker volume rm volume_name \n\n\n\n# 注意事项\n\n * 默认情况下docker compose file使用的是最新版的minio server的docker镜像，你可以修改image tag来拉取指定版本的minio docker image.\n * 默认情况下会创建4个minio实例，你可以添加更多的minio服务（最多总共16个）到你的minio comose deployment。添加一个服务\n   * 复制服务定义并适当地更改新服务的名称。\n   * 更新每个服务中的命令部分。\n   * 更新要为新服务公开的端口号。 另外，请确保分配给新服务的端口尚未使用。 关于分布式minio的更多资料，请访问这里.\n * 默认情况下，minio服务使用的是local volume driver. 更多配置选项，请访问docker documentation 。\n * docker compose file中的minio服务使用的端口是9001到9004，这允许多个服务在主机上运行。更多配置选项，请访问docker documentation.\n * docker swarm默认使用的是ingress做负载均衡，你可以跟据需要配置external load balancer based。\n\n\n# 了解更多\n\n * docker swarm mode概述\n * minio docker快速入门\n * 使用docker compose部署minio\n * minio纠删码快速入门\n\n> 原文: https://docs.min.io/docs/deploy-minio-on-docker-swarm.html',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"使用Kubernetes部署Minio",frontmatter:{title:"使用Kubernetes部署Minio",date:"2022-02-09T11:30:40.000Z",permalink:"/pages/01eafc/"},regularPath:"/12.MinIO/05.Minio%E9%83%A8%E7%BD%B2/03.%E4%BD%BF%E7%94%A8Kubernetes%E9%83%A8%E7%BD%B2Minio.html",relativePath:"12.MinIO/05.Minio部署/03.使用Kubernetes部署Minio.md",key:"v-931f934a",path:"/pages/01eafc/",headers:[{level:2,title:"1. 前提条件",slug:"_1-前提条件",normalizedTitle:"1. 前提条件",charIndex:317},{level:2,title:"2. 使用Helm Chart部署Minio",slug:"_2-使用helm-chart部署minio",normalizedTitle:"2. 使用helm chart部署minio",charIndex:502},{level:3,title:"配置",slug:"配置",normalizedTitle:"配置",charIndex:579},{level:3,title:"分布式Minio",slug:"分布式minio",normalizedTitle:"分布式minio",charIndex:2475},{level:3,title:"Shared Minio",slug:"shared-minio",normalizedTitle:"shared minio",charIndex:2965},{level:3,title:"持久化",slug:"持久化",normalizedTitle:"持久化",charIndex:2852},{level:2,title:"3. 使用Helm更新Minio版本",slug:"_3-使用helm更新minio版本",normalizedTitle:"3. 使用helm更新minio版本",charIndex:3525},{level:2,title:"4. 卸载Chart",slug:"_4-卸载chart",normalizedTitle:"4. 卸载chart",charIndex:3807},{level:3,title:"提示",slug:"提示",normalizedTitle:"提示",charIndex:3926}],headersStr:"1. 前提条件 2. 使用Helm Chart部署Minio 配置 分布式Minio Shared Minio 持久化 3. 使用Helm更新Minio版本 4. 卸载Chart 提示",content:'# 使用Kubernetes部署Minio\n\nKubernetes的部署和状态集提供了在独立，分布式或共享模式下部署Minio服务器的完美平台。 在Kubernetes上部署Minio有多种选择，您可以选择最适合您的。\n\n * Minio Helm Chart通过一个简单的命令即可提供自定义而且简单的Minio部署。更多关于Minio Helm部署的资料，请访问这里.\n * 你也可以浏览Kubernetes Minio示例 ，通过.yaml文件来部署Minio。\n * 如果您想在Kubernetes上开始使用Minio，而无需创建真正的容器集群，您也可以使用Minikube deploy Minio locally。\n\n\n# 1. 前提条件\n\n * 默认standaline模式下，需要开启Beta API的Kubernetes 1.4+。\n\n * distributed 模式，需要开启Beta API的Kubernetes 1.5+。\n\n * 底层支持PV provisioner。\n\n * 你的K8s集群里需要有Helm package manager\n\ninstalled\n\n。\n\n\n# 2. 使用Helm Chart部署Minio\n\n安装 Minio chart\n\n$ helm install stable/minio\n\n\n以上命令以默认配置在Kubernetes群集上部署Minio。 以下部分列出了Minio图表的所有可配置参数及其默认值。\n\n\n# 配置\n\n参数                         描述                                                  默认值\nimage                      Minio镜像名称                                           minio/minio\nimageTag                   Minio镜像tag. 可选值在 这里.                                RELEASE.2017-08-05T00-00-53Z\nimagePullPolicy            Image pull policy                                   Always\nmode                       Minio server模式 (standalone, shared或者 distributed)   standalone\nnumberOfNodes              节点数 (仅对分布式模式生效). 可选值 4 <= x <= 16                   4\naccessKey                  默认access key                                        AKIAIOSFODNN7EXAMPLE\nsecretKey                  默认secret key                                        wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nconfigPath                 默认配置文件路径                                            ~/.minio\nmountPath                  默认挂载路径                                              /export\nserviceType                Kubernetes service type                             LoadBalancer\nservicePort                Kubernetes端口                                        9000\npersistence.enabled        是否使用持久卷存储数据                                         true\npersistence.size           持久卷大小                                               10Gi\npersistence.storageClass   持久卷类型                                               generic\npersistence.accessMode     ReadWriteOnce 或者 ReadOnly                           ReadWriteOnce\nresources                  CPU/Memory 资源需求/限制                                  Memory: 256Mi, CPU: 100m\n\n你可以通过—set key=value[,key=value]给helm install。 比如,\n\n$ helm install --name my-release \\  --set persistence.size=100Gi \\    stable/minio\n\n\n上述命令部署了一个带上100G持久卷的Minio服务。\n\n或者，您可以提供一个YAML文件，用于在安装chart时指定参数值。 例如，\n\n$ helm install --name my-release -f values.yaml stable/minio\n\n\n\n# 分布式Minio\n\n默认情况下，此图表以独立模式提供Minio服务器。 要在分布式模式中配置Minio服务器，请将mode字段设置为distributed,\n\n$ helm install --set mode=distributed stable/minio\n\n\n上述命令部署了个带有4个节点的分布式Minio服务器。 要更改分布式Minio服务器中的节点数，请设置numberOfNodes属性。\n\n$ helm install --set mode=distributed,numberOfNodes=8 stable/minio\n\n\n上述命令部署了个带有8个节点的分布式Minio服务器。注意一下，numberOfNodes取值范围是[4,16]。\n\n# StatefulSet 限制，适用于分布式Minio\n\n * StatefulSets需要持久化存储，所以如果 mode设成 distributed的话，persistence.enabled参数不生效。\n\n * 卸载分布式Minio版本时，需要手动删除与StatefulSet关联的卷。\n   \n   \n   # Shared Minio\n\n如需采用shared mode部署Minio, 将mode 设为shared,\n\n$ helm install --set mode=shared stable/minio\n\n\n上述命令规定了4个Minio服务器节点，一个存储。 要更改共享的Minio部署中的节点数，请设置numberOfNodes字段，\n\n$ helm install --set mode=shared,numberOfNodes=8 stable/minio\n\n\n上述命令规定了Minio服务有8个节点，采用shared模式。\n\n\n# 持久化\n\n这里规定了PersistentVolumeClaim并将相应的持久卷挂载到默认位置/export。 您需要Kubernetes集群中的物理存储才能使其工作。 如果您宁愿使用emptyDir，请通过以下方式禁用PersistentVolumeClaim：\n\n$ helm install --set persistence.enabled=false stable/minio\n\n\n> "当Pod分配给节点时，首先创建一个emptyDir卷，只要该节点上的Pod正在运行，它就会存在。 当某个Pod由于任何原因从节点中删除时，emptyDir中的数据将永久删除。"\n\n\n# 3. 使用Helm更新Minio版本\n\n您可以更新现有的Minio Helm Release以使用较新的Minio Docker镜像。 为此，请使用helm upgrade命令：\n\n$ helm upgrade --set imageTag=<replace-with-minio-docker-image-tag> <helm-release-name> stable/minio\n\n\n如果更新成功，你可以看到下面的输出信息\n\nRelease "your-helm-release" has been upgraded. Happy Helming!\n\n\n\n# 4. 卸载Chart\n\n假设你的版本被命名为my-release，使用下面的命令删除它：\n\n$ helm delete my-release\n\n\n该命令删除与chart关联的所有Kubernetes组件，并删除该release。\n\n\n# 提示\n\n * 在Kubernetes群集中运行的chart的实例称为release。 安装chart后，Helm会自动分配唯一的release名称。 你也可以通过下面的命令设置你心仪的名称：\n   \n   $ helm install --name my-release stable/minio\n   \n\n * 为了覆盖默认的秘钥，可在运行helm install时将access key和secret key做为参数传进去。\n   \n   $ helm install --set accessKey=myaccesskey,secretKey=mysecretkey \\  stable/minio\n   ',normalizedContent:'# 使用kubernetes部署minio\n\nkubernetes的部署和状态集提供了在独立，分布式或共享模式下部署minio服务器的完美平台。 在kubernetes上部署minio有多种选择，您可以选择最适合您的。\n\n * minio helm chart通过一个简单的命令即可提供自定义而且简单的minio部署。更多关于minio helm部署的资料，请访问这里.\n * 你也可以浏览kubernetes minio示例 ，通过.yaml文件来部署minio。\n * 如果您想在kubernetes上开始使用minio，而无需创建真正的容器集群，您也可以使用minikube deploy minio locally。\n\n\n# 1. 前提条件\n\n * 默认standaline模式下，需要开启beta api的kubernetes 1.4+。\n\n * distributed 模式，需要开启beta api的kubernetes 1.5+。\n\n * 底层支持pv provisioner。\n\n * 你的k8s集群里需要有helm package manager\n\ninstalled\n\n。\n\n\n# 2. 使用helm chart部署minio\n\n安装 minio chart\n\n$ helm install stable/minio\n\n\n以上命令以默认配置在kubernetes群集上部署minio。 以下部分列出了minio图表的所有可配置参数及其默认值。\n\n\n# 配置\n\n参数                         描述                                                  默认值\nimage                      minio镜像名称                                           minio/minio\nimagetag                   minio镜像tag. 可选值在 这里.                                release.2017-08-05t00-00-53z\nimagepullpolicy            image pull policy                                   always\nmode                       minio server模式 (standalone, shared或者 distributed)   standalone\nnumberofnodes              节点数 (仅对分布式模式生效). 可选值 4 <= x <= 16                   4\naccesskey                  默认access key                                        akiaiosfodnn7example\nsecretkey                  默认secret key                                        wjalrxutnfemi/k7mdeng/bpxrficyexamplekey\nconfigpath                 默认配置文件路径                                            ~/.minio\nmountpath                  默认挂载路径                                              /export\nservicetype                kubernetes service type                             loadbalancer\nserviceport                kubernetes端口                                        9000\npersistence.enabled        是否使用持久卷存储数据                                         true\npersistence.size           持久卷大小                                               10gi\npersistence.storageclass   持久卷类型                                               generic\npersistence.accessmode     readwriteonce 或者 readonly                           readwriteonce\nresources                  cpu/memory 资源需求/限制                                  memory: 256mi, cpu: 100m\n\n你可以通过—set key=value[,key=value]给helm install。 比如,\n\n$ helm install --name my-release \\  --set persistence.size=100gi \\    stable/minio\n\n\n上述命令部署了一个带上100g持久卷的minio服务。\n\n或者，您可以提供一个yaml文件，用于在安装chart时指定参数值。 例如，\n\n$ helm install --name my-release -f values.yaml stable/minio\n\n\n\n# 分布式minio\n\n默认情况下，此图表以独立模式提供minio服务器。 要在分布式模式中配置minio服务器，请将mode字段设置为distributed,\n\n$ helm install --set mode=distributed stable/minio\n\n\n上述命令部署了个带有4个节点的分布式minio服务器。 要更改分布式minio服务器中的节点数，请设置numberofnodes属性。\n\n$ helm install --set mode=distributed,numberofnodes=8 stable/minio\n\n\n上述命令部署了个带有8个节点的分布式minio服务器。注意一下，numberofnodes取值范围是[4,16]。\n\n# statefulset 限制，适用于分布式minio\n\n * statefulsets需要持久化存储，所以如果 mode设成 distributed的话，persistence.enabled参数不生效。\n\n * 卸载分布式minio版本时，需要手动删除与statefulset关联的卷。\n   \n   \n   # shared minio\n\n如需采用shared mode部署minio, 将mode 设为shared,\n\n$ helm install --set mode=shared stable/minio\n\n\n上述命令规定了4个minio服务器节点，一个存储。 要更改共享的minio部署中的节点数，请设置numberofnodes字段，\n\n$ helm install --set mode=shared,numberofnodes=8 stable/minio\n\n\n上述命令规定了minio服务有8个节点，采用shared模式。\n\n\n# 持久化\n\n这里规定了persistentvolumeclaim并将相应的持久卷挂载到默认位置/export。 您需要kubernetes集群中的物理存储才能使其工作。 如果您宁愿使用emptydir，请通过以下方式禁用persistentvolumeclaim：\n\n$ helm install --set persistence.enabled=false stable/minio\n\n\n> "当pod分配给节点时，首先创建一个emptydir卷，只要该节点上的pod正在运行，它就会存在。 当某个pod由于任何原因从节点中删除时，emptydir中的数据将永久删除。"\n\n\n# 3. 使用helm更新minio版本\n\n您可以更新现有的minio helm release以使用较新的minio docker镜像。 为此，请使用helm upgrade命令：\n\n$ helm upgrade --set imagetag=<replace-with-minio-docker-image-tag> <helm-release-name> stable/minio\n\n\n如果更新成功，你可以看到下面的输出信息\n\nrelease "your-helm-release" has been upgraded. happy helming!\n\n\n\n# 4. 卸载chart\n\n假设你的版本被命名为my-release，使用下面的命令删除它：\n\n$ helm delete my-release\n\n\n该命令删除与chart关联的所有kubernetes组件，并删除该release。\n\n\n# 提示\n\n * 在kubernetes群集中运行的chart的实例称为release。 安装chart后，helm会自动分配唯一的release名称。 你也可以通过下面的命令设置你心仪的名称：\n   \n   $ helm install --name my-release stable/minio\n   \n\n * 为了覆盖默认的秘钥，可在运行helm install时将access key和secret key做为参数传进去。\n   \n   $ helm install --set accesskey=myaccesskey,secretkey=mysecretkey \\  stable/minio\n   ',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"在 Docker Compose 上部署 MinIO",frontmatter:{title:"在 Docker Compose 上部署 MinIO",date:"2022-02-09T11:30:40.000Z",permalink:"/pages/46c251/"},regularPath:"/12.MinIO/05.Minio%E9%83%A8%E7%BD%B2/04.%E5%9C%A8%20Docker%20Compose%20%E4%B8%8A%E9%83%A8%E7%BD%B2%20MinIO.html",relativePath:"12.MinIO/05.Minio部署/04.在 Docker Compose 上部署 MinIO.md",key:"v-4b4c35dc",path:"/pages/46c251/",headers:[{level:2,title:"1. 先决条件",slug:"_1-先决条件",normalizedTitle:"1. 先决条件",charIndex:217},{level:2,title:"2. 在 Docker Compose 上运行分布式 MinIO",slug:"_2-在-docker-compose-上运行分布式-minio",normalizedTitle:"2. 在 docker compose 上运行分布式 minio",charIndex:284},{level:3,title:"GNU/Linux 和 macOS",slug:"gnu-linux-和-macos",normalizedTitle:"gnu/linux 和 macos",charIndex:468},{level:3,title:"Windows",slug:"windows",normalizedTitle:"windows",charIndex:597},{level:3,title:"Notes",slug:"notes",normalizedTitle:"notes",charIndex:824},{level:3,title:"进一步探索",slug:"进一步探索",normalizedTitle:"进一步探索",charIndex:1126}],headersStr:"1. 先决条件 2. 在 Docker Compose 上运行分布式 MinIO GNU/Linux 和 macOS Windows Notes 进一步探索",content:"# 在 Docker Compose 上部署 MinIO\n\nDocker Compose 允许定义和运行单主机、多容器 Docker 应用程序。\n\n使用 Compose，您可以使用 Compose 文件来配置 MinIO 服务。然后，使用单个命令，您可以从您的配置中创建和启动所有分布式 MinIO 实例。分布式 MinIO 实例将部署在同一主机上的多个容器中。这是基于分布式 MinIO 设置开发、测试和登台环境的好方法。\n\n\n# 1. 先决条件\n\n * 熟悉Docker Compose。\n * Docker 安装在您的机器上。从这里下载相关的安装程序。\n\n\n# 2. 在 Docker Compose 上运行分布式 MinIO\n\n要在 Docker Compose 上部署分布式 MinIO，请将docker-compose.yaml和nginx.conf下载到您当前的工作目录。请注意，Docker Compose 会拉取 MinIO Docker 映像，因此无需显式下载 MinIO 二进制文件。然后运行以下命令之一\n\n\n# GNU/Linux 和 macOS\n\ndocker-compose pull\ndocker-compose up\n\n\n或者\n\ndocker stack deploy --compose-file docker-compose.yaml minio\n\n\n\n# Windows\n\ndocker-compose.exe pull\ndocker-compose.exe up\n\n\n或者\n\ndocker stack deploy --compose-file docker-compose.yaml minio\n\n\n现在可以在主机上的 9000 端口访问分布式实例，继续访问 Web 浏览器http://127.0.0.1:9000/。这里 4 个 MinIO 服务器实例通过 Nginx 负载均衡进行反向代理。\n\n\n# Notes\n\n * 默认情况下，Docker Compose 文件使用最新 MinIO 服务器版本的 Docker 镜像。您可以更改镜像标签以拉取特定的MinIO Docker 镜像。\n\n * 默认创建了 4 个 minio 分布式实例。您可以向 MinIO Compose 部署添加更多 MinIO 服务（最多 16 个）。添加服务\n   \n   * 复制服务定义并适当更改新服务的名称。\n   * 更新每个服务中的命令部分。\n   * 在 Nginx 配置文件中的 upstream 指令中添加一个新的 MinIO 服务器实例。\n   \n   在此处阅读有关分布式 MinIO 的更多信息。\n\n\n# 进一步探索\n\n * Docker Compose 概述\n * MinIO Docker 快速入门指南\n * MinIO 纠删码快速入门指南",normalizedContent:"# 在 docker compose 上部署 minio\n\ndocker compose 允许定义和运行单主机、多容器 docker 应用程序。\n\n使用 compose，您可以使用 compose 文件来配置 minio 服务。然后，使用单个命令，您可以从您的配置中创建和启动所有分布式 minio 实例。分布式 minio 实例将部署在同一主机上的多个容器中。这是基于分布式 minio 设置开发、测试和登台环境的好方法。\n\n\n# 1. 先决条件\n\n * 熟悉docker compose。\n * docker 安装在您的机器上。从这里下载相关的安装程序。\n\n\n# 2. 在 docker compose 上运行分布式 minio\n\n要在 docker compose 上部署分布式 minio，请将docker-compose.yaml和nginx.conf下载到您当前的工作目录。请注意，docker compose 会拉取 minio docker 映像，因此无需显式下载 minio 二进制文件。然后运行以下命令之一\n\n\n# gnu/linux 和 macos\n\ndocker-compose pull\ndocker-compose up\n\n\n或者\n\ndocker stack deploy --compose-file docker-compose.yaml minio\n\n\n\n# windows\n\ndocker-compose.exe pull\ndocker-compose.exe up\n\n\n或者\n\ndocker stack deploy --compose-file docker-compose.yaml minio\n\n\n现在可以在主机上的 9000 端口访问分布式实例，继续访问 web 浏览器http://127.0.0.1:9000/。这里 4 个 minio 服务器实例通过 nginx 负载均衡进行反向代理。\n\n\n# notes\n\n * 默认情况下，docker compose 文件使用最新 minio 服务器版本的 docker 镜像。您可以更改镜像标签以拉取特定的minio docker 镜像。\n\n * 默认创建了 4 个 minio 分布式实例。您可以向 minio compose 部署添加更多 minio 服务（最多 16 个）。添加服务\n   \n   * 复制服务定义并适当更改新服务的名称。\n   * 更新每个服务中的命令部分。\n   * 在 nginx 配置文件中的 upstream 指令中添加一个新的 minio 服务器实例。\n   \n   在此处阅读有关分布式 minio 的更多信息。\n\n\n# 进一步探索\n\n * docker compose 概述\n * minio docker 快速入门指南\n * minio 纠删码快速入门指南",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"RabbitMQ 快速入门",frontmatter:{title:"RabbitMQ 快速入门",date:"2022-02-09T11:31:48.000Z",permalink:"/pages/1a684e/"},regularPath:"/13.RabbitMQ/01.%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/02.RabbitMQ%20%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html",relativePath:"13.RabbitMQ/01.快速入门/02.RabbitMQ 快速入门.md",key:"v-0a92eeec",path:"/pages/1a684e/",headers:[{level:2,title:"思维导图",slug:"思维导图",normalizedTitle:"思维导图",charIndex:20},{level:2,title:"什么是消息队列",slug:"什么是消息队列",normalizedTitle:"什么是消息队列",charIndex:31},{level:2,title:"为什么使用消息队列",slug:"为什么使用消息队列",normalizedTitle:"为什么使用消息队列",charIndex:224},{level:2,title:"RabbitMQ 的特点",slug:"rabbitmq-的特点",normalizedTitle:"rabbitmq 的特点",charIndex:824},{level:2,title:"RabbitMQ 的工作原理",slug:"rabbitmq-的工作原理",normalizedTitle:"rabbitmq 的工作原理",charIndex:1303}],headersStr:"思维导图 什么是消息队列 为什么使用消息队列 RabbitMQ 的特点 RabbitMQ 的工作原理",content:"# RabbitMQ 快速入门\n\n\n# 思维导图\n\n\n\n\n# 什么是消息队列\n\n消息指的是两个应用间传递的数据。数据的类型有很多种形式，可能只包含文本字符串，也可能包含嵌入对象。\n\n“消息队列(Message Queue)”是在消息的传输过程中保存消息的容器。在消息队列中，通常有生产者和消费者两个角色。生产者只负责发送数据到消息队列，谁从消息队列中取出数据处理，他不管。消费者只负责从消息队列中取出数据处理，他不管这是谁发送的数据。\n\n\n\n\n# 为什么使用消息队列\n\n主要有三个作用：\n\n * 解耦。如图所示。假设有系统B、C、D都需要系统A的数据，于是系统A调用三个方法发送数据到B、C、D。这时，系统D不需要了，那就需要在系统A把相关的代码删掉。假设这时有个新的系统E需要数据，这时系统A又要增加调用系统E的代码。为了降低这种强耦合，就可以使用MQ，系统A只需要把数据发送到MQ，其他系统如果需要数据，则从MQ中获取即可。\n\n\n\n * 异步。如图所示。一个客户端请求发送进来，系统A会调用系统B、C、D三个系统，同步请求的话，响应时间就是系统A、B、C、D的总和，也就是800ms。如果使用MQ，系统A发送数据到MQ，然后就可以返回响应给客户端，不需要再等待系统B、C、D的响应，可以大大地提高性能。对于一些非必要的业务，比如发送短信，发送邮件等等，就可以采用MQ。\n\n\n\n * 削峰。如图所示。这其实是MQ一个很重要的应用。假设系统A在某一段时间请求数暴增，有5000个请求发送过来，系统A这时就会发送5000条SQL进入MySQL进行执行，MySQL对于如此庞大的请求当然处理不过来，MySQL就会崩溃，导致系统瘫痪。如果使用MQ，系统A不再是直接发送SQL到数据库，而是把数据发送到MQ，MQ短时间积压数据是可以接受的，然后由消费者每次拉取2000条进行处理，防止在请求峰值时期大量的请求直接发送到MySQL导致系统崩溃。\n\n\n\n\n# RabbitMQ 的特点\n\nRabbitMQ是一款使用Erlang语言开发的，实现AMQP(高级消息队列协议)的开源消息中间件。首先要知道一些RabbitMQ的特点，官网可查：\n\n * 可靠性。支持持久化，传输确认，发布确认等保证了MQ的可靠性。\n\n * 灵活的分发消息策略。这应该是RabbitMQ的一大特点。在消息进入MQ前由Exchange(交换机)进行路由消息。分发消息策略有：简单模式、工作队列模式、发布订阅模式、路由模式、通配符模式。\n\n * 支持集群。多台RabbitMQ服务器可以组成一个集群，形成一个逻辑Broker。\n\n * 多种协议。RabbitMQ支持多种消息队列协议，比如 STOMP、MQTT 等等。\n\n * 支持多种语言客户端。RabbitMQ几乎支持所有常用编程语言，包括 Java、.NET、Ruby 等等。\n\n * 可视化管理界面。RabbitMQ提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker。\n\n * 插件机制。RabbitMQ提供了许多插件，可以通过插件进行扩展，也可以编写自己的插件。\n\n\n# RabbitMQ 的工作原理\n\n下图是RabbitMQ的基本结构：\n\n\n\n组成部分说明：\n\n * Broker：消息队列服务进程，此进程包括两个部分：Exchange和Queue\n\n * Exchange：消息队列交换机，按一定的规则将消息路由转发到某个队列，对消息进行过虑。\n\n * Queue：消息队列，存储消息的队列，消息到达队列并转发给指定的\n\n * Producer：消息生产者，即生产方客户端，生产方客户端将消息发送\n\n * Consumer：消息消费者，即消费方客户端，接收MQ转发的消息。\n\n生产者发送消息流程：\n\n1、生产者和Broker建立TCP连接。\n\n2、生产者和Broker建立通道。\n\n3、生产者通过通道消息发送给Broker，由Exchange将消息进行转发。\n\n4、Exchange将消息转发到指定的Queue（队列）\n\n消费者接收消息流程：\n\n1、消费者和Broker建立TCP连接\n\n2、消费者和Broker建立通道\n\n3、消费者监听指定的Queue（队列）\n\n4、当有消息到达Queue时Broker默认将消息推送给消费者。\n\n5、消费者接收到消息。\n\n6、ack回复",normalizedContent:"# rabbitmq 快速入门\n\n\n# 思维导图\n\n\n\n\n# 什么是消息队列\n\n消息指的是两个应用间传递的数据。数据的类型有很多种形式，可能只包含文本字符串，也可能包含嵌入对象。\n\n“消息队列(message queue)”是在消息的传输过程中保存消息的容器。在消息队列中，通常有生产者和消费者两个角色。生产者只负责发送数据到消息队列，谁从消息队列中取出数据处理，他不管。消费者只负责从消息队列中取出数据处理，他不管这是谁发送的数据。\n\n\n\n\n# 为什么使用消息队列\n\n主要有三个作用：\n\n * 解耦。如图所示。假设有系统b、c、d都需要系统a的数据，于是系统a调用三个方法发送数据到b、c、d。这时，系统d不需要了，那就需要在系统a把相关的代码删掉。假设这时有个新的系统e需要数据，这时系统a又要增加调用系统e的代码。为了降低这种强耦合，就可以使用mq，系统a只需要把数据发送到mq，其他系统如果需要数据，则从mq中获取即可。\n\n\n\n * 异步。如图所示。一个客户端请求发送进来，系统a会调用系统b、c、d三个系统，同步请求的话，响应时间就是系统a、b、c、d的总和，也就是800ms。如果使用mq，系统a发送数据到mq，然后就可以返回响应给客户端，不需要再等待系统b、c、d的响应，可以大大地提高性能。对于一些非必要的业务，比如发送短信，发送邮件等等，就可以采用mq。\n\n\n\n * 削峰。如图所示。这其实是mq一个很重要的应用。假设系统a在某一段时间请求数暴增，有5000个请求发送过来，系统a这时就会发送5000条sql进入mysql进行执行，mysql对于如此庞大的请求当然处理不过来，mysql就会崩溃，导致系统瘫痪。如果使用mq，系统a不再是直接发送sql到数据库，而是把数据发送到mq，mq短时间积压数据是可以接受的，然后由消费者每次拉取2000条进行处理，防止在请求峰值时期大量的请求直接发送到mysql导致系统崩溃。\n\n\n\n\n# rabbitmq 的特点\n\nrabbitmq是一款使用erlang语言开发的，实现amqp(高级消息队列协议)的开源消息中间件。首先要知道一些rabbitmq的特点，官网可查：\n\n * 可靠性。支持持久化，传输确认，发布确认等保证了mq的可靠性。\n\n * 灵活的分发消息策略。这应该是rabbitmq的一大特点。在消息进入mq前由exchange(交换机)进行路由消息。分发消息策略有：简单模式、工作队列模式、发布订阅模式、路由模式、通配符模式。\n\n * 支持集群。多台rabbitmq服务器可以组成一个集群，形成一个逻辑broker。\n\n * 多种协议。rabbitmq支持多种消息队列协议，比如 stomp、mqtt 等等。\n\n * 支持多种语言客户端。rabbitmq几乎支持所有常用编程语言，包括 java、.net、ruby 等等。\n\n * 可视化管理界面。rabbitmq提供了一个易用的用户界面，使得用户可以监控和管理消息 broker。\n\n * 插件机制。rabbitmq提供了许多插件，可以通过插件进行扩展，也可以编写自己的插件。\n\n\n# rabbitmq 的工作原理\n\n下图是rabbitmq的基本结构：\n\n\n\n组成部分说明：\n\n * broker：消息队列服务进程，此进程包括两个部分：exchange和queue\n\n * exchange：消息队列交换机，按一定的规则将消息路由转发到某个队列，对消息进行过虑。\n\n * queue：消息队列，存储消息的队列，消息到达队列并转发给指定的\n\n * producer：消息生产者，即生产方客户端，生产方客户端将消息发送\n\n * consumer：消息消费者，即消费方客户端，接收mq转发的消息。\n\n生产者发送消息流程：\n\n1、生产者和broker建立tcp连接。\n\n2、生产者和broker建立通道。\n\n3、生产者通过通道消息发送给broker，由exchange将消息进行转发。\n\n4、exchange将消息转发到指定的queue（队列）\n\n消费者接收消息流程：\n\n1、消费者和broker建立tcp连接\n\n2、消费者和broker建立通道\n\n3、消费者监听指定的queue（队列）\n\n4、当有消息到达queue时broker默认将消息推送给消费者。\n\n5、消费者接收到消息。\n\n6、ack回复",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"RabbitMQ 简介",frontmatter:{title:"RabbitMQ 简介",date:"2022-02-09T11:31:48.000Z",permalink:"/pages/d710e9/"},regularPath:"/13.RabbitMQ/01.%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/01.RabbitMQ%20%E7%AE%80%E4%BB%8B.html",relativePath:"13.RabbitMQ/01.快速入门/01.RabbitMQ 简介.md",key:"v-c205d0b2",path:"/pages/d710e9/",headers:[{level:2,title:"什么是消息队列",slug:"什么是消息队列",normalizedTitle:"什么是消息队列",charIndex:537},{level:2,title:"消息队列应用场景",slug:"消息队列应用场景",normalizedTitle:"消息队列应用场景",charIndex:705},{level:2,title:"AMQP和JMS",slug:"amqp和jms",normalizedTitle:"amqp和jms",charIndex:973},{level:2,title:"常见MQ产品",slug:"常见mq产品",normalizedTitle:"常见mq产品",charIndex:1169}],headersStr:"什么是消息队列 消息队列应用场景 AMQP和JMS 常见MQ产品",content:"# RabbitMQ 简介\n\n在介绍RabbitMQ之前，我们先来看下面一个电商项目的场景：\n\n * 商品的原始数据保存在数据库中，增删改查都在数据库中完成。\n * 搜索服务数据来源是索引库 Elasticsearch，如果数据库商品发生变化，索引库数据不能及时更新。\n * 商品详情做了页面静态化处理，静态页面数据也不会随着数据库商品更新而变化。\n\n如果我们在后台修改了商品的价格，搜索页面和商品详情页显示的依然是旧的价格，这样显然不对。该如何解决？\n\n我们可能会想到这么做：\n\n * 方案1：每当后台对商品做增删改操作，同时修改索引库数据及更新静态页面。\n * 方案2：搜索服务和商品页面静态化服务对外提供操作接口，后台在商品增删改后，调用接口。\n\n这两种方案都有个严重的问题：就是代码耦合，后台服务中需要嵌入搜索和商品页面服务，违背了微服务的独立原则。\n\n这时，我们就会采用另外一种解决办法，那就是消息队列！\n\n商品服务对商品增删改以后，无需去操作索引库和静态页面，只需向MQ发送一条消息（比如包含商品id的消息），也不关心消息被谁接收。搜索服务和静态页面服务监听MQ，接收消息，然后分别去处理索引库和静态页面（根据商品id去更新索引库和商品详情静态页面）。\n\n\n# 什么是消息队列\n\nMQ全称为Message Queue，即消息队列 。“消息队列”是在消息的传输过程中保存消息的容器。它是典型的：生产者、消费者模型。生产者不断向消息队列中生产消息，消费者不断的从队列中获取消息。因为消息的生产和消费都是异步的，而且只关心消息的发送和接收，没有业务逻辑的侵入，这样就实现了生产者和消费者的解耦。\n\n\n# 消息队列应用场景\n\n1、任务异步处理：\n\n高并发环境下，由于来不及同步处理，请求往往会发生堵塞，比如说，大量的insert，update之类的请求同时到达MySQL，直接导致无数的行锁表锁，甚至最后请求会堆积过多，从而触发too many connections错误。通过使用消息队列，我们可以异步处理请求，从而缓解系统的压力。将不需要同步处理的并且耗时长的操作由消息队列通知消息接收方进行异步处理。减少了应用程序的响应时间。\n\n2、应用程序解耦合：\n\nMQ相当于一个中介，生产方通过MQ与消费方交互，它将应用程序进行解耦合。\n\n\n# AMQP和JMS\n\nMQ是消息通信的模型，并发具体实现。现在实现MQ的有两种主流方式：AMQP、JMS。\n\n两者间的区别和联系：\n\n * JMS是定义了统一的接口，来对消息操作进行统一；AMQP是通过规定协议来统一数据交互的格式\n * JMS限定了必须使用Java语言；AMQP只是协议，不规定实现方式，因此是跨语言的。\n * JMS规定了两种消息模型；而AMQP的消息模型更加丰富\n\n\n# 常见MQ产品\n\n * ActiveMQ：基于JMS\n * RabbitMQ：基于AMQP协议，erlang语言开发，稳定性好\n * RocketMQ：基于JMS，阿里巴巴产品，目前交由Apache基金会\n * Kafka：分布式消息系统，高吞吐量",normalizedContent:"# rabbitmq 简介\n\n在介绍rabbitmq之前，我们先来看下面一个电商项目的场景：\n\n * 商品的原始数据保存在数据库中，增删改查都在数据库中完成。\n * 搜索服务数据来源是索引库 elasticsearch，如果数据库商品发生变化，索引库数据不能及时更新。\n * 商品详情做了页面静态化处理，静态页面数据也不会随着数据库商品更新而变化。\n\n如果我们在后台修改了商品的价格，搜索页面和商品详情页显示的依然是旧的价格，这样显然不对。该如何解决？\n\n我们可能会想到这么做：\n\n * 方案1：每当后台对商品做增删改操作，同时修改索引库数据及更新静态页面。\n * 方案2：搜索服务和商品页面静态化服务对外提供操作接口，后台在商品增删改后，调用接口。\n\n这两种方案都有个严重的问题：就是代码耦合，后台服务中需要嵌入搜索和商品页面服务，违背了微服务的独立原则。\n\n这时，我们就会采用另外一种解决办法，那就是消息队列！\n\n商品服务对商品增删改以后，无需去操作索引库和静态页面，只需向mq发送一条消息（比如包含商品id的消息），也不关心消息被谁接收。搜索服务和静态页面服务监听mq，接收消息，然后分别去处理索引库和静态页面（根据商品id去更新索引库和商品详情静态页面）。\n\n\n# 什么是消息队列\n\nmq全称为message queue，即消息队列 。“消息队列”是在消息的传输过程中保存消息的容器。它是典型的：生产者、消费者模型。生产者不断向消息队列中生产消息，消费者不断的从队列中获取消息。因为消息的生产和消费都是异步的，而且只关心消息的发送和接收，没有业务逻辑的侵入，这样就实现了生产者和消费者的解耦。\n\n\n# 消息队列应用场景\n\n1、任务异步处理：\n\n高并发环境下，由于来不及同步处理，请求往往会发生堵塞，比如说，大量的insert，update之类的请求同时到达mysql，直接导致无数的行锁表锁，甚至最后请求会堆积过多，从而触发too many connections错误。通过使用消息队列，我们可以异步处理请求，从而缓解系统的压力。将不需要同步处理的并且耗时长的操作由消息队列通知消息接收方进行异步处理。减少了应用程序的响应时间。\n\n2、应用程序解耦合：\n\nmq相当于一个中介，生产方通过mq与消费方交互，它将应用程序进行解耦合。\n\n\n# amqp和jms\n\nmq是消息通信的模型，并发具体实现。现在实现mq的有两种主流方式：amqp、jms。\n\n两者间的区别和联系：\n\n * jms是定义了统一的接口，来对消息操作进行统一；amqp是通过规定协议来统一数据交互的格式\n * jms限定了必须使用java语言；amqp只是协议，不规定实现方式，因此是跨语言的。\n * jms规定了两种消息模型；而amqp的消息模型更加丰富\n\n\n# 常见mq产品\n\n * activemq：基于jms\n * rabbitmq：基于amqp协议，erlang语言开发，稳定性好\n * rocketmq：基于jms，阿里巴巴产品，目前交由apache基金会\n * kafka：分布式消息系统，高吞吐量",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"RabbitMQ 消息模型",frontmatter:{title:"RabbitMQ 消息模型",date:"2022-02-09T11:31:48.000Z",permalink:"/pages/05a61a/"},regularPath:"/13.RabbitMQ/01.%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/03.RabbitMQ%20%E6%B6%88%E6%81%AF%E6%A8%A1%E5%9E%8B.html",relativePath:"13.RabbitMQ/01.快速入门/03.RabbitMQ 消息模型.md",key:"v-50673fcc",path:"/pages/05a61a/",headers:[{level:2,title:"基本消息模型",slug:"基本消息模型",normalizedTitle:"基本消息模型",charIndex:22},{level:2,title:"消息确认机制（ACK）",slug:"消息确认机制-ack",normalizedTitle:"消息确认机制（ack）",charIndex:32},{level:3,title:"演示手动ACK",slug:"演示手动ack",normalizedTitle:"演示手动ack",charIndex:49},{level:2,title:"work消息模型",slug:"work消息模型",normalizedTitle:"work消息模型",charIndex:60},{level:2,title:"Publish/subscribe（交换机类型：Fanout，也称为广播）",slug:"publish-subscribe-交换机类型-fanout-也称为广播",normalizedTitle:"publish/subscribe（交换机类型：fanout，也称为广播）",charIndex:72},{level:3,title:"生产者",slug:"生产者",normalizedTitle:"生产者",charIndex:115},{level:3,title:"消费者1 （注册成功发给短信服务）",slug:"消费者1-注册成功发给短信服务",normalizedTitle:"消费者1 （注册成功发给短信服务）",charIndex:124},{level:3,title:"消费者2（注册成功发给邮件服务）",slug:"消费者2-注册成功发给邮件服务",normalizedTitle:"消费者2（注册成功发给邮件服务）",charIndex:147},{level:3,title:"思考：",slug:"思考",normalizedTitle:"思考：",charIndex:169},{level:2,title:"Routing 路由模型（交换机类型：direct）",slug:"routing-路由模型-交换机类型-direct",normalizedTitle:"routing 路由模型（交换机类型：direct）",charIndex:176},{level:3,title:"消费者1",slug:"消费者1",normalizedTitle:"消费者1",charIndex:124},{level:3,title:"消费者2",slug:"消费者2",normalizedTitle:"消费者2",charIndex:147},{level:2,title:"Topics 通配符模式（交换机类型：topics）",slug:"topics-通配符模式-交换机类型-topics",normalizedTitle:"topics 通配符模式（交换机类型：topics）",charIndex:226},{level:3,title:"生产者",slug:"生产者-2",normalizedTitle:"生产者",charIndex:115},{level:3,title:"消费者1",slug:"消费者1-2",normalizedTitle:"消费者1",charIndex:124},{level:3,title:"消费者2",slug:"消费者2-2",normalizedTitle:"消费者2",charIndex:147},{level:2,title:"RPC",slug:"rpc",normalizedTitle:"rpc",charIndex:285},{level:3,title:"分享两道面试题：",slug:"分享两道面试题",normalizedTitle:"分享两道面试题：",charIndex:294},{level:3,title:"交换机持久化",slug:"交换机持久化",normalizedTitle:"交换机持久化",charIndex:308},{level:3,title:"队列持久化",slug:"队列持久化",normalizedTitle:"队列持久化",charIndex:320},{level:3,title:"消息持久化",slug:"消息持久化",normalizedTitle:"消息持久化",charIndex:331}],headersStr:"基本消息模型 消息确认机制（ACK） 演示手动ACK work消息模型 Publish/subscribe（交换机类型：Fanout，也称为广播） 生产者 消费者1 （注册成功发给短信服务） 消费者2（注册成功发给邮件服务） 思考： Routing 路由模型（交换机类型：direct） 消费者1 消费者2 Topics 通配符模式（交换机类型：topics） 生产者 消费者1 消费者2 RPC 分享两道面试题： 交换机持久化 队列持久化 消息持久化",content:'# RabbitMQ 消息模型\n\n\n\n * 基本消息模型\n * 消息确认机制（ACK）\n   * 演示手动ACK\n * work消息模型\n * Publish/subscribe（交换机类型：Fanout，也称为广播）\n   * 生产者\n   * 消费者1 （注册成功发给短信服务）\n   * 消费者2（注册成功发给邮件服务）\n   * 思考：\n * Routing 路由模型（交换机类型：direct）\n   * 消费者1\n   * 消费者2\n * Topics 通配符模式（交换机类型：topics）\n   * 生产者\n   * 消费者1\n   * 消费者2\n * RPC\n   * 分享两道面试题：\n   * 交换机持久化\n   * 队列持久化\n   * 消息持久化\n\n\n\n\n# 基本消息模型\n\n在上图的模型中，有以下概念：\n\n\n\n * P：生产者，也就是要发送消息的程序\n\n * C：消费者：消息的接受者，会一直等待消息到来。\n\n * queue：消息队列，图中红色部分。可以缓存消息；生产者向其中投递消息，消费者从其中取出消息。\n\n生产者\n\n新建一个maven工程，添加amqp-client依赖。\n\n<dependency>\n    <groupId>com.rabbitmq</groupId>\n    <artifactId>amqp-client</artifactId>\n    <version>5.14.0</version>\n</dependency>\n\n\n连接工具类：\n\npackage com.pigxcloud.rabbitmq.demo;\n\nimport com.rabbitmq.client.Connection;\nimport com.rabbitmq.client.ConnectionFactory;\n\n/**\n * RabbitMQ 连接工具\n *\n * @author heyuq\n * @date 2021/12/06\n */\npublic class ConnectionUtil {\n\n    /**\n     * 获得RabbitMQ连接\n     *\n     * @return com.rabbitmq.client.Connection\n     * @throws Exception\n     */\n    public static Connection getConnection() throws Exception {\n\n        // 创建连接工厂\n        ConnectionFactory factory = new ConnectionFactory();\n        // 设置服务地址\n        factory.setHost("192.168.100.100");\n        // 设置端口\n        factory.setPort(5672);\n        // 设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq\n        factory.setVirtualHost("/");\n        // 设置账号信息，用户名、密码\n        factory.setUsername("rabbit");\n        factory.setPassword("rabbit123");\n\n        // 通过工厂获取连接\n        return factory.newConnection();\n    }\n}\n\n\n\n生产者发送消息：\n\npackage com.pigxcloud.rabbitmq.demo;\n\n\nimport com.rabbitmq.client.Channel;\nimport com.rabbitmq.client.Connection;\n\n/**\n * 生产者发送消息\n *\n * @author heyuq\n * @date 2021/12/06\n */\npublic class Send {\n\n    /**\n     * 定义消息队列名称\n     */\n    private final static String QUEUE_NAME = "simple_queue";\n\n    public static void main(String[] args) throws Exception {\n        // 1、获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 2、从连接中创建通道，使用通道才能完成消息相关的操作\n        Channel channel = connection.createChannel();\n        /**\n         * 3、声明（创建）队列\n         * 参数：String queue, boolean durable, boolean exclusive, boolean autoDelete, Map<String, Object> arguments\n         * 1、queue 队列名称\n         * 2、durable 是否持久化，如果持久化，mq重启后队列还在\n         * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建\n         * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除）\n         * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间\n         */\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        // 4、消息内容\n        String message = "Hello World!";\n        /**\n         * 向指定的队列中发送消息\n         * 参数：String exchange, String routingKey, BasicProperties props, byte[] body\n         * 1、exchange，交换机，如果不指定将使用mq的默认交换机（设置为""）\n         * 2、routingKey，路由key，交换机根据路由key来将消息转发到指定的队列，如果使用默认交换机，routingKey设置为队列的名称\n         * 3、props，消息的属性\n         * 4、body，消息内容\n         */\n        channel.basicPublish("", QUEUE_NAME, null, message.getBytes());\n        System.out.println(" [x] Sent \'" + message + "\'");\n\n        //关闭通道和连接(资源关闭最好用try-catch-finally语句处理)\n        channel.close();\n        connection.close();\n    }\n}\n\n\n控制台：\n\n"D:\\Program Files\\Java\\jdk-11.0.13\\bin\\java.exe" ...\n [x] Sent \'Hello World!\'\n\n\nweb管理页面：服务器地址/端口号 （本地：127.0.0.1:15672，默认用户及密码：rabbit/rabbit123）\n\n\n\n点击队列名称，进入详情页，可以查看消息：\n\n\n\n消费者接收消息\n\npackage com.pigxcloud.rabbitmq.demo;\n\nimport com.rabbitmq.client.*;\n\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\n\n/**\n * 消费者接收消息\n *\n * @author heyuq\n * @date 2021/12/06\n */\npublic class Recv {\n\n    /**\n     * 定义消息队列名称\n     */\n    private final static String QUEUE_NAME = "simple_queue";\n\n    public static void main(String[] args) throws Exception {\n        // 1、获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 2、从连接中创建通道，使用通道才能完成消息相关的操作\n        Channel channel = connection.createChannel();\n        /**\n         * 3、声明（创建）队列\n         * 参数：String queue, boolean durable, boolean exclusive, boolean autoDelete, Map<String, Object> arguments\n         * 1、queue 队列名称\n         * 2、durable 是否持久化，如果持久化，mq重启后队列还在\n         * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建\n         * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除）\n         * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间\n         */\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n\n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n\n            /**\n             * 当接收到消息后此方法将被调用\n             * @param consumerTag  消费者标签，用来标识消费者的，在监听队列时设置channel.basicConsume\n             * @param envelope 信封，通过envelope\n             * @param properties 消息属性\n             * @param body 消息内容\n             * @throws IOException\n             */\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\n                //交换机\n                String exchange = envelope.getExchange();\n                //消息id，mq在channel中用来标识消息的id，可用于确认消息已接收\n                long deliveryTag = envelope.getDeliveryTag();\n                // body 即消息体\n                String msg = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [x] received : " + msg + "!");\n            }\n        };\n\n        // 监听队列，第二个参数：是否自动进行消息确认。\n        //参数：String queue, boolean autoAck, Consumer callback\n        /**\n         * 参数明细：\n         * 1、queue 队列名称\n         * 2、autoAck 自动回复，当消费者接收到消息后要告诉mq消息已接收，如果将此参数设置为tru表示会自动回复mq，如果设置为false要通过编程实现回复\n         * 3、callback，消费方法，当消费者接收到消息要执行的方法\n         */\n        channel.basicConsume(QUEUE_NAME, true, consumer);\n    }\n}\n\n\n\n控制台打印：\n\n"D:\\Program Files\\Java\\jdk-11.0.13\\bin\\java.exe" ...\n [x] Received \'Hello World!\'\n\n\n再看看队列的消息,已经被消费了\n\n\n\n我们发现，消费者已经获取了消息，但是程序没有停止，一直在监听队列中是否有新的消息。一旦有新的消息进入队列，就会立即打印。\n\n\n# 消息确认机制（ACK）\n\n通过刚才的案例可以看出，消息一旦被消费者接收，队列中的消息就会被删除。\n\n那么问题来了：RabbitMQ怎么知道消息被接收了呢？\n\n如果消费者领取消息后，还没执行操作就挂掉了呢？或者抛出了异常？消息消费失败，但是RabbitMQ无从得知，这样消息就丢失了！\n\n因此，RabbitMQ有一个ACK机制。当消费者获取消息后，会向RabbitMQ发送回执ACK，告知消息已经被接收。不过这种回执ACK分两种情况：\n\n * 自动ACK：消息一旦被接收，消费者自动发送ACK\n\n * 手动ACK：消息接收后，不会发送ACK，需要手动调用\n\n大家觉得哪种更好呢？\n\n这需要看消息的重要性：\n\n * 如果消息不太重要，丢失也没有影响，那么自动ACK会比较方便\n\n * 如果消息非常重要，不容丢失。那么最好在消费完成后手动ACK，否则接收消息后就自动ACK，RabbitMQ就会把消息从队列中删除。如果此时消费者宕机，那么消息就丢失了。\n\n我们之前的测试都是自动ACK的，如果要手动ACK，需要改动我们的代码：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n\npackage com.pigxcloud.rabbitmq.demo;\n\nimport com.rabbitmq.client.*;\n\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\n\n/**\n * 消息确认机制（ACK）\n *\n * @author heyuq\n * @date 2021/12/06\n */\npublic class Recv2 {\n\n    /**\n     * 定义消息队列名称\n     */\n    private final static String QUEUE_NAME = "simple_queue";\n\n    public static void main(String[] args) throws Exception {\n\n        // 1、获取到连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 2、创建通道\n        Channel channel = connection.createChannel();\n        // 3、声明队列\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        // 4、定义队列的消费者\n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties,\n                                       byte[] body) throws IOException {\n                // body 即消息体\n                String message = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [x] received : " + message + "!");\n                // 手动进行ACK\n                /*\n                 *  void basicAck(long deliveryTag, boolean multiple) throws IOException;\n                 *  deliveryTag:用来标识消息的id\n                 *  multiple：是否批量.true:将一次性ack所有小于deliveryTag的消息。\n                 */\n                channel.basicAck(envelope.getDeliveryTag(), false);\n            }\n        };\n        // 监听队列，第二个参数false，手动进行ACK\n        channel.basicConsume(QUEUE_NAME, false, consumer);\n    }\n}\n\n\n最后一行代码设置第二个参数为 false\n\nchannel.basicConsume(QUEUE_NAME, false, consumer);\n\n\n自动ACK存在的问题\n\n修改消费者，添加异常，如下：\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\npublic class Recv {\n    private final static String QUEUE_NAME = "simple_queue";\n    public static void main(String[] args) throws Exception {\n        Connection connection = ConnectionUtil.getConnection();\n        Channel channel = connection.createChannel();\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\n                // 测试自动ack,模拟异常\n                int i = 1 / 0;\n                String exchange = envelope.getExchange();\n                long deliveryTag = envelope.getDeliveryTag();\n                String msg = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [x] received : " + msg + "!");\n            }\n        };\n        channel.basicConsume(QUEUE_NAME, true, consumer);\n    }\n}\n\n\n生产者不做任何修改，直接运行，消息发送成功：\n\n\n\n运行消费者，程序抛出异常：\n\n\n\n管理界面：\n\n\n\n消费者抛出异常，但是消息依然被消费，实际上我们还没获取到消息。\n\n\n# 演示手动ACK\n\n重新运行生产者发送消息：\n\n\n\n同样，在手动进行ack前抛出异常，运行Recv2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\npublic class Recv2 {\n\n    private final static String QUEUE_NAME = "simple_queue";\n    \n    public static void main(String[] args) throws Exception {\n        Connection connection = ConnectionUtil.getConnection();\n        Channel channel = connection.createChannel();\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        \n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties,\n                                       byte[] body) throws IOException {\n                // 测试自动ack,模拟异常\n                int i = 1 / 0;\n                String message = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [x] received : " + message + "!");\n                // 手动进行ACK\n                channel.basicAck(envelope.getDeliveryTag(), false);\n            }\n        };\n        channel.basicConsume(QUEUE_NAME, false, consumer);\n    }\n}\n\n\n再看看管理界面：\n\n\n\n消息没有被消费掉！\n\n\n\n还有另外一种情况：修改消费者Recv2，把监听队列第二个参数自动改成手动,（去掉之前制造的异常） ，并且消费方法中没手动进行ACK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n\n\n\npublic class Recv2 {\n\n    private final static String QUEUE_NAME = "simple_queue";\n    \n    public static void main(String[] args) throws Exception {\n        Connection connection = ConnectionUtil.getConnection();\n        Channel channel = connection.createChannel();\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        \n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties,\n                                       byte[] body) throws IOException {\n                // 测试自动ack,模拟异常\n                // int i = 1 / 0;\n                String message = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [x] received : " + message + "!");\n                // 手动进行ACK\n                // channel.basicAck(envelope.getDeliveryTag(), false);\n            }\n        };\n        channel.basicConsume(QUEUE_NAME, false, consumer);\n    }\n}\n\n\n生产者代码不变，再次运行：\n\n\n\n运行消费者 ：\n\n\n\n但是，查看管理界面，发现：\n\n\n\n停掉消费者的程序，发现：\n\n\n\n这是因为虽然我们设置了手动ACK，但是代码中并没有进行消息确认！所以消息并未被真正消费掉。当我们关掉这个消费者，消息的状态再次变为Ready。\n\n正确的做法是：\n\n我们要在监听队列时设置第二个参数为false,代码中手动进行ACK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\npublic class Recv2 {\n\n    private final static String QUEUE_NAME = "simple_queue";\n    \n    public static void main(String[] args) throws Exception {\n        Connection connection = ConnectionUtil.getConnection();\n        Channel channel = connection.createChannel();\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        \n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties,\n                                       byte[] body) throws IOException {\n                // 测试自动ack,模拟异常\n                // int i = 1 / 0;\n                String message = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [x] received : " + message + "!");\n                // 手动进行ACK\n                channel.basicAck(envelope.getDeliveryTag(), false);\n            }\n        };\n        channel.basicConsume(QUEUE_NAME, false, consumer);\n    }\n}\n\n\n再次运行消费者，查看web管理页面：\n\n\n\n消费者消费成功！\n\n生产者避免数据丢失：https://www.cnblogs.com/vipstone/p/9350075.html\n\n\n# work消息模型\n\n工作队列或者竞争消费者模式\n\n\n\nwork queues与入门程序相比，多了一个消费端，两个消费端共同消费同一个队列中的消息，但是一个消息只能被一个消费者获取。\n\n这个消息模型在Web应用程序中特别有用，可以处理短的HTTP请求窗口中无法处理复杂的任务。\n\n接下来我们来模拟这个流程：\n\nP：生产者：任务的发布者\n\nC1：消费者1：领取任务并且完成任务，假设完成速度较慢（模拟耗时）\n\nC2：消费者2：领取任务并且完成任务，假设完成速度较快\n\n生产者\n\n生产者循环发送50条消息\n\npublic class Send {\n\n    /**\n     * 定义消息队列名称\n     */\n    private final static String QUEUE_NAME = "simple_queue";\n\n    public static void main(String[] args) throws Exception {\n        // 1、获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 2、从连接中创建通道，使用通道才能完成消息相关的操作\n        Channel channel = connection.createChannel();\n        // 3、声明队列\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        // 4、循环发布任务\n        for (int i = 0; i < 50; i++) {\n            // 消息内容\n            String message = "task ... " + i;\n            channel.basicPublish("", QUEUE_NAME, null, message.getBytes());\n            System.out.println(" [x] Sent \'" + message + "\'");\n\n            Thread.sleep(i * 2);\n        }\n\n        //关闭通道和连接\n        channel.close();\n        connection.close();\n    }\n}\n\n\n消费者1\n\npublic class Recv {\n\n    /**\n     * 定义消息队列名称\n     */\n    private final static String QUEUE_NAME = "simple_queue";\n\n    public static void main(String[] args) throws Exception {\n        // 1、获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 2、从连接中创建通道，使用通道才能完成消息相关的操作\n        Channel channel = connection.createChannel();\n        // 3、声明队列\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        // 4、实现消费方法\n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\n                // body 即消息体\n                String msg = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [消费者2] received : " + msg + "!");\n                try {\n                    //模拟任务耗时1s\n                    TimeUnit.SECONDS.sleep(1);\n                } catch (InterruptedException e) {\n                    e.printStackTrace();\n                }\n            }\n        };\n        // 监听队列，第二个参数：是否自动进行消息确认。\n        channel.basicConsume(QUEUE_NAME, true, consumer);\n    }\n}\n\n\n消费者2\n\n代码不贴了，与消费者1基本类似，只是消费者2没有设置消费耗时时间。\n\n接下来，两个消费者一同启动，然后发送50条消息：\n\n\n\n\n\n可以发现，两个消费者各自消费了不同25条消息，这就实现了任务的分发。\n\n能者多劳\n\n刚才的实现有问题吗？\n\n * 消费者1比消费者2的效率要低，一次任务的耗时较长\n * 然而两人最终消费的消息数量是一样的\n * 消费者2大量时间处于空闲状态，消费者1一直忙碌\n\n现在的状态属于是把任务平均分配，正确的做法应该是消费越快的人，消费的越多。\n\n怎么实现呢？\n\n通过 BasicQos 方法设置prefetchCount = 1。这样RabbitMQ就会使得每个Consumer在同一个时间点最多处理1个Message。换句话说，在接收到该Consumer的ack前，他它不会将新的Message分发给它。相反，它会将其分派给不是仍然忙碌的下一个Consumer。\n\n值得注意的是：prefetchCount在手动ack的情况下才生效，自动ack不生效。\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npublic class Recv {\n\n    private final static String QUEUE_NAME = "simple_queue";\n\n    public static void main(String[] args) throws Exception {\n        // 1、获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 2、从连接中创建通道，使用通道才能完成消息相关的操作\n        Channel channel = connection.createChannel();\n        // 3、声明队列\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        // 设置消费者同时只能处理一条消息，在手动ack下才生效\n        channel.basicQos(1);\n        // 4、实现消费方法\n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\n                // body 即消息体\n                String msg = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [消费者2] received : " + msg + "!");\n                try {\n                    //模拟任务耗时1s\n                    TimeUnit.SECONDS.sleep(5);\n                } catch (InterruptedException e) {\n                    e.printStackTrace();\n                }\n                channel.basicAck(envelope.getDeliveryTag(), false);\n            }\n        };\n        // 监听队列，第二个参数：是否自动进行消息确认。\n        channel.basicConsume(QUEUE_NAME, false, consumer);\n    }\n}\n\n\n再次测试：\n\n\n\n\n\n订阅模型分类\n\n说明下：\n\n1、一个生产者多个消费者\n\n2、每个消费者都有一个自己的队列\n\n3、生产者没有将消息直接发送给队列，而是发送给exchange(交换机、转发器)\n\n4、每个队列都需要绑定到交换机上\n\n5、生产者发送的消息，经过交换机到达队列，实现一个消息被多个消费者消费 例子：注册->发邮件、发短信\n\nX（Exchanges）：交换机一方面：接收生产者发送的消息。另一方面：知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。\n\nExchange类型有以下几种：\n\n * Fanout：广播，将消息交给所有绑定到交换机的队列\n\n * Direct：定向，把消息交给符合指定routing key 的队列\n\n * Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列\n\n * Header：header模式与routing不同的地方在于，header模式取消routingkey，使用header中的 key/value（键值对）匹配队列。\n\n * Header模式不展开了，感兴趣可以参考这篇文章https://blog.csdn.net/zhu_tianwei/article/details/40923131\n\n注意\n\nExchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！\n\n\n# Publish/subscribe（交换机类型：Fanout，也称为广播）\n\nPublish/subscribe模型示意图 ：\n\n\n\n\n# 生产者\n\n和前面两种模式不同：\n\n * 1） 声明Exchange，不再声明Queue\n * 2） 发送消息到Exchange，不再发送到Queue\n\npublic class Send {\n\n    /**\n     * 定义交换机名称\n     */\n    private final static String EXCHANGE_NAME = "test_fanout_exchange";\n\n    public static void main(String[] args) throws Exception {\n        // 获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 创建通道\n        Channel channel = connection.createChannel();\n        // 声明 exchange，指定类型为 fanout\n        channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.FANOUT);\n        \n        String message = "注册成功！";\n        channel.basicPublish(EXCHANGE_NAME, "", null, message.getBytes());\n        System.out.println(" [生产者] Sent \'" + message + "\'");\n\n        //关闭通道和连接\n        channel.close();\n        connection.close();\n    }\n}\n\n\n\n# 消费者1 （注册成功发给短信服务）\n\npublic class Recv {\n\n    /**\n     * 交换机名称\n     */\n    private final static String EXCHANGE_NAME = "test_fanout_exchange";\n    /**\n     * 定义消息队列名称\n     */\n    private final static String QUEUE_NAME = "test_fanout_queue_sms";\n\n    public static void main(String[] args) throws Exception {\n        // 获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 创建通道\n        Channel channel = connection.createChannel();\n        // 声明 exchange，指定类型为 fanout\n        channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.FANOUT);\n\n        // 声明队列\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        // 绑定队列到交换机\n        channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, "");\n        // 实现消费方法\n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) {\n                // body 即消息体\n                String msg = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [短信服务] received : " + msg + "!");\n            }\n        };\n        // 监听队列，自动返回完成\n        channel.basicConsume(QUEUE_NAME, true, consumer);\n    }\n}\n\n\n\n# 消费者2（注册成功发给邮件服务）\n\npublic class Recv2 {\n\n    /**\n     * 交换机名称\n     */\n    private final static String EXCHANGE_NAME = "test_fanout_exchange";\n    /**\n     * 定义消息队列名称\n     */\n    private final static String QUEUE_NAME = "test_fanout_queue_email";\n\n    public static void main(String[] args) throws Exception {\n\n        // 获取到连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 创建通道\n        Channel channel = connection.createChannel();\n        // 声明 exchange，指定类型为 fanout\n        channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.FANOUT);\n        // 声明队列\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        // 绑定队列到交换机\n        channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, "");\n        // 定义队列的消费者\n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) {\n                // body 即消息体\n                String message = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [邮件服务] received : " + message + "!");\n            }\n        };\n        // 监听队列，自动返回完成\n        channel.basicConsume(QUEUE_NAME, true, consumer);\n    }\n}\n\n\n我们运行两个消费者，然后发送1条消息：\n\n\n\n\n\n\n# 思考：\n\n1、publish/subscribe与work queues有什么区别。\n\n区别：\n\n1）work queues不用定义交换机，而publish/subscribe需要定义交换机。\n\n2）publish/subscribe的生产方是面向交换机发送消息，work queues的生产方是面向队列发送消息(底层使用默认交换机)。\n\n3）publish/subscribe需要设置队列和交换机的绑定，work queues不需要设置，实际上work queues会将队列绑定到默认的交换机 。\n\n相同点：\n\n所以两者实现的发布/订阅的效果是一样的，多个消费端监听同一个队列不会重复消费消息。\n\n2、实际工作用 publish/subscribe还是work queues。\n\n建议使用 publish/subscribe，发布订阅模式比工作队列模式更强大（也可以做到同一队列竞争），并且发布订阅模式可以指定自己专用的交换机。\n\n\n# Routing 路由模型（交换机类型：direct）\n\nRouting模型示意图：\n\n\n\nP：生产者，向Exchange发送消息，发送消息时，会指定一个routing key。\n\nX：Exchange（交换机），接收生产者的消息，然后把消息递交给 与routing key完全匹配的队列\n\nC1：消费者，其所在队列指定了需要routing key 为 error 的消息\n\nC2：消费者，其所在队列指定了需要routing key 为 info、error、warning 的消息\n\n接下来看代码：\n\npublic class Send {\n\n    /**\n     * 定义交换机名称\n     */\n    private final static String EXCHANGE_NAME = "test_direct_exchange";\n\n    public static void main(String[] args) throws Exception {\n        // 获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 创建通道\n        Channel channel = connection.createChannel();\n        // 声明 exchange，指定类型为 direct\n        channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.DIRECT);\n        String message = "【XXX开发平台】恭喜注册成功！";\n        // 发送消息，并且指定routing key 为：sms，只有短信服务才能接收到消息\n        channel.basicPublish(EXCHANGE_NAME, "sms", null, message.getBytes());\n        System.out.println(" [x] Sent \'" + message + "\'");\n\n        //关闭通道和连接\n        channel.close();\n        connection.close();\n    }\n}\n\n\n\n# 消费者1\n\npublic class Send {\n\n    /**\n     * 定义交换机名称\n     */\n    private final static String EXCHANGE_NAME = "test_direct_exchange";\n\n    public static void main(String[] args) throws Exception {\n        // 获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 创建通道\n        Channel channel = connection.createChannel();\n        // 声明 exchange，指定类型为 direct\n        channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.DIRECT);\n        String message = "【XXX开发平台】恭喜注册成功！";\n        // 发送消息，并且指定routing key 为：sms，只有短信服务才能接收到消息\n        channel.basicPublish(EXCHANGE_NAME, "sms", null, message.getBytes());\n        // channel.basicPublish(EXCHANGE_NAME, "email", null, message.getBytes());\n        System.out.println(" [x] Sent \'" + message + "\'");\n\n        //关闭通道和连接\n        channel.close();\n        connection.close();\n    }\n}\n\n\n\n# 消费者2\n\npublic class Recv {\n\n    /**\n     * 交换机名称\n     */\n    private final static String EXCHANGE_NAME = "test_direct_exchange";\n    /**\n     * 定义消息队列名称\n     */\n    private final static String QUEUE_NAME = "test_direct_queue_sms";\n\n    public static void main(String[] args) throws Exception {\n        // 获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 创建通道\n        Channel channel = connection.createChannel();\n        // 声明 exchange，指定类型为 fanout\n        channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.DIRECT);\n        // 声明队列\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        // 绑定队列到交换机，同时指定需要订阅的routing key，指定接收发送方指定routing key为sms的消息，可以指定多个。\n        channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, "sms");\n        // channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, "email");\n\n        // 实现消费方法\n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) {\n                // body 即消息体\n                String msg = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [短信服务] received : " + msg + "!");\n            }\n        };\n        // 监听队列，自动返回完成\n        channel.basicConsume(QUEUE_NAME, true, consumer);\n    }\n}\n\n\n我们发送sms的RoutingKey，发现结果：只有指定短信的消费者1收到消息了\n\n\n\n\n# Topics 通配符模式（交换机类型：topics）\n\nTopics模型示意图：\n\n\n\n每个消费者监听自己的队列，并且设置带统配符的routingkey,生产者将消息发给broker，由交换机根据routingkey来转发消息到指定的队列。\n\nRoutingkey一般都是有一个或者多个单词组成，多个单词之间以“.”分割，例如：inform.sms\n\n通配符规则：\n\n#：匹配一个或多个词\n\n*：匹配不多不少恰好1个词\n\n举例：\n\n> audit.#：能够匹配 audit.irs.corporate 或者 audit.irs\n\n> audit.*：只能匹配 audit.irs\n\n从示意图可知，我们将发送所有描述动物的消息。消息将使用由三个字（两个点）组成的Routing key发送。路由关键字中的第一个单词将描述速度，第二个颜色和第三个种类：<speed>.<color>.<species>。\n\n我们创建了三个绑定：Q1绑定了 *.orange.* ，Q2绑定了 *.*.rabbit 和 lazy.＃。\n\nQ1匹配所有的橙色动物。\n\nQ2匹配关于兔子以及懒惰动物的消息。\n\n下面做个小练习，假如生产者发送如下消息，会进入哪个队列：\n\nquick.orange.rabbit Q1 Q2 routingKey="quick.orange.rabbit" 的消息会同时路由到Q1与Q2\n\nlazy.orange.elephant Q1 Q2\n\nquick.orange.fox Q1\n\nlazy.pink.rabbit Q2 (值得注意的是，虽然这个routingKey与Q2的两个bindingKey都匹配，但是只会投递Q2一次)\n\nquick.brown.fox 不匹配任意队列，被丢弃\n\nquick.orange.male.rabbit 不匹配任意队列，被丢弃\n\norange 不匹配任意队列，被丢弃\n\n下面我们以指定Routing key="quick.orange.rabbit"为例，验证上面的答案\n\n\n# 生产者\n\npackage com.pigxcloud.rabbitmq.demo;\n\n\nimport com.rabbitmq.client.BuiltinExchangeType;\nimport com.rabbitmq.client.Channel;\nimport com.rabbitmq.client.Connection;\n\n/**\n * 生产者发送消息\n *\n * @author heyuq\n * @date 2021/12/06\n */\npublic class Send {\n\n    /**\n     * 定义交换机名称\n     */\n    private final static String EXCHANGE_NAME = "test_topic_exchange";\n\n    public static void main(String[] args) throws Exception {\n        // 获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 创建通道\n        Channel channel = connection.createChannel();\n        // 声明 exchange，指定类型为 topic\n        channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.TOPIC);\n        String message = "这是一只行动迅速的橙色的兔子";\n        // 发送消息，并且指定routing key 为：quick.orange.rabbit\n        channel.basicPublish(EXCHANGE_NAME, "quick.orange.rabbit", null, message.getBytes());\n        System.out.println(" [动物描述] Sent \'" + message + "\'");\n\n        //关闭通道和连接\n        channel.close();\n        connection.close();\n    }\n}\n\n\n\n# 消费者1\n\npublic class Recv {\n\n    /**\n     * 交换机名称\n     */\n    private final static String EXCHANGE_NAME = "test_topic_exchange";\n    /**\n     * 定义消息队列名称\n     */\n    private final static String QUEUE_NAME = "test_topic_queue_Q1";\n\n    public static void main(String[] args) throws Exception {\n        // 获取连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 创建通道\n        Channel channel = connection.createChannel();\n        // 声明 exchange，指定类型为 topic\n        channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.TOPIC);\n        // 声明队列\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        // 绑定队列到交换机，同时指定需要订阅的routing key，订阅所有的橙色动物\n        channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, "*.orange.*");\n\n        // 实现消费方法\n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) {\n                // body 即消息体\n                String msg = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [消费者1] received : " + msg + "!");\n            }\n        };\n        // 监听队列，自动返回完成\n        channel.basicConsume(QUEUE_NAME, true, consumer);\n    }\n}\n\n\n\n# 消费者2\n\npublic class Recv2 {\n\n    /**\n     * 交换机名称\n     */\n    private final static String EXCHANGE_NAME = "test_topic_exchange";\n    /**\n     * 定义消息队列名称\n     */\n    private final static String QUEUE_NAME = "test_topic_queue_Q2";\n\n    public static void main(String[] args) throws Exception {\n\n        // 获取到连接\n        Connection connection = ConnectionUtil.getConnection();\n        // 创建通道\n        Channel channel = connection.createChannel();\n        // 声明 exchange，指定类型为 topic\n        channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.TOPIC);\n        // 声明队列\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        // 绑定队列到交换机，同时指定需要订阅的routing key。订阅关于兔子以及懒惰动物的消息\n        channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, "*.*.rabbit");\n        channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, "lazy.#");\n        // 定义队列的消费者\n        DefaultConsumer consumer = new DefaultConsumer(channel) {\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) {\n                // body 即消息体\n                String message = new String(body, StandardCharsets.UTF_8);\n                System.out.println(" [消费者2] received : " + message + "!");\n            }\n        };\n        // 监听队列，自动返回完成\n        channel.basicConsume(QUEUE_NAME, true, consumer);\n    }\n}\n\n\n结果C1、C2是都接收到消息了：\n\nRecv 控制台\n\n[消费者1] received : 这是一只行动迅速的橙色的兔子!\n\n\nRecv2 控制台\n\n[消费者2] received : 这是一只行动迅速的橙色的兔子!\n\n\n\n# RPC\n\nRPC模型示意图：\n\n\n\n基本概念：\n\nCallback queue 回调队列，客户端向服务器发送请求，服务器端处理请求后，将其处理结果保存在一个存储体中。而客户端为了获得处理结果，那么客户在向服务器发送请求时，同时发送一个回调队列地址reply_to。\n\nCorrelation id 关联标识，客户端可能会发送多个请求给服务器，当服务器处理完后，客户端无法辨别在回调队列中的响应具体和那个请求时对应的。为了处理这种情况，客户端在发送每个请求时，同时会附带一个独有correlation_id属性，这样客户端在回调队列中根据correlation_id字段的值就可以分辨此响应属于哪个请求。\n\n流程说明：\n\n * 当客户端启动的时候，它创建一个匿名独享的回调队列。\n\n * 在 RPC 请求中，客户端发送带有两个属性的消息：一个是设置回调队列的 reply_to 属性，另一个是设置唯一值的 correlation_id 属性。\n\n * 将请求发送到一个 rpc_queue 队列中。\n\n * 服务器等待请求发送到这个队列中来。当请求出现的时候，它执行他的工作并且将带有执行结果的消息发送给 reply_to 字段指定的队列。\n\n * 客户端等待回调队列里的数据。当有消息出现的时候，它会检查 correlation_id 属性。如果此属性的值与请求匹配，将它返回给应用\n\n\n# 分享两道面试题：\n\n面试题：\n\n避免消息堆积？\n\n> 1）采用workqueue，多个消费者监听同一队列。\n\n> 2）接收到消息以后，而是通过线程池，异步消费。\n\n如何避免消息丢失？\n\n> 1） 消费者的ACK机制。可以防止消费者丢失消息。\n\n但是，如果在消费者消费之前，MQ就宕机了，消息就没了？\n\n> 2）可以将消息进行持久化。要将消息持久化，前提是：队列、Exchange都持久化\n\n\n# 交换机持久化\n\n声明 exchange，第三个参数设置为true 表示将交换机持久化，MQ重启后仍然生效。\n\nchannel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.TOPIC, true);\n\n\n\n# 队列持久化\n\n声明队列, 第二个参数设置为true, 表示将声明的队列持久化，MQ重启后仍然生效。\n\nchannel.queueDeclare(QUEUE_NAME, true, false, false, null);\n\n\n\n# 消息持久化\n\n发送消息，并且指定routing key 为：quick.orange.rabbit, 第三个参数设置消息持久化\n\nchannel.basicPublish(EXCHANGE_NAME, "quick.orange.rabbit", MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes());\n',normalizedContent:'# rabbitmq 消息模型\n\n\n\n * 基本消息模型\n * 消息确认机制（ack）\n   * 演示手动ack\n * work消息模型\n * publish/subscribe（交换机类型：fanout，也称为广播）\n   * 生产者\n   * 消费者1 （注册成功发给短信服务）\n   * 消费者2（注册成功发给邮件服务）\n   * 思考：\n * routing 路由模型（交换机类型：direct）\n   * 消费者1\n   * 消费者2\n * topics 通配符模式（交换机类型：topics）\n   * 生产者\n   * 消费者1\n   * 消费者2\n * rpc\n   * 分享两道面试题：\n   * 交换机持久化\n   * 队列持久化\n   * 消息持久化\n\n\n\n\n# 基本消息模型\n\n在上图的模型中，有以下概念：\n\n\n\n * p：生产者，也就是要发送消息的程序\n\n * c：消费者：消息的接受者，会一直等待消息到来。\n\n * queue：消息队列，图中红色部分。可以缓存消息；生产者向其中投递消息，消费者从其中取出消息。\n\n生产者\n\n新建一个maven工程，添加amqp-client依赖。\n\n<dependency>\n    <groupid>com.rabbitmq</groupid>\n    <artifactid>amqp-client</artifactid>\n    <version>5.14.0</version>\n</dependency>\n\n\n连接工具类：\n\npackage com.pigxcloud.rabbitmq.demo;\n\nimport com.rabbitmq.client.connection;\nimport com.rabbitmq.client.connectionfactory;\n\n/**\n * rabbitmq 连接工具\n *\n * @author heyuq\n * @date 2021/12/06\n */\npublic class connectionutil {\n\n    /**\n     * 获得rabbitmq连接\n     *\n     * @return com.rabbitmq.client.connection\n     * @throws exception\n     */\n    public static connection getconnection() throws exception {\n\n        // 创建连接工厂\n        connectionfactory factory = new connectionfactory();\n        // 设置服务地址\n        factory.sethost("192.168.100.100");\n        // 设置端口\n        factory.setport(5672);\n        // 设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq\n        factory.setvirtualhost("/");\n        // 设置账号信息，用户名、密码\n        factory.setusername("rabbit");\n        factory.setpassword("rabbit123");\n\n        // 通过工厂获取连接\n        return factory.newconnection();\n    }\n}\n\n\n\n生产者发送消息：\n\npackage com.pigxcloud.rabbitmq.demo;\n\n\nimport com.rabbitmq.client.channel;\nimport com.rabbitmq.client.connection;\n\n/**\n * 生产者发送消息\n *\n * @author heyuq\n * @date 2021/12/06\n */\npublic class send {\n\n    /**\n     * 定义消息队列名称\n     */\n    private final static string queue_name = "simple_queue";\n\n    public static void main(string[] args) throws exception {\n        // 1、获取连接\n        connection connection = connectionutil.getconnection();\n        // 2、从连接中创建通道，使用通道才能完成消息相关的操作\n        channel channel = connection.createchannel();\n        /**\n         * 3、声明（创建）队列\n         * 参数：string queue, boolean durable, boolean exclusive, boolean autodelete, map<string, object> arguments\n         * 1、queue 队列名称\n         * 2、durable 是否持久化，如果持久化，mq重启后队列还在\n         * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建\n         * 4、autodelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除）\n         * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间\n         */\n        channel.queuedeclare(queue_name, false, false, false, null);\n        // 4、消息内容\n        string message = "hello world!";\n        /**\n         * 向指定的队列中发送消息\n         * 参数：string exchange, string routingkey, basicproperties props, byte[] body\n         * 1、exchange，交换机，如果不指定将使用mq的默认交换机（设置为""）\n         * 2、routingkey，路由key，交换机根据路由key来将消息转发到指定的队列，如果使用默认交换机，routingkey设置为队列的名称\n         * 3、props，消息的属性\n         * 4、body，消息内容\n         */\n        channel.basicpublish("", queue_name, null, message.getbytes());\n        system.out.println(" [x] sent \'" + message + "\'");\n\n        //关闭通道和连接(资源关闭最好用try-catch-finally语句处理)\n        channel.close();\n        connection.close();\n    }\n}\n\n\n控制台：\n\n"d:\\program files\\java\\jdk-11.0.13\\bin\\java.exe" ...\n [x] sent \'hello world!\'\n\n\nweb管理页面：服务器地址/端口号 （本地：127.0.0.1:15672，默认用户及密码：rabbit/rabbit123）\n\n\n\n点击队列名称，进入详情页，可以查看消息：\n\n\n\n消费者接收消息\n\npackage com.pigxcloud.rabbitmq.demo;\n\nimport com.rabbitmq.client.*;\n\nimport java.io.ioexception;\nimport java.nio.charset.standardcharsets;\n\n/**\n * 消费者接收消息\n *\n * @author heyuq\n * @date 2021/12/06\n */\npublic class recv {\n\n    /**\n     * 定义消息队列名称\n     */\n    private final static string queue_name = "simple_queue";\n\n    public static void main(string[] args) throws exception {\n        // 1、获取连接\n        connection connection = connectionutil.getconnection();\n        // 2、从连接中创建通道，使用通道才能完成消息相关的操作\n        channel channel = connection.createchannel();\n        /**\n         * 3、声明（创建）队列\n         * 参数：string queue, boolean durable, boolean exclusive, boolean autodelete, map<string, object> arguments\n         * 1、queue 队列名称\n         * 2、durable 是否持久化，如果持久化，mq重启后队列还在\n         * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建\n         * 4、autodelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除）\n         * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间\n         */\n        channel.queuedeclare(queue_name, false, false, false, null);\n\n        defaultconsumer consumer = new defaultconsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n\n            /**\n             * 当接收到消息后此方法将被调用\n             * @param consumertag  消费者标签，用来标识消费者的，在监听队列时设置channel.basicconsume\n             * @param envelope 信封，通过envelope\n             * @param properties 消息属性\n             * @param body 消息内容\n             * @throws ioexception\n             */\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) throws ioexception {\n                //交换机\n                string exchange = envelope.getexchange();\n                //消息id，mq在channel中用来标识消息的id，可用于确认消息已接收\n                long deliverytag = envelope.getdeliverytag();\n                // body 即消息体\n                string msg = new string(body, standardcharsets.utf_8);\n                system.out.println(" [x] received : " + msg + "!");\n            }\n        };\n\n        // 监听队列，第二个参数：是否自动进行消息确认。\n        //参数：string queue, boolean autoack, consumer callback\n        /**\n         * 参数明细：\n         * 1、queue 队列名称\n         * 2、autoack 自动回复，当消费者接收到消息后要告诉mq消息已接收，如果将此参数设置为tru表示会自动回复mq，如果设置为false要通过编程实现回复\n         * 3、callback，消费方法，当消费者接收到消息要执行的方法\n         */\n        channel.basicconsume(queue_name, true, consumer);\n    }\n}\n\n\n\n控制台打印：\n\n"d:\\program files\\java\\jdk-11.0.13\\bin\\java.exe" ...\n [x] received \'hello world!\'\n\n\n再看看队列的消息,已经被消费了\n\n\n\n我们发现，消费者已经获取了消息，但是程序没有停止，一直在监听队列中是否有新的消息。一旦有新的消息进入队列，就会立即打印。\n\n\n# 消息确认机制（ack）\n\n通过刚才的案例可以看出，消息一旦被消费者接收，队列中的消息就会被删除。\n\n那么问题来了：rabbitmq怎么知道消息被接收了呢？\n\n如果消费者领取消息后，还没执行操作就挂掉了呢？或者抛出了异常？消息消费失败，但是rabbitmq无从得知，这样消息就丢失了！\n\n因此，rabbitmq有一个ack机制。当消费者获取消息后，会向rabbitmq发送回执ack，告知消息已经被接收。不过这种回执ack分两种情况：\n\n * 自动ack：消息一旦被接收，消费者自动发送ack\n\n * 手动ack：消息接收后，不会发送ack，需要手动调用\n\n大家觉得哪种更好呢？\n\n这需要看消息的重要性：\n\n * 如果消息不太重要，丢失也没有影响，那么自动ack会比较方便\n\n * 如果消息非常重要，不容丢失。那么最好在消费完成后手动ack，否则接收消息后就自动ack，rabbitmq就会把消息从队列中删除。如果此时消费者宕机，那么消息就丢失了。\n\n我们之前的测试都是自动ack的，如果要手动ack，需要改动我们的代码：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n\npackage com.pigxcloud.rabbitmq.demo;\n\nimport com.rabbitmq.client.*;\n\nimport java.io.ioexception;\nimport java.nio.charset.standardcharsets;\n\n/**\n * 消息确认机制（ack）\n *\n * @author heyuq\n * @date 2021/12/06\n */\npublic class recv2 {\n\n    /**\n     * 定义消息队列名称\n     */\n    private final static string queue_name = "simple_queue";\n\n    public static void main(string[] args) throws exception {\n\n        // 1、获取到连接\n        connection connection = connectionutil.getconnection();\n        // 2、创建通道\n        channel channel = connection.createchannel();\n        // 3、声明队列\n        channel.queuedeclare(queue_name, false, false, false, null);\n        // 4、定义队列的消费者\n        defaultconsumer consumer = new defaultconsumer(channel) {\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties,\n                                       byte[] body) throws ioexception {\n                // body 即消息体\n                string message = new string(body, standardcharsets.utf_8);\n                system.out.println(" [x] received : " + message + "!");\n                // 手动进行ack\n                /*\n                 *  void basicack(long deliverytag, boolean multiple) throws ioexception;\n                 *  deliverytag:用来标识消息的id\n                 *  multiple：是否批量.true:将一次性ack所有小于deliverytag的消息。\n                 */\n                channel.basicack(envelope.getdeliverytag(), false);\n            }\n        };\n        // 监听队列，第二个参数false，手动进行ack\n        channel.basicconsume(queue_name, false, consumer);\n    }\n}\n\n\n最后一行代码设置第二个参数为 false\n\nchannel.basicconsume(queue_name, false, consumer);\n\n\n自动ack存在的问题\n\n修改消费者，添加异常，如下：\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\npublic class recv {\n    private final static string queue_name = "simple_queue";\n    public static void main(string[] args) throws exception {\n        connection connection = connectionutil.getconnection();\n        channel channel = connection.createchannel();\n        channel.queuedeclare(queue_name, false, false, false, null);\n        defaultconsumer consumer = new defaultconsumer(channel) {\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) throws ioexception {\n                // 测试自动ack,模拟异常\n                int i = 1 / 0;\n                string exchange = envelope.getexchange();\n                long deliverytag = envelope.getdeliverytag();\n                string msg = new string(body, standardcharsets.utf_8);\n                system.out.println(" [x] received : " + msg + "!");\n            }\n        };\n        channel.basicconsume(queue_name, true, consumer);\n    }\n}\n\n\n生产者不做任何修改，直接运行，消息发送成功：\n\n\n\n运行消费者，程序抛出异常：\n\n\n\n管理界面：\n\n\n\n消费者抛出异常，但是消息依然被消费，实际上我们还没获取到消息。\n\n\n# 演示手动ack\n\n重新运行生产者发送消息：\n\n\n\n同样，在手动进行ack前抛出异常，运行recv2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\npublic class recv2 {\n\n    private final static string queue_name = "simple_queue";\n    \n    public static void main(string[] args) throws exception {\n        connection connection = connectionutil.getconnection();\n        channel channel = connection.createchannel();\n        channel.queuedeclare(queue_name, false, false, false, null);\n        \n        defaultconsumer consumer = new defaultconsumer(channel) {\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties,\n                                       byte[] body) throws ioexception {\n                // 测试自动ack,模拟异常\n                int i = 1 / 0;\n                string message = new string(body, standardcharsets.utf_8);\n                system.out.println(" [x] received : " + message + "!");\n                // 手动进行ack\n                channel.basicack(envelope.getdeliverytag(), false);\n            }\n        };\n        channel.basicconsume(queue_name, false, consumer);\n    }\n}\n\n\n再看看管理界面：\n\n\n\n消息没有被消费掉！\n\n\n\n还有另外一种情况：修改消费者recv2，把监听队列第二个参数自动改成手动,（去掉之前制造的异常） ，并且消费方法中没手动进行ack\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n\n\n\npublic class recv2 {\n\n    private final static string queue_name = "simple_queue";\n    \n    public static void main(string[] args) throws exception {\n        connection connection = connectionutil.getconnection();\n        channel channel = connection.createchannel();\n        channel.queuedeclare(queue_name, false, false, false, null);\n        \n        defaultconsumer consumer = new defaultconsumer(channel) {\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties,\n                                       byte[] body) throws ioexception {\n                // 测试自动ack,模拟异常\n                // int i = 1 / 0;\n                string message = new string(body, standardcharsets.utf_8);\n                system.out.println(" [x] received : " + message + "!");\n                // 手动进行ack\n                // channel.basicack(envelope.getdeliverytag(), false);\n            }\n        };\n        channel.basicconsume(queue_name, false, consumer);\n    }\n}\n\n\n生产者代码不变，再次运行：\n\n\n\n运行消费者 ：\n\n\n\n但是，查看管理界面，发现：\n\n\n\n停掉消费者的程序，发现：\n\n\n\n这是因为虽然我们设置了手动ack，但是代码中并没有进行消息确认！所以消息并未被真正消费掉。当我们关掉这个消费者，消息的状态再次变为ready。\n\n正确的做法是：\n\n我们要在监听队列时设置第二个参数为false,代码中手动进行ack\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\npublic class recv2 {\n\n    private final static string queue_name = "simple_queue";\n    \n    public static void main(string[] args) throws exception {\n        connection connection = connectionutil.getconnection();\n        channel channel = connection.createchannel();\n        channel.queuedeclare(queue_name, false, false, false, null);\n        \n        defaultconsumer consumer = new defaultconsumer(channel) {\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties,\n                                       byte[] body) throws ioexception {\n                // 测试自动ack,模拟异常\n                // int i = 1 / 0;\n                string message = new string(body, standardcharsets.utf_8);\n                system.out.println(" [x] received : " + message + "!");\n                // 手动进行ack\n                channel.basicack(envelope.getdeliverytag(), false);\n            }\n        };\n        channel.basicconsume(queue_name, false, consumer);\n    }\n}\n\n\n再次运行消费者，查看web管理页面：\n\n\n\n消费者消费成功！\n\n生产者避免数据丢失：https://www.cnblogs.com/vipstone/p/9350075.html\n\n\n# work消息模型\n\n工作队列或者竞争消费者模式\n\n\n\nwork queues与入门程序相比，多了一个消费端，两个消费端共同消费同一个队列中的消息，但是一个消息只能被一个消费者获取。\n\n这个消息模型在web应用程序中特别有用，可以处理短的http请求窗口中无法处理复杂的任务。\n\n接下来我们来模拟这个流程：\n\np：生产者：任务的发布者\n\nc1：消费者1：领取任务并且完成任务，假设完成速度较慢（模拟耗时）\n\nc2：消费者2：领取任务并且完成任务，假设完成速度较快\n\n生产者\n\n生产者循环发送50条消息\n\npublic class send {\n\n    /**\n     * 定义消息队列名称\n     */\n    private final static string queue_name = "simple_queue";\n\n    public static void main(string[] args) throws exception {\n        // 1、获取连接\n        connection connection = connectionutil.getconnection();\n        // 2、从连接中创建通道，使用通道才能完成消息相关的操作\n        channel channel = connection.createchannel();\n        // 3、声明队列\n        channel.queuedeclare(queue_name, false, false, false, null);\n        // 4、循环发布任务\n        for (int i = 0; i < 50; i++) {\n            // 消息内容\n            string message = "task ... " + i;\n            channel.basicpublish("", queue_name, null, message.getbytes());\n            system.out.println(" [x] sent \'" + message + "\'");\n\n            thread.sleep(i * 2);\n        }\n\n        //关闭通道和连接\n        channel.close();\n        connection.close();\n    }\n}\n\n\n消费者1\n\npublic class recv {\n\n    /**\n     * 定义消息队列名称\n     */\n    private final static string queue_name = "simple_queue";\n\n    public static void main(string[] args) throws exception {\n        // 1、获取连接\n        connection connection = connectionutil.getconnection();\n        // 2、从连接中创建通道，使用通道才能完成消息相关的操作\n        channel channel = connection.createchannel();\n        // 3、声明队列\n        channel.queuedeclare(queue_name, false, false, false, null);\n        // 4、实现消费方法\n        defaultconsumer consumer = new defaultconsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) throws ioexception {\n                // body 即消息体\n                string msg = new string(body, standardcharsets.utf_8);\n                system.out.println(" [消费者2] received : " + msg + "!");\n                try {\n                    //模拟任务耗时1s\n                    timeunit.seconds.sleep(1);\n                } catch (interruptedexception e) {\n                    e.printstacktrace();\n                }\n            }\n        };\n        // 监听队列，第二个参数：是否自动进行消息确认。\n        channel.basicconsume(queue_name, true, consumer);\n    }\n}\n\n\n消费者2\n\n代码不贴了，与消费者1基本类似，只是消费者2没有设置消费耗时时间。\n\n接下来，两个消费者一同启动，然后发送50条消息：\n\n\n\n\n\n可以发现，两个消费者各自消费了不同25条消息，这就实现了任务的分发。\n\n能者多劳\n\n刚才的实现有问题吗？\n\n * 消费者1比消费者2的效率要低，一次任务的耗时较长\n * 然而两人最终消费的消息数量是一样的\n * 消费者2大量时间处于空闲状态，消费者1一直忙碌\n\n现在的状态属于是把任务平均分配，正确的做法应该是消费越快的人，消费的越多。\n\n怎么实现呢？\n\n通过 basicqos 方法设置prefetchcount = 1。这样rabbitmq就会使得每个consumer在同一个时间点最多处理1个message。换句话说，在接收到该consumer的ack前，他它不会将新的message分发给它。相反，它会将其分派给不是仍然忙碌的下一个consumer。\n\n值得注意的是：prefetchcount在手动ack的情况下才生效，自动ack不生效。\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npublic class recv {\n\n    private final static string queue_name = "simple_queue";\n\n    public static void main(string[] args) throws exception {\n        // 1、获取连接\n        connection connection = connectionutil.getconnection();\n        // 2、从连接中创建通道，使用通道才能完成消息相关的操作\n        channel channel = connection.createchannel();\n        // 3、声明队列\n        channel.queuedeclare(queue_name, false, false, false, null);\n        // 设置消费者同时只能处理一条消息，在手动ack下才生效\n        channel.basicqos(1);\n        // 4、实现消费方法\n        defaultconsumer consumer = new defaultconsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) throws ioexception {\n                // body 即消息体\n                string msg = new string(body, standardcharsets.utf_8);\n                system.out.println(" [消费者2] received : " + msg + "!");\n                try {\n                    //模拟任务耗时1s\n                    timeunit.seconds.sleep(5);\n                } catch (interruptedexception e) {\n                    e.printstacktrace();\n                }\n                channel.basicack(envelope.getdeliverytag(), false);\n            }\n        };\n        // 监听队列，第二个参数：是否自动进行消息确认。\n        channel.basicconsume(queue_name, false, consumer);\n    }\n}\n\n\n再次测试：\n\n\n\n\n\n订阅模型分类\n\n说明下：\n\n1、一个生产者多个消费者\n\n2、每个消费者都有一个自己的队列\n\n3、生产者没有将消息直接发送给队列，而是发送给exchange(交换机、转发器)\n\n4、每个队列都需要绑定到交换机上\n\n5、生产者发送的消息，经过交换机到达队列，实现一个消息被多个消费者消费 例子：注册->发邮件、发短信\n\nx（exchanges）：交换机一方面：接收生产者发送的消息。另一方面：知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于exchange的类型。\n\nexchange类型有以下几种：\n\n * fanout：广播，将消息交给所有绑定到交换机的队列\n\n * direct：定向，把消息交给符合指定routing key 的队列\n\n * topic：通配符，把消息交给符合routing pattern（路由模式） 的队列\n\n * header：header模式与routing不同的地方在于，header模式取消routingkey，使用header中的 key/value（键值对）匹配队列。\n\n * header模式不展开了，感兴趣可以参考这篇文章https://blog.csdn.net/zhu_tianwei/article/details/40923131\n\n注意\n\nexchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！\n\n\n# publish/subscribe（交换机类型：fanout，也称为广播）\n\npublish/subscribe模型示意图 ：\n\n\n\n\n# 生产者\n\n和前面两种模式不同：\n\n * 1） 声明exchange，不再声明queue\n * 2） 发送消息到exchange，不再发送到queue\n\npublic class send {\n\n    /**\n     * 定义交换机名称\n     */\n    private final static string exchange_name = "test_fanout_exchange";\n\n    public static void main(string[] args) throws exception {\n        // 获取连接\n        connection connection = connectionutil.getconnection();\n        // 创建通道\n        channel channel = connection.createchannel();\n        // 声明 exchange，指定类型为 fanout\n        channel.exchangedeclare(exchange_name, builtinexchangetype.fanout);\n        \n        string message = "注册成功！";\n        channel.basicpublish(exchange_name, "", null, message.getbytes());\n        system.out.println(" [生产者] sent \'" + message + "\'");\n\n        //关闭通道和连接\n        channel.close();\n        connection.close();\n    }\n}\n\n\n\n# 消费者1 （注册成功发给短信服务）\n\npublic class recv {\n\n    /**\n     * 交换机名称\n     */\n    private final static string exchange_name = "test_fanout_exchange";\n    /**\n     * 定义消息队列名称\n     */\n    private final static string queue_name = "test_fanout_queue_sms";\n\n    public static void main(string[] args) throws exception {\n        // 获取连接\n        connection connection = connectionutil.getconnection();\n        // 创建通道\n        channel channel = connection.createchannel();\n        // 声明 exchange，指定类型为 fanout\n        channel.exchangedeclare(exchange_name, builtinexchangetype.fanout);\n\n        // 声明队列\n        channel.queuedeclare(queue_name, false, false, false, null);\n        // 绑定队列到交换机\n        channel.queuebind(queue_name, exchange_name, "");\n        // 实现消费方法\n        defaultconsumer consumer = new defaultconsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) {\n                // body 即消息体\n                string msg = new string(body, standardcharsets.utf_8);\n                system.out.println(" [短信服务] received : " + msg + "!");\n            }\n        };\n        // 监听队列，自动返回完成\n        channel.basicconsume(queue_name, true, consumer);\n    }\n}\n\n\n\n# 消费者2（注册成功发给邮件服务）\n\npublic class recv2 {\n\n    /**\n     * 交换机名称\n     */\n    private final static string exchange_name = "test_fanout_exchange";\n    /**\n     * 定义消息队列名称\n     */\n    private final static string queue_name = "test_fanout_queue_email";\n\n    public static void main(string[] args) throws exception {\n\n        // 获取到连接\n        connection connection = connectionutil.getconnection();\n        // 创建通道\n        channel channel = connection.createchannel();\n        // 声明 exchange，指定类型为 fanout\n        channel.exchangedeclare(exchange_name, builtinexchangetype.fanout);\n        // 声明队列\n        channel.queuedeclare(queue_name, false, false, false, null);\n        // 绑定队列到交换机\n        channel.queuebind(queue_name, exchange_name, "");\n        // 定义队列的消费者\n        defaultconsumer consumer = new defaultconsumer(channel) {\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) {\n                // body 即消息体\n                string message = new string(body, standardcharsets.utf_8);\n                system.out.println(" [邮件服务] received : " + message + "!");\n            }\n        };\n        // 监听队列，自动返回完成\n        channel.basicconsume(queue_name, true, consumer);\n    }\n}\n\n\n我们运行两个消费者，然后发送1条消息：\n\n\n\n\n\n\n# 思考：\n\n1、publish/subscribe与work queues有什么区别。\n\n区别：\n\n1）work queues不用定义交换机，而publish/subscribe需要定义交换机。\n\n2）publish/subscribe的生产方是面向交换机发送消息，work queues的生产方是面向队列发送消息(底层使用默认交换机)。\n\n3）publish/subscribe需要设置队列和交换机的绑定，work queues不需要设置，实际上work queues会将队列绑定到默认的交换机 。\n\n相同点：\n\n所以两者实现的发布/订阅的效果是一样的，多个消费端监听同一个队列不会重复消费消息。\n\n2、实际工作用 publish/subscribe还是work queues。\n\n建议使用 publish/subscribe，发布订阅模式比工作队列模式更强大（也可以做到同一队列竞争），并且发布订阅模式可以指定自己专用的交换机。\n\n\n# routing 路由模型（交换机类型：direct）\n\nrouting模型示意图：\n\n\n\np：生产者，向exchange发送消息，发送消息时，会指定一个routing key。\n\nx：exchange（交换机），接收生产者的消息，然后把消息递交给 与routing key完全匹配的队列\n\nc1：消费者，其所在队列指定了需要routing key 为 error 的消息\n\nc2：消费者，其所在队列指定了需要routing key 为 info、error、warning 的消息\n\n接下来看代码：\n\npublic class send {\n\n    /**\n     * 定义交换机名称\n     */\n    private final static string exchange_name = "test_direct_exchange";\n\n    public static void main(string[] args) throws exception {\n        // 获取连接\n        connection connection = connectionutil.getconnection();\n        // 创建通道\n        channel channel = connection.createchannel();\n        // 声明 exchange，指定类型为 direct\n        channel.exchangedeclare(exchange_name, builtinexchangetype.direct);\n        string message = "【xxx开发平台】恭喜注册成功！";\n        // 发送消息，并且指定routing key 为：sms，只有短信服务才能接收到消息\n        channel.basicpublish(exchange_name, "sms", null, message.getbytes());\n        system.out.println(" [x] sent \'" + message + "\'");\n\n        //关闭通道和连接\n        channel.close();\n        connection.close();\n    }\n}\n\n\n\n# 消费者1\n\npublic class send {\n\n    /**\n     * 定义交换机名称\n     */\n    private final static string exchange_name = "test_direct_exchange";\n\n    public static void main(string[] args) throws exception {\n        // 获取连接\n        connection connection = connectionutil.getconnection();\n        // 创建通道\n        channel channel = connection.createchannel();\n        // 声明 exchange，指定类型为 direct\n        channel.exchangedeclare(exchange_name, builtinexchangetype.direct);\n        string message = "【xxx开发平台】恭喜注册成功！";\n        // 发送消息，并且指定routing key 为：sms，只有短信服务才能接收到消息\n        channel.basicpublish(exchange_name, "sms", null, message.getbytes());\n        // channel.basicpublish(exchange_name, "email", null, message.getbytes());\n        system.out.println(" [x] sent \'" + message + "\'");\n\n        //关闭通道和连接\n        channel.close();\n        connection.close();\n    }\n}\n\n\n\n# 消费者2\n\npublic class recv {\n\n    /**\n     * 交换机名称\n     */\n    private final static string exchange_name = "test_direct_exchange";\n    /**\n     * 定义消息队列名称\n     */\n    private final static string queue_name = "test_direct_queue_sms";\n\n    public static void main(string[] args) throws exception {\n        // 获取连接\n        connection connection = connectionutil.getconnection();\n        // 创建通道\n        channel channel = connection.createchannel();\n        // 声明 exchange，指定类型为 fanout\n        channel.exchangedeclare(exchange_name, builtinexchangetype.direct);\n        // 声明队列\n        channel.queuedeclare(queue_name, false, false, false, null);\n        // 绑定队列到交换机，同时指定需要订阅的routing key，指定接收发送方指定routing key为sms的消息，可以指定多个。\n        channel.queuebind(queue_name, exchange_name, "sms");\n        // channel.queuebind(queue_name, exchange_name, "email");\n\n        // 实现消费方法\n        defaultconsumer consumer = new defaultconsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) {\n                // body 即消息体\n                string msg = new string(body, standardcharsets.utf_8);\n                system.out.println(" [短信服务] received : " + msg + "!");\n            }\n        };\n        // 监听队列，自动返回完成\n        channel.basicconsume(queue_name, true, consumer);\n    }\n}\n\n\n我们发送sms的routingkey，发现结果：只有指定短信的消费者1收到消息了\n\n\n\n\n# topics 通配符模式（交换机类型：topics）\n\ntopics模型示意图：\n\n\n\n每个消费者监听自己的队列，并且设置带统配符的routingkey,生产者将消息发给broker，由交换机根据routingkey来转发消息到指定的队列。\n\nroutingkey一般都是有一个或者多个单词组成，多个单词之间以“.”分割，例如：inform.sms\n\n通配符规则：\n\n#：匹配一个或多个词\n\n*：匹配不多不少恰好1个词\n\n举例：\n\n> audit.#：能够匹配 audit.irs.corporate 或者 audit.irs\n\n> audit.*：只能匹配 audit.irs\n\n从示意图可知，我们将发送所有描述动物的消息。消息将使用由三个字（两个点）组成的routing key发送。路由关键字中的第一个单词将描述速度，第二个颜色和第三个种类：<speed>.<color>.<species>。\n\n我们创建了三个绑定：q1绑定了 *.orange.* ，q2绑定了 *.*.rabbit 和 lazy.＃。\n\nq1匹配所有的橙色动物。\n\nq2匹配关于兔子以及懒惰动物的消息。\n\n下面做个小练习，假如生产者发送如下消息，会进入哪个队列：\n\nquick.orange.rabbit q1 q2 routingkey="quick.orange.rabbit" 的消息会同时路由到q1与q2\n\nlazy.orange.elephant q1 q2\n\nquick.orange.fox q1\n\nlazy.pink.rabbit q2 (值得注意的是，虽然这个routingkey与q2的两个bindingkey都匹配，但是只会投递q2一次)\n\nquick.brown.fox 不匹配任意队列，被丢弃\n\nquick.orange.male.rabbit 不匹配任意队列，被丢弃\n\norange 不匹配任意队列，被丢弃\n\n下面我们以指定routing key="quick.orange.rabbit"为例，验证上面的答案\n\n\n# 生产者\n\npackage com.pigxcloud.rabbitmq.demo;\n\n\nimport com.rabbitmq.client.builtinexchangetype;\nimport com.rabbitmq.client.channel;\nimport com.rabbitmq.client.connection;\n\n/**\n * 生产者发送消息\n *\n * @author heyuq\n * @date 2021/12/06\n */\npublic class send {\n\n    /**\n     * 定义交换机名称\n     */\n    private final static string exchange_name = "test_topic_exchange";\n\n    public static void main(string[] args) throws exception {\n        // 获取连接\n        connection connection = connectionutil.getconnection();\n        // 创建通道\n        channel channel = connection.createchannel();\n        // 声明 exchange，指定类型为 topic\n        channel.exchangedeclare(exchange_name, builtinexchangetype.topic);\n        string message = "这是一只行动迅速的橙色的兔子";\n        // 发送消息，并且指定routing key 为：quick.orange.rabbit\n        channel.basicpublish(exchange_name, "quick.orange.rabbit", null, message.getbytes());\n        system.out.println(" [动物描述] sent \'" + message + "\'");\n\n        //关闭通道和连接\n        channel.close();\n        connection.close();\n    }\n}\n\n\n\n# 消费者1\n\npublic class recv {\n\n    /**\n     * 交换机名称\n     */\n    private final static string exchange_name = "test_topic_exchange";\n    /**\n     * 定义消息队列名称\n     */\n    private final static string queue_name = "test_topic_queue_q1";\n\n    public static void main(string[] args) throws exception {\n        // 获取连接\n        connection connection = connectionutil.getconnection();\n        // 创建通道\n        channel channel = connection.createchannel();\n        // 声明 exchange，指定类型为 topic\n        channel.exchangedeclare(exchange_name, builtinexchangetype.topic);\n        // 声明队列\n        channel.queuedeclare(queue_name, false, false, false, null);\n        // 绑定队列到交换机，同时指定需要订阅的routing key，订阅所有的橙色动物\n        channel.queuebind(queue_name, exchange_name, "*.orange.*");\n\n        // 实现消费方法\n        defaultconsumer consumer = new defaultconsumer(channel) {\n            // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) {\n                // body 即消息体\n                string msg = new string(body, standardcharsets.utf_8);\n                system.out.println(" [消费者1] received : " + msg + "!");\n            }\n        };\n        // 监听队列，自动返回完成\n        channel.basicconsume(queue_name, true, consumer);\n    }\n}\n\n\n\n# 消费者2\n\npublic class recv2 {\n\n    /**\n     * 交换机名称\n     */\n    private final static string exchange_name = "test_topic_exchange";\n    /**\n     * 定义消息队列名称\n     */\n    private final static string queue_name = "test_topic_queue_q2";\n\n    public static void main(string[] args) throws exception {\n\n        // 获取到连接\n        connection connection = connectionutil.getconnection();\n        // 创建通道\n        channel channel = connection.createchannel();\n        // 声明 exchange，指定类型为 topic\n        channel.exchangedeclare(exchange_name, builtinexchangetype.topic);\n        // 声明队列\n        channel.queuedeclare(queue_name, false, false, false, null);\n        // 绑定队列到交换机，同时指定需要订阅的routing key。订阅关于兔子以及懒惰动物的消息\n        channel.queuebind(queue_name, exchange_name, "*.*.rabbit");\n        channel.queuebind(queue_name, exchange_name, "lazy.#");\n        // 定义队列的消费者\n        defaultconsumer consumer = new defaultconsumer(channel) {\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) {\n                // body 即消息体\n                string message = new string(body, standardcharsets.utf_8);\n                system.out.println(" [消费者2] received : " + message + "!");\n            }\n        };\n        // 监听队列，自动返回完成\n        channel.basicconsume(queue_name, true, consumer);\n    }\n}\n\n\n结果c1、c2是都接收到消息了：\n\nrecv 控制台\n\n[消费者1] received : 这是一只行动迅速的橙色的兔子!\n\n\nrecv2 控制台\n\n[消费者2] received : 这是一只行动迅速的橙色的兔子!\n\n\n\n# rpc\n\nrpc模型示意图：\n\n\n\n基本概念：\n\ncallback queue 回调队列，客户端向服务器发送请求，服务器端处理请求后，将其处理结果保存在一个存储体中。而客户端为了获得处理结果，那么客户在向服务器发送请求时，同时发送一个回调队列地址reply_to。\n\ncorrelation id 关联标识，客户端可能会发送多个请求给服务器，当服务器处理完后，客户端无法辨别在回调队列中的响应具体和那个请求时对应的。为了处理这种情况，客户端在发送每个请求时，同时会附带一个独有correlation_id属性，这样客户端在回调队列中根据correlation_id字段的值就可以分辨此响应属于哪个请求。\n\n流程说明：\n\n * 当客户端启动的时候，它创建一个匿名独享的回调队列。\n\n * 在 rpc 请求中，客户端发送带有两个属性的消息：一个是设置回调队列的 reply_to 属性，另一个是设置唯一值的 correlation_id 属性。\n\n * 将请求发送到一个 rpc_queue 队列中。\n\n * 服务器等待请求发送到这个队列中来。当请求出现的时候，它执行他的工作并且将带有执行结果的消息发送给 reply_to 字段指定的队列。\n\n * 客户端等待回调队列里的数据。当有消息出现的时候，它会检查 correlation_id 属性。如果此属性的值与请求匹配，将它返回给应用\n\n\n# 分享两道面试题：\n\n面试题：\n\n避免消息堆积？\n\n> 1）采用workqueue，多个消费者监听同一队列。\n\n> 2）接收到消息以后，而是通过线程池，异步消费。\n\n如何避免消息丢失？\n\n> 1） 消费者的ack机制。可以防止消费者丢失消息。\n\n但是，如果在消费者消费之前，mq就宕机了，消息就没了？\n\n> 2）可以将消息进行持久化。要将消息持久化，前提是：队列、exchange都持久化\n\n\n# 交换机持久化\n\n声明 exchange，第三个参数设置为true 表示将交换机持久化，mq重启后仍然生效。\n\nchannel.exchangedeclare(exchange_name, builtinexchangetype.topic, true);\n\n\n\n# 队列持久化\n\n声明队列, 第二个参数设置为true, 表示将声明的队列持久化，mq重启后仍然生效。\n\nchannel.queuedeclare(queue_name, true, false, false, null);\n\n\n\n# 消息持久化\n\n发送消息，并且指定routing key 为：quick.orange.rabbit, 第三个参数设置消息持久化\n\nchannel.basicpublish(exchange_name, "quick.orange.rabbit", messageproperties.persistent_text_plain, message.getbytes());\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Spring整合RabbitMQ",frontmatter:{title:"Spring整合RabbitMQ",date:"2022-02-09T11:31:48.000Z",permalink:"/pages/36085c/"},regularPath:"/13.RabbitMQ/01.%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/04.Spring%E6%95%B4%E5%90%88RabbitMQ.html",relativePath:"13.RabbitMQ/01.快速入门/04.Spring整合RabbitMQ.md",key:"v-6680c492",path:"/pages/36085c/",headers:[{level:2,title:"搭建SpringBoot环境",slug:"搭建springboot环境",normalizedTitle:"搭建springboot环境",charIndex:59},{level:3,title:"1、添加AMQP的启动器：",slug:"_1、添加amqp的启动器",normalizedTitle:"1、添加amqp的启动器：",charIndex:159},{level:3,title:"2、在application.yml中添加RabbitMQ的配置：",slug:"_2、在application-yml中添加rabbitmq的配置",normalizedTitle:"2、在application.yml中添加rabbitmq的配置：",charIndex:437},{level:3,title:"3、定义RabbitConfig配置类，配置Exchange、Queue、及绑定交换机。",slug:"_3、定义rabbitconfig配置类-配置exchange、queue、及绑定交换机。",normalizedTitle:"3、定义rabbitconfig配置类，配置exchange、queue、及绑定交换机。",charIndex:1244},{level:3,title:"生产者(mq-rabbitmq-producer)",slug:"生产者-mq-rabbitmq-producer",normalizedTitle:"生产者(mq-rabbitmq-producer)",charIndex:2906},{level:3,title:"消费者(mq-rabbitmq-consumer)",slug:"消费者-mq-rabbitmq-consumer",normalizedTitle:"消费者(mq-rabbitmq-consumer)",charIndex:3716}],headersStr:"搭建SpringBoot环境 1、添加AMQP的启动器： 2、在application.yml中添加RabbitMQ的配置： 3、定义RabbitConfig配置类，配置Exchange、Queue、及绑定交换机。 生产者(mq-rabbitmq-producer) 消费者(mq-rabbitmq-consumer)",content:'# Spring整合RabbitMQ\n\n下面还是模拟注册服务当用户注册成功后，向短信和邮件服务推送消息的场景\n\n\n# 搭建SpringBoot环境\n\n创建两个工程 mq-rabbitmq-producer和mq-rabbitmq-consumer，分别配置1、2、3（第三步本例消费者用注解形式，可以不用配）\n\n\n# 1、添加AMQP的启动器：\n\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-amqp</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring‐boot‐starter‐test</artifactId>\n</dependency>\n\n\n\n# 2、在application.yml中添加RabbitMQ的配置：\n\nserver:\n  port: 10086  \nspring:\n  application:\n    name: mq-rabbitmq-producer\n  rabbitmq:\n    host: 192.168.100.100\n    port: 5672\n    username: rabbit\n    password: rabbit123\n    virtualHost: /\n    template:\n      retry:\n        enabled: true\n        initial-interval: 10000ms\n        max-interval: 300000ms\n        multiplier: 2\n      exchange: topic.exchange\n    publisher-confirms: true\n\n\n属性说明：\n\n * template：有关AmqpTemplate的配置\n   \n   * retry：失败重试\n   \n   * enabled：开启失败重试\n     \n     * initial-interval：第一次重试的间隔时长\n     \n     * max-interval：最长重试间隔，超过这个间隔将不再重试\n     \n     * multiplier：下次重试间隔的倍数，此处是2即下次重试间隔是上次的2倍\n   \n   * exchange：缺省的交换机名称，此处配置后，发送消息如果不指定交换机就会使用这个\n\n * publisher-confirms：生产者确认机制，确保消息会正确发送，如果发送失败会有错误回执，从而触发重试\n\n当然如果consumer只是接收消息而不发送，就不用配置template相关内容。\n\n\n# 3、定义RabbitConfig配置类，配置Exchange、Queue、及绑定交换机。\n\n\n@Configuration\npublic class RabbitmqConfig {\n    public static final String QUEUE_EMAIL = "queue_email";//email队列\n    public static final String QUEUE_SMS = "queue_sms";//sms队列\n    public static final String EXCHANGE_NAME="topic.exchange";//topics类型交换机\n    public static final String ROUTINGKEY_EMAIL="topic.#.email.#";\n    public static final String ROUTINGKEY_SMS="topic.#.sms.#";\n \n    //声明交换机\n    @Bean(EXCHANGE_NAME)\n    public Exchange exchange(){\n        //durable(true) 持久化，mq重启之后交换机还在\n        return ExchangeBuilder.topicExchange(EXCHANGE_NAME).durable(true).build();\n    }\n \n    //声明email队列\n    /*\n     *   new Queue(QUEUE_EMAIL,true,false,false)\n     *   durable="true" 持久化 rabbitmq重启的时候不需要创建新的队列\n     *   auto-delete 表示消息队列没有在使用时将被自动删除 默认是false\n     *   exclusive  表示该消息队列是否只在当前connection生效,默认是false\n     */\n    @Bean(QUEUE_EMAIL)\n    public Queue emailQueue(){\n        return new Queue(QUEUE_EMAIL);\n    }\n    //声明sms队列\n    @Bean(QUEUE_SMS)\n    public Queue smsQueue(){\n        return new Queue(QUEUE_SMS);\n    }\n \n    //ROUTINGKEY_EMAIL队列绑定交换机，指定routingKey\n    @Bean\n    public Binding bindingEmail(@Qualifier(QUEUE_EMAIL) Queue queue,\n                                @Qualifier(EXCHANGE_NAME) Exchange exchange){\n        return BindingBuilder.bind(queue).to(exchange).with(ROUTINGKEY_EMAIL).noargs();\n    }\n    //ROUTINGKEY_SMS队列绑定交换机，指定routingKey\n    @Bean\n    public Binding bindingSMS(@Qualifier(QUEUE_SMS) Queue queue,\n                              @Qualifier(EXCHANGE_NAME) Exchange exchange){\n        return BindingBuilder.bind(queue).to(exchange).with(ROUTINGKEY_SMS).noargs();\n    }\n \n}\n\n\n\n# 生产者(mq-rabbitmq-producer)\n\n为了方便测试，我直接把生产者代码放工程测试类：发送routing key是"topic.sms.email"的消息，那么mq-rabbitmq-consumer下那些监听的（与交换机(topic.exchange)绑定，并且订阅的routingkey中匹配了"topic.sms.email"规则的） 队列就会收到消息。\n\n@SpringBootTest\n@RunWith(SpringRunner.class)\npublic class Send {\n \n    @Autowired\n    RabbitTemplate rabbitTemplate;\n    \n    @Test\n    public void sendMsgByTopics(){\n \n        /**\n         * 参数：\n         * 1、交换机名称\n         * 2、routingKey\n         * 3、消息内容\n         */\n        for (int i=0;i<5;i++){\n            String message = "恭喜您，注册成功！userid="+i;\n            rabbitTemplate.convertAndSend(RabbitmqConfig.EXCHANGE_NAME,"topic.sms.email",message);\n            System.out.println(" [x] Sent \'" + message + "\'");\n        }\n \n    }\n}\n\n\n运行测试类发送5条消息：\n\nweb管理界面： 可以看到已经创建了交换机以及queue_email、queue_sms 2个队列，并且向这两个队列分别发送了5条消息\n\n\n# 消费者(mq-rabbitmq-consumer)\n\n编写一个监听器组件，通过注解配置消费者队列，以及队列与交换机之间绑定关系。（也可以像生产者那样通过配置类配置）\n\n在SpringAmqp中，对消息的消费者进行了封装和抽象。一个JavaBean的方法，只要添加@RabbitListener注解，就可以成为了一个消费者。\n\n@Component\npublic class ReceiveHandler {\n \n    //监听邮件队列\n    @RabbitListener(bindings = @QueueBinding(\n            value = @Queue(value = "queue_email", durable = "true"),\n            exchange = @Exchange(\n                    value = "topic.exchange",\n                    ignoreDeclarationExceptions = "true",\n                    type = ExchangeTypes.TOPIC\n            ),\n            key = {"topic.#.email.#","email.*"}))\n    public void rece_email(String msg){\n        System.out.println(" [邮件服务] received : " + msg + "!");\n    }\n \n    //监听短信队列\n    @RabbitListener(bindings = @QueueBinding(\n            value = @Queue(value = "queue_sms", durable = "true"),\n            exchange = @Exchange(\n                    value = "topic.exchange",\n                    ignoreDeclarationExceptions = "true",\n                    type = ExchangeTypes.TOPIC\n            ),\n            key = {"topic.#.sms.#"}))\n    public void rece_sms(String msg){\n        System.out.println(" [短信服务] received : " + msg + "!");\n    }\n}\n\n\n属性说明：\n\n * @Componet：类上的注解，注册到Spring容器\n\n * @RabbitListener：方法上的注解，声明这个方法是一个消费者方法，需要指定下面的属性：\n   \n   * bindings：指定绑定关系，可以有多个。值是@QueueBinding的数组。@QueueBinding包含下面属性：\n     \n     * value：这个消费者关联的队列。值是@Queue，代表一个队列\n     \n     * exchange：队列所绑定的交换机，值是@Exchange类型\n     \n     * key：队列和交换机绑定的RoutingKey，可指定多个\n\n启动mq-rabbitmq-comsumer项目\n\nok,邮件服务和短息服务接收到消息后，就可以各自开展自己的业务了。',normalizedContent:'# spring整合rabbitmq\n\n下面还是模拟注册服务当用户注册成功后，向短信和邮件服务推送消息的场景\n\n\n# 搭建springboot环境\n\n创建两个工程 mq-rabbitmq-producer和mq-rabbitmq-consumer，分别配置1、2、3（第三步本例消费者用注解形式，可以不用配）\n\n\n# 1、添加amqp的启动器：\n\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter-amqp</artifactid>\n</dependency>\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring‐boot‐starter‐test</artifactid>\n</dependency>\n\n\n\n# 2、在application.yml中添加rabbitmq的配置：\n\nserver:\n  port: 10086  \nspring:\n  application:\n    name: mq-rabbitmq-producer\n  rabbitmq:\n    host: 192.168.100.100\n    port: 5672\n    username: rabbit\n    password: rabbit123\n    virtualhost: /\n    template:\n      retry:\n        enabled: true\n        initial-interval: 10000ms\n        max-interval: 300000ms\n        multiplier: 2\n      exchange: topic.exchange\n    publisher-confirms: true\n\n\n属性说明：\n\n * template：有关amqptemplate的配置\n   \n   * retry：失败重试\n   \n   * enabled：开启失败重试\n     \n     * initial-interval：第一次重试的间隔时长\n     \n     * max-interval：最长重试间隔，超过这个间隔将不再重试\n     \n     * multiplier：下次重试间隔的倍数，此处是2即下次重试间隔是上次的2倍\n   \n   * exchange：缺省的交换机名称，此处配置后，发送消息如果不指定交换机就会使用这个\n\n * publisher-confirms：生产者确认机制，确保消息会正确发送，如果发送失败会有错误回执，从而触发重试\n\n当然如果consumer只是接收消息而不发送，就不用配置template相关内容。\n\n\n# 3、定义rabbitconfig配置类，配置exchange、queue、及绑定交换机。\n\n\n@configuration\npublic class rabbitmqconfig {\n    public static final string queue_email = "queue_email";//email队列\n    public static final string queue_sms = "queue_sms";//sms队列\n    public static final string exchange_name="topic.exchange";//topics类型交换机\n    public static final string routingkey_email="topic.#.email.#";\n    public static final string routingkey_sms="topic.#.sms.#";\n \n    //声明交换机\n    @bean(exchange_name)\n    public exchange exchange(){\n        //durable(true) 持久化，mq重启之后交换机还在\n        return exchangebuilder.topicexchange(exchange_name).durable(true).build();\n    }\n \n    //声明email队列\n    /*\n     *   new queue(queue_email,true,false,false)\n     *   durable="true" 持久化 rabbitmq重启的时候不需要创建新的队列\n     *   auto-delete 表示消息队列没有在使用时将被自动删除 默认是false\n     *   exclusive  表示该消息队列是否只在当前connection生效,默认是false\n     */\n    @bean(queue_email)\n    public queue emailqueue(){\n        return new queue(queue_email);\n    }\n    //声明sms队列\n    @bean(queue_sms)\n    public queue smsqueue(){\n        return new queue(queue_sms);\n    }\n \n    //routingkey_email队列绑定交换机，指定routingkey\n    @bean\n    public binding bindingemail(@qualifier(queue_email) queue queue,\n                                @qualifier(exchange_name) exchange exchange){\n        return bindingbuilder.bind(queue).to(exchange).with(routingkey_email).noargs();\n    }\n    //routingkey_sms队列绑定交换机，指定routingkey\n    @bean\n    public binding bindingsms(@qualifier(queue_sms) queue queue,\n                              @qualifier(exchange_name) exchange exchange){\n        return bindingbuilder.bind(queue).to(exchange).with(routingkey_sms).noargs();\n    }\n \n}\n\n\n\n# 生产者(mq-rabbitmq-producer)\n\n为了方便测试，我直接把生产者代码放工程测试类：发送routing key是"topic.sms.email"的消息，那么mq-rabbitmq-consumer下那些监听的（与交换机(topic.exchange)绑定，并且订阅的routingkey中匹配了"topic.sms.email"规则的） 队列就会收到消息。\n\n@springboottest\n@runwith(springrunner.class)\npublic class send {\n \n    @autowired\n    rabbittemplate rabbittemplate;\n    \n    @test\n    public void sendmsgbytopics(){\n \n        /**\n         * 参数：\n         * 1、交换机名称\n         * 2、routingkey\n         * 3、消息内容\n         */\n        for (int i=0;i<5;i++){\n            string message = "恭喜您，注册成功！userid="+i;\n            rabbittemplate.convertandsend(rabbitmqconfig.exchange_name,"topic.sms.email",message);\n            system.out.println(" [x] sent \'" + message + "\'");\n        }\n \n    }\n}\n\n\n运行测试类发送5条消息：\n\nweb管理界面： 可以看到已经创建了交换机以及queue_email、queue_sms 2个队列，并且向这两个队列分别发送了5条消息\n\n\n# 消费者(mq-rabbitmq-consumer)\n\n编写一个监听器组件，通过注解配置消费者队列，以及队列与交换机之间绑定关系。（也可以像生产者那样通过配置类配置）\n\n在springamqp中，对消息的消费者进行了封装和抽象。一个javabean的方法，只要添加@rabbitlistener注解，就可以成为了一个消费者。\n\n@component\npublic class receivehandler {\n \n    //监听邮件队列\n    @rabbitlistener(bindings = @queuebinding(\n            value = @queue(value = "queue_email", durable = "true"),\n            exchange = @exchange(\n                    value = "topic.exchange",\n                    ignoredeclarationexceptions = "true",\n                    type = exchangetypes.topic\n            ),\n            key = {"topic.#.email.#","email.*"}))\n    public void rece_email(string msg){\n        system.out.println(" [邮件服务] received : " + msg + "!");\n    }\n \n    //监听短信队列\n    @rabbitlistener(bindings = @queuebinding(\n            value = @queue(value = "queue_sms", durable = "true"),\n            exchange = @exchange(\n                    value = "topic.exchange",\n                    ignoredeclarationexceptions = "true",\n                    type = exchangetypes.topic\n            ),\n            key = {"topic.#.sms.#"}))\n    public void rece_sms(string msg){\n        system.out.println(" [短信服务] received : " + msg + "!");\n    }\n}\n\n\n属性说明：\n\n * @componet：类上的注解，注册到spring容器\n\n * @rabbitlistener：方法上的注解，声明这个方法是一个消费者方法，需要指定下面的属性：\n   \n   * bindings：指定绑定关系，可以有多个。值是@queuebinding的数组。@queuebinding包含下面属性：\n     \n     * value：这个消费者关联的队列。值是@queue，代表一个队列\n     \n     * exchange：队列所绑定的交换机，值是@exchange类型\n     \n     * key：队列和交换机绑定的routingkey，可指定多个\n\n启动mq-rabbitmq-comsumer项目\n\nok,邮件服务和短息服务接收到消息后，就可以各自开展自己的业务了。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"构建单体应用模型",frontmatter:{title:"构建单体应用模型",date:"2022-02-09T11:40:48.000Z",permalink:"/pages/593778/"},regularPath:"/14.%E5%BE%AE%E6%9C%8D%E5%8A%A1/01.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%AE%80%E4%BB%8B/01.%E6%9E%84%E5%BB%BA%E5%8D%95%E4%BD%93%E5%BA%94%E7%94%A8%E6%A8%A1%E5%9E%8B.html",relativePath:"14.微服务/01.微服务简介/01.构建单体应用模型.md",key:"v-12cc11ea",path:"/pages/593778/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:15}],headersStr:"概述",content:"# 构建单体应用模型\n\n\n# 概述\n\n我们假设，您开始开发一个打车应用，打算与 Uber 和 Hailo 竞争。经过初步交流和需求收集，您开始手动或者使用类似 Rails、Spring Boot、Play 或者 Maven 等平台来生成一个新项目。\n\n该新应用是一个模块化的六边形架构，如下图（一个简单的打车应用）所示：\n\n\n\n该应用的核心是由模块实现的业务逻辑，它定义了服务、领域对象和事件。围绕核心的是与外部世界接口对接的适配器。适配器示例包括数据库访问组件、生产和消费消息的消息组件和暴露了 API 或实现了一个 UI 的 web 组件。\n\n尽管有一个逻辑模块化架构，但应用程序被作为一个单体进行打包和部署。实际格式取决于应用程序的语言和框架。例如，许多 Java 应用程序被打包成 WAR 文件部署在如 Tomcat 或者 Jetty 之类的应用服务器上。其他 Java 应用程序被打包成自包含 (self-contained) 的可执行 JAR。类似地， Rails 和 Node.js 应用程序被打包为有目录层次的结构\n\n以这种风格编写的应用是很常见的。他们很容易开发，因为我们的 IDE 和其他工具就是专注于构建单体应用。这些应用程序也很容易测试， 您可以通过简单地启动并使用如 Selenium 测试包来测试 UI 以轻松地实现端到端 (end-to-end) 测试。单体应用同样易于部署。您只需拷贝打包好的应用程序到服务器上。您还可以通过运行多个副本和结合负载均衡器来扩展应用。在项目的早期阶段，它可以良好运作。",normalizedContent:"# 构建单体应用模型\n\n\n# 概述\n\n我们假设，您开始开发一个打车应用，打算与 uber 和 hailo 竞争。经过初步交流和需求收集，您开始手动或者使用类似 rails、spring boot、play 或者 maven 等平台来生成一个新项目。\n\n该新应用是一个模块化的六边形架构，如下图（一个简单的打车应用）所示：\n\n\n\n该应用的核心是由模块实现的业务逻辑，它定义了服务、领域对象和事件。围绕核心的是与外部世界接口对接的适配器。适配器示例包括数据库访问组件、生产和消费消息的消息组件和暴露了 api 或实现了一个 ui 的 web 组件。\n\n尽管有一个逻辑模块化架构，但应用程序被作为一个单体进行打包和部署。实际格式取决于应用程序的语言和框架。例如，许多 java 应用程序被打包成 war 文件部署在如 tomcat 或者 jetty 之类的应用服务器上。其他 java 应用程序被打包成自包含 (self-contained) 的可执行 jar。类似地， rails 和 node.js 应用程序被打包为有目录层次的结构\n\n以这种风格编写的应用是很常见的。他们很容易开发，因为我们的 ide 和其他工具就是专注于构建单体应用。这些应用程序也很容易测试， 您可以通过简单地启动并使用如 selenium 测试包来测试 ui 以轻松地实现端到端 (end-to-end) 测试。单体应用同样易于部署。您只需拷贝打包好的应用程序到服务器上。您还可以通过运行多个副本和结合负载均衡器来扩展应用。在项目的早期阶段，它可以良好运作。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"TTL队列_死信队列",frontmatter:{title:"TTL队列_死信队列",date:"2022-02-09T11:31:48.000Z",permalink:"/pages/c5c502/"},regularPath:"/13.RabbitMQ/02.%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/01.TTL%E9%98%9F%E5%88%97_%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97.html",relativePath:"13.RabbitMQ/02.高级用法/01.TTL队列_死信队列.md",key:"v-8f47802a",path:"/pages/c5c502/",headers:[{level:2,title:"TTL队列(Time To Live)",slug:"ttl队列-time-to-live",normalizedTitle:"ttl队列(time to live)",charIndex:19},{level:3,title:"业务场景",slug:"业务场景",normalizedTitle:"业务场景",charIndex:44},{level:3,title:"代码实现",slug:"代码实现",normalizedTitle:"代码实现",charIndex:54},{level:3,title:"小结",slug:"小结",normalizedTitle:"小结",charIndex:64},{level:2,title:"死信队列",slug:"死信队列",normalizedTitle:"死信队列",charIndex:8},{level:3,title:"什么是死信队列",slug:"什么是死信队列",normalizedTitle:"什么是死信队列",charIndex:80},{level:3,title:"代码实现",slug:"代码实现-2",normalizedTitle:"代码实现",charIndex:54},{level:3,title:"小结",slug:"小结-2",normalizedTitle:"小结",charIndex:64}],headersStr:"TTL队列(Time To Live) 业务场景 代码实现 小结 死信队列 什么是死信队列 代码实现 小结",content:'# TTL队列_死信队列\n\n\n\n * TTL队列(Time To Live)\n   * 业务场景\n   * 代码实现\n   * 小结\n * 死信队列\n   * 什么是死信队列\n   * 代码实现\n   * 小结\n\n\n\n\n# TTL队列(Time To Live)\n\nTTL队列\n\n简单的说就是队列中的消息是有时间限制的，如果超时，那么这个消息将会被队列删除\n\n\n# 业务场景\n\n淘宝购物，提交订单之后，在一定的时间内（30分钟）、订单如果没有支付，那么订单就会自动关闭，如果想要购买商品，就需要重新下单。实现这个功能，就使用到了这个 TTL队列\n\n\n# 代码实现\n\n# 编写生产者\n\n// 生产者\npublic class Producer {\n    private static final String QUEUE_NAME = "queue_ttl_1";\n\n    public static void main(String[] args) throws IOException, TimeoutException {\n        Connection connection = ConnectionUtils.getConnection();\n        Channel channel = connection.createChannel();\n        // 设置队列的属性\n        Map<String, Object> map = new HashMap<>(1);\n        // 如果是 ttl 队列，需要设置键为：x-message-ttl，值为：过期时间（毫秒）\n        map.put("x-message-ttl", 10000);\n        channel.queueDeclare(QUEUE_NAME, false, false, false, map);\n        channel.basicPublish("", QUEUE_NAME, null, "ttl队列...".getBytes());\n        channel.close();\n        connection.close();\n    }\n}\n\n\n# 编写消费者\n\n// 消费者\npublic class Consumer {\n    private static final String QUEUE_NAME = "queue_ttl_1";\n\n    public static void main(String[] args) throws IOException, TimeoutException {\n        Connection connection = ConnectionUtils.getConnection();\n        Channel channel = connection.createChannel();\n        Map<String, Object> map = new HashMap<>(1);\n        // 如果是 ttl 队列，需要设置键为：x-message-ttl，值为：过期时间（毫秒）\n        map.put("x-message-ttl", 10000);\n        channel.queueDeclare(QUEUE_NAME, false, false, false, map);\n        DefaultConsumer defaultConsumer = new DefaultConsumer(channel) {\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\n                System.out.println("消费者接收到消息：" + new String(body));\n                System.out.println(new SimpleDateFormat("hh: mm: ss").format(new Date()));\n                channel.basicAck(envelope.getDeliveryTag(), false);\n            }\n        };\n        System.out.println(new SimpleDateFormat("hh: mm: ss").format(new Date()));\n        channel.basicConsume(QUEUE_NAME, false, defaultConsumer);\n    }\n}\n\n\n# 测试TTL\n\n启动消费者，再启动生产者，然后重新启动消费者\n\n可以看出，生产者发送消息时，消费者可以接收到，当重新启动消费者时，由于间隔时间大于 TTL 设置的过期时间10秒，所以第二次并没有得到值，也就证明了，队列中的消息已经被删除。\n\n\n# 小结\n\n 1. 设置TTL队列，需要在创建队列的时候，设置上一个属性 x-message-ttl\n 2. 设置方式：map.put("x-message-ttl", 10000);\n\n\n# 死信队列\n\n\n# 什么是死信队列\n\n当消息在队列中编程死信之后、可以定义它重新push 到另外一个交换机上、这个交换机 也有自己对应的队列 这个队列就称为死信队列\n\n消息变成死信的原因：\n\n 1. 发送到队列中的消息被拒绝了\n 2. 消息的 ttl 时间过期了\n 3. 队列达到了最大长度，再往里面放信息，放不进去的消息\n\n在满足死信的前提下，定义一个队列，这个队列专门用来存放死信，就是死信队列\n\n死信队列也是一个正常的队列，和一般的队列没有什么区别\n\n当这个队列中如果有这个死信的时候，rabbitmq 就会将这个消息自动发送到我们提前定义好的死信队列中去（简单的说就是路由到另外一个队列）\n\n死信的流程：\n\n> 生产者 --\x3e 发送消息 --\x3e TTL交换机 --\x3e TTL队列 --\x3e 变成死信队列 --\x3e DLX交换机 --\x3e DLX队列 --\x3e 消费者\n\n\n\n\n# 代码实现\n\n这里使用 TTL 测试死信队列\n\n# 编写生产者\n\n// 生产者\npublic class Producer {\n    // ttl队列的交换机\n    private static final String EXCHANGE_NAME_TTL = "exchange_ttl";\n    // 路由的key\n    private static final String ROUTING_KEY = "dlx.km";\n\n    public static void main(String[] args) throws IOException, TimeoutException {\n        Connection connection = ConnectionUtils.getConnection();\n        Channel channel = connection.createChannel();\n        for (int i = 0; i < 5; i++) {\n            channel.basicPublish(EXCHANGE_NAME_TTL, ROUTING_KEY, null, ("TTL队列测试死信队列" + i).getBytes());\n        }\n        channel.close();\n        connection.close();\n    }\n}\n\n\n# 编写消费者\n\n// 消费者\npublic class Consumer {\n    // ttl队列的交换机\n    private static final String EXCHANGE_NAME_TTL = "exchange_ttl";\n    // ttl队列\n    private static final String QUEUE_NAME_TTL = "queue_ttl";\n    // dlx交换机\n    private static final String EXCHANGE_NAME_DLX = "exchange_dlx";\n    // dlx队列\n    private static final String QUEUE_NAME_DLX = "queue_dlx";\n\n    public static void main(String[] args) throws IOException, TimeoutException {\n        Connection connection = ConnectionUtils.getConnection();\n        final Channel channel = connection.createChannel();\n        // 创建topic模式交换机，开启持久化\n        channel.exchangeDeclare(EXCHANGE_NAME_TTL, "topic", true);\n        Map<String, Object> map = new HashMap<>();\n        map.put("x-message-ttl", 10000);\n        map.put("x-dead-letter-exchange", EXCHANGE_NAME_DLX);\n        channel.queueDeclare(QUEUE_NAME_TTL, true, false, false, map);\n        // 将队列和交换机进行绑定\n        channel.queueBind(QUEUE_NAME_TTL, EXCHANGE_NAME_TTL, "dlx.#");\n\n        // 开始声明死信交换机和死信队列\n        // 添加一个死信的属性  后面这个值就是死信队列交换机的名字\n        channel.exchangeDeclare(EXCHANGE_NAME_DLX, "topic", true);\n        channel.queueDeclare(QUEUE_NAME_DLX, true, false, false, null);\n        channel.queueBind(QUEUE_NAME_DLX, EXCHANGE_NAME_DLX, "#");\n\n        DefaultConsumer defaultConsumer = new DefaultConsumer(channel) {\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\n                System.out.println("成功获取到数据：" + new String(body));\n            }\n        };\n        channel.basicConsume(QUEUE_NAME_TTL, true, defaultConsumer);\n//        channel.basicConsume(QUEUE_NAME_DLX, true, defaultConsumer);\n    }\n}\n\n\n# 测试结果\n\n先启动消费者，再启动生产者\n\n当设置 channel.basicConsume(QUEUE_NAME_TTL, true, defaultConsumer); 时，也就是监听 TTL 队列消息的时候，控制台马上打印出下图所示信息。\n\n当设置 channel.basicConsume(QUEUE_NAME_DLX, true, defaultConsumer); 时，监听死信队列的消息的时候，控制台在消息发送了 10秒钟之后才打印出以下信息。\n\n证明 TTL过期的信息，成功进入到死信队列中。动态效果也可以在 RabbitMQ控制台中看出来。\n\n\n\n\n# 小结\n\n 1. 核心代码：map.put("x-dead-letter-exchange", EXCHANGE_NAME_DLX); 一定要设置 ttl 队列的死信属性\n 2. 死信队列就是一个普通的队列\n 3. 应用场景：消息的延迟发送，定时关闭订单等。',normalizedContent:'# ttl队列_死信队列\n\n\n\n * ttl队列(time to live)\n   * 业务场景\n   * 代码实现\n   * 小结\n * 死信队列\n   * 什么是死信队列\n   * 代码实现\n   * 小结\n\n\n\n\n# ttl队列(time to live)\n\nttl队列\n\n简单的说就是队列中的消息是有时间限制的，如果超时，那么这个消息将会被队列删除\n\n\n# 业务场景\n\n淘宝购物，提交订单之后，在一定的时间内（30分钟）、订单如果没有支付，那么订单就会自动关闭，如果想要购买商品，就需要重新下单。实现这个功能，就使用到了这个 ttl队列\n\n\n# 代码实现\n\n# 编写生产者\n\n// 生产者\npublic class producer {\n    private static final string queue_name = "queue_ttl_1";\n\n    public static void main(string[] args) throws ioexception, timeoutexception {\n        connection connection = connectionutils.getconnection();\n        channel channel = connection.createchannel();\n        // 设置队列的属性\n        map<string, object> map = new hashmap<>(1);\n        // 如果是 ttl 队列，需要设置键为：x-message-ttl，值为：过期时间（毫秒）\n        map.put("x-message-ttl", 10000);\n        channel.queuedeclare(queue_name, false, false, false, map);\n        channel.basicpublish("", queue_name, null, "ttl队列...".getbytes());\n        channel.close();\n        connection.close();\n    }\n}\n\n\n# 编写消费者\n\n// 消费者\npublic class consumer {\n    private static final string queue_name = "queue_ttl_1";\n\n    public static void main(string[] args) throws ioexception, timeoutexception {\n        connection connection = connectionutils.getconnection();\n        channel channel = connection.createchannel();\n        map<string, object> map = new hashmap<>(1);\n        // 如果是 ttl 队列，需要设置键为：x-message-ttl，值为：过期时间（毫秒）\n        map.put("x-message-ttl", 10000);\n        channel.queuedeclare(queue_name, false, false, false, map);\n        defaultconsumer defaultconsumer = new defaultconsumer(channel) {\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) throws ioexception {\n                system.out.println("消费者接收到消息：" + new string(body));\n                system.out.println(new simpledateformat("hh: mm: ss").format(new date()));\n                channel.basicack(envelope.getdeliverytag(), false);\n            }\n        };\n        system.out.println(new simpledateformat("hh: mm: ss").format(new date()));\n        channel.basicconsume(queue_name, false, defaultconsumer);\n    }\n}\n\n\n# 测试ttl\n\n启动消费者，再启动生产者，然后重新启动消费者\n\n可以看出，生产者发送消息时，消费者可以接收到，当重新启动消费者时，由于间隔时间大于 ttl 设置的过期时间10秒，所以第二次并没有得到值，也就证明了，队列中的消息已经被删除。\n\n\n# 小结\n\n 1. 设置ttl队列，需要在创建队列的时候，设置上一个属性 x-message-ttl\n 2. 设置方式：map.put("x-message-ttl", 10000);\n\n\n# 死信队列\n\n\n# 什么是死信队列\n\n当消息在队列中编程死信之后、可以定义它重新push 到另外一个交换机上、这个交换机 也有自己对应的队列 这个队列就称为死信队列\n\n消息变成死信的原因：\n\n 1. 发送到队列中的消息被拒绝了\n 2. 消息的 ttl 时间过期了\n 3. 队列达到了最大长度，再往里面放信息，放不进去的消息\n\n在满足死信的前提下，定义一个队列，这个队列专门用来存放死信，就是死信队列\n\n死信队列也是一个正常的队列，和一般的队列没有什么区别\n\n当这个队列中如果有这个死信的时候，rabbitmq 就会将这个消息自动发送到我们提前定义好的死信队列中去（简单的说就是路由到另外一个队列）\n\n死信的流程：\n\n> 生产者 --\x3e 发送消息 --\x3e ttl交换机 --\x3e ttl队列 --\x3e 变成死信队列 --\x3e dlx交换机 --\x3e dlx队列 --\x3e 消费者\n\n\n\n\n# 代码实现\n\n这里使用 ttl 测试死信队列\n\n# 编写生产者\n\n// 生产者\npublic class producer {\n    // ttl队列的交换机\n    private static final string exchange_name_ttl = "exchange_ttl";\n    // 路由的key\n    private static final string routing_key = "dlx.km";\n\n    public static void main(string[] args) throws ioexception, timeoutexception {\n        connection connection = connectionutils.getconnection();\n        channel channel = connection.createchannel();\n        for (int i = 0; i < 5; i++) {\n            channel.basicpublish(exchange_name_ttl, routing_key, null, ("ttl队列测试死信队列" + i).getbytes());\n        }\n        channel.close();\n        connection.close();\n    }\n}\n\n\n# 编写消费者\n\n// 消费者\npublic class consumer {\n    // ttl队列的交换机\n    private static final string exchange_name_ttl = "exchange_ttl";\n    // ttl队列\n    private static final string queue_name_ttl = "queue_ttl";\n    // dlx交换机\n    private static final string exchange_name_dlx = "exchange_dlx";\n    // dlx队列\n    private static final string queue_name_dlx = "queue_dlx";\n\n    public static void main(string[] args) throws ioexception, timeoutexception {\n        connection connection = connectionutils.getconnection();\n        final channel channel = connection.createchannel();\n        // 创建topic模式交换机，开启持久化\n        channel.exchangedeclare(exchange_name_ttl, "topic", true);\n        map<string, object> map = new hashmap<>();\n        map.put("x-message-ttl", 10000);\n        map.put("x-dead-letter-exchange", exchange_name_dlx);\n        channel.queuedeclare(queue_name_ttl, true, false, false, map);\n        // 将队列和交换机进行绑定\n        channel.queuebind(queue_name_ttl, exchange_name_ttl, "dlx.#");\n\n        // 开始声明死信交换机和死信队列\n        // 添加一个死信的属性  后面这个值就是死信队列交换机的名字\n        channel.exchangedeclare(exchange_name_dlx, "topic", true);\n        channel.queuedeclare(queue_name_dlx, true, false, false, null);\n        channel.queuebind(queue_name_dlx, exchange_name_dlx, "#");\n\n        defaultconsumer defaultconsumer = new defaultconsumer(channel) {\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) throws ioexception {\n                system.out.println("成功获取到数据：" + new string(body));\n            }\n        };\n        channel.basicconsume(queue_name_ttl, true, defaultconsumer);\n//        channel.basicconsume(queue_name_dlx, true, defaultconsumer);\n    }\n}\n\n\n# 测试结果\n\n先启动消费者，再启动生产者\n\n当设置 channel.basicconsume(queue_name_ttl, true, defaultconsumer); 时，也就是监听 ttl 队列消息的时候，控制台马上打印出下图所示信息。\n\n当设置 channel.basicconsume(queue_name_dlx, true, defaultconsumer); 时，监听死信队列的消息的时候，控制台在消息发送了 10秒钟之后才打印出以下信息。\n\n证明 ttl过期的信息，成功进入到死信队列中。动态效果也可以在 rabbitmq控制台中看出来。\n\n\n\n\n# 小结\n\n 1. 核心代码：map.put("x-dead-letter-exchange", exchange_name_dlx); 一定要设置 ttl 队列的死信属性\n 2. 死信队列就是一个普通的队列\n 3. 应用场景：消息的延迟发送，定时关闭订单等。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"走向单体地狱",frontmatter:{title:"走向单体地狱",date:"2022-02-09T11:40:48.000Z",permalink:"/pages/72d00f/"},regularPath:"/14.%E5%BE%AE%E6%9C%8D%E5%8A%A1/01.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%AE%80%E4%BB%8B/02.%E8%B5%B0%E5%90%91%E5%8D%95%E4%BD%93%E5%9C%B0%E7%8B%B1.html",relativePath:"14.微服务/01.微服务简介/02.走向单体地狱.md",key:"v-dc3c5474",path:"/pages/72d00f/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:13}],headersStr:"概述",content:"# 走向单体地狱\n\n\n# 概述\n\n不幸的是，这种简单的方法有很大的局限性。成功的应用有一个趋势，随着时间推移而变得越来越臃肿。您的开发团队在每个冲刺阶段都要实现更多的用户需求，这意味着需要添加了许多行代码。几年之后，小而简单的应用将会逐渐成长成一个庞大的单体。为了给出一个极端示例，我最近和一位开发者做了交谈，他正在编写一个工具，该工具用于从他们的数百万行代码 (lines of code， LOC) 应用中分析出数千个 JAR 之间的依赖。我相信这是大量开发者在多年齐心协力下创造出了这样的野兽。\n\n一旦您的应用程序成为了一个庞大、复杂的单体，您的开发组织可能会陷入了一个痛苦的境地，敏捷开发和交付的任何一次尝试都将原地徘徊。一个主要问题是应用程序实在非常复杂。对于任何一个开发人员来说显得过于庞大，这是可以理解的。最终，正确修复 bug 和实现新功能变得非常困难而耗时。此外， 这种趋势就像是往下的螺旋。如果基本代码都令人难以理解，那么改变也不会变得正确，您最终得到的将是一个巨大且不可思议的大泥球。\n\n应用程序的规模也将减缓发展。应用程序越大，启动时间越长。我调查过开发者们的单体应用的大小和性能，一些报告的启动时间为 12 分钟。我也听说过应用程序启动需要 40 分钟以上的怪事。如果开发人员经常要重启应用服务器，那么很大一部分时间都是在等待中度过，他们的生产力将受到限制。\n\n另一个大问题是，复杂的单体应用本身就是持续部署的障碍。如今， SaaS 应用发展到了可以每天多次将变更推送到生产环境中。这对于复杂的单体来说非常困难，因为您需要重新部署整个应用程序才能更新其中任何一部分。 联想到我之前提到的漫长启动时间，这也不会是什么好事。此外，因变更所产生的影响通常不是很明确，您很可能需要做大量的手工测试。因此，持续部署是不可能做到的。\n\n当不同模块存在资源需求冲突时，单体应用可能难以扩展。例如，一个模块可能会执行 CPU 密集型图像处理逻辑，理想情况下是部署在 Amazon EC2 Compute Optimized 实例中。另一个模块可能是一个内存数据库，最适合部署到 EC2 Memory-optimized 实例。然而， 由于这些模块被部署在一起，您必须在硬件选择上做出妥协。\n\n单体应用的另一个问题是可靠性。因为所有模块都运行在同一进程中。任何模块的一个 bug，比如内存泄漏，可能会拖垮整个进程。此外，由于应用程序的所有实例都是相同的，该错误将影响到整个应用的可用性。\n\n最后但同样重要，单体应用使得采用新框架和语言变得非常困难。例如，我们假设您有 200 万行代码使用了 XYZ 框架编写。如果使用较新的 ABC 框架来重写整个应用，这将非常昂贵（在时间和成本方面），即使框架非常好。因此，这对于采用新技术是一个非常大的障碍。在项目开始时， 您无论选择何种新技术都会感到困扰。\n\n总结一下：您有一个成功的关键业务应用程序，它已经发展成为一个只有少数开发人员（如果有的话）能够理解的巨大单体。它使用了过时、非生产性技术编写，这使得招聘优秀开发人员变得非常困难。应用程序变得难以扩展，不可靠。因此敏捷开发和应用交付是不可能的。",normalizedContent:"# 走向单体地狱\n\n\n# 概述\n\n不幸的是，这种简单的方法有很大的局限性。成功的应用有一个趋势，随着时间推移而变得越来越臃肿。您的开发团队在每个冲刺阶段都要实现更多的用户需求，这意味着需要添加了许多行代码。几年之后，小而简单的应用将会逐渐成长成一个庞大的单体。为了给出一个极端示例，我最近和一位开发者做了交谈，他正在编写一个工具，该工具用于从他们的数百万行代码 (lines of code， loc) 应用中分析出数千个 jar 之间的依赖。我相信这是大量开发者在多年齐心协力下创造出了这样的野兽。\n\n一旦您的应用程序成为了一个庞大、复杂的单体，您的开发组织可能会陷入了一个痛苦的境地，敏捷开发和交付的任何一次尝试都将原地徘徊。一个主要问题是应用程序实在非常复杂。对于任何一个开发人员来说显得过于庞大，这是可以理解的。最终，正确修复 bug 和实现新功能变得非常困难而耗时。此外， 这种趋势就像是往下的螺旋。如果基本代码都令人难以理解，那么改变也不会变得正确，您最终得到的将是一个巨大且不可思议的大泥球。\n\n应用程序的规模也将减缓发展。应用程序越大，启动时间越长。我调查过开发者们的单体应用的大小和性能，一些报告的启动时间为 12 分钟。我也听说过应用程序启动需要 40 分钟以上的怪事。如果开发人员经常要重启应用服务器，那么很大一部分时间都是在等待中度过，他们的生产力将受到限制。\n\n另一个大问题是，复杂的单体应用本身就是持续部署的障碍。如今， saas 应用发展到了可以每天多次将变更推送到生产环境中。这对于复杂的单体来说非常困难，因为您需要重新部署整个应用程序才能更新其中任何一部分。 联想到我之前提到的漫长启动时间，这也不会是什么好事。此外，因变更所产生的影响通常不是很明确，您很可能需要做大量的手工测试。因此，持续部署是不可能做到的。\n\n当不同模块存在资源需求冲突时，单体应用可能难以扩展。例如，一个模块可能会执行 cpu 密集型图像处理逻辑，理想情况下是部署在 amazon ec2 compute optimized 实例中。另一个模块可能是一个内存数据库，最适合部署到 ec2 memory-optimized 实例。然而， 由于这些模块被部署在一起，您必须在硬件选择上做出妥协。\n\n单体应用的另一个问题是可靠性。因为所有模块都运行在同一进程中。任何模块的一个 bug，比如内存泄漏，可能会拖垮整个进程。此外，由于应用程序的所有实例都是相同的，该错误将影响到整个应用的可用性。\n\n最后但同样重要，单体应用使得采用新框架和语言变得非常困难。例如，我们假设您有 200 万行代码使用了 xyz 框架编写。如果使用较新的 abc 框架来重写整个应用，这将非常昂贵（在时间和成本方面），即使框架非常好。因此，这对于采用新技术是一个非常大的障碍。在项目开始时， 您无论选择何种新技术都会感到困扰。\n\n总结一下：您有一个成功的关键业务应用程序，它已经发展成为一个只有少数开发人员（如果有的话）能够理解的巨大单体。它使用了过时、非生产性技术编写，这使得招聘优秀开发人员变得非常困难。应用程序变得难以扩展，不可靠。因此敏捷开发和应用交付是不可能的。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"微服务-解决复杂问题",frontmatter:{title:"微服务-解决复杂问题",date:"2022-02-09T11:40:48.000Z",permalink:"/pages/0b2d6e/"},regularPath:"/14.%E5%BE%AE%E6%9C%8D%E5%8A%A1/01.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%AE%80%E4%BB%8B/03.%E5%BE%AE%E6%9C%8D%E5%8A%A1-%E8%A7%A3%E5%86%B3%E5%A4%8D%E6%9D%82%E9%97%AE%E9%A2%98.html",relativePath:"14.微服务/01.微服务简介/03.微服务-解决复杂问题.md",key:"v-08740c32",path:"/pages/0b2d6e/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:17}],headersStr:"概述",content:"# 微服务-解决复杂问题\n\n\n# 概述\n\n许多如 Amazon、 eBay 和 Netflix 这样的组织，已经采用现在所谓的微服务架构模式解决了这个问题，而不是构建一个臃肿的单体应用。它的思路是将应用程序分解成一套较小的互连服务。一个服务通常实现了一组不同的特性或功能，例如订单管理、客户管理等。每一个微服务都是一个迷你应用，它自己的六边形架构包括了业务逻辑以及多个适配器。\n\n一些微服务会暴露一个供其他微服务或应用客户端消费的 API。其他微服务可能实现了一个 web UI。在运行时，每个实例通常是一个云虚拟机 (virtual machine， VM) 或者一个 Docker 容器。\n\n例如，前面描述的系统可能分解成如下图（一个单体应用分解成微服务) 所示：\n\n\n\n应用程序的每个功能区域现在都由自己的微服务实现。此外，Web 应用程序被划分为一组更简单的 Web 应用程序。例如，以我们的出租车为例，一个是乘客的应用，一个是司机的应用。这使得它更容易地为特定的用户、司机、设备或者专门的用例部署不同的场景。每个后端服务暴露一个 REST API，大部分服务消费的 API 由其他服务提供。例如， Driver Management 使用了 Notification 服务器来通知一个可用司机一个可选路程。UI 服务调用了其他服务来渲染页面。服务也可以使用异步、基于消息的通信。\n\n一些 REST API 也暴露给移动端应用以供司机和乘客使用。然而，应用不能直接访问后端服务。相反，他们之间的通信是由一个称为 API 网关 (API Gateway) 的中介负责。 API 网关负责负载均衡、缓存、访问控制、 API 计量和监控， 可以通过使用 NGINX 来实现。\n\n开发和交付中的伸缩立方：\n\n\n\n微服务架构模式相当于此伸缩立方的 Y 轴坐标，此立方是一个来自《架构即未来》 的三维伸缩模型。另外两个坐标轴是由运行多个相同应用程序副本的负载均衡器组成的X 轴坐标和 Z 轴坐标（或数据分区) ，其中请求的属性（例如，一行记录的主键或者客户标识) 用于将请求路由到特定的服务器。\n\n在运行时，X 坐标轴上运行着服务的多个实例，每个服务配合负载均衡器以满足吞吐量和可用性。某些应用程序也有可能使用 Z 坐标轴来进行分区服务。下图展示了如何用 Docker 将 Trip Management 服务部署到 Amazon EC2 上运行。\n\n使用 Docker 部署 Trip Management 服务：\n\n\n\n在运行时，Trip Management 服务由多个服务实例组成，每个服务实例是一个 Docker容器。为了实现高可用，容器是在多个云虚拟机上运行的。服务实例的之前是一个类似 NGINX 的负载均衡器，用于跨实例分发请求。负载均衡器也可以处理其他问题，如缓存、访问控制、API 度量和监控。\n\n微服务架构模式明显影响到了应用程序与数据库之间的关系。与其他共享单个数据库模式 (schema) 服务有所不同， 其每一个服务都有自己的数据库模式。一方面，这种做法与企业级数据库数据模型的想法相背，此外，它经常导致部分数据冗余。然而，如果您想从微服务中受益，每一个服务都应该有自己的数据库模式。因为它能实现松耦合。下图展示了数据库架构示例应用程序。\n\n每个服务都拥有各自的数据库。而且，服务可以使用一种最适合其需求、号称多语言持久架构 （polyglot persistence architecture ) 的数据库。例如，DriverManagement，要找到与潜在乘客接近的司机，就必须使用支持高效地理查询的数据库。\n\n打车应用的数据库架构：\n\n\n\n从表面上看，微服务架构模式类似于 SOA。微服务是由一组服务组成。然而，换另一种方式去思考微服务架构模式，它是没有商业化的 SOA，没有集成 Web 服务规范 (WS-) 和企业服务总线 (Enterprise Service Bus， ESB) 。基于微服务的应用支持更简单、轻量级的协议，例如，REST，而不是 WS-。他们也尽量避免使用 ESB，而是实现微服务本身具有类似 ESB 的功能。微服务架构也拒绝了 SOA 的其他部分，例如，数据访问规范模式概念。",normalizedContent:"# 微服务-解决复杂问题\n\n\n# 概述\n\n许多如 amazon、 ebay 和 netflix 这样的组织，已经采用现在所谓的微服务架构模式解决了这个问题，而不是构建一个臃肿的单体应用。它的思路是将应用程序分解成一套较小的互连服务。一个服务通常实现了一组不同的特性或功能，例如订单管理、客户管理等。每一个微服务都是一个迷你应用，它自己的六边形架构包括了业务逻辑以及多个适配器。\n\n一些微服务会暴露一个供其他微服务或应用客户端消费的 api。其他微服务可能实现了一个 web ui。在运行时，每个实例通常是一个云虚拟机 (virtual machine， vm) 或者一个 docker 容器。\n\n例如，前面描述的系统可能分解成如下图（一个单体应用分解成微服务) 所示：\n\n\n\n应用程序的每个功能区域现在都由自己的微服务实现。此外，web 应用程序被划分为一组更简单的 web 应用程序。例如，以我们的出租车为例，一个是乘客的应用，一个是司机的应用。这使得它更容易地为特定的用户、司机、设备或者专门的用例部署不同的场景。每个后端服务暴露一个 rest api，大部分服务消费的 api 由其他服务提供。例如， driver management 使用了 notification 服务器来通知一个可用司机一个可选路程。ui 服务调用了其他服务来渲染页面。服务也可以使用异步、基于消息的通信。\n\n一些 rest api 也暴露给移动端应用以供司机和乘客使用。然而，应用不能直接访问后端服务。相反，他们之间的通信是由一个称为 api 网关 (api gateway) 的中介负责。 api 网关负责负载均衡、缓存、访问控制、 api 计量和监控， 可以通过使用 nginx 来实现。\n\n开发和交付中的伸缩立方：\n\n\n\n微服务架构模式相当于此伸缩立方的 y 轴坐标，此立方是一个来自《架构即未来》 的三维伸缩模型。另外两个坐标轴是由运行多个相同应用程序副本的负载均衡器组成的x 轴坐标和 z 轴坐标（或数据分区) ，其中请求的属性（例如，一行记录的主键或者客户标识) 用于将请求路由到特定的服务器。\n\n在运行时，x 坐标轴上运行着服务的多个实例，每个服务配合负载均衡器以满足吞吐量和可用性。某些应用程序也有可能使用 z 坐标轴来进行分区服务。下图展示了如何用 docker 将 trip management 服务部署到 amazon ec2 上运行。\n\n使用 docker 部署 trip management 服务：\n\n\n\n在运行时，trip management 服务由多个服务实例组成，每个服务实例是一个 docker容器。为了实现高可用，容器是在多个云虚拟机上运行的。服务实例的之前是一个类似 nginx 的负载均衡器，用于跨实例分发请求。负载均衡器也可以处理其他问题，如缓存、访问控制、api 度量和监控。\n\n微服务架构模式明显影响到了应用程序与数据库之间的关系。与其他共享单个数据库模式 (schema) 服务有所不同， 其每一个服务都有自己的数据库模式。一方面，这种做法与企业级数据库数据模型的想法相背，此外，它经常导致部分数据冗余。然而，如果您想从微服务中受益，每一个服务都应该有自己的数据库模式。因为它能实现松耦合。下图展示了数据库架构示例应用程序。\n\n每个服务都拥有各自的数据库。而且，服务可以使用一种最适合其需求、号称多语言持久架构 （polyglot persistence architecture ) 的数据库。例如，drivermanagement，要找到与潜在乘客接近的司机，就必须使用支持高效地理查询的数据库。\n\n打车应用的数据库架构：\n\n\n\n从表面上看，微服务架构模式类似于 soa。微服务是由一组服务组成。然而，换另一种方式去思考微服务架构模式，它是没有商业化的 soa，没有集成 web 服务规范 (ws-) 和企业服务总线 (enterprise service bus， esb) 。基于微服务的应用支持更简单、轻量级的协议，例如，rest，而不是 ws-。他们也尽量避免使用 esb，而是实现微服务本身具有类似 esb 的功能。微服务架构也拒绝了 soa 的其他部分，例如，数据访问规范模式概念。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"微服务的优点",frontmatter:{title:"微服务的优点",date:"2022-02-09T11:40:48.000Z",permalink:"/pages/47dac2/"},regularPath:"/14.%E5%BE%AE%E6%9C%8D%E5%8A%A1/01.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%AE%80%E4%BB%8B/04.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E4%BC%98%E7%82%B9.html",relativePath:"14.微服务/01.微服务简介/04.微服务的优点.md",key:"v-3704fefa",path:"/pages/47dac2/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:13}],headersStr:"概述",content:"# 微服务的优点\n\n\n# 概述\n\n微服务架构模式有许多非常好的地方。\n\n第一，它解决了复杂问题。它把可能会变得庞大的单体应用程序分解成一套服务。虽然功能数量不变，但是应用程序已经被分解成可管理的块或者服务。每个服务都有一个明确定义边界的方式，如远程过程调用（RPC）驱动或消息驱动 API。微服务架构模式强制一定程度的模块化，实际上，使用单体代码来实现是极其困难的。因此，使用微服务架构模式，个体服务能被更快地开发，并更容易理解与维护。\n\n第二，这种架构使得每个服务都可以由一个团队独立专注开发。开发者可以自由选择任何符合服务 API 契约的技术。当然，更多的组织是希望通过技术选型限制来避免完全混乱的状态。然而，这种自由意味着开发人员不再有可能在这种自由的新项目开始时使用过时的技术。当编写一个新服务时，他们可以选择当前的技术。此外，由于服务较小，使用当前技术重写旧服务将变得更加可行。\n\n第三，微服务架构模式可以实现每个微服务独立部署。开发人员根本不需要去协调部署本地变更到服务。这些变更一经测试即可立即部署。比如，UI 团队可以执行 A|B 测试，并快速迭代 UI 变更。微服务架构模式使得持续部署成为可能。\n\n最后，微服务架构模式使得每个服务能够独立扩展。您可以仅部署满足每个服务的容量和可用性约束的实例数目。此外，您可以使用与服务资源要求最匹配的硬件。例如，您可以在 EC2 Compute Optimized 实例上部署一个 CPU 密集型图像处理服务，并且在 EC2 Memory-optimized 实例上部署一个内存数据库服务。",normalizedContent:"# 微服务的优点\n\n\n# 概述\n\n微服务架构模式有许多非常好的地方。\n\n第一，它解决了复杂问题。它把可能会变得庞大的单体应用程序分解成一套服务。虽然功能数量不变，但是应用程序已经被分解成可管理的块或者服务。每个服务都有一个明确定义边界的方式，如远程过程调用（rpc）驱动或消息驱动 api。微服务架构模式强制一定程度的模块化，实际上，使用单体代码来实现是极其困难的。因此，使用微服务架构模式，个体服务能被更快地开发，并更容易理解与维护。\n\n第二，这种架构使得每个服务都可以由一个团队独立专注开发。开发者可以自由选择任何符合服务 api 契约的技术。当然，更多的组织是希望通过技术选型限制来避免完全混乱的状态。然而，这种自由意味着开发人员不再有可能在这种自由的新项目开始时使用过时的技术。当编写一个新服务时，他们可以选择当前的技术。此外，由于服务较小，使用当前技术重写旧服务将变得更加可行。\n\n第三，微服务架构模式可以实现每个微服务独立部署。开发人员根本不需要去协调部署本地变更到服务。这些变更一经测试即可立即部署。比如，ui 团队可以执行 a|b 测试，并快速迭代 ui 变更。微服务架构模式使得持续部署成为可能。\n\n最后，微服务架构模式使得每个服务能够独立扩展。您可以仅部署满足每个服务的容量和可用性约束的实例数目。此外，您可以使用与服务资源要求最匹配的硬件。例如，您可以在 ec2 compute optimized 实例上部署一个 cpu 密集型图像处理服务，并且在 ec2 memory-optimized 实例上部署一个内存数据库服务。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Spring Cloud 之 Gateway",frontmatter:{title:"Spring Cloud 之 Gateway",date:"2022-02-09T11:44:07.000Z",permalink:"/pages/aa8849/"},regularPath:"/15.Spring%20Cloud%20Gateway/01.Spring%20Cloud%20Gateway/01.Spring%20Cloud%20%E4%B9%8B%20Gateway.html",relativePath:"15.Spring Cloud Gateway/01.Spring Cloud Gateway/01.Spring Cloud 之 Gateway.md",key:"v-fc4d58d8",path:"/pages/aa8849/",headers:[{level:2,title:"Gateway 和 Zuul 的区别",slug:"gateway-和-zuul-的区别",normalizedTitle:"gateway 和 zuul 的区别",charIndex:29},{level:2,title:"快速入门",slug:"快速入门",normalizedTitle:"快速入门",charIndex:529},{level:3,title:"1. pom.xml",slug:"_1-pom-xml",normalizedTitle:"1. pom.xml",charIndex:682},{level:3,title:"2. application.yml",slug:"_2-application-yml",normalizedTitle:"2. application.yml",charIndex:1830},{level:3,title:"3. GatewayApplication.java",slug:"_3-gatewayapplication-java",normalizedTitle:"3. gatewayapplication.java",charIndex:4253},{level:2,title:"过滤器",slug:"过滤器",normalizedTitle:"过滤器",charIndex:3842},{level:3,title:"全局过滤器",slug:"全局过滤器",normalizedTitle:"全局过滤器",charIndex:5056},{level:3,title:"局部过滤器",slug:"局部过滤器",normalizedTitle:"局部过滤器",charIndex:6125},{level:2,title:"动态路由",slug:"动态路由",normalizedTitle:"动态路由",charIndex:7480}],headersStr:"Gateway 和 Zuul 的区别 快速入门 1. pom.xml 2. application.yml 3. GatewayApplication.java 过滤器 全局过滤器 局部过滤器 动态路由",content:'# Spring Cloud 之 Gateway\n\n\n# Gateway 和 Zuul 的区别\n\nZuul 基于servlet 2.5 (works with 3.x)，使用阻塞API。它不支持任何长期的连接，如websocket。\n\nGateway建立在Spring Framework 5，Project Reactor 和Spring Boot 2 上，使用非阻塞API。支持Websocket，因为它与Spring紧密集成，所以它是一个更好的开发者体验。\n\n为什么 Spring Cloud 最初选择了使用 Netflix 几年前开源的 Zuul 作为网关，之后又选择了自建 Gateway 呢？有一种说法是，高性能版的 Zuul2 在经过了多次跳票之后，对于 Spring 这样的整合专家可能也不愿意再继续等待，所以 Spring Cloud Gateway 应运而生。\n\n本文不对 Spring Cloud Gateway 和 Zuul 的性能作太多赘述，基本可以肯定的是 Gateway 作为现在 Spring Cloud 主推的网关方案， Finchley 版本后的 Gateway 比 zuul 1.x 系列的性能和功能整体要好。\n\n\n# 快速入门\n\n我们来搭建一个基于 Eureka 注册中心的简单网关，不对 Gateway 的全部功能做过多解读，毕竟官方文档已经写的很详细了，或者可以阅读中文翻译文档。\n\nSpringBoot 版本号：2.1.6.RELEASE\n\nSpringCloud 版本号：Greenwich.RELEASE\n\n\n# 1. pom.xml\n\n<dependencies>\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-gateway</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-data-redis-reactive</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-actuator</artifactId>\n    </dependency>\n</dependencies>\n\n\n * spring-cloud-starter-gateway：Spring Cloud Gateway 的启动类\n * spring-cloud-starter-netflix-hystrix：Hystrix 作为网关的熔断方案\n * spring-cloud-starter-netflix-eureka-client：将网关纳入 Eureka 注册中心管理\n * spring-boot-starter-data-redis-reactive：限流方案，Spring Cloud Gateway 默认以 redis 实现限流\n * spring-boot-starter-actuator：用来监控 Gateway 的路由信息。\n\n\n# 2. application.yml\n\nspring:\n  application:\n    name: cloud-gateway\n  redis:\n    host: 127.0.0.1\n    timeout: 3000\n    password: xxxx\n    jedis:\n      pool:\n        max-active: 8\n        max-idle: 4\n  cloud:\n    gateway:\n      enabled: true\n      metrics:\n        enabled: true\n      discovery:\n        locator:\n          enabled: true\n      routes:\n        # 普通服务的路由配置\n        - id: cloud-eureka-client\n          uri: lb://cloud-eureka-client\n          order: 0\n          predicates:\n            - Path=/client/**\n          filters:\n            #  parts 参数指示在将请求发送到下游之前，要从请求中去除的路径中的节数。比如我们访问 /client/hello，调用的时候变成 http://localhost:2222/hello\n            - StripPrefix=1\n            # 熔断器\n            - name: Hystrix\n              args:\n                name: fallbackcmd\n                # 降级处理\n                fallbackUri: forward:/fallback\n            # 限流器\n            # 这定义了每个用户 10 个请求的限制。允许 20 个突发，但下一秒只有 10 个请求可用。\n            - name: RequestRateLimiter\n              args:\n                # SPEL 表达式获取 Spring 中的 Bean，这个参数表示根据什么来限流\n                key-resolver: \'#{@ipKeyResolver}\'\n                # 允许用户每秒执行多少请求（令牌桶的填充速率）\n                redis-rate-limiter.replenishRate: 10\n                # 允许用户在一秒内执行的最大请求数。（令牌桶可以保存的令牌数）。将此值设置为零将阻止所有请求。\n                redis-rate-limiter.burstCapacity: 20\n        # websocket 的路由配置\n        - id: websocket service\n          uri: lb:ws://serviceid\n          predicates:\n            - Path=/websocket/**\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        # 开启指定端点\n        include: gateway,metrics\neureka:\n  client:\n    service-url:\n      defaultZone: http://user:password@localhost:1111/eureka/\n\n\n * spring.redis.*： redis 相关配置是为了实现 Gateway 的限流方案。\n * eureka.client.*：eureka 注册中心信息。\n * spring.cloud.gateway.discovery.locator.enabled：将网关配置为基于使用兼容 DiscoveryClient 注册中心注册的服务来创建路由。\n * spring.cloud.gateway.routes.*：配置路由信息\n   * id：路由唯一标识\n   * uri：路由转发地址，以 lb 开头的路由，会由 ribbon 处理，转发到 cloud-eureka-client 的服务处理。也可配置成 http 的单机路由 — http://localhost:2222。\n   * order：路由执行顺序（也可理解成过滤器的执行顺序），执行顺序是从小到大执行，较高的值被解释为较低的优先级。\n   * predicates：路由断言，匹配访问路径为 "/client/**" 的请求。\n   * filters：网关的过滤器配置\n * management.endpoints.web.exposure.include：暴露 actuator 可以访问的端点\n   * /actuator/gateway/routes 查看路由列表\n   * /actuator/gateway/globalfilters 检索全局路由 — 对所有路由生效\n   * /actuator/gateway/routefilters 检索局部路由 — 可配置只对单个路由生效\n   * /actuator/gateway/refresh 清理路由缓存\n   * /actuator/metrics/gateway.requests 获得路由请求数据\n\n\n# 3. GatewayApplication.java\n\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class GatewayApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(GatewayApplication.class, args);\n    }\n\n    /**\n     * 限流的键定义，根据什么来限流\n     */\n    @Bean\n    public KeyResolver ipKeyResolver() {\n        return exchange -> Mono.just(exchange.getRequest().getRemoteAddress().getHostName());\n    }\n\n}\n\n\n\n# 过滤器\n\nSpring Cloud Gateway 同 Zuul 类似，有 “pre” 和 “post” 两种方式的 filter。客户端的请求先经过 “pre” 类型的 filter，然后将请求转发到具体的业务服务，收到业务服务的响应之后，再经过“post”类型的filter处理，最后返回响应到客户端。\n\n与 Zuul 不同的是，filter 除了分为 “pre” 和 “post” 两种方式的 filter 外，在 Spring Cloud Gateway 中，filter 从作用范围可分为另外两种，一种是针对于单个路由的 gateway filter，它需要像上面 application.yml 中的 filters 那样在单个路由中配置；另外一种是针对于全部路由的global gateway filter，不需要单独配置，对所有路由生效。\n\n\n# 全局过滤器\n\n我们通常用全局过滤器实现鉴权、验签、限流、日志输出等。\n\n通过实现 GlobalFilter 接口来自定义 Gateway 的全局过滤器；通过实现 Ordered 接口或者使用 @Order 注解来定义过滤器的执行顺序，执行顺序是从小到大执行，较高的值被解释为较低的优先级。\n\n    @Bean\n    @Order(-1)\n    public GlobalFilter a() {\n        return (exchange, chain) -> {\n            log.info("first pre filter");\n            return chain.filter(exchange).then(Mono.fromRunnable(() -> {\n                log.info("third post filter");\n            }));\n        };\n    }\n\n    @Bean\n    @Order(0)\n    public GlobalFilter b() {\n        return (exchange, chain) -> {\n            log.info("second pre filter");\n            return chain.filter(exchange).then(Mono.fromRunnable(() -> {\n                log.info("second post filter");\n            }));\n        };\n    }\n\n    @Bean\n    @Order(1)\n    public GlobalFilter c() {\n        return (exchange, chain) -> {\n            log.info("third pre filter");\n            return chain.filter(exchange).then(Mono.fromRunnable(() -> {\n                log.info("first post filter");\n            }));\n        };\n    }\n\n\n优先级最高的 filter ，它的 “pre” 过滤器最先执行，“post” 过滤器最晚执行。\n\n\n# 局部过滤器\n\n我们来定义一个 “pre” 类型的局部过滤器：\n\n@Component\npublic class PreGatewayFilterFactory extends AbstractGatewayFilterFactory<PreGatewayFilterFactory.Config> {\n    \n    public PreGatewayFilterFactory() {\n        super(Config.class);\n    }\n\n    @Override\n    public GatewayFilter apply(Config config) {\n        // grab configuration from Config object\n        return (exchange, chain) -> {\n            //If you want to build a "pre" filter you need to manipulate the\n            //request before calling chain.filter\n            ServerHttpRequest.Builder builder = exchange.getRequest().mutate();\n            //use builder to manipulate the request\n            ServerHttpRequest request = builder.build();\n            return chain.filter(exchange.mutate().request(request).build());\n        };\n    }\n\n    public static class Config {\n        //Put the configuration properties for your filter here\n    }\n}\n\n\n其中，需要的过滤器参数配置在 PreGatewayFilterFactory.Config 中。然后，接下来我们要做的，就是把局部过滤器配置在需要的路由上，根据 SpringBoot 约定大于配置的思想，我们只需要配置 PreGatewayFilterFactory.java 中，前面的参数就行了，即\n\nspring:\n  cloud:\n    gateway:\n      routes:\n        - id: cloud-eureka-client\n          uri: lb://cloud-eureka-client\n          order: 0\n          predicates:\n            - Path=/client/**\n          filters:\n            - pre\n\n\n> 可以去阅读下 Gateway 中默认提供的几种过滤器，比如 StripPrefixGatewayFilterFactory.java 等。\n\n\n# 动态路由\n\nSpring Cloud Gateway 实现动态路由主要利用 RouteDefinitionWriter 这个 Bean：\n\npublic interface RouteDefinitionWriter {\n\n\tMono<Void> save(Mono<RouteDefinition> route);\n\n\tMono<Void> delete(Mono<String> routeId);\n}\n\n\n之前翻阅了网上的一些文章，基本都是通过自定义 controller 和出入参，然后利用 RouteDefinitionWriter 实现动态网关。但是，我在翻阅 Spring Cloud Gateway 文档的时候，发现 Gateway 已经提供了类似的功能：\n\n@RestControllerEndpoint(id = "gateway")\npublic class GatewayControllerEndpoint implements ApplicationEventPublisherAware {\n\n    /*---省略前面代码---*/\n\n\t@PostMapping("/routes/{id}")\n\t@SuppressWarnings("unchecked")\n\tpublic Mono<ResponseEntity<Void>> save(@PathVariable String id, @RequestBody Mono<RouteDefinition> route) {\n\t\treturn this.routeDefinitionWriter.save(route.map(r ->  {\n\t\t\tr.setId(id);\n\t\t\tlog.debug("Saving route: " + route);\n\t\t\treturn r;\n\t\t})).then(Mono.defer(() ->\n\t\t\tMono.just(ResponseEntity.created(URI.create("/routes/"+id)).build())\n\t\t));\n\t}\n\n\t@DeleteMapping("/routes/{id}")\n\tpublic Mono<ResponseEntity<Object>> delete(@PathVariable String id) {\n\t\treturn this.routeDefinitionWriter.delete(Mono.just(id))\n\t\t\t\t.then(Mono.defer(() -> Mono.just(ResponseEntity.ok().build())))\n\t\t\t\t.onErrorResume(t -> t instanceof NotFoundException, t -> Mono.just(ResponseEntity.notFound().build()));\n\t}\n\n     /*---省略后面代码---*/\n}\n\n\n要创建一个路由，发送POST请求 /actuator/gateway/routes/{id_route_to_create}，参数为JSON结构，具体参数数据结构：\n\n{\n  "id": "first_route",\n  "predicates": [{\n    "name": "Path",\n    "args": {"_genkey_0":"/first"}\n  }],\n  "filters": [],\n  "uri": "http://www.uri-destination.org",\n  "order": 0\n}]\n\n\n要删除一个路由，发送 DELETE请求 /actuator/gateway/routes/{id_route_to_delete}',normalizedContent:'# spring cloud 之 gateway\n\n\n# gateway 和 zuul 的区别\n\nzuul 基于servlet 2.5 (works with 3.x)，使用阻塞api。它不支持任何长期的连接，如websocket。\n\ngateway建立在spring framework 5，project reactor 和spring boot 2 上，使用非阻塞api。支持websocket，因为它与spring紧密集成，所以它是一个更好的开发者体验。\n\n为什么 spring cloud 最初选择了使用 netflix 几年前开源的 zuul 作为网关，之后又选择了自建 gateway 呢？有一种说法是，高性能版的 zuul2 在经过了多次跳票之后，对于 spring 这样的整合专家可能也不愿意再继续等待，所以 spring cloud gateway 应运而生。\n\n本文不对 spring cloud gateway 和 zuul 的性能作太多赘述，基本可以肯定的是 gateway 作为现在 spring cloud 主推的网关方案， finchley 版本后的 gateway 比 zuul 1.x 系列的性能和功能整体要好。\n\n\n# 快速入门\n\n我们来搭建一个基于 eureka 注册中心的简单网关，不对 gateway 的全部功能做过多解读，毕竟官方文档已经写的很详细了，或者可以阅读中文翻译文档。\n\nspringboot 版本号：2.1.6.release\n\nspringcloud 版本号：greenwich.release\n\n\n# 1. pom.xml\n\n<dependencies>\n    <dependency>\n        <groupid>org.springframework.cloud</groupid>\n        <artifactid>spring-cloud-starter-gateway</artifactid>\n    </dependency>\n    <dependency>\n        <groupid>org.springframework.cloud</groupid>\n        <artifactid>spring-cloud-starter-netflix-hystrix</artifactid>\n    </dependency>\n    <dependency>\n        <groupid>org.springframework.cloud</groupid>\n        <artifactid>spring-cloud-starter-netflix-eureka-client</artifactid>\n    </dependency>\n    <dependency>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-data-redis-reactive</artifactid>\n    </dependency>\n    <dependency>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-actuator</artifactid>\n    </dependency>\n</dependencies>\n\n\n * spring-cloud-starter-gateway：spring cloud gateway 的启动类\n * spring-cloud-starter-netflix-hystrix：hystrix 作为网关的熔断方案\n * spring-cloud-starter-netflix-eureka-client：将网关纳入 eureka 注册中心管理\n * spring-boot-starter-data-redis-reactive：限流方案，spring cloud gateway 默认以 redis 实现限流\n * spring-boot-starter-actuator：用来监控 gateway 的路由信息。\n\n\n# 2. application.yml\n\nspring:\n  application:\n    name: cloud-gateway\n  redis:\n    host: 127.0.0.1\n    timeout: 3000\n    password: xxxx\n    jedis:\n      pool:\n        max-active: 8\n        max-idle: 4\n  cloud:\n    gateway:\n      enabled: true\n      metrics:\n        enabled: true\n      discovery:\n        locator:\n          enabled: true\n      routes:\n        # 普通服务的路由配置\n        - id: cloud-eureka-client\n          uri: lb://cloud-eureka-client\n          order: 0\n          predicates:\n            - path=/client/**\n          filters:\n            #  parts 参数指示在将请求发送到下游之前，要从请求中去除的路径中的节数。比如我们访问 /client/hello，调用的时候变成 http://localhost:2222/hello\n            - stripprefix=1\n            # 熔断器\n            - name: hystrix\n              args:\n                name: fallbackcmd\n                # 降级处理\n                fallbackuri: forward:/fallback\n            # 限流器\n            # 这定义了每个用户 10 个请求的限制。允许 20 个突发，但下一秒只有 10 个请求可用。\n            - name: requestratelimiter\n              args:\n                # spel 表达式获取 spring 中的 bean，这个参数表示根据什么来限流\n                key-resolver: \'#{@ipkeyresolver}\'\n                # 允许用户每秒执行多少请求（令牌桶的填充速率）\n                redis-rate-limiter.replenishrate: 10\n                # 允许用户在一秒内执行的最大请求数。（令牌桶可以保存的令牌数）。将此值设置为零将阻止所有请求。\n                redis-rate-limiter.burstcapacity: 20\n        # websocket 的路由配置\n        - id: websocket service\n          uri: lb:ws://serviceid\n          predicates:\n            - path=/websocket/**\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        # 开启指定端点\n        include: gateway,metrics\neureka:\n  client:\n    service-url:\n      defaultzone: http://user:password@localhost:1111/eureka/\n\n\n * spring.redis.*： redis 相关配置是为了实现 gateway 的限流方案。\n * eureka.client.*：eureka 注册中心信息。\n * spring.cloud.gateway.discovery.locator.enabled：将网关配置为基于使用兼容 discoveryclient 注册中心注册的服务来创建路由。\n * spring.cloud.gateway.routes.*：配置路由信息\n   * id：路由唯一标识\n   * uri：路由转发地址，以 lb 开头的路由，会由 ribbon 处理，转发到 cloud-eureka-client 的服务处理。也可配置成 http 的单机路由 — http://localhost:2222。\n   * order：路由执行顺序（也可理解成过滤器的执行顺序），执行顺序是从小到大执行，较高的值被解释为较低的优先级。\n   * predicates：路由断言，匹配访问路径为 "/client/**" 的请求。\n   * filters：网关的过滤器配置\n * management.endpoints.web.exposure.include：暴露 actuator 可以访问的端点\n   * /actuator/gateway/routes 查看路由列表\n   * /actuator/gateway/globalfilters 检索全局路由 — 对所有路由生效\n   * /actuator/gateway/routefilters 检索局部路由 — 可配置只对单个路由生效\n   * /actuator/gateway/refresh 清理路由缓存\n   * /actuator/metrics/gateway.requests 获得路由请求数据\n\n\n# 3. gatewayapplication.java\n\n@springbootapplication\n@enablediscoveryclient\npublic class gatewayapplication {\n\n    public static void main(string[] args) {\n        springapplication.run(gatewayapplication.class, args);\n    }\n\n    /**\n     * 限流的键定义，根据什么来限流\n     */\n    @bean\n    public keyresolver ipkeyresolver() {\n        return exchange -> mono.just(exchange.getrequest().getremoteaddress().gethostname());\n    }\n\n}\n\n\n\n# 过滤器\n\nspring cloud gateway 同 zuul 类似，有 “pre” 和 “post” 两种方式的 filter。客户端的请求先经过 “pre” 类型的 filter，然后将请求转发到具体的业务服务，收到业务服务的响应之后，再经过“post”类型的filter处理，最后返回响应到客户端。\n\n与 zuul 不同的是，filter 除了分为 “pre” 和 “post” 两种方式的 filter 外，在 spring cloud gateway 中，filter 从作用范围可分为另外两种，一种是针对于单个路由的 gateway filter，它需要像上面 application.yml 中的 filters 那样在单个路由中配置；另外一种是针对于全部路由的global gateway filter，不需要单独配置，对所有路由生效。\n\n\n# 全局过滤器\n\n我们通常用全局过滤器实现鉴权、验签、限流、日志输出等。\n\n通过实现 globalfilter 接口来自定义 gateway 的全局过滤器；通过实现 ordered 接口或者使用 @order 注解来定义过滤器的执行顺序，执行顺序是从小到大执行，较高的值被解释为较低的优先级。\n\n    @bean\n    @order(-1)\n    public globalfilter a() {\n        return (exchange, chain) -> {\n            log.info("first pre filter");\n            return chain.filter(exchange).then(mono.fromrunnable(() -> {\n                log.info("third post filter");\n            }));\n        };\n    }\n\n    @bean\n    @order(0)\n    public globalfilter b() {\n        return (exchange, chain) -> {\n            log.info("second pre filter");\n            return chain.filter(exchange).then(mono.fromrunnable(() -> {\n                log.info("second post filter");\n            }));\n        };\n    }\n\n    @bean\n    @order(1)\n    public globalfilter c() {\n        return (exchange, chain) -> {\n            log.info("third pre filter");\n            return chain.filter(exchange).then(mono.fromrunnable(() -> {\n                log.info("first post filter");\n            }));\n        };\n    }\n\n\n优先级最高的 filter ，它的 “pre” 过滤器最先执行，“post” 过滤器最晚执行。\n\n\n# 局部过滤器\n\n我们来定义一个 “pre” 类型的局部过滤器：\n\n@component\npublic class pregatewayfilterfactory extends abstractgatewayfilterfactory<pregatewayfilterfactory.config> {\n    \n    public pregatewayfilterfactory() {\n        super(config.class);\n    }\n\n    @override\n    public gatewayfilter apply(config config) {\n        // grab configuration from config object\n        return (exchange, chain) -> {\n            //if you want to build a "pre" filter you need to manipulate the\n            //request before calling chain.filter\n            serverhttprequest.builder builder = exchange.getrequest().mutate();\n            //use builder to manipulate the request\n            serverhttprequest request = builder.build();\n            return chain.filter(exchange.mutate().request(request).build());\n        };\n    }\n\n    public static class config {\n        //put the configuration properties for your filter here\n    }\n}\n\n\n其中，需要的过滤器参数配置在 pregatewayfilterfactory.config 中。然后，接下来我们要做的，就是把局部过滤器配置在需要的路由上，根据 springboot 约定大于配置的思想，我们只需要配置 pregatewayfilterfactory.java 中，前面的参数就行了，即\n\nspring:\n  cloud:\n    gateway:\n      routes:\n        - id: cloud-eureka-client\n          uri: lb://cloud-eureka-client\n          order: 0\n          predicates:\n            - path=/client/**\n          filters:\n            - pre\n\n\n> 可以去阅读下 gateway 中默认提供的几种过滤器，比如 stripprefixgatewayfilterfactory.java 等。\n\n\n# 动态路由\n\nspring cloud gateway 实现动态路由主要利用 routedefinitionwriter 这个 bean：\n\npublic interface routedefinitionwriter {\n\n\tmono<void> save(mono<routedefinition> route);\n\n\tmono<void> delete(mono<string> routeid);\n}\n\n\n之前翻阅了网上的一些文章，基本都是通过自定义 controller 和出入参，然后利用 routedefinitionwriter 实现动态网关。但是，我在翻阅 spring cloud gateway 文档的时候，发现 gateway 已经提供了类似的功能：\n\n@restcontrollerendpoint(id = "gateway")\npublic class gatewaycontrollerendpoint implements applicationeventpublisheraware {\n\n    /*---省略前面代码---*/\n\n\t@postmapping("/routes/{id}")\n\t@suppresswarnings("unchecked")\n\tpublic mono<responseentity<void>> save(@pathvariable string id, @requestbody mono<routedefinition> route) {\n\t\treturn this.routedefinitionwriter.save(route.map(r ->  {\n\t\t\tr.setid(id);\n\t\t\tlog.debug("saving route: " + route);\n\t\t\treturn r;\n\t\t})).then(mono.defer(() ->\n\t\t\tmono.just(responseentity.created(uri.create("/routes/"+id)).build())\n\t\t));\n\t}\n\n\t@deletemapping("/routes/{id}")\n\tpublic mono<responseentity<object>> delete(@pathvariable string id) {\n\t\treturn this.routedefinitionwriter.delete(mono.just(id))\n\t\t\t\t.then(mono.defer(() -> mono.just(responseentity.ok().build())))\n\t\t\t\t.onerrorresume(t -> t instanceof notfoundexception, t -> mono.just(responseentity.notfound().build()));\n\t}\n\n     /*---省略后面代码---*/\n}\n\n\n要创建一个路由，发送post请求 /actuator/gateway/routes/{id_route_to_create}，参数为json结构，具体参数数据结构：\n\n{\n  "id": "first_route",\n  "predicates": [{\n    "name": "path",\n    "args": {"_genkey_0":"/first"}\n  }],\n  "filters": [],\n  "uri": "http://www.uri-destination.org",\n  "order": 0\n}]\n\n\n要删除一个路由，发送 delete请求 /actuator/gateway/routes/{id_route_to_delete}',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:47:02",lastUpdatedTimestamp:1644378422e3},{title:"Spring Cloud Gateway 配置大全",frontmatter:{title:"Spring Cloud Gateway 配置大全",date:"2022-02-09T11:44:07.000Z",permalink:"/pages/f0610e/"},regularPath:"/15.Spring%20Cloud%20Gateway/01.Spring%20Cloud%20Gateway/02.Spring%20Cloud%20Gateway%20%E9%85%8D%E7%BD%AE%E5%A4%A7%E5%85%A8.html",relativePath:"15.Spring Cloud Gateway/01.Spring Cloud Gateway/02.Spring Cloud Gateway 配置大全.md",key:"v-1dd07656",path:"/pages/f0610e/",headers:[{level:2,title:"Predicates",slug:"predicates",normalizedTitle:"predicates",charIndex:84},{level:2,title:"Path",slug:"path",normalizedTitle:"path",charIndex:140},{level:2,title:"Cookie",slug:"cookie",normalizedTitle:"cookie",charIndex:282},{level:2,title:"Header",slug:"header",normalizedTitle:"header",charIndex:474},{level:2,title:"Host",slug:"host",normalizedTitle:"host",charIndex:673},{level:2,title:"Method",slug:"method",normalizedTitle:"method",charIndex:825},{level:2,title:"Query",slug:"query",normalizedTitle:"query",charIndex:928},{level:2,title:"RemoteAddr",slug:"remoteaddr",normalizedTitle:"remoteaddr",charIndex:1090},{level:2,title:"After",slug:"after",normalizedTitle:"after",charIndex:1240},{level:2,title:"Before",slug:"before",normalizedTitle:"before",charIndex:1425},{level:2,title:"Before",slug:"before-2",normalizedTitle:"before",charIndex:1425},{level:2,title:"Filters",slug:"filters",normalizedTitle:"filters",charIndex:2061}],headersStr:"Predicates Path Cookie Header Host Method Query RemoteAddr After Before Before Filters",content:'# Spring Cloud Gateway 配置大全\n\n> 了解Gateway的配置才可以理解使用Gateway可以做什么事情，才能更好地应用在产品开发中。\n\n\n# Predicates\n\n> Predicates主要起的作用是：配置路由匹配请求的规则\n\nHttp相关\n\n\n# Path\n\n> 配置对于请求路径的匹配规则\n\nyml配置，多个参数用逗号隔开\n\n- Path = /aa/**,/bb/**\n\n\njson配置\n\n{"name":"Path","args":{"pattern":"/aa/**","pattern1":"/bb/**"}}\n\n\n\n# Cookie\n\n> 配置对Cookie中值的匹配，第一个为key，第二个为value。下例匹配cookie设置chocolate:ch.p的请求\n\nyml配置\n\n- Cookie = chocolate,ch.p\n\n\njson配置\n\n{"name":"Cookie","args":{"_genkey_0":"chocolate","_genkey_1":"ch.p"}}\n\n\n\n# Header\n\n> 匹配Http请求中设置的内容，http-header设置X-Request-Id:\\d+可以匹配，第二个参数第二个参数是正则表达式\n\nyml配置\n\n- Header = X-Request-Id,\\d+\n\n\njson配置\n\n{"name":"Header","args":{"_genkey_0":"X-Request-Id","_genkey_1":"\\d+"}}\n\n\n\n# Host\n\n> 匹配Http请求Host，匹配所有host为**.somehost.com的请求\n\nyml配置\n\n- Host = **.somehost.com\n\n\njson配置\n\n{"name":"Host","args":{"_genkey_0":"**.somehost.com"}}\n\n\n\n# Method\n\n> 匹配Http请求头\n\nyml配置\n\n- Method = GET\n\n\njson配置\n\n{"name":"Method","args":{"_genkey_0":"GET"}}\n\n\n\n# Query\n\n> 匹配Http请求中的查询参数，请求中携带param1=value的请求可以匹配\n\nyml配置\n\n- Query = param1,value\n\n\njson配置\n\n{"name":"Query","args":{"_genkey_0":"param1","_genkey_1":"value"}}\n\n\n\n# RemoteAddr\n\n> 匹配请求中的RemoteAddr\n\nyml配置\n\n- RemoteAddr = 192.168.1.1/24\n\n\njson配置\n\n{"name":"RemoteAddr","args":{"_genkey_0":"192.168.1.1/24"}}\n\n\n时间相关\n\n\n# After\n\n> 设置时间之后可以访问\n\nyml配置\n\n- After = 2017-01-20T17:42:47.789-07:00[America/Denver]\n\n\njson配置\n\n{"name":"After","args":{"_genkey_0":"2017-01-20T17:42:47.789-07:00[America/Denver]"}}\n\n\n\n# Before\n\n> 设置时间之前可以访问\n\nyml配置\n\n- Before = 2017-01-20T17:42:47.789-07:00[America/Denver]\n\n\njson配置\n\n{"name":"Before","args":{"_genkey_0":"2017-01-20T17:42:47.789-07:00[America/Denver]"}}\n\n\n\n# Before\n\n> 设置时间段内可以访问\n\nyml配置\n\n- Between = 2017-01-20T17:42:47.789-07:00[America/Denver],2017-01-21T17:42:47.789-07:00[America/Denver]\n\n\njson配置\n\n{"name":"Between","args":{"_genkey_0":"2017-01-20T17:42:47.789-07:00[America/Denver]"，"_genkey_1":"2017-01-21T17:42:47.789-07:00[America/Denver]"}}\n\n\n权重路由\n\n> 至少两组以上路由可以配置权重路由，配置后会根据权重随机访问几个路由\n\nyml配置\n\n- Weight = service1,80\n\n\njson配置\n\n{"name":"Weight","args":{"_genkey_0":"service1","_genkey_1":"80"}}\n\n\n\n# Filters\n\n路径重写\n\nyml配置\n\n- RewritePath = /path/(?<segment>.*), /$\\{segment}\n\n\njson配置\n\n{"name":"RewritePath","args":{"_genkey_0":"/foo/(?<segment>.*)","_genkey_1":"/$\\\\{segment}"}}\n\n\n修改请求头\n\nyml配置\n\n- AddRequestHeader = X-Request-Foo,Bar\n\n\njson配置\n\n{"name":"AddRequestHeader","args":{"_genkey_0":"X-Request-Foo","_genkey_1":"Bar"}}\n\n\n修改请求参数\n\nyml配置\n\n- AddRequestParameter = foo,bar\n\n\njson配置\n\n{"name":"AddRequestParameter","args":{"_genkey_0":"foo","_genkey_1":"bar"}}\n\n\n修改响应参数\n\nyml配置\n\n- AddResponseHeader = X-Request-Foo,Bar\n\n\njson配置\n\n{"name":"AddResponseHeader","args":{"_genkey_0":"X-Request-Foo","_genkey_1":"Bar"}}\n\n\n路径前缀增强\n\n> 请求路径/hello, 将会被替换为 /mypath/hello\n\nyml配置\n\n- PrefixPath = /mypath\n\n\njson配置\n\n{"name":"PrefixPath","args":{"_genkey_0":"/mypath"}}\n\n\n路径前缀删除\n\n> 请求/name/bar/foo，去除掉前面两个前缀之后，最后转发到目标服务的路径为/foo\n\nyml配置\n\n- StripPrefix = 2\n\n\njson配置\n\n{"name":"StripPrefix","args":{"_genkey_0":"2"}}\n\n\n请求携带保留原始Host\n\nyml配置\n\n- PreserveHostHeader\n\n\njson配置\n\n{"name":"PreserveHostHeader","args":{}}\n\n\n重定向\n\nyml配置\n\n- RedirectTo = 302,http://acme.org\n\n\njson配置\n\n{"name":"RedirectTo","args":{"_genkey_0":"302","_genkey_1":"http://acme.org"}}\n\n\n断路器\n\nyml配置\n\n- name: Hystrix\n  args:\n\t  ## 断路后跳转地址\n\t  name: fallbackcmd\n      fallbackUri: forward:/incaseoffailureusethis\n\n\njson配置\n\n{"name":"Hystrix","args":{"name":"fallbackcmd","fallbackUri":"forward:/incaseoffailureusethis"}}\n\n\n集成Redis原生支持请求限流\n\nyml配置\n\n - name: RequestRateLimiter\n   args:\n     redis-rate-limiter.replenishRate: 10  \n     redis-rate-limiter.burstCapacity: 20\n\n\njson配置\n\n{"name":"RequestRateLimiter","args":{"redis-rate-limiter.replenishRate":"10","redis-rate-limiter.burstCapacity":"20"}}\n\n\n删除请求头属性\n\nyml配置\n\n- RemoveRequestHeader = X-Request-Foo\n\n\njson配置\n\n{"name":"RemoveRequestHeader","args":{"_genkey_0":"X-Request-Foo"}}\n\n\n删除响应头属性\n\nyml配置\n\n- RemoveResponseHeader = X-Request-Foo\n\n\njson配置\n\n{"name":"RemoveResponseHeader","args":{"_genkey_0":"X-Request-Foo"}}\n\n\n重写响应头\n\n> 将请求 /42?user=ford&password=omg!what&flag=true, 改为 /42?user=ford&password=***&flag=true\n\nyml配置\n\n- RewriteResponseHeader = X-Response-Foo,password=[^&]+,password=***\n\n\njson配置\n\n{"name":"RewriteResponseHeader","args":{"_genkey_0":"X-Response-Foo","_genkey_1":"password=[^&]+","_genkey_2":"password=***"}}\n\n\n重设请求路径\n\n> 请求/foo/bar，在接下来的处理中被改为/bar\n\nyml配置\n\n- SetPath =/{segment}\n\n\njson配置\n\n{"name":"SetPath","args":{"_genkey_0":"/{segment}"}}\n\n\n设置响应头\n\n> 在接下来的处理中修改响应头X-Response-Foo为Bar\n\nyml配置\n\n- SetResponseHeader =X-Request-Foo,Bar\n\n\njson配置\n\n{"name":"SetResponseHeader","args":{"_genkey_0":"X-Response-Foo","_genkey_1":"Bar"}}\n\n\n设置Http状态\n\nyml配置\n\n- name: SetStatus\n  args:\n\t  status: 401\n\n\njson配置\n\n{"name":"SetStatus","args":{"_genkey_0":"302"}}\n\n\n设置文件传输大小\n\nyml配置\n\n - name: RequestSize\n   args:\n\t   maxSize: 5000000\n\n\njson配置\n\n{"name":"RequestSize","args":{"_genkey_0":"5000000"}}\n\n\n失败重试\n\nyml配置\n\n- name: Retry\n  args:\n\t  retries: 3\n      statuses: BAD_GATEWAY\n',normalizedContent:'# spring cloud gateway 配置大全\n\n> 了解gateway的配置才可以理解使用gateway可以做什么事情，才能更好地应用在产品开发中。\n\n\n# predicates\n\n> predicates主要起的作用是：配置路由匹配请求的规则\n\nhttp相关\n\n\n# path\n\n> 配置对于请求路径的匹配规则\n\nyml配置，多个参数用逗号隔开\n\n- path = /aa/**,/bb/**\n\n\njson配置\n\n{"name":"path","args":{"pattern":"/aa/**","pattern1":"/bb/**"}}\n\n\n\n# cookie\n\n> 配置对cookie中值的匹配，第一个为key，第二个为value。下例匹配cookie设置chocolate:ch.p的请求\n\nyml配置\n\n- cookie = chocolate,ch.p\n\n\njson配置\n\n{"name":"cookie","args":{"_genkey_0":"chocolate","_genkey_1":"ch.p"}}\n\n\n\n# header\n\n> 匹配http请求中设置的内容，http-header设置x-request-id:\\d+可以匹配，第二个参数第二个参数是正则表达式\n\nyml配置\n\n- header = x-request-id,\\d+\n\n\njson配置\n\n{"name":"header","args":{"_genkey_0":"x-request-id","_genkey_1":"\\d+"}}\n\n\n\n# host\n\n> 匹配http请求host，匹配所有host为**.somehost.com的请求\n\nyml配置\n\n- host = **.somehost.com\n\n\njson配置\n\n{"name":"host","args":{"_genkey_0":"**.somehost.com"}}\n\n\n\n# method\n\n> 匹配http请求头\n\nyml配置\n\n- method = get\n\n\njson配置\n\n{"name":"method","args":{"_genkey_0":"get"}}\n\n\n\n# query\n\n> 匹配http请求中的查询参数，请求中携带param1=value的请求可以匹配\n\nyml配置\n\n- query = param1,value\n\n\njson配置\n\n{"name":"query","args":{"_genkey_0":"param1","_genkey_1":"value"}}\n\n\n\n# remoteaddr\n\n> 匹配请求中的remoteaddr\n\nyml配置\n\n- remoteaddr = 192.168.1.1/24\n\n\njson配置\n\n{"name":"remoteaddr","args":{"_genkey_0":"192.168.1.1/24"}}\n\n\n时间相关\n\n\n# after\n\n> 设置时间之后可以访问\n\nyml配置\n\n- after = 2017-01-20t17:42:47.789-07:00[america/denver]\n\n\njson配置\n\n{"name":"after","args":{"_genkey_0":"2017-01-20t17:42:47.789-07:00[america/denver]"}}\n\n\n\n# before\n\n> 设置时间之前可以访问\n\nyml配置\n\n- before = 2017-01-20t17:42:47.789-07:00[america/denver]\n\n\njson配置\n\n{"name":"before","args":{"_genkey_0":"2017-01-20t17:42:47.789-07:00[america/denver]"}}\n\n\n\n# before\n\n> 设置时间段内可以访问\n\nyml配置\n\n- between = 2017-01-20t17:42:47.789-07:00[america/denver],2017-01-21t17:42:47.789-07:00[america/denver]\n\n\njson配置\n\n{"name":"between","args":{"_genkey_0":"2017-01-20t17:42:47.789-07:00[america/denver]"，"_genkey_1":"2017-01-21t17:42:47.789-07:00[america/denver]"}}\n\n\n权重路由\n\n> 至少两组以上路由可以配置权重路由，配置后会根据权重随机访问几个路由\n\nyml配置\n\n- weight = service1,80\n\n\njson配置\n\n{"name":"weight","args":{"_genkey_0":"service1","_genkey_1":"80"}}\n\n\n\n# filters\n\n路径重写\n\nyml配置\n\n- rewritepath = /path/(?<segment>.*), /$\\{segment}\n\n\njson配置\n\n{"name":"rewritepath","args":{"_genkey_0":"/foo/(?<segment>.*)","_genkey_1":"/$\\\\{segment}"}}\n\n\n修改请求头\n\nyml配置\n\n- addrequestheader = x-request-foo,bar\n\n\njson配置\n\n{"name":"addrequestheader","args":{"_genkey_0":"x-request-foo","_genkey_1":"bar"}}\n\n\n修改请求参数\n\nyml配置\n\n- addrequestparameter = foo,bar\n\n\njson配置\n\n{"name":"addrequestparameter","args":{"_genkey_0":"foo","_genkey_1":"bar"}}\n\n\n修改响应参数\n\nyml配置\n\n- addresponseheader = x-request-foo,bar\n\n\njson配置\n\n{"name":"addresponseheader","args":{"_genkey_0":"x-request-foo","_genkey_1":"bar"}}\n\n\n路径前缀增强\n\n> 请求路径/hello, 将会被替换为 /mypath/hello\n\nyml配置\n\n- prefixpath = /mypath\n\n\njson配置\n\n{"name":"prefixpath","args":{"_genkey_0":"/mypath"}}\n\n\n路径前缀删除\n\n> 请求/name/bar/foo，去除掉前面两个前缀之后，最后转发到目标服务的路径为/foo\n\nyml配置\n\n- stripprefix = 2\n\n\njson配置\n\n{"name":"stripprefix","args":{"_genkey_0":"2"}}\n\n\n请求携带保留原始host\n\nyml配置\n\n- preservehostheader\n\n\njson配置\n\n{"name":"preservehostheader","args":{}}\n\n\n重定向\n\nyml配置\n\n- redirectto = 302,http://acme.org\n\n\njson配置\n\n{"name":"redirectto","args":{"_genkey_0":"302","_genkey_1":"http://acme.org"}}\n\n\n断路器\n\nyml配置\n\n- name: hystrix\n  args:\n\t  ## 断路后跳转地址\n\t  name: fallbackcmd\n      fallbackuri: forward:/incaseoffailureusethis\n\n\njson配置\n\n{"name":"hystrix","args":{"name":"fallbackcmd","fallbackuri":"forward:/incaseoffailureusethis"}}\n\n\n集成redis原生支持请求限流\n\nyml配置\n\n - name: requestratelimiter\n   args:\n     redis-rate-limiter.replenishrate: 10  \n     redis-rate-limiter.burstcapacity: 20\n\n\njson配置\n\n{"name":"requestratelimiter","args":{"redis-rate-limiter.replenishrate":"10","redis-rate-limiter.burstcapacity":"20"}}\n\n\n删除请求头属性\n\nyml配置\n\n- removerequestheader = x-request-foo\n\n\njson配置\n\n{"name":"removerequestheader","args":{"_genkey_0":"x-request-foo"}}\n\n\n删除响应头属性\n\nyml配置\n\n- removeresponseheader = x-request-foo\n\n\njson配置\n\n{"name":"removeresponseheader","args":{"_genkey_0":"x-request-foo"}}\n\n\n重写响应头\n\n> 将请求 /42?user=ford&password=omg!what&flag=true, 改为 /42?user=ford&password=***&flag=true\n\nyml配置\n\n- rewriteresponseheader = x-response-foo,password=[^&]+,password=***\n\n\njson配置\n\n{"name":"rewriteresponseheader","args":{"_genkey_0":"x-response-foo","_genkey_1":"password=[^&]+","_genkey_2":"password=***"}}\n\n\n重设请求路径\n\n> 请求/foo/bar，在接下来的处理中被改为/bar\n\nyml配置\n\n- setpath =/{segment}\n\n\njson配置\n\n{"name":"setpath","args":{"_genkey_0":"/{segment}"}}\n\n\n设置响应头\n\n> 在接下来的处理中修改响应头x-response-foo为bar\n\nyml配置\n\n- setresponseheader =x-request-foo,bar\n\n\njson配置\n\n{"name":"setresponseheader","args":{"_genkey_0":"x-response-foo","_genkey_1":"bar"}}\n\n\n设置http状态\n\nyml配置\n\n- name: setstatus\n  args:\n\t  status: 401\n\n\njson配置\n\n{"name":"setstatus","args":{"_genkey_0":"302"}}\n\n\n设置文件传输大小\n\nyml配置\n\n - name: requestsize\n   args:\n\t   maxsize: 5000000\n\n\njson配置\n\n{"name":"requestsize","args":{"_genkey_0":"5000000"}}\n\n\n失败重试\n\nyml配置\n\n- name: retry\n  args:\n\t  retries: 3\n      statuses: bad_gateway\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:47:02",lastUpdatedTimestamp:1644378422e3},{title:"Spring Cloud Gateway 中文官网文档",frontmatter:{title:"Spring Cloud Gateway 中文官网文档",date:"2022-02-09T11:44:07.000Z",permalink:"/pages/f7b8bf/"},regularPath:"/15.Spring%20Cloud%20Gateway/01.Spring%20Cloud%20Gateway/03.Spring%20Cloud%20Gateway%20%E4%B8%AD%E6%96%87%E5%AE%98%E7%BD%91%E6%96%87%E6%A1%A3.html",relativePath:"15.Spring Cloud Gateway/01.Spring Cloud Gateway/03.Spring Cloud Gateway 中文官网文档.md",key:"v-13de5e4a",path:"/pages/f7b8bf/",headers:[{level:2,title:"文档版本 2.1.0",slug:"文档版本-2-1-0",normalizedTitle:"文档版本 2.1.0",charIndex:34},{level:2,title:"1. 如何在工程中引用Spring Cloud Gateway",slug:"_1-如何在工程中引用spring-cloud-gateway",normalizedTitle:"1. 如何在工程中引用spring cloud gateway",charIndex:466},{level:2,title:"2. 词汇表",slug:"_2-词汇表",normalizedTitle:"2. 词汇表",charIndex:847},{level:2,title:"3. 如何工作的",slug:"_3-如何工作的",normalizedTitle:"3. 如何工作的",charIndex:1140},{level:2,title:"4. 路由断言Factories",slug:"_4-路由断言factories",normalizedTitle:"4. 路由断言factories",charIndex:1439},{level:2,title:"4.1 After 路由断言 Factory",slug:"_4-1-after-路由断言-factory",normalizedTitle:"4.1 after 路由断言 factory",charIndex:1618},{level:2,title:"4.2 Before 路由断言 Factory",slug:"_4-2-before-路由断言-factory",normalizedTitle:"4.2 before 路由断言 factory",charIndex:1909},{level:2,title:"4.3 Between 路由断言 Factory",slug:"_4-3-between-路由断言-factory",normalizedTitle:"4.3 between 路由断言 factory",charIndex:2205},{level:2,title:"4.4 Cookie 路由断言 Factory",slug:"_4-4-cookie-路由断言-factory",normalizedTitle:"4.4 cookie 路由断言 factory",charIndex:2596},{level:2,title:"4.5 Header  路由断言 Factory",slug:"_4-5-header-路由断言-factory",normalizedTitle:"4.5 header  路由断言 factory",charIndex:null},{level:2,title:"4.6 Host 路由断言 Factory",slug:"_4-6-host-路由断言-factory",normalizedTitle:"4.6 host 路由断言 factory",charIndex:3134},{level:2,title:"4.7 Method 路由断言 Factory",slug:"_4-7-method-路由断言-factory",normalizedTitle:"4.7 method 路由断言 factory",charIndex:3405},{level:2,title:"4.8 Path 路由断言 Factory",slug:"_4-8-path-路由断言-factory",normalizedTitle:"4.8 path 路由断言 factory",charIndex:3652},{level:2,title:"4.9 Query 路由断言 Factory",slug:"_4-9-query-路由断言-factory",normalizedTitle:"4.9 query 路由断言 factory",charIndex:4325},{level:2,title:"4.10 RemoteAddr 路由断言 Factory",slug:"_4-10-remoteaddr-路由断言-factory",normalizedTitle:"4.10 remoteaddr 路由断言 factory",charIndex:4799},{level:3,title:"4.10.1 修改远程地址的解析方式",slug:"_4-10-1-修改远程地址的解析方式",normalizedTitle:"4.10.1 修改远程地址的解析方式",charIndex:5162},{level:2,title:"5.1 AddRequestHeader GatewayFilter Factory",slug:"_5-1-addrequestheader-gatewayfilter-factory",normalizedTitle:"5.1 addrequestheader gatewayfilter factory",charIndex:6693},{level:2,title:"5.2 AddRequestParameter GatewayFilter Factory",slug:"_5-2-addrequestparameter-gatewayfilter-factory",normalizedTitle:"5.2 addrequestparameter gatewayfilter factory",charIndex:6998},{level:2,title:"5.3 AddResponseHeader GatewayFilter Factory",slug:"_5-3-addresponseheader-gatewayfilter-factory",normalizedTitle:"5.3 addresponseheader gatewayfilter factory",charIndex:7286},{level:2,title:"5.4 Hystrix GatewayFilter Factory",slug:"_5-4-hystrix-gatewayfilter-factory",normalizedTitle:"5.4 hystrix gatewayfilter factory",charIndex:7595},{level:2,title:"5.5 FallbackHeaders GatewayFilter Factory",slug:"_5-5-fallbackheaders-gatewayfilter-factory",normalizedTitle:"5.5 fallbackheaders gatewayfilter factory",charIndex:9753},{level:2,title:"5.6 PrefixPath GatewayFilter Factory",slug:"_5-6-prefixpath-gatewayfilter-factory",normalizedTitle:"5.6 prefixpath gatewayfilter factory",charIndex:10933},{level:2,title:"5.7 PreserveHostHeader GatewayFilter Factory",slug:"_5-7-preservehostheader-gatewayfilter-factory",normalizedTitle:"5.7 preservehostheader gatewayfilter factory",charIndex:11245},{level:2,title:"5.8 RequestRateLimiter GatewayFilter Factory",slug:"_5-8-requestratelimiter-gatewayfilter-factory",normalizedTitle:"5.8 requestratelimiter gatewayfilter factory",charIndex:11554},{level:3,title:"5.8.1 Redis RateLimiter",slug:"_5-8-1-redis-ratelimiter",normalizedTitle:"5.8.1 redis ratelimiter",charIndex:12531},{level:2,title:"5.9 RedirectTo GatewayFilter Factory",slug:"_5-9-redirectto-gatewayfilter-factory",normalizedTitle:"5.9 redirectto gatewayfilter factory",charIndex:13947},{level:2,title:"5.10 RemoveNonProxyHeaders GatewayFilter Factory",slug:"_5-10-removenonproxyheaders-gatewayfilter-factory",normalizedTitle:"5.10 removenonproxyheaders gatewayfilter factory",charIndex:14315},{level:2,title:"5.11 RemoveRequestHeader GatewayFilter Factory",slug:"_5-11-removerequestheader-gatewayfilter-factory",normalizedTitle:"5.11 removerequestheader gatewayfilter factory",charIndex:14692},{level:2,title:"5.12 RemoveResponseHeader GatewayFilter Factory",slug:"_5-12-removeresponseheader-gatewayfilter-factory",normalizedTitle:"5.12 removeresponseheader gatewayfilter factory",charIndex:15004},{level:2,title:"5.13 RewritePath GatewayFilter Factory",slug:"_5-13-rewritepath-gatewayfilter-factory",normalizedTitle:"5.13 rewritepath gatewayfilter factory",charIndex:15323},{level:2,title:"5.14 RewriteResponseHeader GatewayFilter Factory",slug:"_5-14-rewriteresponseheader-gatewayfilter-factory",normalizedTitle:"5.14 rewriteresponseheader gatewayfilter factory",charIndex:15732},{level:2,title:"5.15 SaveSession GatewayFilter Factory",slug:"_5-15-savesession-gatewayfilter-factory",normalizedTitle:"5.15 savesession gatewayfilter factory",charIndex:16201},{level:2,title:"5.16 SecureHeaders GatewayFilter Factory",slug:"_5-16-secureheaders-gatewayfilter-factory",normalizedTitle:"5.16 secureheaders gatewayfilter factory",charIndex:16681},{level:2,title:"5.17 SetPath GatewayFilter Factory",slug:"_5-17-setpath-gatewayfilter-factory",normalizedTitle:"5.17 setpath gatewayfilter factory",charIndex:17548},{level:2,title:"5.18 SetResponseHeader GatewayFilter Factory",slug:"_5-18-setresponseheader-gatewayfilter-factory",normalizedTitle:"5.18 setresponseheader gatewayfilter factory",charIndex:17968},{level:2,title:"5.19 SetStatus GatewayFilter Factory",slug:"_5-19-setstatus-gatewayfilter-factory",normalizedTitle:"5.19 setstatus gatewayfilter factory",charIndex:18388},{level:2,title:"5.20 StripPrefix GatewayFilter Factory",slug:"_5-20-stripprefix-gatewayfilter-factory",normalizedTitle:"5.20 stripprefix gatewayfilter factory",charIndex:18837},{level:2,title:"5.21 Retry GatewayFilter Factory",slug:"_5-21-retry-gatewayfilter-factory",normalizedTitle:"5.21 retry gatewayfilter factory",charIndex:19235},{level:2,title:"5.22 RequestSize GatewayFilter Factory",slug:"_5-22-requestsize-gatewayfilter-factory",normalizedTitle:"5.22 requestsize gatewayfilter factory",charIndex:20037},{level:2,title:"5.23 Modify Request Body GatewayFilter Factory",slug:"_5-23-modify-request-body-gatewayfilter-factory",normalizedTitle:"5.23 modify request body gatewayfilter factory",charIndex:20737},{level:2,title:"5.24 Modify Response Body GatewayFilter Factory",slug:"_5-24-modify-response-body-gatewayfilter-factory",normalizedTitle:"5.24 modify response body gatewayfilter factory",charIndex:21595},{level:2,title:"6.1 全局Filter和GatewayFilter组合排序",slug:"_6-1-全局filter和gatewayfilter组合排序",normalizedTitle:"6.1 全局filter和gatewayfilter组合排序",charIndex:22221},{level:2,title:"6.2 Forward Routing Filter",slug:"_6-2-forward-routing-filter",normalizedTitle:"6.2 forward routing filter",charIndex:23292},{level:2,title:"6.3 LoadBalancerClient Filter",slug:"_6-3-loadbalancerclient-filter",normalizedTitle:"6.3 loadbalancerclient filter",charIndex:23598},{level:2,title:"6.4 Netty Routing Filter",slug:"_6-4-netty-routing-filter",normalizedTitle:"6.4 netty routing filter",charIndex:24489},{level:2,title:"6.5 Netty Write Response Filter",slug:"_6-5-netty-write-response-filter",normalizedTitle:"6.5 netty write response filter",charIndex:24781},{level:2,title:"6.6 RouteToRequestUrl Filter",slug:"_6-6-routetorequesturl-filter",normalizedTitle:"6.6 routetorequesturl filter",charIndex:25018},{level:2,title:"6.7 Websocket Routing Filter",slug:"_6-7-websocket-routing-filter",normalizedTitle:"6.7 websocket routing filter",charIndex:25384},{level:2,title:"6.8 Gateway Metrics Filter",slug:"_6-8-gateway-metrics-filter",normalizedTitle:"6.8 gateway metrics filter",charIndex:26029},{level:2,title:"6.9 Making An Exchange As Routed",slug:"_6-9-making-an-exchange-as-routed",normalizedTitle:"6.9 making an exchange as routed",charIndex:26494},{level:2,title:"7.1 TLS 握手",slug:"_7-1-tls-握手",normalizedTitle:"7.1 tls 握手",charIndex:27604},{level:2,title:"8.1 Fluent Java Routes API",slug:"_8-1-fluent-java-routes-api",normalizedTitle:"8.1 fluent java routes api",charIndex:28668},{level:2,title:"8.2 DiscoveryClient Route Definition Locator",slug:"_8-2-discoveryclient-route-definition-locator",normalizedTitle:"8.2 discoveryclient route definition locator",charIndex:29896},{level:3,title:"8.2.1 Configuring Predicates and Filters For DiscoveryClient Routes",slug:"_8-2-1-configuring-predicates-and-filters-for-discoveryclient-routes",normalizedTitle:"8.2.1 configuring predicates and filters for discoveryclient routes",charIndex:30126},{level:2,title:"11.1 Retrieving route filters",slug:"_11-1-retrieving-route-filters",normalizedTitle:"11.1 retrieving route filters",charIndex:32633},{level:3,title:"11.1.1 Global Filters",slug:"_11-1-1-global-filters",normalizedTitle:"11.1.1 global filters",charIndex:32667},{level:3,title:"11.1.2 Route Filters",slug:"_11-1-2-route-filters",normalizedTitle:"11.1.2 route filters",charIndex:33621},{level:2,title:"11.2 Refreshing the route cache",slug:"_11-2-refreshing-the-route-cache",normalizedTitle:"11.2 refreshing the route cache",charIndex:34230},{level:2,title:"11.3 Retrieving the routes defined in the gateway",slug:"_11-3-retrieving-the-routes-defined-in-the-gateway",normalizedTitle:"11.3 retrieving the routes defined in the gateway",charIndex:34334},{level:2,title:"11.4 Retrieving information about a particular route",slug:"_11-4-retrieving-information-about-a-particular-route",normalizedTitle:"11.4 retrieving information about a particular route",charIndex:35380},{level:2,title:"11.5 Creating and deleting a particular route",slug:"_11-5-creating-and-deleting-a-particular-route",normalizedTitle:"11.5 creating and deleting a particular route",charIndex:36122},{level:2,title:"11.6 Recap: list of all endpoints",slug:"_11-6-recap-list-of-all-endpoints",normalizedTitle:"11.6 recap: list of all endpoints",charIndex:36312},{level:2,title:"12.1 Writing Custom Route Predicate Factories",slug:"_12-1-writing-custom-route-predicate-factories",normalizedTitle:"12.1 writing custom route predicate factories",charIndex:37126},{level:2,title:"12.2 Writing Custom GatewayFilter Factories",slug:"_12-2-writing-custom-gatewayfilter-factories",normalizedTitle:"12.2 writing custom gatewayfilter factories",charIndex:37233},{level:2,title:"12.3 Writing Custom Global Filters",slug:"_12-3-writing-custom-global-filters",normalizedTitle:"12.3 writing custom global filters",charIndex:38908},{level:2,title:"12.4 Writing Custom Route Locators and Writers",slug:"_12-4-writing-custom-route-locators-and-writers",normalizedTitle:"12.4 writing custom route locators and writers",charIndex:38993}],headersStr:"文档版本 2.1.0 1. 如何在工程中引用Spring Cloud Gateway 2. 词汇表 3. 如何工作的 4. 路由断言Factories 4.1 After 路由断言 Factory 4.2 Before 路由断言 Factory 4.3 Between 路由断言 Factory 4.4 Cookie 路由断言 Factory 4.5 Header  路由断言 Factory 4.6 Host 路由断言 Factory 4.7 Method 路由断言 Factory 4.8 Path 路由断言 Factory 4.9 Query 路由断言 Factory 4.10 RemoteAddr 路由断言 Factory 4.10.1 修改远程地址的解析方式 5.1 AddRequestHeader GatewayFilter Factory 5.2 AddRequestParameter GatewayFilter Factory 5.3 AddResponseHeader GatewayFilter Factory 5.4 Hystrix GatewayFilter Factory 5.5 FallbackHeaders GatewayFilter Factory 5.6 PrefixPath GatewayFilter Factory 5.7 PreserveHostHeader GatewayFilter Factory 5.8 RequestRateLimiter GatewayFilter Factory 5.8.1 Redis RateLimiter 5.9 RedirectTo GatewayFilter Factory 5.10 RemoveNonProxyHeaders GatewayFilter Factory 5.11 RemoveRequestHeader GatewayFilter Factory 5.12 RemoveResponseHeader GatewayFilter Factory 5.13 RewritePath GatewayFilter Factory 5.14 RewriteResponseHeader GatewayFilter Factory 5.15 SaveSession GatewayFilter Factory 5.16 SecureHeaders GatewayFilter Factory 5.17 SetPath GatewayFilter Factory 5.18 SetResponseHeader GatewayFilter Factory 5.19 SetStatus GatewayFilter Factory 5.20 StripPrefix GatewayFilter Factory 5.21 Retry GatewayFilter Factory 5.22 RequestSize GatewayFilter Factory 5.23 Modify Request Body GatewayFilter Factory 5.24 Modify Response Body GatewayFilter Factory 6.1 全局Filter和GatewayFilter组合排序 6.2 Forward Routing Filter 6.3 LoadBalancerClient Filter 6.4 Netty Routing Filter 6.5 Netty Write Response Filter 6.6 RouteToRequestUrl Filter 6.7 Websocket Routing Filter 6.8 Gateway Metrics Filter 6.9 Making An Exchange As Routed 7.1 TLS 握手 8.1 Fluent Java Routes API 8.2 DiscoveryClient Route Definition Locator 8.2.1 Configuring Predicates and Filters For DiscoveryClient Routes 11.1 Retrieving route filters 11.1.1 Global Filters 11.1.2 Route Filters 11.2 Refreshing the route cache 11.3 Retrieving the routes defined in the gateway 11.4 Retrieving information about a particular route 11.5 Creating and deleting a particular route 11.6 Recap: list of all endpoints 12.1 Writing Custom Route Predicate Factories 12.2 Writing Custom GatewayFilter Factories 12.3 Writing Custom Global Filters 12.4 Writing Custom Route Locators and Writers",content:'# Spring Cloud Gateway 中文官网文档\n\n\n# 文档版本 2.1.0\n\n目录\n\n1. How to Include Spring Cloud Gateway\n2. Glossary\n3. How It Works\n4. Route Predicate Factories\n5. GatewayFilter Factories\n6. Global Filters\n7. TLS / SSL\n8. Configuration\n9. Reactor Netty Access Logs\n10. CORS Configuration\n11. Actuator API\n12. Developer Guide\n\n\n该项目提供了一个建立在Spring Ecosystem之上的API网关，包括：Spring 5，Spring Boot 2和Project Reactor。Spring Cloud Gateway旨在提供一种简单而有效的方式来对API进行路由，并为他们提供切面，例如：安全性，监控/指标 和弹性等。\n\n\n# 1. 如何在工程中引用Spring Cloud Gateway\n\n要在项目中引入Spring Cloud Gateway，需要引用 group org.springframework.cloud 和 artifact id为spring-cloud-starter-gateway starter。最新的Spring Cloud Release 构建信息，请参阅Spring Cloud Project page。\n\n如果应用了该starter，但由于某种原因不希望启用网关，请进行设置spring.cloud.gateway.enabled=false。\n\n> 重要 Spring Cloud Gateway依赖Spring Boot和Spring Webflux提供的Netty runtime。它不能在传统的Servlet容器中工作或构建为WAR\n\n\n# 2. 词汇表\n\n * Route 路由：gateway的基本构建模块。它由ID、目标URI、断言集合和过滤器集合组成。如果聚合断言结果为真，则匹配到该路由。\n * Predicate 断言：这是一个Java 8 Function Predicate。输入类型是 Spring Framework ServerWebExchange。这允许开发人员可以匹配来自HTTP请求的任何内容，例如Header或参数。\n * Filter 过滤器：这些是使用特定工厂构建的 Spring FrameworkGatewayFilter实例。所以可以在返回请求之前或之后修改请求和响应的内容。\n\n\n# 3. 如何工作的\n\n\n\nSpring Cloud Gateway Diagram\n\n客户端向Spring Cloud Gateway发出请求。如果Gateway Handler Mapping确定请求与路由匹配，则将其发送到Gateway Web Handler。此handler通过特定于该请求的过滤器链处理请求。图中filters被虚线划分的原因是filters可以在发送代理请求之前或之后执行逻辑。先执行所有“pre filter”逻辑，然后进行请求代理。在请求代理执行完后，执行“post filter”逻辑。\n\n> 注意 HTTP和HTTPS URI默认端口设置是80和443。\n\n\n# 4. 路由断言Factories\n\nSpring Cloud Gateway将路由作为Spring WebFlux HandlerMapping基础结构的一部分进行匹配。Spring Cloud Gateway包含许多内置的路由断言Factories。这些断言都匹配HTTP请求的不同属性。多个路由断言Factories可以通过 and 组合使用。\n\n\n# 4.1 After 路由断言 Factory\n\nAfter Route Predicate Factory采用一个参数——日期时间。在该日期时间之后发生的请求都将被匹配。\n\napplication.yml\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: after_route\n        uri: http://example.org\n        predicates:\n        - After=2017-01-20T17:42:47.789-07:00[America/Denver]\n\n\n\n# 4.2 Before 路由断言 Factory\n\nBefore Route Predicate Factory采用一个参数——日期时间。在该日期时间之前发生的请求都将被匹配。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: before_route\n        uri: http://example.org\n        predicates:\n        - Before=2017-01-20T17:42:47.789-07:00[America/Denver]\n\n\n\n# 4.3 Between 路由断言 Factory\n\nBetween 路由断言 Factory有两个参数，datetime1和datetime2。在datetime1和datetime2之间的请求将被匹配。datetime2参数的实际时间必须在datetime1之后。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: between_route\n        uri: http://example.org\n        predicates:\n        - Between=2017-01-20T17:42:47.789-07:00[America/Denver], 2017-01-21T17:42:47.789-07:00[America/Denver]\n\n\n\n# 4.4 Cookie 路由断言 Factory\n\nCookie 路由断言 Factory有两个参数，cookie名称和正则表达式。请求包含次cookie名称且正则表达式为真的将会被匹配。\n\napplication.yml\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: cookie_route\n        uri: http://example.org\n        predicates:\n        - Cookie=chocolate, ch.p\n\n\n\n# 4.5 Header 路由断言 Factory\n\nHeader 路由断言 Factory有两个参数，header名称和正则表达式。请求包含次header名称且正则表达式为真的将会被匹配。\n\napplication.yml.\n\nspring:\n cloud:\n   gateway:\n     routes:\n     - id: header_route\n       uri: http://example.org\n       predicates:\n       - Header=X-Request-Id, \\d+\n\n\n\n# 4.6 Host 路由断言 Factory\n\nHost 路由断言 Factory包括一个参数：host name列表。使用Ant路径匹配规则，.作为分隔符。 application.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: host_route\n        uri: http://example.org\n        predicates:\n        - Host=**.somehost.org,**.anotherhost.org\n\n\n\n# 4.7 Method 路由断言 Factory\n\nMethod 路由断言 Factory只包含一个参数: 需要匹配的HTTP请求方式\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: method_route\n        uri: http://example.org\n        predicates:\n        - Method=GET\n\n\n所有GET请求都将被路由\n\n\n# 4.8 Path 路由断言 Factory\n\nPath 路由断言 Factory 有2个参数: 一个Spring PathMatcher表达式列表和可选matchOptionalTrailingSeparator标识 .\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: host_route\n        uri: http://example.org\n        predicates:\n        - Path=/foo/{segment},/bar/{segment}\n\n\n例如: /foo/1 or /foo/bar or /bar/baz的请求都将被匹配\n\nURI 模板变量 (如上例中的 segment ) 将以Map的方式保存于ServerWebExchange.getAttributes() key为ServerWebExchangeUtils.URI_TEMPLATE_VARIABLES_ATTRIBUTE. 这些值将在GatewayFilter Factories使用\n\n可以使用以下方法来更方便地访问这些变量。\n\nMap<String, String> uriVariables = ServerWebExchangeUtils.getPathPredicateVariables(exchange);\n\nString segment = uriVariables.get("segment");\n\n\n\n# 4.9 Query 路由断言 Factory\n\nQuery 路由断言 Factory 有2个参数: 必选项 param 和可选项 regexp.\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: query_route\n        uri: http://example.org\n        predicates:\n        - Query=baz\n\n\n则包含了请求参数 baz的都将被匹配。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: query_route\n        uri: http://example.org\n        predicates:\n        - Query=foo, ba.\n\n\n如果请求参数里包含foo参数，并且值匹配为ba. 表达式，则将会被路由，如：bar and baz\n\n\n# 4.10 RemoteAddr 路由断言 Factory\n\nRemoteAddr 路由断言 Factory的参数为 一个CIDR符号（IPv4或IPv6）字符串的列表，最小值为1，例如192.168.0.1/16（其中192.168.0.1是IP地址并且16是子网掩码）。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: remoteaddr_route\n        uri: http://example.org\n        predicates:\n        - RemoteAddr=192.168.1.1/24\n\n\n如果请求的remote address 为 192.168.1.10则将被路由\n\n\n# 4.10.1 修改远程地址的解析方式\n\n默认情况下，RemoteAddr 路由断言 Factory使用传入请求中的remote address。如果Spring Cloud Gateway位于代理层后面，则可能与实际客户端IP地址不匹配。\n\n可以通过设置自定义RemoteAddressResolver来自定义解析远程地址的方式。Spring Cloud Gateway网关附带一个非默认远程地址解析程序，它基于X-Forwarded-For header, XForwardedRemoteAddressResolver.\n\nXForwardedRemoteAddressResolver 有两个静态构造函数方法，采用不同的安全方法：\n\n 1. XForwardedRemoteAddressResolver:：TrustAll返回一个RemoteAddressResolver，它始终采用X-Forwarded-for头中找到的第一个IP地址。这种方法容易受到欺骗，因为恶意客户端可能会为解析程序接受的“x-forwarded-for”设置初始值。\n 2. XForwardedRemoteAddressResolver:：MaxTrustedIndex获取一个索引，该索引与在Spring Cloud网关前运行的受信任基础设施数量相关。例如，如果SpringCloudGateway只能通过haproxy访问，则应使用值1。如果在访问Spring Cloud Gateway之前需要两个受信任的基础架构跃点，那么应该使用2。\n\n给定以下的header值：\n\nX-Forwarded-For: 0.0.0.1, 0.0.0.2, 0.0.0.3\n\n\n下面的` maxTrustedIndex值将生成以下远程地址:\n\nMAXTRUSTEDINDEX          RESULT\n[Integer.MIN_VALUE,0]    (invalid, IllegalArgumentException during initialization)\n1                        0.0.0.3\n2                        0.0.0.2\n3                        0.0.0.1\n[4, Integer.MAX_VALUE]   0.0.0.1\n\nJava 配置方式:\n\nGatewayConfig.java\n\nRemoteAddressResolver resolver = XForwardedRemoteAddressResolver\n    .maxTrustedIndex(1);\n\n...\n\n.route("direct-route",\n    r -> r.remoteAddr("10.1.1.1", "10.10.1.1/24")\n        .uri("https://downstream1")\n.route("proxied-route",\n    r -> r.remoteAddr(resolver,  "10.10.1.1", "10.10.1.1/24")\n        .uri("https://downstream2")\n)\n\n\n\n# 5. GatewayFilter Factories\n\n过滤器允许以某种方式修改传入的HTTP请求或返回的HTTP响应。过滤器的作用域是某些特定路由。Spring Cloud Gateway包括许多内置的 Filter工厂。\n\n注意：有关如何使用以下任何过滤器的更详细示例，请查看unit tests.。\n\n\n# 5.1 AddRequestHeader GatewayFilter Factory\n\n采用一对名称和值作为参数 application.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: add_request_header_route\n        uri: http://example.org\n        filters:\n        - AddRequestHeader=X-Request-Foo, Bar\n\n\n对于所有匹配的请求，这将向下游请求的头中添加 x-request-foo:bar header\n\n\n# 5.2 AddRequestParameter GatewayFilter Factory\n\n采用一对名称和值作为参数 application.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: add_request_parameter_route\n        uri: http://example.org\n        filters:\n        - AddRequestParameter=foo, bar\n\n\n对于所有匹配的请求，这将向下游请求添加foo=bar查询字符串\n\n\n# 5.3 AddResponseHeader GatewayFilter Factory\n\n采用一对名称和值作为参数\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: add_request_header_route\n        uri: http://example.org\n        filters:\n        - AddResponseHeader=X-Response-Foo, Bar\n\n\n对于所有匹配的请求，这会将x-response-foo:bar头添加到下游响应的header中\n\n\n# 5.4 Hystrix GatewayFilter Factory\n\nHystrix 是Netflix开源的断路器组件。Hystrix GatewayFilter允许你向网关路由引入断路器，保护你的服务不受级联故障的影响，并允许你在下游故障时提供fallback响应。\n\n要在项目中启用Hystrix网关过滤器，需要添加对 spring-cloud-starter-netflix-hystrix的依赖 Spring Cloud Netflix.\n\nHystrix GatewayFilter Factory 需要一个name参数，即HystrixCommand的名称。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: hystrix_route\n        uri: http://example.org\n        filters:\n        - Hystrix=myCommandName\n\n\n这将剩余的过滤器包装在命令名为“myCommandName”的HystrixCommand中。\n\nhystrix过滤器还可以接受可选的fallbackUri 参数。目前，仅支持forward: 预设的URI，如果调用fallback，则请求将转发到与URI匹配的控制器。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: hystrix_route\n        uri: lb://backing-service:8088\n        predicates:\n        - Path=/consumingserviceendpoint\n        filters:\n        - name: Hystrix\n          args:\n            name: fallbackcmd\n            fallbackUri: forward:/incaseoffailureusethis\n        - RewritePath=/consumingserviceendpoint, /backingserviceendpoint\n\n\n当调用hystrix fallback时，这将转发到/incaseoffailureusethis uri。注意，这个示例还演示了（可选）通过目标URI上的\'lb`前缀,使用Spring Cloud Netflix Ribbon 客户端负载均衡。\n\n主要场景是使用fallbackUri 到网关应用程序中的内部控制器或处理程序。但是，也可以将请求重新路由到外部应用程序中的控制器或处理程序，如：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: ingredients\n        uri: lb://ingredients\n        predicates:\n        - Path=//ingredients/**\n        filters:\n        - name: Hystrix\n          args:\n            name: fetchIngredients\n            fallbackUri: forward:/fallback\n      - id: ingredients-fallback\n        uri: http://localhost:9994\n        predicates:\n        - Path=/fallback\n\n\n在本例中，gateway应用程序中没有 fallback 实现，但是另一个应用程序中有一个接口实现，注册为“http://localhost:9994”。\n\n在将请求转发到fallback的情况下，Hystrix Gateway过滤还支持直接抛出Throwable 。它被作为ServerWebExchangeUtils.HYSTRIX_EXECUTION_EXCEPTION_ATTR属性添加到ServerWebExchange中，可以在处理网关应用程序中的fallback时使用。\n\n对于外部控制器/处理程序方案，可以添加带有异常详细信息的header。可以在 FallbackHeaders GatewayFilter Factory section.中找到有关它的更多信息。\n\nhystrix配置参数（如 timeouts）可以使用全局默认值配置，也可以使用Hystrix wiki中所述属性进行配置。\n\n要为上面的示例路由设置5秒超时，将使用以下配置：\n\napplication.yml.\n\nhystrix.command.fallbackcmd.execution.isolation.thread.timeoutInMilliseconds: 5000\n\n\n\n# 5.5 FallbackHeaders GatewayFilter Factory\n\nFallbackHeaders允许在转发到外部应用程序中的FallbackUri的请求的header中添加Hystrix异常详细信息，如下所示：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: ingredients\n        uri: lb://ingredients\n        predicates:\n        - Path=//ingredients/**\n        filters:\n        - name: Hystrix\n          args:\n            name: fetchIngredients\n            fallbackUri: forward:/fallback\n      - id: ingredients-fallback\n        uri: http://localhost:9994\n        predicates:\n        - Path=/fallback\n        filters:\n        - name: FallbackHeaders\n          args:\n            executionExceptionTypeHeaderName: Test-Header\n\n\n在本例中，在运行HystrixCommand发生执行异常后，请求将被转发到 localhost:9994应用程序中的 fallback终端或程序。异常类型、消息（如果可用）cause exception类型和消息的头，将由FallbackHeaders filter添加到该请求中。\n\n通过设置下面列出的参数值及其默认值，可以在配置中覆盖headers的名称：\n\n * executionExceptionTypeHeaderName ("Execution-Exception-Type")\n * executionExceptionMessageHeaderName ("Execution-Exception-Message")\n * rootCauseExceptionTypeHeaderName ("Root-Cause-Exception-Type")\n * rootCauseExceptionMessageHeaderName ("Root-Cause-Exception-Message")\n\nHystrix 如何实现的更多细节可以参考 Hystrix GatewayFilter Factory section.\n\n\n# 5.6 PrefixPath GatewayFilter Factory\n\nPrefixPath GatewayFilter 只有一个 prefix 参数.\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: prefixpath_route\n        uri: http://example.org\n        filters:\n        - PrefixPath=/mypath\n\n\n这将给所有匹配请求的路径加前缀/mypath。因此，向/hello发送的请求将发送到/mypath/hello。\n\n\n# 5.7 PreserveHostHeader GatewayFilter Factory\n\n该filter没有参数。设置了该Filter后，GatewayFilter将不使用由HTTP客户端确定的host header ，而是发送原始host header 。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: preserve_host_route\n        uri: http://example.org\n        filters:\n        - PreserveHostHeader\n\n\n\n# 5.8 RequestRateLimiter GatewayFilter Factory\n\nRequestRateLimiter使用RateLimiter实现是否允许继续执行当前请求。如果不允许继续执行，则返回HTTP 429 - Too Many Requests （默认情况下）。\n\n这个过滤器可以配置一个可选的keyResolver 参数和rate limiter参数（见下文）。\n\nkeyResolver 是 KeyResolver 接口的实现类.在配置中，按名称使用SpEL引用bean。#{@myKeyResolver} 是引用名为\'myKeyResolver\'的bean的SpEL表达式。\n\nKeyResolver.java.\n\npublic interface KeyResolver {\n    Mono<String> resolve(ServerWebExchange exchange);\n}\n\n\nKeyResolver接口允许使用可插拔策略来派生限制请求的key。在未来的里程碑版本中，将有一些KeyResolver实现。\n\nKeyResolver的默认实现是PrincipalNameKeyResolver，它从ServerWebExchange检索Principal并调用Principal.getName()。\n\n默认情况下，如果KeyResolver 没有获取到key，请求将被拒绝。此行为可以使用 spring.cloud.gateway.filter.request-rate-limiter.deny-empty-key (true or false) 和 spring.cloud.gateway.filter.request-rate-limiter.empty-key-status-code属性进行调整。\n\n> 说明 无法通过"shortcut" 配置RequestRateLimiter。以下示例无效\n\napplication.properties.\n\n# INVALID SHORTCUT CONFIGURATION\nspring.cloud.gateway.routes[0].filters[0]=RequestRateLimiter=2, 2, #{@userkeyresolver}\n\n\n\n# 5.8.1 Redis RateLimiter\n\nRedis的实现基于 Stripe实现。它需要使用 spring-boot-starter-data-redis-reactive Spring Boot starter。\n\n使用的算法是Token Bucket Algorithm.。\n\nredis-rate-limiter.replenishRate是你允许用户每秒执行多少请求，而丢弃任何请求。这是令牌桶的填充速率。\n\n``redis-rate-limiter.burstCapacity`是允许用户在一秒钟内执行的最大请求数。这是令牌桶可以保存的令牌数。将此值设置为零将阻止所有请求。\n\n稳定速率是通过在replenishRate 和 burstCapacity中设置相同的值来实现的。可通过设置burstCapacity高于replenishRate来允许临时突发流浪。在这种情况下，限流器需要在两次突发之间留出一段时间（根据replenishRate），因为连续两次突发将导致请求丢失 (HTTP 429 - Too Many Requests).。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: requestratelimiter_route\n        uri: http://example.org\n        filters:\n        - name: RequestRateLimiter\n          args:\n            redis-rate-limiter.replenishRate: 10\n            redis-rate-limiter.burstCapacity: 20\n\n\nConfig.java.\n\n@Bean\nKeyResolver userKeyResolver() {\n    return exchange -> Mono.just(exchange.getRequest().getQueryParams().getFirst("user"));\n}\n\n\n这定义了每个用户10个请求的限制。允许20个突发，但下一秒只有10个请求可用。KeyResolver是一个简单的获取user请求参数的工具（注意：不建议用于生产）。\n\n限流器也可以定义为RateLimiter接口的实现 bean。在配置中，按名称使用SpEL引用bean。#{@myRateLimiter}是引用名为\'myRateLimiter\'的bean的SpEL表达式。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: requestratelimiter_route\n        uri: http://example.org\n        filters:\n        - name: RequestRateLimiter\n          args:\n            rate-limiter: "#{@myRateLimiter}"\n            key-resolver: "#{@userKeyResolver}"\n\n\n\n# 5.9 RedirectTo GatewayFilter Factory\n\n该过滤器有一个 status 和一个 url参数。status是300类重定向HTTP代码，如301。该URL应为有效的URL，这将是 Location header的值。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: prefixpath_route\n        uri: http://example.org\n        filters:\n        - RedirectTo=302, http://acme.org\n\n\n这将发送一个302状态码和一个Location:http://acme.org header来执行重定向。\n\n\n# 5.10 RemoveNonProxyHeaders GatewayFilter Factory\n\nRemoveNonProxyHeaders GatewayFilter Factory 从转发请求中删除headers。删除的默认头列表来自 IETF.\n\nThe default removed headers are:\n\n * Connection\n * Keep-Alive\n * Proxy-Authenticate\n * Proxy-Authorization\n * TE\n * Trailer\n * Transfer-Encoding\n * Upgrade 要更改此设置，请将 spring.cloud.gateway.filter.remove-non-proxy-headers.headers属性设置为要删除的header名称。\n\n\n# 5.11 RemoveRequestHeader GatewayFilter Factory\n\n有一个name参数. 这是要删除的header的名称。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: removerequestheader_route\n        uri: http://example.org\n        filters:\n        - RemoveRequestHeader=X-Request-Foo\n\n\n这将在X-Request-Foo header被发送到下游之前删除它。\n\n\n# 5.12 RemoveResponseHeader GatewayFilter Factory\n\n有一个name参数. 这是要删除的header的名称。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: removeresponseheader_route\n        uri: http://example.org\n        filters:\n        - RemoveResponseHeader=X-Response-Foo\n\n\n这将在返回到网关client之前从响应中删除x-response-foo头。\n\n\n# 5.13 RewritePath GatewayFilter Factory\n\n包含一个 regexp正则表达式参数和一个 replacement 参数. 通过使用Java正则表达式灵活地重写请求路径。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: rewritepath_route\n        uri: http://example.org\n        predicates:\n        - Path=/foo/**\n        filters:\n        - RewritePath=/foo/(?<segment>.*), /$\\{segment}\n\n\n对于请求路径/foo/bar，将在发出下游请求之前将路径设置为/bar。注意,由于YAML规范，请使用 $\\替换 $。\n\n\n# 5.14 RewriteResponseHeader GatewayFilter Factory\n\n包含 name, regexp和 replacement 参数.。通过使用Java正则表达式灵活地重写响应头的值。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: rewriteresponseheader_route\n        uri: http://example.org\n        filters:\n        - RewriteResponseHeader=X-Response-Foo, , password=[^&]+, password=***\n\n\n对于一个/42?user=ford&password=omg!what&flag=true的header值，在做下游请求时将被设置为/42?user=ford&password=***&flag=true，由于YAML规范，请使用 $\\替换 $。\n\n\n# 5.15 SaveSession GatewayFilter Factory\n\nSaveSession GatewayFilter Factory将调用转发到下游之前强制执行WebSession::save 操作。这在使用 Spring Session 之类时特别有用，需要确保会话状态在进行转发调用之前已保存。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: save_session\n        uri: http://example.org\n        predicates:\n        - Path=/foo/**\n        filters:\n        - SaveSession\n\n\n如果你希望要将[Spring Security]（https://projects.spring.io/Spring Security/）与Spring Session集成,并确保安全详细信息已转发到远程的进程，这一点至关重要。\n\n\n# 5.16 SecureHeaders GatewayFilter Factory\n\nSecureHeaders GatewayFilter Factory 将许多headers添加到reccomedation处的响应中，从this blog post.\n\n添加以下标题（使用默认值分配）:\n\n * X-Xss-Protection:1; mode=block\n * Strict-Transport-Security:max-age=631138519\n * X-Frame-Options:DENY\n * X-Content-Type-Options:nosniff\n * Referrer-Policy:no-referrer\n * Content-Security-Policy:default-src \'self\' https:; font-src \'self\' https: data:; img-src \'self\' https: data:; object-src \'none\'; script-src https:; style-src \'self\' https: \'unsafe-inline\'\n * X-Download-Options:noopen\n * X-Permitted-Cross-Domain-Policies:none\n\n要更改默认值，请在spring.cloud.gateway.filter.secure-headers 命名空间中设置相应的属性：\n\nProperty to change:\n\n * xss-protection-header\n * strict-transport-security\n * frame-options\n * content-type-options\n * referrer-policy\n * content-security-policy\n * download-options\n * permitted-cross-domain-policies\n\n\n# 5.17 SetPath GatewayFilter Factory\n\nSetPath GatewayFilter Factory 采用 template路径参数。它提供了一种通过允许路径的模板化segments来操作请求路径的简单方法。使用Spring Framework中的URI模板，允许多个匹配segments。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: setpath_route\n        uri: http://example.org\n        predicates:\n        - Path=/foo/{segment}\n        filters:\n        - SetPath=/{segment}\n\n\n对于一个 /foo/bar请求，在做下游请求前，路径将被设置为/bar\n\n\n# 5.18 SetResponseHeader GatewayFilter Factory\n\nSetResponseHeader GatewayFilter Factory 包括 name 和 value 参数.\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: setresponseheader_route\n        uri: http://example.org\n        filters:\n        - SetResponseHeader=X-Response-Foo, Bar\n\n\n此GatewayFilter使用给定的名称替换所有header，而不是添加。因此，如果下游服务器响应为X-Response-Foo:1234，则会将其替换为X-Response-Foo:Bar,这是网关客户端将接收的内容。\n\n\n# 5.19 SetStatus GatewayFilter Factory\n\nSetStatus GatewayFilter Factory 包括唯一的 status参数.必须是一个可用的Spring HttpStatus。它可以是整数值404或字符串枚举NOT_FOUND。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: setstatusstring_route\n        uri: http://example.org\n        filters:\n        - SetStatus=BAD_REQUEST\n      - id: setstatusint_route\n        uri: http://example.org\n        filters:\n        - SetStatus=401\n\n\n在这个例子中，HTTP返回码将设置为401.\n\n\n# 5.20 StripPrefix GatewayFilter Factory\n\nStripPrefix GatewayFilter Factory 包括一个parts参数。 parts参数指示在将请求发送到下游之前，要从请求中去除的路径中的节数。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: nameRoot\n        uri: http://nameservice\n        predicates:\n        - Path=/name/**\n        filters:\n        - StripPrefix=2\n\n\n当通过网关发出/name/bar/foo请求时，向nameservice发出的请求将是http://nameservice/foo。\n\n\n# 5.21 Retry GatewayFilter Factory\n\nRetry GatewayFilter Factory包括 retries, statuses, methods和 series 参数.\n\n * retries: 应尝试的重试次数\n * statuses: 应该重试的HTTP状态代码，用org.springframework.http.HttpStatus标识\n * methods: 应该重试的HTTP方法，用 org.springframework.http.HttpMethod标识\n * series: 要重试的一系列状态码，用 org.springframework.http.HttpStatus.Series标识\n\napplication.yml.\n\nspring: cloud: gateway: routes: - id: retry_test uri: http://localhost:8080/flakey predicates: - Host=*.retry.com filters: - name: Retry args: retries: 3 statuses: BAD_GATEWAY\n\n> 注意 retry filter 不支持body请求的重试，如通过body的POST 或 PUT请求\n> \n> 注意 在使用带有前缀为 forward: 的retry filter时，应仔细编写目标端点，以便在出现错误时不会执行任何可能导致将响应发送到客户端并提交的操作。例如，如果目标端点是带注解的controller，则目标controller方法不应返回带有错误状态代码的ResponseEntity。相反，它应该抛出一个Exception，或者发出一个错误信号，例如通过Mono.error(ex) 返回值，重试过滤器可以配置为通过重试来处理。\n\n\n# 5.22 RequestSize GatewayFilter Factory\n\n当请求大小大于允许的限制时，RequestSize GatewayFilter Factory可以限制请求不到达下游服务。过滤器以RequestSize作为参数，这是定义请求的允许大小限制(以字节为单位)。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: request_size_route\n      uri: http://localhost:8080/upload\n      predicates:\n      - Path=/upload\n      filters:\n      - name: RequestSize\n        args:\n          maxSize: 5000000\n\n\n当请求因大小而被拒绝时， RequestSize GatewayFilter Factory 将响应状态设置为413 Payload Too Large，并带有额外的header errorMessage 。下面是一个 errorMessage的例子。\n\nerrorMessage` : `Request size is larger than permissible limit. Request size is 6.0 MB where permissible limit is 5.0 MB\n\n\n> 注意 如果未在路由定义中作为filter参数提供，则默认请求大小将设置为5 MB。\n\n\n# 5.23 Modify Request Body GatewayFilter Factory\n\n这个过滤器被定义为beta版本，将来API可能会改变。\n\n此过滤器可用于在请求主体被网关发送到下游之前对其进行修改。\n\n> 注意 只能使用Java DSL配置此过滤器\n\n@Bean\npublic RouteLocator routes(RouteLocatorBuilder builder) {\n    return builder.routes()\n        .route("rewrite_request_obj", r -> r.host("*.rewriterequestobj.org")\n            .filters(f -> f.prefixPath("/httpbin")\n                .modifyRequestBody(String.class, Hello.class, MediaType.APPLICATION_JSON_VALUE,\n                    (exchange, s) -> return Mono.just(new Hello(s.toUpperCase())))).uri(uri))\n        .build();\n}\n\nstatic class Hello {\n    String message;\n\n    public Hello() { }\n\n    public Hello(String message) {\n        this.message = message;\n    }\n\n    public String getMessage() {\n        return message;\n    }\n\n    public void setMessage(String message) {\n        this.message = message;\n    }\n}\n\n\n\n# 5.24 Modify Response Body GatewayFilter Factory\n\n这个过滤器被定义为beta版本，将来API可能会改变。 此过滤器可用于在将响应正文发送回客户端之前对其进行修改。\n\n> 注意 只能使用Java DSL配置此过滤器\n\n@Bean\npublic RouteLocator routes(RouteLocatorBuilder builder) {\n    return builder.routes()\n        .route("rewrite_response_upper", r -> r.host("*.rewriteresponseupper.org")\n            .filters(f -> f.prefixPath("/httpbin")\n                .modifyResponseBody(String.class, String.class,\n                    (exchange, s) -> Mono.just(s.toUpperCase()))).uri(uri)\n        .build();\n}\n\n\n\n# 6. Global Filters\n\nGlobalFilter接口与GatewayFilter具有相同的签名。是有条件地应用于所有路由的特殊过滤器。（此接口和用法可能在将来的里程碑版本中发生更改）。\n\n\n# 6.1 全局Filter和GatewayFilter组合排序\n\n当请求进入（并与路由匹配）时，筛选Web Handler 会将GlobalFilter的所有实例和所有的GatewayFilter路由特定实例添加到 filter chain。filter组合的排序由org.springframework.core.Ordered接口决定，可以通过实现getOrde()方法或使用@Order注释来设置。\n\n由于Spring Cloud Gateway将用于执行过滤器逻辑区分为“前置”和“后置”阶段，具有最高优先级的过滤器将是“前置”阶段的第一个，而“后置”阶段的最后一个。\n\nExampleConfiguration.java.\n\n@Bean\n@Order(-1)\npublic GlobalFilter a() {\n    return (exchange, chain) -> {\n        log.info("first pre filter");\n        return chain.filter(exchange).then(Mono.fromRunnable(() -> {\n            log.info("third post filter");\n        }));\n    };\n}\n\n@Bean\n@Order(0)\npublic GlobalFilter b() {\n    return (exchange, chain) -> {\n        log.info("second pre filter");\n        return chain.filter(exchange).then(Mono.fromRunnable(() -> {\n            log.info("second post filter");\n        }));\n    };\n}\n\n@Bean\n@Order(1)\npublic GlobalFilter c() {\n    return (exchange, chain) -> {\n        log.info("third pre filter");\n        return chain.filter(exchange).then(Mono.fromRunnable(() -> {\n            log.info("first post filter");\n        }));\n    };\n}\n\n\n\n# 6.2 Forward Routing Filter\n\nForwardRoutingFilter在exchange属性ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR中查找URI。如果URL有一个forwardscheme (如 forward:///localendpoint)，它将使用Spring DispatcherHandler 来处理请求。请求URL的路径部分将被转发URL中的路径覆盖。未修改的原始URL将附加到 ServerWebExchangeUtils.GATEWAY_ORIGINAL_REQUEST_URL_ATTR属性中的列表中。\n\n\n# 6.3 LoadBalancerClient Filter\n\nLoadBalancerClientFilter在exchange属性ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR中查找URI。如果URL有一个lbscheme (如 lb://myservice），它将使用Spring Cloud LoadBalancerClient 将名称（在前一个示例中为\'myservice）解析为实际主机和端口，并替换URI。未修改的原始URL将附加到ServerWebExchangeUtils.GATEWAY_ORIGINAL_REQUEST_URL_ATTR属性中的列表中。过滤器还将查看ServerWebExchangeUtils.GATEWAY_SCHEME_PREFIX_ATTR属性，查看它是否等于lb`，然后应用相同的规则。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: myRoute\n        uri: lb://service\n        predicates:\n        - Path=/service/**\n\n\n> 注意 默认情况下，如果一个服务实例在LoadBalancer 中没有发现，则返回503。可以通过设置spring.cloud.gateway.loadbalancer.use404=true来让网管返回404.\n> \n> 注意\n\n从LoadBalancer返回的ServiceInstance的isSecure 值将覆盖在对网关发出的请求中指定的scheme。例如，如果请求通过HTTPS进入网关，但ServiceInstance表示它不安全，则下游请求将通过HTTP协议。相反的情况也适用。但是，如果在网关配置中为路由指定了GATEWAY_SCHEME_PREFIX_ATTR，则前缀将被删除，并且路由URL生成的scheme将覆盖ServiceInstance配置。\n\n\n# 6.4 Netty Routing Filter\n\n如果位于 ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR属性中的URL具有http 或https 模式，则会运行Netty Routing Filter。它使用Netty HttpClient 发出下游代理请求。响应放在 ServerWebExchangeUtils.CLIENT_RESPONSE_ATTR exchange属性中，以便在以后的过滤器中使用。（有一个实验阶段不需要Netty的相同的功能的Filter，WebClientHttpRoutingFilter）\n\n\n# 6.5 Netty Write Response Filter\n\n如果ServerWebExchangeUtils.CLIENT_RESPONSE_ATTR exchange属性中存在 Netty HttpClientResponse，则运行 NettyWriteResponseFilter 。它在其他所有过滤器完成后将代理响应写回网关客户端响应之后运行。（有一个不需要netty的实验性的WebClientWriteResponseFilter执行相同的功能）\n\n\n# 6.6 RouteToRequestUrl Filter\n\n如果ServerWebExchangeUtils.GATEWAY_ROUTE_ATTR exchange属性中存在Route对象，RouteToRequestUrlFilter将运行。它基于请求URI创建一个新的URI，使用Route对象的uri属性进行更新。新的URI被放置在ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR exchange属性中。\n\n如果该URI有一个前缀scheme，例如lb:ws://serviceid，则会从该URI中剥离该 lb scheme，并将其放置在ServerWebExchangeUtils.GATEWAY_SCHEME_PREFIX_ATTR中，以便稍后在过滤器链中使用。\n\n\n# 6.7 Websocket Routing Filter\n\n如果ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTRexchange属性中有 ws 、 wssscheme，则Websocket Routing Filter将被运行。它使用Spring Web Socket基础模块将Websocket转发到下游。\n\nURI前缀为lb的Websockets可以被负载均衡，如 lb:ws://serviceid.\n\n> 注意 如果使用 SockJS 作为普通HTTP的fallback，则应配置普通HTTP路由以及WebSocket路由。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      # SockJS route\n      - id: websocket_sockjs_route\n        uri: http://localhost:3001\n        predicates:\n        - Path=/websocket/info/**\n      # Normwal Websocket route\n      - id: websocket_route\n        uri: ws://localhost:3001\n        predicates:\n        - Path=/websocket/**\n\n\n\n# 6.8 Gateway Metrics Filter\n\n要启用网关指标，请将spring-boot-starter-actuator添加为项目依赖项。然后，默认情况下，只要属性spring.cloud.gateway.metrics.enabled未设置为false，网关指标过滤器就会运行。此过滤器添加名为“gateway.requests”的计时器指标，并带有以下标记：\n\n * routeId: The route id\n * routeUri: API 将被转发的URI\n * outcome: 结果分类依据 HttpStatus.Series\n * status: 返回client的请求的Http Status\n\n这些指标可以从/actuator/metrics/gateway.requests中获取，可以很容易地与Prometheus集成以创建Grafana dashboard.\n\n> 注意 要将pometheus启用，需要添加 micrometer-registry-prometheus为项目依赖。\n\n\n# 6.9 Making An Exchange As Routed\n\n网关路由ServerWebExchange之后，它将通过向Exchange属性添加gatewayAlreadyRouted，将该exchange标记为“routed”。一旦一个请求被标记为routed，其他路由过滤器将不会再次路由该请求，将跳过该过滤器。有一些方便的方法可以用来将exchange标记为routed，或者检查exchange是否已经routed。\n\n * ServerWebExchangeUtils.isAlreadyRouted 有一个 ServerWebExchange对象并检查它是否已"routed"\n * ServerWebExchangeUtils.setAlreadyRouted 有一个 ServerWebExchange 对象并将其标记为"routed"\n\n\n# 7. TLS / SSL\n\n网关可以通过常规的 Spring server configuration 来侦听HTTPS上的请求。例子：\n\napplication.yml.\n\nserver:\n  ssl:\n    enabled: true\n    key-alias: scg\n    key-store-password: scg1234\n    key-store: classpath:scg-keystore.p12\n    key-store-type: PKCS12\n\n\n网关路由可以路由到HTTP和HTTPS后端。如果路由到HTTPS后端，则可以将网关配置为信任所有具有证书的下游服务：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      httpclient:\n        ssl:\n          useInsecureTrustManager: true\n\n\n不建议在生产环境使用不安全的信任管理器。对于生产部署，可以使用一组已知证书配置网关，这些证书可以通过以下方式进行配置：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      httpclient:\n        ssl:\n          trustedX509Certificates:\n          - cert1.pem\n          - cert2.pem\n\n\n如果Spring Cloud Gateway未配置受信任证书，则使用默认信任库（可以使用系统属性javax.net.ssl.trustStore覆盖）。\n\n\n# 7.1 TLS 握手\n\n网关维护一个用于路由到后端的client池。当通过HTTPS通信时，客户端启动一个TLS握手，其中可能会有很多超时。这些超时可以这样配置（显示默认值）：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      httpclient:\n        ssl:\n          handshake-timeout-millis: 10000\n          close-notify-flush-timeout-millis: 3000\n          close-notify-read-timeout-millis: 0\n\n\n\n# 8. Configuration\n\nSpring Cloud Gateway的配置由RouteDefinitionLocator的集合驱动。\n\nRouteDefinitionLocator.java.\n\npublic interface RouteDefinitionLocator {\n    Flux<RouteDefinition> getRouteDefinitions();\n}\n\n\n默认情况下，PropertiesRouteDefinitionLocator使用Spring Boot的@ConfigurationProperties机制加载属性。\n\n以下两个示例是等效的：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: setstatus_route\n        uri: http://example.org\n        filters:\n        - name: SetStatus\n          args:\n            status: 401\n      - id: setstatusshortcut_route\n        uri: http://example.org\n        filters:\n        - SetStatus=401\n\n\n对于网关的大部分用法，配置文件方式是够用的，但一些生产用例更建议从外部源（如数据库）加载配置。未来的里程碑版本将有基于Spring Data Repositories （如Redis、MongoDB和Cassandra）的RouteDefinitionLocator实现。\n\n\n# 8.1 Fluent Java Routes API\n\n为了可以更简单在Java中配置，在RouteLocatorBuilder bean中定义了一个fluent API。\n\nGatewaySampleApplication.java.\n\n// static imports from GatewayFilters and RoutePredicates\n@Bean\npublic RouteLocator customRouteLocator(RouteLocatorBuilder builder, ThrottleGatewayFilterFactory throttle) {\n    return builder.routes()\n            .route(r -> r.host("**.abc.org").and().path("/image/png")\n                .filters(f ->\n                        f.addResponseHeader("X-TestHeader", "foobar"))\n                .uri("http://httpbin.org:80")\n            )\n            .route(r -> r.path("/image/webp")\n                .filters(f ->\n                        f.addResponseHeader("X-AnotherHeader", "baz"))\n                .uri("http://httpbin.org:80")\n            )\n            .route(r -> r.order(-1)\n                .host("**.throttle.org").and().path("/get")\n                .filters(f -> f.filter(throttle.apply(1,\n                        1,\n                        10,\n                        TimeUnit.SECONDS)))\n                .uri("http://httpbin.org:80")\n            )\n            .build();\n}\n\n\n这种样式还允许使用更多的自定义断言。由RouteDefinitionLocator beans定义的断言使用逻辑 and组合。通过使用fluent Java API，可以在 Predicate类上使用 and()、or() 、 negate()运算符。\n\n\n# 8.2 DiscoveryClient Route Definition Locator\n\n可以将网关配置为基于使用兼容DiscoveryClient注册中心注册的服务来创建路由。\n\n要启用此功能，请设置spring.cloud.gateway.discovery.locator.enabled=true，并确保DiscoveryClient实现位于classpath上并已启用（如netflix eureka、consul或zookeeper）。\n\n\n# 8.2.1 Configuring Predicates and Filters For DiscoveryClient Routes\n\n默认情况下，网关为通过DiscoveryClient创建的路由定义单个断言和过滤器。\n\n默认断言是使用/serviceId/**定义的path断言，其中serviceId是DiscoveryClient中服务的ID。\n\n默认过滤器是使用正则表达式 /serviceId/(?<remaining>.*)和替换的/${remaining}进行重写。这只是在请求被发送到下游之前从路径中截取掉 service id 。\n\n可以通过设置spring.cloud.gateway.discovery.locator.predicates[x] and spring.cloud.gateway.discovery.locator.filters[y]来实现自定义DiscoveryClient路由使用的断言and/or过滤器。当你这样做时，如果你想要保留这个功能，你需要确保包括上面的默认断言和过滤器。下面是这样一个例子。\n\napplication.properties.\n\nspring.cloud.gateway.discovery.locator.predicates[0].name: Path\nspring.cloud.gateway.discovery.locator.predicates[0].args[pattern]: "\'/\'+serviceId+\'/**\'"\nspring.cloud.gateway.discovery.locator.predicates[1].name: Host\nspring.cloud.gateway.discovery.locator.predicates[1].args[pattern]: "\'**.foo.com\'"\nspring.cloud.gateway.discovery.locator.filters[0].name: Hystrix\nspring.cloud.gateway.discovery.locator.filters[0].args[name]: serviceId\nspring.cloud.gateway.discovery.locator.filters[1].name: RewritePath\nspring.cloud.gateway.discovery.locator.filters[1].args[regexp]: "\'/\' + serviceId + \'/(?<remaining>.*)\'"\nspring.cloud.gateway.discovery.locator.filters[1].args[replacement]: "\'/${remaining}\'"\n\n\n\n# 9. Reactor Netty Access Logs\n\n设置 -Dreactor.netty.http.server.accessLogEnabled=true 来开启Reactor Netty access logs，注意必须是Java System Property而不是Spring Boot property。\n\nlogging 模块也可以通过配置单独输出一个access log文件，下面是logback的配置例子：\n\nlogback.xml.\n\n <appender name="accessLog" class="ch.qos.logback.core.FileAppender">\n        <file>access_log.log</file>\n        <encoder>\n            <pattern>%msg%n</pattern>\n        </encoder>\n    </appender>\n    <appender name="async" class="ch.qos.logback.classic.AsyncAppender">\n        <appender-ref ref="accessLog" />\n    </appender>\n\n    <logger name="reactor.netty.http.server.AccessLog" level="INFO" additivity="false">\n        <appender-ref ref="async"/>\n    </logger>\n\n\n\n# 10. CORS Configuration\n\n我们可以通过配置网关来控制CORS行为，全局CORS配置是 Spring Framework CorsConfiguration模式的URL MAP。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      globalcors:\n        corsConfigurations:\n          \'[/**]\':\n            allowedOrigins: "http://docs.spring.io"\n            allowedMethods:\n            - GET\n\n\n例子中将允许从docs.spring.io发出的所有GET请求进行CORS请求。\n\n\n# 11. Actuator API\n\n/gateway的actuator端点允许监视Spring Cloud Gateway应用程序并与之交互。要进行远程访问，必须在应用程序属性中暴露HTTP或JMX 端口。\n\napplication.properties.\n\nmanagement.endpoint.gateway.enabled=true # default value\nmanagement.endpoints.web.exposure.include=gateway\n\n\n\n# 11.1 Retrieving route filters\n\n\n# 11.1.1 Global Filters\n\n要检索应用于所有路由的 [global filters]，请get请求 /actuator/gateway/globalfilters。返回的结果类似于以下内容：\n\n{\n  "org.springframework.cloud.gateway.filter.LoadBalancerClientFilter@77856cc5": 10100,\n  "org.springframework.cloud.gateway.filter.RouteToRequestUrlFilter@4f6fd101": 10000,\n  "org.springframework.cloud.gateway.filter.NettyWriteResponseFilter@32d22650": -1,\n  "org.springframework.cloud.gateway.filter.ForwardRoutingFilter@106459d9": 2147483647,\n  "org.springframework.cloud.gateway.filter.NettyRoutingFilter@1fbd5e0": 2147483647,\n  "org.springframework.cloud.gateway.filter.ForwardPathFilter@33a71d23": 0,\n  "org.springframework.cloud.gateway.filter.AdaptCachedBodyGlobalFilter@135064ea": 2147483637,\n  "org.springframework.cloud.gateway.filter.WebsocketRoutingFilter@23c05889": 2147483646\n}\n\n\n返回结果包含已就绪的global filters的详细信息(如 org.springframework.cloud.gateway.filter.LoadBalancerClientFilter@77856cc5)。对于每个global filters，返回结果字符串对应过滤器链中的相应顺序。\n\n\n# 11.1.2 Route Filters\n\n要检索应用于路由的 [GatewayFilter factories] ，请get请求/actuator/gateway/routefilters。返回结果类似于以下内容：\n\n{\n  "[AddRequestHeaderGatewayFilterFactory@570ed9c configClass = AbstractNameValueGatewayFilterFactory.NameValueConfig]": null,\n  "[SecureHeadersGatewayFilterFactory@fceab5d configClass = Object]": null,\n  "[SaveSessionGatewayFilterFactory@4449b273 configClass = Object]": null\n}\n\n\n返回结果包含应用于所有路由的GatewayFilter的详细信息。显示每个工厂提供字符串格式的相应对象（例如， [SecureHeadersGatewayFilterFactory@fceab5d configClass = Object]）。请注意，null值是由于endpoint controller实现不完整造成的，因为它尝试在filter chain中设置对象的顺序，这不适用于GatewayFilter工厂对象。\n\n\n# 11.2 Refreshing the route cache\n\n如果要清理路由的缓存，请POST请求/actuator/gateway/refresh。该请求将返回一个没有body的200返回码。\n\n\n# 11.3 Retrieving the routes defined in the gateway\n\n要检索网关中定义的路由，发送GET请求/actuator/gateway/routes，返回结果如下所示：\n\n[{\n  "route_id": "first_route",\n  "route_object": {\n    "predicate": "org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory$$Lambda$432/1736826640@1e9d7e7d",\n    "filters": [\n      "OrderedGatewayFilter{delegate=org.springframework.cloud.gateway.filter.factory.PreserveHostHeaderGatewayFilterFactory$$Lambda$436/674480275@6631ef72, order=0}"\n    ]\n  },\n  "order": 0\n},\n{\n  "route_id": "second_route",\n  "route_object": {\n    "predicate": "org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory$$Lambda$432/1736826640@cd8d298",\n    "filters": []\n  },\n  "order": 0\n}]\n\n\n返回结果中包含网关中所有定义的路由信息，下面表格中描述了返回结果信息：\n\nPATH                     TYPE     DESCRIPTION\nroute_id                 String   The route ID.\nroute_object.predicate   Object   The route predicate.\nroute_object.filters     Array    The GatewayFilter factories applied to the route.\norder                    Number   The route order.\n\n\n# 11.4 Retrieving information about a particular route\n\n要获取单个路由的信息，发送GET请求 /actuator/gateway/routes/{id} (如： /actuator/gateway/routes/first_route)，返回结果如下所示：\n\n{\n  "id": "first_route",\n  "predicates": [{\n    "name": "Path",\n    "args": {"_genkey_0":"/first"}\n  }],\n  "filters": [],\n  "uri": "http://www.uri-destination.org",\n  "order": 0\n}]\n\n\n下面表格中描述了返回结果信息：\n\nPATH         TYPE     DESCRIPTION\nid           String   The route ID.\npredicates   Array    The collection of route predicates. Each item defines the\n                      name and the arguments of a given predicate.\nfilters      Array    The collection of filters applied to the route.\nuri          String   The destination URI of the route.\norder        Number   The route order.\n\n\n# 11.5 Creating and deleting a particular route\n\n要创建一个路由，发送POST请求 /gateway/routes/{id_route_to_create}，参数为JSON结构，具体参数数据结构参考上面章节。\n\n要删除一个路由，发送 DELETE请求 /gateway/routes/{id_route_to_delete}。\n\n\n# 11.6 Recap: list of all endpoints\n\n下表总结了Spring Cloud Gateway actuator endpoints。注意，每个endpoint都是/actuator/gateway作为基本路径。\n\nID              HTTP METHOD   DESCRIPTION\nglobalfilters   GET           Displays the list of global filters applied to the routes.\nroutefilters    GET           Displays the list of GatewayFilter factories applied to a\n                              particular route.\nrefresh         POST          Clears the routes cache.\nroutes          GET           Displays the list of routes defined in the gateway.\nroutes/{id}     GET           Displays information about a particular route.\nroutes/{id}     POST          Adds a new route to the gateway.\nroutes/{id}     DELETE        Removes an existing route from the gateway.\n\n\n# 12. Developer Guide\n\nTODO: overview of writing custom integrations\n\n\n# 12.1 Writing Custom Route Predicate Factories\n\nTODO: document writing Custom Route Predicate Factories\n\n\n# 12.2 Writing Custom GatewayFilter Factories\n\n如果要自定义一个GatewayFilter，需要实现GatewayFilterFactory。下面是一个你需要集成的抽象类 AbstractGatewayFilterFactory。\n\nPreGatewayFilterFactory.java.\n\npublic class PreGatewayFilterFactory extends AbstractGatewayFilterFactory<PreGatewayFilterFactory.Config> {\n\n    public PreGatewayFilterFactory() {\n        super(Config.class);\n    }\n\n    @Override\n    public GatewayFilter apply(Config config) {\n        // grab configuration from Config object\n        return (exchange, chain) -> {\n            //If you want to build a "pre" filter you need to manipulate the\n            //request before calling chain.filter\n            ServerHttpRequest.Builder builder = exchange.getRequest().mutate();\n            //use builder to manipulate the request\n            return chain.filter(exchange.mutate().request(request).build());\n        };\n    }\n\n    public static class Config {\n        //Put the configuration properties for your filter here\n    }\n\n}\n\n\nPostGatewayFilterFactory.java.\n\npublic class PostGatewayFilterFactory extends AbstractGatewayFilterFactory<PostGatewayFilterFactory.Config> {\n\n    public PostGatewayFilterFactory() {\n        super(Config.class);\n    }\n\n    @Override\n    public GatewayFilter apply(Config config) {\n        // grab configuration from Config object\n        return (exchange, chain) -> {\n            return chain.filter(exchange).then(Mono.fromRunnable(() -> {\n                ServerHttpResponse response = exchange.getResponse();\n                //Manipulate the response in some way\n            }));\n        };\n    }\n\n    public static class Config {\n        //Put the configuration properties for your filter here\n    }\n\n}\n\n\n\n# 12.3 Writing Custom Global Filters\n\nTODO: document writing Custom Global Filters\n\n\n# 12.4 Writing Custom Route Locators and Writers\n\nTODO: document writing Custom Route Locators and Writers',normalizedContent:'# spring cloud gateway 中文官网文档\n\n\n# 文档版本 2.1.0\n\n目录\n\n1. how to include spring cloud gateway\n2. glossary\n3. how it works\n4. route predicate factories\n5. gatewayfilter factories\n6. global filters\n7. tls / ssl\n8. configuration\n9. reactor netty access logs\n10. cors configuration\n11. actuator api\n12. developer guide\n\n\n该项目提供了一个建立在spring ecosystem之上的api网关，包括：spring 5，spring boot 2和project reactor。spring cloud gateway旨在提供一种简单而有效的方式来对api进行路由，并为他们提供切面，例如：安全性，监控/指标 和弹性等。\n\n\n# 1. 如何在工程中引用spring cloud gateway\n\n要在项目中引入spring cloud gateway，需要引用 group org.springframework.cloud 和 artifact id为spring-cloud-starter-gateway starter。最新的spring cloud release 构建信息，请参阅spring cloud project page。\n\n如果应用了该starter，但由于某种原因不希望启用网关，请进行设置spring.cloud.gateway.enabled=false。\n\n> 重要 spring cloud gateway依赖spring boot和spring webflux提供的netty runtime。它不能在传统的servlet容器中工作或构建为war\n\n\n# 2. 词汇表\n\n * route 路由：gateway的基本构建模块。它由id、目标uri、断言集合和过滤器集合组成。如果聚合断言结果为真，则匹配到该路由。\n * predicate 断言：这是一个java 8 function predicate。输入类型是 spring framework serverwebexchange。这允许开发人员可以匹配来自http请求的任何内容，例如header或参数。\n * filter 过滤器：这些是使用特定工厂构建的 spring frameworkgatewayfilter实例。所以可以在返回请求之前或之后修改请求和响应的内容。\n\n\n# 3. 如何工作的\n\n\n\nspring cloud gateway diagram\n\n客户端向spring cloud gateway发出请求。如果gateway handler mapping确定请求与路由匹配，则将其发送到gateway web handler。此handler通过特定于该请求的过滤器链处理请求。图中filters被虚线划分的原因是filters可以在发送代理请求之前或之后执行逻辑。先执行所有“pre filter”逻辑，然后进行请求代理。在请求代理执行完后，执行“post filter”逻辑。\n\n> 注意 http和https uri默认端口设置是80和443。\n\n\n# 4. 路由断言factories\n\nspring cloud gateway将路由作为spring webflux handlermapping基础结构的一部分进行匹配。spring cloud gateway包含许多内置的路由断言factories。这些断言都匹配http请求的不同属性。多个路由断言factories可以通过 and 组合使用。\n\n\n# 4.1 after 路由断言 factory\n\nafter route predicate factory采用一个参数——日期时间。在该日期时间之后发生的请求都将被匹配。\n\napplication.yml\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: after_route\n        uri: http://example.org\n        predicates:\n        - after=2017-01-20t17:42:47.789-07:00[america/denver]\n\n\n\n# 4.2 before 路由断言 factory\n\nbefore route predicate factory采用一个参数——日期时间。在该日期时间之前发生的请求都将被匹配。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: before_route\n        uri: http://example.org\n        predicates:\n        - before=2017-01-20t17:42:47.789-07:00[america/denver]\n\n\n\n# 4.3 between 路由断言 factory\n\nbetween 路由断言 factory有两个参数，datetime1和datetime2。在datetime1和datetime2之间的请求将被匹配。datetime2参数的实际时间必须在datetime1之后。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: between_route\n        uri: http://example.org\n        predicates:\n        - between=2017-01-20t17:42:47.789-07:00[america/denver], 2017-01-21t17:42:47.789-07:00[america/denver]\n\n\n\n# 4.4 cookie 路由断言 factory\n\ncookie 路由断言 factory有两个参数，cookie名称和正则表达式。请求包含次cookie名称且正则表达式为真的将会被匹配。\n\napplication.yml\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: cookie_route\n        uri: http://example.org\n        predicates:\n        - cookie=chocolate, ch.p\n\n\n\n# 4.5 header 路由断言 factory\n\nheader 路由断言 factory有两个参数，header名称和正则表达式。请求包含次header名称且正则表达式为真的将会被匹配。\n\napplication.yml.\n\nspring:\n cloud:\n   gateway:\n     routes:\n     - id: header_route\n       uri: http://example.org\n       predicates:\n       - header=x-request-id, \\d+\n\n\n\n# 4.6 host 路由断言 factory\n\nhost 路由断言 factory包括一个参数：host name列表。使用ant路径匹配规则，.作为分隔符。 application.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: host_route\n        uri: http://example.org\n        predicates:\n        - host=**.somehost.org,**.anotherhost.org\n\n\n\n# 4.7 method 路由断言 factory\n\nmethod 路由断言 factory只包含一个参数: 需要匹配的http请求方式\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: method_route\n        uri: http://example.org\n        predicates:\n        - method=get\n\n\n所有get请求都将被路由\n\n\n# 4.8 path 路由断言 factory\n\npath 路由断言 factory 有2个参数: 一个spring pathmatcher表达式列表和可选matchoptionaltrailingseparator标识 .\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: host_route\n        uri: http://example.org\n        predicates:\n        - path=/foo/{segment},/bar/{segment}\n\n\n例如: /foo/1 or /foo/bar or /bar/baz的请求都将被匹配\n\nuri 模板变量 (如上例中的 segment ) 将以map的方式保存于serverwebexchange.getattributes() key为serverwebexchangeutils.uri_template_variables_attribute. 这些值将在gatewayfilter factories使用\n\n可以使用以下方法来更方便地访问这些变量。\n\nmap<string, string> urivariables = serverwebexchangeutils.getpathpredicatevariables(exchange);\n\nstring segment = urivariables.get("segment");\n\n\n\n# 4.9 query 路由断言 factory\n\nquery 路由断言 factory 有2个参数: 必选项 param 和可选项 regexp.\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: query_route\n        uri: http://example.org\n        predicates:\n        - query=baz\n\n\n则包含了请求参数 baz的都将被匹配。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: query_route\n        uri: http://example.org\n        predicates:\n        - query=foo, ba.\n\n\n如果请求参数里包含foo参数，并且值匹配为ba. 表达式，则将会被路由，如：bar and baz\n\n\n# 4.10 remoteaddr 路由断言 factory\n\nremoteaddr 路由断言 factory的参数为 一个cidr符号（ipv4或ipv6）字符串的列表，最小值为1，例如192.168.0.1/16（其中192.168.0.1是ip地址并且16是子网掩码）。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: remoteaddr_route\n        uri: http://example.org\n        predicates:\n        - remoteaddr=192.168.1.1/24\n\n\n如果请求的remote address 为 192.168.1.10则将被路由\n\n\n# 4.10.1 修改远程地址的解析方式\n\n默认情况下，remoteaddr 路由断言 factory使用传入请求中的remote address。如果spring cloud gateway位于代理层后面，则可能与实际客户端ip地址不匹配。\n\n可以通过设置自定义remoteaddressresolver来自定义解析远程地址的方式。spring cloud gateway网关附带一个非默认远程地址解析程序，它基于x-forwarded-for header, xforwardedremoteaddressresolver.\n\nxforwardedremoteaddressresolver 有两个静态构造函数方法，采用不同的安全方法：\n\n 1. xforwardedremoteaddressresolver:：trustall返回一个remoteaddressresolver，它始终采用x-forwarded-for头中找到的第一个ip地址。这种方法容易受到欺骗，因为恶意客户端可能会为解析程序接受的“x-forwarded-for”设置初始值。\n 2. xforwardedremoteaddressresolver:：maxtrustedindex获取一个索引，该索引与在spring cloud网关前运行的受信任基础设施数量相关。例如，如果springcloudgateway只能通过haproxy访问，则应使用值1。如果在访问spring cloud gateway之前需要两个受信任的基础架构跃点，那么应该使用2。\n\n给定以下的header值：\n\nx-forwarded-for: 0.0.0.1, 0.0.0.2, 0.0.0.3\n\n\n下面的` maxtrustedindex值将生成以下远程地址:\n\nmaxtrustedindex          result\n[integer.min_value,0]    (invalid, illegalargumentexception during initialization)\n1                        0.0.0.3\n2                        0.0.0.2\n3                        0.0.0.1\n[4, integer.max_value]   0.0.0.1\n\njava 配置方式:\n\ngatewayconfig.java\n\nremoteaddressresolver resolver = xforwardedremoteaddressresolver\n    .maxtrustedindex(1);\n\n...\n\n.route("direct-route",\n    r -> r.remoteaddr("10.1.1.1", "10.10.1.1/24")\n        .uri("https://downstream1")\n.route("proxied-route",\n    r -> r.remoteaddr(resolver,  "10.10.1.1", "10.10.1.1/24")\n        .uri("https://downstream2")\n)\n\n\n\n# 5. gatewayfilter factories\n\n过滤器允许以某种方式修改传入的http请求或返回的http响应。过滤器的作用域是某些特定路由。spring cloud gateway包括许多内置的 filter工厂。\n\n注意：有关如何使用以下任何过滤器的更详细示例，请查看unit tests.。\n\n\n# 5.1 addrequestheader gatewayfilter factory\n\n采用一对名称和值作为参数 application.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: add_request_header_route\n        uri: http://example.org\n        filters:\n        - addrequestheader=x-request-foo, bar\n\n\n对于所有匹配的请求，这将向下游请求的头中添加 x-request-foo:bar header\n\n\n# 5.2 addrequestparameter gatewayfilter factory\n\n采用一对名称和值作为参数 application.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: add_request_parameter_route\n        uri: http://example.org\n        filters:\n        - addrequestparameter=foo, bar\n\n\n对于所有匹配的请求，这将向下游请求添加foo=bar查询字符串\n\n\n# 5.3 addresponseheader gatewayfilter factory\n\n采用一对名称和值作为参数\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: add_request_header_route\n        uri: http://example.org\n        filters:\n        - addresponseheader=x-response-foo, bar\n\n\n对于所有匹配的请求，这会将x-response-foo:bar头添加到下游响应的header中\n\n\n# 5.4 hystrix gatewayfilter factory\n\nhystrix 是netflix开源的断路器组件。hystrix gatewayfilter允许你向网关路由引入断路器，保护你的服务不受级联故障的影响，并允许你在下游故障时提供fallback响应。\n\n要在项目中启用hystrix网关过滤器，需要添加对 spring-cloud-starter-netflix-hystrix的依赖 spring cloud netflix.\n\nhystrix gatewayfilter factory 需要一个name参数，即hystrixcommand的名称。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: hystrix_route\n        uri: http://example.org\n        filters:\n        - hystrix=mycommandname\n\n\n这将剩余的过滤器包装在命令名为“mycommandname”的hystrixcommand中。\n\nhystrix过滤器还可以接受可选的fallbackuri 参数。目前，仅支持forward: 预设的uri，如果调用fallback，则请求将转发到与uri匹配的控制器。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: hystrix_route\n        uri: lb://backing-service:8088\n        predicates:\n        - path=/consumingserviceendpoint\n        filters:\n        - name: hystrix\n          args:\n            name: fallbackcmd\n            fallbackuri: forward:/incaseoffailureusethis\n        - rewritepath=/consumingserviceendpoint, /backingserviceendpoint\n\n\n当调用hystrix fallback时，这将转发到/incaseoffailureusethis uri。注意，这个示例还演示了（可选）通过目标uri上的\'lb`前缀,使用spring cloud netflix ribbon 客户端负载均衡。\n\n主要场景是使用fallbackuri 到网关应用程序中的内部控制器或处理程序。但是，也可以将请求重新路由到外部应用程序中的控制器或处理程序，如：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: ingredients\n        uri: lb://ingredients\n        predicates:\n        - path=//ingredients/**\n        filters:\n        - name: hystrix\n          args:\n            name: fetchingredients\n            fallbackuri: forward:/fallback\n      - id: ingredients-fallback\n        uri: http://localhost:9994\n        predicates:\n        - path=/fallback\n\n\n在本例中，gateway应用程序中没有 fallback 实现，但是另一个应用程序中有一个接口实现，注册为“http://localhost:9994”。\n\n在将请求转发到fallback的情况下，hystrix gateway过滤还支持直接抛出throwable 。它被作为serverwebexchangeutils.hystrix_execution_exception_attr属性添加到serverwebexchange中，可以在处理网关应用程序中的fallback时使用。\n\n对于外部控制器/处理程序方案，可以添加带有异常详细信息的header。可以在 fallbackheaders gatewayfilter factory section.中找到有关它的更多信息。\n\nhystrix配置参数（如 timeouts）可以使用全局默认值配置，也可以使用hystrix wiki中所述属性进行配置。\n\n要为上面的示例路由设置5秒超时，将使用以下配置：\n\napplication.yml.\n\nhystrix.command.fallbackcmd.execution.isolation.thread.timeoutinmilliseconds: 5000\n\n\n\n# 5.5 fallbackheaders gatewayfilter factory\n\nfallbackheaders允许在转发到外部应用程序中的fallbackuri的请求的header中添加hystrix异常详细信息，如下所示：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: ingredients\n        uri: lb://ingredients\n        predicates:\n        - path=//ingredients/**\n        filters:\n        - name: hystrix\n          args:\n            name: fetchingredients\n            fallbackuri: forward:/fallback\n      - id: ingredients-fallback\n        uri: http://localhost:9994\n        predicates:\n        - path=/fallback\n        filters:\n        - name: fallbackheaders\n          args:\n            executionexceptiontypeheadername: test-header\n\n\n在本例中，在运行hystrixcommand发生执行异常后，请求将被转发到 localhost:9994应用程序中的 fallback终端或程序。异常类型、消息（如果可用）cause exception类型和消息的头，将由fallbackheaders filter添加到该请求中。\n\n通过设置下面列出的参数值及其默认值，可以在配置中覆盖headers的名称：\n\n * executionexceptiontypeheadername ("execution-exception-type")\n * executionexceptionmessageheadername ("execution-exception-message")\n * rootcauseexceptiontypeheadername ("root-cause-exception-type")\n * rootcauseexceptionmessageheadername ("root-cause-exception-message")\n\nhystrix 如何实现的更多细节可以参考 hystrix gatewayfilter factory section.\n\n\n# 5.6 prefixpath gatewayfilter factory\n\nprefixpath gatewayfilter 只有一个 prefix 参数.\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: prefixpath_route\n        uri: http://example.org\n        filters:\n        - prefixpath=/mypath\n\n\n这将给所有匹配请求的路径加前缀/mypath。因此，向/hello发送的请求将发送到/mypath/hello。\n\n\n# 5.7 preservehostheader gatewayfilter factory\n\n该filter没有参数。设置了该filter后，gatewayfilter将不使用由http客户端确定的host header ，而是发送原始host header 。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: preserve_host_route\n        uri: http://example.org\n        filters:\n        - preservehostheader\n\n\n\n# 5.8 requestratelimiter gatewayfilter factory\n\nrequestratelimiter使用ratelimiter实现是否允许继续执行当前请求。如果不允许继续执行，则返回http 429 - too many requests （默认情况下）。\n\n这个过滤器可以配置一个可选的keyresolver 参数和rate limiter参数（见下文）。\n\nkeyresolver 是 keyresolver 接口的实现类.在配置中，按名称使用spel引用bean。#{@mykeyresolver} 是引用名为\'mykeyresolver\'的bean的spel表达式。\n\nkeyresolver.java.\n\npublic interface keyresolver {\n    mono<string> resolve(serverwebexchange exchange);\n}\n\n\nkeyresolver接口允许使用可插拔策略来派生限制请求的key。在未来的里程碑版本中，将有一些keyresolver实现。\n\nkeyresolver的默认实现是principalnamekeyresolver，它从serverwebexchange检索principal并调用principal.getname()。\n\n默认情况下，如果keyresolver 没有获取到key，请求将被拒绝。此行为可以使用 spring.cloud.gateway.filter.request-rate-limiter.deny-empty-key (true or false) 和 spring.cloud.gateway.filter.request-rate-limiter.empty-key-status-code属性进行调整。\n\n> 说明 无法通过"shortcut" 配置requestratelimiter。以下示例无效\n\napplication.properties.\n\n# invalid shortcut configuration\nspring.cloud.gateway.routes[0].filters[0]=requestratelimiter=2, 2, #{@userkeyresolver}\n\n\n\n# 5.8.1 redis ratelimiter\n\nredis的实现基于 stripe实现。它需要使用 spring-boot-starter-data-redis-reactive spring boot starter。\n\n使用的算法是token bucket algorithm.。\n\nredis-rate-limiter.replenishrate是你允许用户每秒执行多少请求，而丢弃任何请求。这是令牌桶的填充速率。\n\n``redis-rate-limiter.burstcapacity`是允许用户在一秒钟内执行的最大请求数。这是令牌桶可以保存的令牌数。将此值设置为零将阻止所有请求。\n\n稳定速率是通过在replenishrate 和 burstcapacity中设置相同的值来实现的。可通过设置burstcapacity高于replenishrate来允许临时突发流浪。在这种情况下，限流器需要在两次突发之间留出一段时间（根据replenishrate），因为连续两次突发将导致请求丢失 (http 429 - too many requests).。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: requestratelimiter_route\n        uri: http://example.org\n        filters:\n        - name: requestratelimiter\n          args:\n            redis-rate-limiter.replenishrate: 10\n            redis-rate-limiter.burstcapacity: 20\n\n\nconfig.java.\n\n@bean\nkeyresolver userkeyresolver() {\n    return exchange -> mono.just(exchange.getrequest().getqueryparams().getfirst("user"));\n}\n\n\n这定义了每个用户10个请求的限制。允许20个突发，但下一秒只有10个请求可用。keyresolver是一个简单的获取user请求参数的工具（注意：不建议用于生产）。\n\n限流器也可以定义为ratelimiter接口的实现 bean。在配置中，按名称使用spel引用bean。#{@myratelimiter}是引用名为\'myratelimiter\'的bean的spel表达式。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: requestratelimiter_route\n        uri: http://example.org\n        filters:\n        - name: requestratelimiter\n          args:\n            rate-limiter: "#{@myratelimiter}"\n            key-resolver: "#{@userkeyresolver}"\n\n\n\n# 5.9 redirectto gatewayfilter factory\n\n该过滤器有一个 status 和一个 url参数。status是300类重定向http代码，如301。该url应为有效的url，这将是 location header的值。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: prefixpath_route\n        uri: http://example.org\n        filters:\n        - redirectto=302, http://acme.org\n\n\n这将发送一个302状态码和一个location:http://acme.org header来执行重定向。\n\n\n# 5.10 removenonproxyheaders gatewayfilter factory\n\nremovenonproxyheaders gatewayfilter factory 从转发请求中删除headers。删除的默认头列表来自 ietf.\n\nthe default removed headers are:\n\n * connection\n * keep-alive\n * proxy-authenticate\n * proxy-authorization\n * te\n * trailer\n * transfer-encoding\n * upgrade 要更改此设置，请将 spring.cloud.gateway.filter.remove-non-proxy-headers.headers属性设置为要删除的header名称。\n\n\n# 5.11 removerequestheader gatewayfilter factory\n\n有一个name参数. 这是要删除的header的名称。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: removerequestheader_route\n        uri: http://example.org\n        filters:\n        - removerequestheader=x-request-foo\n\n\n这将在x-request-foo header被发送到下游之前删除它。\n\n\n# 5.12 removeresponseheader gatewayfilter factory\n\n有一个name参数. 这是要删除的header的名称。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: removeresponseheader_route\n        uri: http://example.org\n        filters:\n        - removeresponseheader=x-response-foo\n\n\n这将在返回到网关client之前从响应中删除x-response-foo头。\n\n\n# 5.13 rewritepath gatewayfilter factory\n\n包含一个 regexp正则表达式参数和一个 replacement 参数. 通过使用java正则表达式灵活地重写请求路径。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: rewritepath_route\n        uri: http://example.org\n        predicates:\n        - path=/foo/**\n        filters:\n        - rewritepath=/foo/(?<segment>.*), /$\\{segment}\n\n\n对于请求路径/foo/bar，将在发出下游请求之前将路径设置为/bar。注意,由于yaml规范，请使用 $\\替换 $。\n\n\n# 5.14 rewriteresponseheader gatewayfilter factory\n\n包含 name, regexp和 replacement 参数.。通过使用java正则表达式灵活地重写响应头的值。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: rewriteresponseheader_route\n        uri: http://example.org\n        filters:\n        - rewriteresponseheader=x-response-foo, , password=[^&]+, password=***\n\n\n对于一个/42?user=ford&password=omg!what&flag=true的header值，在做下游请求时将被设置为/42?user=ford&password=***&flag=true，由于yaml规范，请使用 $\\替换 $。\n\n\n# 5.15 savesession gatewayfilter factory\n\nsavesession gatewayfilter factory将调用转发到下游之前强制执行websession::save 操作。这在使用 spring session 之类时特别有用，需要确保会话状态在进行转发调用之前已保存。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: save_session\n        uri: http://example.org\n        predicates:\n        - path=/foo/**\n        filters:\n        - savesession\n\n\n如果你希望要将[spring security]（https://projects.spring.io/spring security/）与spring session集成,并确保安全详细信息已转发到远程的进程，这一点至关重要。\n\n\n# 5.16 secureheaders gatewayfilter factory\n\nsecureheaders gatewayfilter factory 将许多headers添加到reccomedation处的响应中，从this blog post.\n\n添加以下标题（使用默认值分配）:\n\n * x-xss-protection:1; mode=block\n * strict-transport-security:max-age=631138519\n * x-frame-options:deny\n * x-content-type-options:nosniff\n * referrer-policy:no-referrer\n * content-security-policy:default-src \'self\' https:; font-src \'self\' https: data:; img-src \'self\' https: data:; object-src \'none\'; script-src https:; style-src \'self\' https: \'unsafe-inline\'\n * x-download-options:noopen\n * x-permitted-cross-domain-policies:none\n\n要更改默认值，请在spring.cloud.gateway.filter.secure-headers 命名空间中设置相应的属性：\n\nproperty to change:\n\n * xss-protection-header\n * strict-transport-security\n * frame-options\n * content-type-options\n * referrer-policy\n * content-security-policy\n * download-options\n * permitted-cross-domain-policies\n\n\n# 5.17 setpath gatewayfilter factory\n\nsetpath gatewayfilter factory 采用 template路径参数。它提供了一种通过允许路径的模板化segments来操作请求路径的简单方法。使用spring framework中的uri模板，允许多个匹配segments。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: setpath_route\n        uri: http://example.org\n        predicates:\n        - path=/foo/{segment}\n        filters:\n        - setpath=/{segment}\n\n\n对于一个 /foo/bar请求，在做下游请求前，路径将被设置为/bar\n\n\n# 5.18 setresponseheader gatewayfilter factory\n\nsetresponseheader gatewayfilter factory 包括 name 和 value 参数.\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: setresponseheader_route\n        uri: http://example.org\n        filters:\n        - setresponseheader=x-response-foo, bar\n\n\n此gatewayfilter使用给定的名称替换所有header，而不是添加。因此，如果下游服务器响应为x-response-foo:1234，则会将其替换为x-response-foo:bar,这是网关客户端将接收的内容。\n\n\n# 5.19 setstatus gatewayfilter factory\n\nsetstatus gatewayfilter factory 包括唯一的 status参数.必须是一个可用的spring httpstatus。它可以是整数值404或字符串枚举not_found。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: setstatusstring_route\n        uri: http://example.org\n        filters:\n        - setstatus=bad_request\n      - id: setstatusint_route\n        uri: http://example.org\n        filters:\n        - setstatus=401\n\n\n在这个例子中，http返回码将设置为401.\n\n\n# 5.20 stripprefix gatewayfilter factory\n\nstripprefix gatewayfilter factory 包括一个parts参数。 parts参数指示在将请求发送到下游之前，要从请求中去除的路径中的节数。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: nameroot\n        uri: http://nameservice\n        predicates:\n        - path=/name/**\n        filters:\n        - stripprefix=2\n\n\n当通过网关发出/name/bar/foo请求时，向nameservice发出的请求将是http://nameservice/foo。\n\n\n# 5.21 retry gatewayfilter factory\n\nretry gatewayfilter factory包括 retries, statuses, methods和 series 参数.\n\n * retries: 应尝试的重试次数\n * statuses: 应该重试的http状态代码，用org.springframework.http.httpstatus标识\n * methods: 应该重试的http方法，用 org.springframework.http.httpmethod标识\n * series: 要重试的一系列状态码，用 org.springframework.http.httpstatus.series标识\n\napplication.yml.\n\nspring: cloud: gateway: routes: - id: retry_test uri: http://localhost:8080/flakey predicates: - host=*.retry.com filters: - name: retry args: retries: 3 statuses: bad_gateway\n\n> 注意 retry filter 不支持body请求的重试，如通过body的post 或 put请求\n> \n> 注意 在使用带有前缀为 forward: 的retry filter时，应仔细编写目标端点，以便在出现错误时不会执行任何可能导致将响应发送到客户端并提交的操作。例如，如果目标端点是带注解的controller，则目标controller方法不应返回带有错误状态代码的responseentity。相反，它应该抛出一个exception，或者发出一个错误信号，例如通过mono.error(ex) 返回值，重试过滤器可以配置为通过重试来处理。\n\n\n# 5.22 requestsize gatewayfilter factory\n\n当请求大小大于允许的限制时，requestsize gatewayfilter factory可以限制请求不到达下游服务。过滤器以requestsize作为参数，这是定义请求的允许大小限制(以字节为单位)。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: request_size_route\n      uri: http://localhost:8080/upload\n      predicates:\n      - path=/upload\n      filters:\n      - name: requestsize\n        args:\n          maxsize: 5000000\n\n\n当请求因大小而被拒绝时， requestsize gatewayfilter factory 将响应状态设置为413 payload too large，并带有额外的header errormessage 。下面是一个 errormessage的例子。\n\nerrormessage` : `request size is larger than permissible limit. request size is 6.0 mb where permissible limit is 5.0 mb\n\n\n> 注意 如果未在路由定义中作为filter参数提供，则默认请求大小将设置为5 mb。\n\n\n# 5.23 modify request body gatewayfilter factory\n\n这个过滤器被定义为beta版本，将来api可能会改变。\n\n此过滤器可用于在请求主体被网关发送到下游之前对其进行修改。\n\n> 注意 只能使用java dsl配置此过滤器\n\n@bean\npublic routelocator routes(routelocatorbuilder builder) {\n    return builder.routes()\n        .route("rewrite_request_obj", r -> r.host("*.rewriterequestobj.org")\n            .filters(f -> f.prefixpath("/httpbin")\n                .modifyrequestbody(string.class, hello.class, mediatype.application_json_value,\n                    (exchange, s) -> return mono.just(new hello(s.touppercase())))).uri(uri))\n        .build();\n}\n\nstatic class hello {\n    string message;\n\n    public hello() { }\n\n    public hello(string message) {\n        this.message = message;\n    }\n\n    public string getmessage() {\n        return message;\n    }\n\n    public void setmessage(string message) {\n        this.message = message;\n    }\n}\n\n\n\n# 5.24 modify response body gatewayfilter factory\n\n这个过滤器被定义为beta版本，将来api可能会改变。 此过滤器可用于在将响应正文发送回客户端之前对其进行修改。\n\n> 注意 只能使用java dsl配置此过滤器\n\n@bean\npublic routelocator routes(routelocatorbuilder builder) {\n    return builder.routes()\n        .route("rewrite_response_upper", r -> r.host("*.rewriteresponseupper.org")\n            .filters(f -> f.prefixpath("/httpbin")\n                .modifyresponsebody(string.class, string.class,\n                    (exchange, s) -> mono.just(s.touppercase()))).uri(uri)\n        .build();\n}\n\n\n\n# 6. global filters\n\nglobalfilter接口与gatewayfilter具有相同的签名。是有条件地应用于所有路由的特殊过滤器。（此接口和用法可能在将来的里程碑版本中发生更改）。\n\n\n# 6.1 全局filter和gatewayfilter组合排序\n\n当请求进入（并与路由匹配）时，筛选web handler 会将globalfilter的所有实例和所有的gatewayfilter路由特定实例添加到 filter chain。filter组合的排序由org.springframework.core.ordered接口决定，可以通过实现getorde()方法或使用@order注释来设置。\n\n由于spring cloud gateway将用于执行过滤器逻辑区分为“前置”和“后置”阶段，具有最高优先级的过滤器将是“前置”阶段的第一个，而“后置”阶段的最后一个。\n\nexampleconfiguration.java.\n\n@bean\n@order(-1)\npublic globalfilter a() {\n    return (exchange, chain) -> {\n        log.info("first pre filter");\n        return chain.filter(exchange).then(mono.fromrunnable(() -> {\n            log.info("third post filter");\n        }));\n    };\n}\n\n@bean\n@order(0)\npublic globalfilter b() {\n    return (exchange, chain) -> {\n        log.info("second pre filter");\n        return chain.filter(exchange).then(mono.fromrunnable(() -> {\n            log.info("second post filter");\n        }));\n    };\n}\n\n@bean\n@order(1)\npublic globalfilter c() {\n    return (exchange, chain) -> {\n        log.info("third pre filter");\n        return chain.filter(exchange).then(mono.fromrunnable(() -> {\n            log.info("first post filter");\n        }));\n    };\n}\n\n\n\n# 6.2 forward routing filter\n\nforwardroutingfilter在exchange属性serverwebexchangeutils.gateway_request_url_attr中查找uri。如果url有一个forwardscheme (如 forward:///localendpoint)，它将使用spring dispatcherhandler 来处理请求。请求url的路径部分将被转发url中的路径覆盖。未修改的原始url将附加到 serverwebexchangeutils.gateway_original_request_url_attr属性中的列表中。\n\n\n# 6.3 loadbalancerclient filter\n\nloadbalancerclientfilter在exchange属性serverwebexchangeutils.gateway_request_url_attr中查找uri。如果url有一个lbscheme (如 lb://myservice），它将使用spring cloud loadbalancerclient 将名称（在前一个示例中为\'myservice）解析为实际主机和端口，并替换uri。未修改的原始url将附加到serverwebexchangeutils.gateway_original_request_url_attr属性中的列表中。过滤器还将查看serverwebexchangeutils.gateway_scheme_prefix_attr属性，查看它是否等于lb`，然后应用相同的规则。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: myroute\n        uri: lb://service\n        predicates:\n        - path=/service/**\n\n\n> 注意 默认情况下，如果一个服务实例在loadbalancer 中没有发现，则返回503。可以通过设置spring.cloud.gateway.loadbalancer.use404=true来让网管返回404.\n> \n> 注意\n\n从loadbalancer返回的serviceinstance的issecure 值将覆盖在对网关发出的请求中指定的scheme。例如，如果请求通过https进入网关，但serviceinstance表示它不安全，则下游请求将通过http协议。相反的情况也适用。但是，如果在网关配置中为路由指定了gateway_scheme_prefix_attr，则前缀将被删除，并且路由url生成的scheme将覆盖serviceinstance配置。\n\n\n# 6.4 netty routing filter\n\n如果位于 serverwebexchangeutils.gateway_request_url_attr属性中的url具有http 或https 模式，则会运行netty routing filter。它使用netty httpclient 发出下游代理请求。响应放在 serverwebexchangeutils.client_response_attr exchange属性中，以便在以后的过滤器中使用。（有一个实验阶段不需要netty的相同的功能的filter，webclienthttproutingfilter）\n\n\n# 6.5 netty write response filter\n\n如果serverwebexchangeutils.client_response_attr exchange属性中存在 netty httpclientresponse，则运行 nettywriteresponsefilter 。它在其他所有过滤器完成后将代理响应写回网关客户端响应之后运行。（有一个不需要netty的实验性的webclientwriteresponsefilter执行相同的功能）\n\n\n# 6.6 routetorequesturl filter\n\n如果serverwebexchangeutils.gateway_route_attr exchange属性中存在route对象，routetorequesturlfilter将运行。它基于请求uri创建一个新的uri，使用route对象的uri属性进行更新。新的uri被放置在serverwebexchangeutils.gateway_request_url_attr exchange属性中。\n\n如果该uri有一个前缀scheme，例如lb:ws://serviceid，则会从该uri中剥离该 lb scheme，并将其放置在serverwebexchangeutils.gateway_scheme_prefix_attr中，以便稍后在过滤器链中使用。\n\n\n# 6.7 websocket routing filter\n\n如果serverwebexchangeutils.gateway_request_url_attrexchange属性中有 ws 、 wssscheme，则websocket routing filter将被运行。它使用spring web socket基础模块将websocket转发到下游。\n\nuri前缀为lb的websockets可以被负载均衡，如 lb:ws://serviceid.\n\n> 注意 如果使用 sockjs 作为普通http的fallback，则应配置普通http路由以及websocket路由。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      # sockjs route\n      - id: websocket_sockjs_route\n        uri: http://localhost:3001\n        predicates:\n        - path=/websocket/info/**\n      # normwal websocket route\n      - id: websocket_route\n        uri: ws://localhost:3001\n        predicates:\n        - path=/websocket/**\n\n\n\n# 6.8 gateway metrics filter\n\n要启用网关指标，请将spring-boot-starter-actuator添加为项目依赖项。然后，默认情况下，只要属性spring.cloud.gateway.metrics.enabled未设置为false，网关指标过滤器就会运行。此过滤器添加名为“gateway.requests”的计时器指标，并带有以下标记：\n\n * routeid: the route id\n * routeuri: api 将被转发的uri\n * outcome: 结果分类依据 httpstatus.series\n * status: 返回client的请求的http status\n\n这些指标可以从/actuator/metrics/gateway.requests中获取，可以很容易地与prometheus集成以创建grafana dashboard.\n\n> 注意 要将pometheus启用，需要添加 micrometer-registry-prometheus为项目依赖。\n\n\n# 6.9 making an exchange as routed\n\n网关路由serverwebexchange之后，它将通过向exchange属性添加gatewayalreadyrouted，将该exchange标记为“routed”。一旦一个请求被标记为routed，其他路由过滤器将不会再次路由该请求，将跳过该过滤器。有一些方便的方法可以用来将exchange标记为routed，或者检查exchange是否已经routed。\n\n * serverwebexchangeutils.isalreadyrouted 有一个 serverwebexchange对象并检查它是否已"routed"\n * serverwebexchangeutils.setalreadyrouted 有一个 serverwebexchange 对象并将其标记为"routed"\n\n\n# 7. tls / ssl\n\n网关可以通过常规的 spring server configuration 来侦听https上的请求。例子：\n\napplication.yml.\n\nserver:\n  ssl:\n    enabled: true\n    key-alias: scg\n    key-store-password: scg1234\n    key-store: classpath:scg-keystore.p12\n    key-store-type: pkcs12\n\n\n网关路由可以路由到http和https后端。如果路由到https后端，则可以将网关配置为信任所有具有证书的下游服务：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      httpclient:\n        ssl:\n          useinsecuretrustmanager: true\n\n\n不建议在生产环境使用不安全的信任管理器。对于生产部署，可以使用一组已知证书配置网关，这些证书可以通过以下方式进行配置：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      httpclient:\n        ssl:\n          trustedx509certificates:\n          - cert1.pem\n          - cert2.pem\n\n\n如果spring cloud gateway未配置受信任证书，则使用默认信任库（可以使用系统属性javax.net.ssl.truststore覆盖）。\n\n\n# 7.1 tls 握手\n\n网关维护一个用于路由到后端的client池。当通过https通信时，客户端启动一个tls握手，其中可能会有很多超时。这些超时可以这样配置（显示默认值）：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      httpclient:\n        ssl:\n          handshake-timeout-millis: 10000\n          close-notify-flush-timeout-millis: 3000\n          close-notify-read-timeout-millis: 0\n\n\n\n# 8. configuration\n\nspring cloud gateway的配置由routedefinitionlocator的集合驱动。\n\nroutedefinitionlocator.java.\n\npublic interface routedefinitionlocator {\n    flux<routedefinition> getroutedefinitions();\n}\n\n\n默认情况下，propertiesroutedefinitionlocator使用spring boot的@configurationproperties机制加载属性。\n\n以下两个示例是等效的：\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: setstatus_route\n        uri: http://example.org\n        filters:\n        - name: setstatus\n          args:\n            status: 401\n      - id: setstatusshortcut_route\n        uri: http://example.org\n        filters:\n        - setstatus=401\n\n\n对于网关的大部分用法，配置文件方式是够用的，但一些生产用例更建议从外部源（如数据库）加载配置。未来的里程碑版本将有基于spring data repositories （如redis、mongodb和cassandra）的routedefinitionlocator实现。\n\n\n# 8.1 fluent java routes api\n\n为了可以更简单在java中配置，在routelocatorbuilder bean中定义了一个fluent api。\n\ngatewaysampleapplication.java.\n\n// static imports from gatewayfilters and routepredicates\n@bean\npublic routelocator customroutelocator(routelocatorbuilder builder, throttlegatewayfilterfactory throttle) {\n    return builder.routes()\n            .route(r -> r.host("**.abc.org").and().path("/image/png")\n                .filters(f ->\n                        f.addresponseheader("x-testheader", "foobar"))\n                .uri("http://httpbin.org:80")\n            )\n            .route(r -> r.path("/image/webp")\n                .filters(f ->\n                        f.addresponseheader("x-anotherheader", "baz"))\n                .uri("http://httpbin.org:80")\n            )\n            .route(r -> r.order(-1)\n                .host("**.throttle.org").and().path("/get")\n                .filters(f -> f.filter(throttle.apply(1,\n                        1,\n                        10,\n                        timeunit.seconds)))\n                .uri("http://httpbin.org:80")\n            )\n            .build();\n}\n\n\n这种样式还允许使用更多的自定义断言。由routedefinitionlocator beans定义的断言使用逻辑 and组合。通过使用fluent java api，可以在 predicate类上使用 and()、or() 、 negate()运算符。\n\n\n# 8.2 discoveryclient route definition locator\n\n可以将网关配置为基于使用兼容discoveryclient注册中心注册的服务来创建路由。\n\n要启用此功能，请设置spring.cloud.gateway.discovery.locator.enabled=true，并确保discoveryclient实现位于classpath上并已启用（如netflix eureka、consul或zookeeper）。\n\n\n# 8.2.1 configuring predicates and filters for discoveryclient routes\n\n默认情况下，网关为通过discoveryclient创建的路由定义单个断言和过滤器。\n\n默认断言是使用/serviceid/**定义的path断言，其中serviceid是discoveryclient中服务的id。\n\n默认过滤器是使用正则表达式 /serviceid/(?<remaining>.*)和替换的/${remaining}进行重写。这只是在请求被发送到下游之前从路径中截取掉 service id 。\n\n可以通过设置spring.cloud.gateway.discovery.locator.predicates[x] and spring.cloud.gateway.discovery.locator.filters[y]来实现自定义discoveryclient路由使用的断言and/or过滤器。当你这样做时，如果你想要保留这个功能，你需要确保包括上面的默认断言和过滤器。下面是这样一个例子。\n\napplication.properties.\n\nspring.cloud.gateway.discovery.locator.predicates[0].name: path\nspring.cloud.gateway.discovery.locator.predicates[0].args[pattern]: "\'/\'+serviceid+\'/**\'"\nspring.cloud.gateway.discovery.locator.predicates[1].name: host\nspring.cloud.gateway.discovery.locator.predicates[1].args[pattern]: "\'**.foo.com\'"\nspring.cloud.gateway.discovery.locator.filters[0].name: hystrix\nspring.cloud.gateway.discovery.locator.filters[0].args[name]: serviceid\nspring.cloud.gateway.discovery.locator.filters[1].name: rewritepath\nspring.cloud.gateway.discovery.locator.filters[1].args[regexp]: "\'/\' + serviceid + \'/(?<remaining>.*)\'"\nspring.cloud.gateway.discovery.locator.filters[1].args[replacement]: "\'/${remaining}\'"\n\n\n\n# 9. reactor netty access logs\n\n设置 -dreactor.netty.http.server.accesslogenabled=true 来开启reactor netty access logs，注意必须是java system property而不是spring boot property。\n\nlogging 模块也可以通过配置单独输出一个access log文件，下面是logback的配置例子：\n\nlogback.xml.\n\n <appender name="accesslog" class="ch.qos.logback.core.fileappender">\n        <file>access_log.log</file>\n        <encoder>\n            <pattern>%msg%n</pattern>\n        </encoder>\n    </appender>\n    <appender name="async" class="ch.qos.logback.classic.asyncappender">\n        <appender-ref ref="accesslog" />\n    </appender>\n\n    <logger name="reactor.netty.http.server.accesslog" level="info" additivity="false">\n        <appender-ref ref="async"/>\n    </logger>\n\n\n\n# 10. cors configuration\n\n我们可以通过配置网关来控制cors行为，全局cors配置是 spring framework corsconfiguration模式的url map。\n\napplication.yml.\n\nspring:\n  cloud:\n    gateway:\n      globalcors:\n        corsconfigurations:\n          \'[/**]\':\n            allowedorigins: "http://docs.spring.io"\n            allowedmethods:\n            - get\n\n\n例子中将允许从docs.spring.io发出的所有get请求进行cors请求。\n\n\n# 11. actuator api\n\n/gateway的actuator端点允许监视spring cloud gateway应用程序并与之交互。要进行远程访问，必须在应用程序属性中暴露http或jmx 端口。\n\napplication.properties.\n\nmanagement.endpoint.gateway.enabled=true # default value\nmanagement.endpoints.web.exposure.include=gateway\n\n\n\n# 11.1 retrieving route filters\n\n\n# 11.1.1 global filters\n\n要检索应用于所有路由的 [global filters]，请get请求 /actuator/gateway/globalfilters。返回的结果类似于以下内容：\n\n{\n  "org.springframework.cloud.gateway.filter.loadbalancerclientfilter@77856cc5": 10100,\n  "org.springframework.cloud.gateway.filter.routetorequesturlfilter@4f6fd101": 10000,\n  "org.springframework.cloud.gateway.filter.nettywriteresponsefilter@32d22650": -1,\n  "org.springframework.cloud.gateway.filter.forwardroutingfilter@106459d9": 2147483647,\n  "org.springframework.cloud.gateway.filter.nettyroutingfilter@1fbd5e0": 2147483647,\n  "org.springframework.cloud.gateway.filter.forwardpathfilter@33a71d23": 0,\n  "org.springframework.cloud.gateway.filter.adaptcachedbodyglobalfilter@135064ea": 2147483637,\n  "org.springframework.cloud.gateway.filter.websocketroutingfilter@23c05889": 2147483646\n}\n\n\n返回结果包含已就绪的global filters的详细信息(如 org.springframework.cloud.gateway.filter.loadbalancerclientfilter@77856cc5)。对于每个global filters，返回结果字符串对应过滤器链中的相应顺序。\n\n\n# 11.1.2 route filters\n\n要检索应用于路由的 [gatewayfilter factories] ，请get请求/actuator/gateway/routefilters。返回结果类似于以下内容：\n\n{\n  "[addrequestheadergatewayfilterfactory@570ed9c configclass = abstractnamevaluegatewayfilterfactory.namevalueconfig]": null,\n  "[secureheadersgatewayfilterfactory@fceab5d configclass = object]": null,\n  "[savesessiongatewayfilterfactory@4449b273 configclass = object]": null\n}\n\n\n返回结果包含应用于所有路由的gatewayfilter的详细信息。显示每个工厂提供字符串格式的相应对象（例如， [secureheadersgatewayfilterfactory@fceab5d configclass = object]）。请注意，null值是由于endpoint controller实现不完整造成的，因为它尝试在filter chain中设置对象的顺序，这不适用于gatewayfilter工厂对象。\n\n\n# 11.2 refreshing the route cache\n\n如果要清理路由的缓存，请post请求/actuator/gateway/refresh。该请求将返回一个没有body的200返回码。\n\n\n# 11.3 retrieving the routes defined in the gateway\n\n要检索网关中定义的路由，发送get请求/actuator/gateway/routes，返回结果如下所示：\n\n[{\n  "route_id": "first_route",\n  "route_object": {\n    "predicate": "org.springframework.cloud.gateway.handler.predicate.pathroutepredicatefactory$$lambda$432/1736826640@1e9d7e7d",\n    "filters": [\n      "orderedgatewayfilter{delegate=org.springframework.cloud.gateway.filter.factory.preservehostheadergatewayfilterfactory$$lambda$436/674480275@6631ef72, order=0}"\n    ]\n  },\n  "order": 0\n},\n{\n  "route_id": "second_route",\n  "route_object": {\n    "predicate": "org.springframework.cloud.gateway.handler.predicate.pathroutepredicatefactory$$lambda$432/1736826640@cd8d298",\n    "filters": []\n  },\n  "order": 0\n}]\n\n\n返回结果中包含网关中所有定义的路由信息，下面表格中描述了返回结果信息：\n\npath                     type     description\nroute_id                 string   the route id.\nroute_object.predicate   object   the route predicate.\nroute_object.filters     array    the gatewayfilter factories applied to the route.\norder                    number   the route order.\n\n\n# 11.4 retrieving information about a particular route\n\n要获取单个路由的信息，发送get请求 /actuator/gateway/routes/{id} (如： /actuator/gateway/routes/first_route)，返回结果如下所示：\n\n{\n  "id": "first_route",\n  "predicates": [{\n    "name": "path",\n    "args": {"_genkey_0":"/first"}\n  }],\n  "filters": [],\n  "uri": "http://www.uri-destination.org",\n  "order": 0\n}]\n\n\n下面表格中描述了返回结果信息：\n\npath         type     description\nid           string   the route id.\npredicates   array    the collection of route predicates. each item defines the\n                      name and the arguments of a given predicate.\nfilters      array    the collection of filters applied to the route.\nuri          string   the destination uri of the route.\norder        number   the route order.\n\n\n# 11.5 creating and deleting a particular route\n\n要创建一个路由，发送post请求 /gateway/routes/{id_route_to_create}，参数为json结构，具体参数数据结构参考上面章节。\n\n要删除一个路由，发送 delete请求 /gateway/routes/{id_route_to_delete}。\n\n\n# 11.6 recap: list of all endpoints\n\n下表总结了spring cloud gateway actuator endpoints。注意，每个endpoint都是/actuator/gateway作为基本路径。\n\nid              http method   description\nglobalfilters   get           displays the list of global filters applied to the routes.\nroutefilters    get           displays the list of gatewayfilter factories applied to a\n                              particular route.\nrefresh         post          clears the routes cache.\nroutes          get           displays the list of routes defined in the gateway.\nroutes/{id}     get           displays information about a particular route.\nroutes/{id}     post          adds a new route to the gateway.\nroutes/{id}     delete        removes an existing route from the gateway.\n\n\n# 12. developer guide\n\ntodo: overview of writing custom integrations\n\n\n# 12.1 writing custom route predicate factories\n\ntodo: document writing custom route predicate factories\n\n\n# 12.2 writing custom gatewayfilter factories\n\n如果要自定义一个gatewayfilter，需要实现gatewayfilterfactory。下面是一个你需要集成的抽象类 abstractgatewayfilterfactory。\n\npregatewayfilterfactory.java.\n\npublic class pregatewayfilterfactory extends abstractgatewayfilterfactory<pregatewayfilterfactory.config> {\n\n    public pregatewayfilterfactory() {\n        super(config.class);\n    }\n\n    @override\n    public gatewayfilter apply(config config) {\n        // grab configuration from config object\n        return (exchange, chain) -> {\n            //if you want to build a "pre" filter you need to manipulate the\n            //request before calling chain.filter\n            serverhttprequest.builder builder = exchange.getrequest().mutate();\n            //use builder to manipulate the request\n            return chain.filter(exchange.mutate().request(request).build());\n        };\n    }\n\n    public static class config {\n        //put the configuration properties for your filter here\n    }\n\n}\n\n\npostgatewayfilterfactory.java.\n\npublic class postgatewayfilterfactory extends abstractgatewayfilterfactory<postgatewayfilterfactory.config> {\n\n    public postgatewayfilterfactory() {\n        super(config.class);\n    }\n\n    @override\n    public gatewayfilter apply(config config) {\n        // grab configuration from config object\n        return (exchange, chain) -> {\n            return chain.filter(exchange).then(mono.fromrunnable(() -> {\n                serverhttpresponse response = exchange.getresponse();\n                //manipulate the response in some way\n            }));\n        };\n    }\n\n    public static class config {\n        //put the configuration properties for your filter here\n    }\n\n}\n\n\n\n# 12.3 writing custom global filters\n\ntodo: document writing custom global filters\n\n\n# 12.4 writing custom route locators and writers\n\ntodo: document writing custom route locators and writers',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"简介",frontmatter:{title:"简介",date:"2022-02-09T11:45:40.000Z",permalink:"/pages/b62716/"},regularPath:"/16.Spring%20Cloud%20Alibaba/01.Spring%20Cloud%20Alibaba/01.%E7%AE%80%E4%BB%8B.html",relativePath:"16.Spring Cloud Alibaba/01.Spring Cloud Alibaba/01.简介.md",key:"v-c2d87842",path:"/pages/b62716/",headers:[{level:2,title:"主要功能",slug:"主要功能",normalizedTitle:"主要功能",charIndex:210},{level:2,title:"组件",slug:"组件",normalizedTitle:"组件",charIndex:64}],headersStr:"主要功能 组件",content:"# 简介\n\nSpring Cloud Alibaba 致力于提供微服务开发的一站式解决方案。此项目包含开发分布式应用微服务的必需组件，方便开发者通过 Spring Cloud 编程模型轻松使用这些组件来开发分布式应用服务。\n\n依托 Spring Cloud Alibaba，您只需要添加一些注解和少量配置，就可以将 Spring Cloud 应用接入阿里微服务解决方案，通过阿里中间件来迅速搭建分布式应用系统。\n\n\n# 主要功能\n\n * 服务限流降级：默认支持 WebServlet、WebFlux, OpenFeign、RestTemplate、Spring Cloud Gateway, Zuul, Dubbo 和 RocketMQ 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 Metrics 监控。\n * 服务注册与发现：适配 Spring Cloud 服务注册与发现标准，默认集成了 Ribbon 的支持。\n * 分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新。\n * 消息驱动能力：基于 Spring Cloud Stream 为微服务应用构建消息驱动能力。\n * 分布式事务：使用 @GlobalTransactional 注解， 高效并且对业务零侵入地解决分布式事务问题。。\n * 阿里云对象存储：阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。\n * 分布式任务调度：提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。同时提供分布式的任务执行模型，如网格任务。网格任务支持海量子任务均匀分配到所有 Worker（schedulerx-client）上执行。\n * 阿里云短信服务：覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。\n\n\n# 组件\n\n * Sentinel：把流量作为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。\n\n * Nacos：一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。\n\n * RocketMQ：一款开源的分布式消息系统，基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。\n\n * Dubbo：Apache Dubbo™ 是一款高性能 Java RPC 框架。\n\n * Seata：阿里巴巴开源产品，一个易于使用的高性能微服务分布式事务解决方案。\n\n * Alibaba Cloud ACM：一款在分布式架构环境中对应用配置进行集中管理和推送的应用配置中心产品。\n\n * Alibaba Cloud OSS: 阿里云对象存储服务（Object Storage Service，简称 OSS），是阿里云提供的海量、安全、低成本、高可靠的云存储服务。您可以在任何应用、任何时间、任何地点存储和访问任意类型的数据。\n\n * Alibaba Cloud SchedulerX: 阿里中间件团队开发的一款分布式任务调度产品，提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。\n\n * Alibaba Cloud SMS: 覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。",normalizedContent:"# 简介\n\nspring cloud alibaba 致力于提供微服务开发的一站式解决方案。此项目包含开发分布式应用微服务的必需组件，方便开发者通过 spring cloud 编程模型轻松使用这些组件来开发分布式应用服务。\n\n依托 spring cloud alibaba，您只需要添加一些注解和少量配置，就可以将 spring cloud 应用接入阿里微服务解决方案，通过阿里中间件来迅速搭建分布式应用系统。\n\n\n# 主要功能\n\n * 服务限流降级：默认支持 webservlet、webflux, openfeign、resttemplate、spring cloud gateway, zuul, dubbo 和 rocketmq 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 metrics 监控。\n * 服务注册与发现：适配 spring cloud 服务注册与发现标准，默认集成了 ribbon 的支持。\n * 分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新。\n * 消息驱动能力：基于 spring cloud stream 为微服务应用构建消息驱动能力。\n * 分布式事务：使用 @globaltransactional 注解， 高效并且对业务零侵入地解决分布式事务问题。。\n * 阿里云对象存储：阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。\n * 分布式任务调度：提供秒级、精准、高可靠、高可用的定时（基于 cron 表达式）任务调度服务。同时提供分布式的任务执行模型，如网格任务。网格任务支持海量子任务均匀分配到所有 worker（schedulerx-client）上执行。\n * 阿里云短信服务：覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。\n\n\n# 组件\n\n * sentinel：把流量作为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。\n\n * nacos：一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。\n\n * rocketmq：一款开源的分布式消息系统，基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。\n\n * dubbo：apache dubbo™ 是一款高性能 java rpc 框架。\n\n * seata：阿里巴巴开源产品，一个易于使用的高性能微服务分布式事务解决方案。\n\n * alibaba cloud acm：一款在分布式架构环境中对应用配置进行集中管理和推送的应用配置中心产品。\n\n * alibaba cloud oss: 阿里云对象存储服务（object storage service，简称 oss），是阿里云提供的海量、安全、低成本、高可靠的云存储服务。您可以在任何应用、任何时间、任何地点存储和访问任意类型的数据。\n\n * alibaba cloud schedulerx: 阿里中间件团队开发的一款分布式任务调度产品，提供秒级、精准、高可靠、高可用的定时（基于 cron 表达式）任务调度服务。\n\n * alibaba cloud sms: 覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:47:02",lastUpdatedTimestamp:1644378422e3},{title:"服务注册与发现",frontmatter:{title:"服务注册与发现",date:"2022-02-09T11:45:40.000Z",permalink:"/pages/e48516/"},regularPath:"/16.Spring%20Cloud%20Alibaba/01.Spring%20Cloud%20Alibaba/02.%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0.html",relativePath:"16.Spring Cloud Alibaba/01.Spring Cloud Alibaba/02.服务注册与发现.md",key:"v-c1a66828",path:"/pages/e48516/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:14},{level:2,title:"什么是 Nacos",slug:"什么是-nacos",normalizedTitle:"什么是 nacos",charIndex:179},{level:2,title:"Nacos 地图",slug:"nacos-地图",normalizedTitle:"nacos 地图",charIndex:1369},{level:2,title:"Nacos 生态图",slug:"nacos-生态图",normalizedTitle:"nacos 生态图",charIndex:1593},{level:2,title:"Nacos Docker 快速开始",slug:"nacos-docker-快速开始",normalizedTitle:"nacos docker 快速开始",charIndex:1911},{level:3,title:"操作步骤",slug:"操作步骤",normalizedTitle:"操作步骤",charIndex:1933},{level:3,title:"Common property configuration",slug:"common-property-configuration",normalizedTitle:"common property configuration",charIndex:2869},{level:2,title:"Nacos + Grafana + Prometheus",slug:"nacos-grafana-prometheus",normalizedTitle:"nacos + grafana + prometheus",charIndex:4618},{level:2,title:"相关项目",slug:"相关项目",normalizedTitle:"相关项目",charIndex:4721}],headersStr:"概述 什么是 Nacos Nacos 地图 Nacos 生态图 Nacos Docker 快速开始 操作步骤 Common property configuration Nacos + Grafana + Prometheus 相关项目",content:"# 服务注册与发现\n\n\n# 概述\n\nNacos 致力于帮助您发现、配置和管理微服务。Nacos 提供了一组简单易用的特性集，帮助您快速实现动态服务发现、服务配置、服务元数据及流量管理。\n\nNacos 帮助您更敏捷和容易地构建、交付和管理微服务平台。 Nacos 是构建以“服务”为中心的现代应用架构 (例如微服务范式、云原生范式) 的服务基础设施。\n\n\n# 什么是 Nacos\n\n服务（Service）是 Nacos 世界的一等公民。Nacos 支持几乎所有主流类型的“服务”的发现、配置和管理：\n\nKubernetes Service\n\ngRPC & Dubbo RPC Service\n\nSpring Cloud RESTful Service\n\nNacos 的关键特性包括:\n\n * 服务发现和服务健康监测\n   \n   Nacos 支持基于 DNS 和基于 RPC 的服务发现。服务提供者使用 原生SDK、OpenAPI、或一个独立的Agent TODO注册 Service 后，服务消费者可以使用DNS TODO 或HTTP&API查找和发现服务。\n   \n   Nacos 提供对服务的实时的健康检查，阻止向不健康的主机或服务实例发送请求。Nacos 支持传输层 (PING 或 TCP)和应用层 (如 HTTP、MySQL、用户自定义）的健康检查。 对于复杂的云环境和网络拓扑环境中（如 VPC、边缘网络等）服务的健康检查，Nacos 提供了 agent 上报模式和服务端主动检测2种健康检查模式。Nacos 还提供了统一的健康检查仪表盘，帮助您根据健康状态管理服务的可用性及流量。\n\n * 动态配置服务\n   \n   动态配置服务可以让您以中心化、外部化和动态化的方式管理所有环境的应用配置和服务配置。\n   \n   动态配置消除了配置变更时重新部署应用和服务的需要，让配置管理变得更加高效和敏捷。\n   \n   配置中心化管理让实现无状态服务变得更简单，让服务按需弹性扩展变得更容易。\n   \n   Nacos 提供了一个简洁易用的UI (控制台样例 Demo) 帮助您管理所有的服务和应用的配置。Nacos 还提供包括配置版本跟踪、金丝雀发布、一键回滚配置以及客户端配置更新状态跟踪在内的一系列开箱即用的配置管理特性，帮助您更安全地在生产环境中管理配置变更和降低配置变更带来的风险。\n\n * 动态 DNS 服务\n   \n   动态 DNS 服务支持权重路由，让您更容易地实现中间层负载均衡、更灵活的路由策略、流量控制以及数据中心内网的简单DNS解析服务。动态DNS服务还能让您更容易地实现以 DNS 协议为基础的服务发现，以帮助您消除耦合到厂商私有服务发现 API 上的风险。\n   \n   Nacos 提供了一些简单的 DNS APIs TODO 帮助您管理服务的关联域名和可用的 IP:PORT 列表.\n\n * 服务及其元数据管理\n   \n   Nacos 能让您从微服务平台建设的视角管理数据中心的所有服务及元数据，包括管理服务的描述、生命周期、服务的静态依赖分析、服务的健康状态、服务的流量管理、路由及安全策略、服务的 SLA 以及最首要的 metrics 统计数据。\n\n * 更多的特性列表 ...\n\n\n# Nacos 地图\n\n一图看懂 Nacos，下面架构部分会详细介绍。\n\n * 特性大图：要从功能特性，非功能特性，全面介绍我们要解的问题域的特性诉求\n * 架构大图：通过清晰架构，让您快速进入 Nacos 世界\n * 业务大图：利用当前特性可以支持的业务场景，及其最佳实践\n * 生态大图：系统梳理 Nacos 和主流技术生态的关系\n * 优势大图：展示 Nacos 核心竞争力\n * 战略大图：要从战略到战术层面讲 Nacos 的宏观优势\n\n\n# Nacos 生态图\n\n\n\n如 Nacos 全景图所示，Nacos 无缝支持一些主流的开源生态，例如\n\n * Spring Cloud\n * Apache Dubbo and Dubbo Mesh TODO\n * Kubernetes and CNCF TODO。\n\n使用 Nacos 简化服务发现、配置管理、服务治理及管理的解决方案，让微服务的发现、管理、共享、组合更加容易。\n\n关于如何在这些生态中使用 Nacos，请参考以下文档：\n\nNacos与Spring Cloud一起使用\n\nNacos与Kubernetes一起使用\n\nNacos与Dubbo一起使用\n\nNacos与gRPC一起使用\n\nNacos与Istio一起使用\n\n\n# Nacos Docker 快速开始\n\n\n# 操作步骤\n\n * Clone 项目\n   \n   git clone https://github.com/nacos-group/nacos-docker.git\n   cd nacos-docker\n   \n\n * 单机模式 Derby\n   \n   docker-compose -f example/standalone-derby.yaml up\n   \n\n * 单机模式 Mysql\n   \n   docker-compose -f example/standalone-mysql.yaml up\n   \n\n * 集群模式\n   \n   docker-compose -f example/cluster-hostname.yaml up \n   \n\n * 服务注册\n   \n   curl -X POST 'http://127.0.0.1:8848/nacos/v1/ns/instance?serviceName=nacos.naming.serviceName&ip=20.18.7.10&port=8080'\n   \n\n * 服务发现\n   \n   curl -X GET 'http://127.0.0.1:8848/nacos/v1/ns/instances?serviceName=nacos.naming.serviceName'\n   \n\n * 发布配置\n   \n   curl -X POST \"http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=helloWorld\"\n   \n\n * 获取配置\n   \n     curl -X GET \"http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test\"\n   \n\n * Nacos 控制台\n   \n   link：http://127.0.0.1:8848/nacos/\n   \n   提示\n   \n   默认账号密码为 nacos/nacos\n\n登录页\n\n控制台\n\n\n# Common property configuration\n\nNAME                            DESCRIPTION                       OPTION\nMODE                            cluster模式/standalone模式            cluster/standalone default cluster\nNACOS_SERVERS                   nacos cluster地址                   eg. ip1,ip2,ip3\nPREFER_HOST_MODE                是否支持hostname                      hostname/ip default ip\nNACOS_SERVER_PORT               nacos服务器端口                        default 8848\nNACOS_SERVER_IP                 多网卡下的自定义nacos服务器IP                \nSPRING_DATASOURCE_PLATFORM      standalone 支持 mysql               mysql / empty default empty\nMYSQL_MASTER_SERVICE_HOST       mysql 主节点host                     \nMYSQL_MASTER_SERVICE_PORT       mysql 主节点端口                       default : 3306\nMYSQL_MASTER_SERVICE_DB_NAME    mysql 主节点数据库                      \nMYSQL_MASTER_SERVICE_USER       数据库用户名                            \nMYSQL_MASTER_SERVICE_PASSWORD   数据库密码                             \nMYSQL_SLAVE_SERVICE_HOST        mysql从节点host                      \nMYSQL_SLAVE_SERVICE_PORT        mysql从节点端口                        default :3306\nMYSQL_DATABASE_NUM              数据库数量                             default :2\nJVM_XMS                         -Xms                              default :2g\nJVM_XMX                         -Xmx                              default :2g\nJVM_XMN                         -Xmn                              default :1g\nJVM_MS                          -XX:MetaspaceSize                 default :128m\nJVM_MMS                         -XX:MaxMetaspaceSize              default :320m\nNACOS_DEBUG                     开启远程调试                            y/n default :n\nTOMCAT_ACCESSLOG_ENABLED        server.tomcat.accesslog.enabled   default :false\n\n\n# Nacos + Grafana + Prometheus\n\n参考：Nacos监控指南\n\nNote: grafana创建一个新数据源时，数据源地址必须是 http://prometheus:9090\n\n\n# 相关项目\n\n * Nacos\n * Nacos Docker",normalizedContent:"# 服务注册与发现\n\n\n# 概述\n\nnacos 致力于帮助您发现、配置和管理微服务。nacos 提供了一组简单易用的特性集，帮助您快速实现动态服务发现、服务配置、服务元数据及流量管理。\n\nnacos 帮助您更敏捷和容易地构建、交付和管理微服务平台。 nacos 是构建以“服务”为中心的现代应用架构 (例如微服务范式、云原生范式) 的服务基础设施。\n\n\n# 什么是 nacos\n\n服务（service）是 nacos 世界的一等公民。nacos 支持几乎所有主流类型的“服务”的发现、配置和管理：\n\nkubernetes service\n\ngrpc & dubbo rpc service\n\nspring cloud restful service\n\nnacos 的关键特性包括:\n\n * 服务发现和服务健康监测\n   \n   nacos 支持基于 dns 和基于 rpc 的服务发现。服务提供者使用 原生sdk、openapi、或一个独立的agent todo注册 service 后，服务消费者可以使用dns todo 或http&api查找和发现服务。\n   \n   nacos 提供对服务的实时的健康检查，阻止向不健康的主机或服务实例发送请求。nacos 支持传输层 (ping 或 tcp)和应用层 (如 http、mysql、用户自定义）的健康检查。 对于复杂的云环境和网络拓扑环境中（如 vpc、边缘网络等）服务的健康检查，nacos 提供了 agent 上报模式和服务端主动检测2种健康检查模式。nacos 还提供了统一的健康检查仪表盘，帮助您根据健康状态管理服务的可用性及流量。\n\n * 动态配置服务\n   \n   动态配置服务可以让您以中心化、外部化和动态化的方式管理所有环境的应用配置和服务配置。\n   \n   动态配置消除了配置变更时重新部署应用和服务的需要，让配置管理变得更加高效和敏捷。\n   \n   配置中心化管理让实现无状态服务变得更简单，让服务按需弹性扩展变得更容易。\n   \n   nacos 提供了一个简洁易用的ui (控制台样例 demo) 帮助您管理所有的服务和应用的配置。nacos 还提供包括配置版本跟踪、金丝雀发布、一键回滚配置以及客户端配置更新状态跟踪在内的一系列开箱即用的配置管理特性，帮助您更安全地在生产环境中管理配置变更和降低配置变更带来的风险。\n\n * 动态 dns 服务\n   \n   动态 dns 服务支持权重路由，让您更容易地实现中间层负载均衡、更灵活的路由策略、流量控制以及数据中心内网的简单dns解析服务。动态dns服务还能让您更容易地实现以 dns 协议为基础的服务发现，以帮助您消除耦合到厂商私有服务发现 api 上的风险。\n   \n   nacos 提供了一些简单的 dns apis todo 帮助您管理服务的关联域名和可用的 ip:port 列表.\n\n * 服务及其元数据管理\n   \n   nacos 能让您从微服务平台建设的视角管理数据中心的所有服务及元数据，包括管理服务的描述、生命周期、服务的静态依赖分析、服务的健康状态、服务的流量管理、路由及安全策略、服务的 sla 以及最首要的 metrics 统计数据。\n\n * 更多的特性列表 ...\n\n\n# nacos 地图\n\n一图看懂 nacos，下面架构部分会详细介绍。\n\n * 特性大图：要从功能特性，非功能特性，全面介绍我们要解的问题域的特性诉求\n * 架构大图：通过清晰架构，让您快速进入 nacos 世界\n * 业务大图：利用当前特性可以支持的业务场景，及其最佳实践\n * 生态大图：系统梳理 nacos 和主流技术生态的关系\n * 优势大图：展示 nacos 核心竞争力\n * 战略大图：要从战略到战术层面讲 nacos 的宏观优势\n\n\n# nacos 生态图\n\n\n\n如 nacos 全景图所示，nacos 无缝支持一些主流的开源生态，例如\n\n * spring cloud\n * apache dubbo and dubbo mesh todo\n * kubernetes and cncf todo。\n\n使用 nacos 简化服务发现、配置管理、服务治理及管理的解决方案，让微服务的发现、管理、共享、组合更加容易。\n\n关于如何在这些生态中使用 nacos，请参考以下文档：\n\nnacos与spring cloud一起使用\n\nnacos与kubernetes一起使用\n\nnacos与dubbo一起使用\n\nnacos与grpc一起使用\n\nnacos与istio一起使用\n\n\n# nacos docker 快速开始\n\n\n# 操作步骤\n\n * clone 项目\n   \n   git clone https://github.com/nacos-group/nacos-docker.git\n   cd nacos-docker\n   \n\n * 单机模式 derby\n   \n   docker-compose -f example/standalone-derby.yaml up\n   \n\n * 单机模式 mysql\n   \n   docker-compose -f example/standalone-mysql.yaml up\n   \n\n * 集群模式\n   \n   docker-compose -f example/cluster-hostname.yaml up \n   \n\n * 服务注册\n   \n   curl -x post 'http://127.0.0.1:8848/nacos/v1/ns/instance?servicename=nacos.naming.servicename&ip=20.18.7.10&port=8080'\n   \n\n * 服务发现\n   \n   curl -x get 'http://127.0.0.1:8848/nacos/v1/ns/instances?servicename=nacos.naming.servicename'\n   \n\n * 发布配置\n   \n   curl -x post \"http://127.0.0.1:8848/nacos/v1/cs/configs?dataid=nacos.cfg.dataid&group=test&content=helloworld\"\n   \n\n * 获取配置\n   \n     curl -x get \"http://127.0.0.1:8848/nacos/v1/cs/configs?dataid=nacos.cfg.dataid&group=test\"\n   \n\n * nacos 控制台\n   \n   link：http://127.0.0.1:8848/nacos/\n   \n   提示\n   \n   默认账号密码为 nacos/nacos\n\n登录页\n\n控制台\n\n\n# common property configuration\n\nname                            description                       option\nmode                            cluster模式/standalone模式            cluster/standalone default cluster\nnacos_servers                   nacos cluster地址                   eg. ip1,ip2,ip3\nprefer_host_mode                是否支持hostname                      hostname/ip default ip\nnacos_server_port               nacos服务器端口                        default 8848\nnacos_server_ip                 多网卡下的自定义nacos服务器ip                \nspring_datasource_platform      standalone 支持 mysql               mysql / empty default empty\nmysql_master_service_host       mysql 主节点host                     \nmysql_master_service_port       mysql 主节点端口                       default : 3306\nmysql_master_service_db_name    mysql 主节点数据库                      \nmysql_master_service_user       数据库用户名                            \nmysql_master_service_password   数据库密码                             \nmysql_slave_service_host        mysql从节点host                      \nmysql_slave_service_port        mysql从节点端口                        default :3306\nmysql_database_num              数据库数量                             default :2\njvm_xms                         -xms                              default :2g\njvm_xmx                         -xmx                              default :2g\njvm_xmn                         -xmn                              default :1g\njvm_ms                          -xx:metaspacesize                 default :128m\njvm_mms                         -xx:maxmetaspacesize              default :320m\nnacos_debug                     开启远程调试                            y/n default :n\ntomcat_accesslog_enabled        server.tomcat.accesslog.enabled   default :false\n\n\n# nacos + grafana + prometheus\n\n参考：nacos监控指南\n\nnote: grafana创建一个新数据源时，数据源地址必须是 http://prometheus:9090\n\n\n# 相关项目\n\n * nacos\n * nacos docker",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Nacos discovery",frontmatter:{title:"Nacos discovery",date:"2022-02-09T11:45:40.000Z",permalink:"/pages/01bdf6/"},regularPath:"/16.Spring%20Cloud%20Alibaba/01.Spring%20Cloud%20Alibaba/03.Nacos%20discovery.html",relativePath:"16.Spring Cloud Alibaba/01.Spring Cloud Alibaba/03.Nacos discovery.md",key:"v-ed96cd98",path:"/pages/01bdf6/",headers:[{level:2,title:"Spring Cloud Alibaba Nacos Discovery",slug:"spring-cloud-alibaba-nacos-discovery",normalizedTitle:"spring cloud alibaba nacos discovery",charIndex:22},{level:3,title:"服务注册发现: Nacos Discovery Starter",slug:"服务注册发现-nacos-discovery-starter",normalizedTitle:"服务注册发现: nacos discovery starter",charIndex:202},{level:3,title:"服务的 EndPoint",slug:"服务的-endpoint",normalizedTitle:"服务的 endpoint",charIndex:4264},{level:3,title:"启动一个 Consumer 应用",slug:"启动一个-consumer-应用",normalizedTitle:"启动一个 consumer 应用",charIndex:5890},{level:3,title:"关于 Nacos Starter 更多的配置项信息",slug:"关于-nacos-starter-更多的配置项信息",normalizedTitle:"关于 nacos starter 更多的配置项信息",charIndex:7626}],headersStr:"Spring Cloud Alibaba Nacos Discovery 服务注册发现: Nacos Discovery Starter 服务的 EndPoint 启动一个 Consumer 应用 关于 Nacos Starter 更多的配置项信息",content:'# Nacos discovery\n\n\n# Spring Cloud Alibaba Nacos Discovery\n\n该项目通过自动配置以及其他 Spring 编程模型的习惯用法为 Spring Boot 应用程序在服务注册与发现方面提供和 Nacos 的无缝集成。 通过一些简单的注解，您可以快速来注册一个服务，并使用经过双十一考验的 Nacos 组件来作为大规模分布式系统的服务注册中心。\n\n\n# 服务注册发现: Nacos Discovery Starter\n\n服务发现是微服务架构体系中最关键的组件之一。如果尝试着用手动的方式来给每一个客户端来配置所有服务提供者的服务列表是一件非常困难的事，而且也不利于 服务的动态扩缩容。Nacos Discovery Starter 可以帮助您将服务自动注册到 Nacos 服务端并且能够动态感知和刷新某个服务实例的服务列表。除此之外，Nacos Discovery Starter 也将服务实例自身的一些元数据信息-例如 host，port,健康检查URL，主页等-注册到 Nacos 。Nacos 的获取和启动方式可以参考 Nacos 官网。\n\n# 如何引入 Nacos Discovery Starter\n\n如果要在您的项目中使用 Nacos 来实现服务发现，使用 group ID 为 com.alibaba.cloud 和 artifact ID 为 spring-cloud-starter-alibaba-nacos-discovery 的 starter。\n\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n\n\n# 启动一个 Provider 应用\n\n以下步骤向您展示了如何将一个服务注册到 Nacos。\n\n 1. pom.xml的配置。一个完整的 pom.xml 配置如下所示：\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>open.source.test</groupId>\n    <artifactId>nacos-discovery-test</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <name>nacos-discovery-test</name>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>${spring.boot.version}</version>\n        <relativePath/>\n    </parent>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n    </properties>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>${spring.cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            <dependency>\n                <groupId>com.alibaba.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring.cloud.alibaba.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-actuator</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>com.alibaba.cloud</groupId>\n            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n 1. application.properties 配置。一些关于 Nacos 基本的配置也必须在 application.properties(也可以是application.yaml)配置，如下所示： application.properties\n\nserver.port=8081\nspring.application.name=nacos-producer\nspring.cloud.nacos.discovery.server-addr=127.0.0.1:8848\nmanagement.endpoints.web.exposure.include=*\n\n\nNOTE   如果不想使用 NACOS 作为您的服务注册与发现，可以将\n       SPRING.CLOUD.NACOS.DISCOVERY.ENABLED 设置为 FALSE。\n\n 1. 启动 Provider 示例。如下所示：\n\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProducerDemoApplication.class, args);\n    }\n\n    @RestController\n    public class EchoController {\n        @GetMapping(value = "/echo/{string}")\n        public String echo(@PathVariable String string) {\n            return "Hello Nacos Discovery " + string;\n        }\n    }\n}\n\n\n这个时候你就可以在 Nacos的控制台上看到注册上来的服务信息了。\n\nNOTE   再启动 PROVIDER 应用之前 请先将 NACOS 服务启动。具体启动方式可参考 NACOS 官网。\n\n\n# 服务的 EndPoint\n\nspring-cloud-starter-alibaba-nacos-discovery 在实现的时候提供了一个EndPoint,EndPoint的访问地址为 http://ip:port/actuator/nacos-discovery。 EndPoint 的信息主要提供了两类:\n\n1、subscribe: 显示了当前有哪些服务订阅者\n2、NacosDiscoveryProperties: 显示了当前服务实例关于 Nacos 的基础配置\n\n\n一个服务实例访问 EndPoint 的信息如下所示：\n\n{\n  "subscribe": [\n    {\n      "jsonFromServer": "",\n      "name": "nacos-provider",\n      "clusters": "",\n      "cacheMillis": 10000,\n      "hosts": [\n        {\n          "instanceId": "30.5.124.156#8081#DEFAULT#nacos-provider",\n          "ip": "30.5.124.156",\n          "port": 8081,\n          "weight": 1.0,\n          "healthy": true,\n          "enabled": true,\n          "cluster": {\n            "serviceName": null,\n            "name": null,\n            "healthChecker": {\n              "type": "TCP"\n            },\n            "defaultPort": 80,\n            "defaultCheckPort": 80,\n            "useIPPort4Check": true,\n            "metadata": {\n\n            }\n          },\n          "service": null,\n          "metadata": {\n\n          }\n        }\n      ],\n      "lastRefTime": 1541755293119,\n      "checksum": "e5a699c9201f5328241c178e804657e11541755293119",\n      "allIPs": false,\n      "key": "nacos-producer",\n      "valid": true\n    }\n  ],\n  "NacosDiscoveryProperties": {\n    "serverAddr": "127.0.0.1:8848",\n    "endpoint": "",\n    "namespace": "",\n    "logName": "",\n    "service": "nacos-provider",\n    "weight": 1.0,\n    "clusterName": "DEFAULT",\n    "metadata": {\n\n    },\n    "registerEnabled": true,\n    "ip": "30.5.124.201",\n    "networkInterface": "",\n    "port": 8082,\n    "secure": false,\n    "accessKey": "",\n    "secretKey": ""\n  }\n}\n\n\n\n# 启动一个 Consumer 应用\n\nConsumer 的应用可能还没像启动一个 Provider 应用那么简单。因为在 Consumer 端需要去调用 Provider 端提供的REST 服务。例子中我们使用最原始的一种方式， 即显示的使用 LoadBalanceClient 和 RestTemolate 结合的方式来访问。 pom.xml 和 application.properties 的配置可以参考 1.2 小结。启动一个 Consumer应用的示例代码如下所示：\n\nNOTE   通过带有负载均衡的RESTTEMPLATE 和 FEIGNCLIENT 也是可以访问的。\n\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosConsumerApp {\n\n    @RestController\n    public class NacosController{\n\n        @Autowired\n        private LoadBalancerClient loadBalancerClient;\n        @Autowired\n        private RestTemplate restTemplate;\n\n        @Value("${spring.application.name}")\n        private String appName;\n\n        @GetMapping("/echo/app-name")\n        public String echoAppName(){\n            //使用 LoadBalanceClient 和 RestTemolate 结合的方式来访问\n            ServiceInstance serviceInstance = loadBalancerClient.choose("nacos-provider");\n            String url = String.format("http://%s:%s/echo/%s",serviceInstance.getHost(),serviceInstance.getPort(),appName);\n            System.out.println("request url:"+url);\n            return restTemplate.getForObject(url,String.class);\n        }\n\n    }\n\n    //实例化 RestTemplate 实例\n    @Bean\n    public RestTemplate restTemplate(){\n\n        return new RestTemplate();\n    }\n\n    public static void main(String[] args) {\n\n        SpringApplication.run(NacosConsumerApp.class,args);\n    }\n}\n\n\n这个例子中我们注入了一个 LoadBalancerClient 的实例，并且手动的实例化一个 RestTemplate，同时将 spring.application.name 的配置值 注入到应用中来， 目的是调用 Provider 提供的服务时，希望将当前配置的应用名给显示出来。\n\nNOTE   在启动 CONSUMER 应用之前请先将 NACOS 服务启动好。具体启动方式可参考 NACOS 官网。\n\n启动后，访问 Consumer 提供出来的 http://ip:port/echo/app-name 接口。我这里测试启动的 port是 8082。访问结果如下所示：\n\n访问地址：http://127.0.0.1:8082/echo/app-name\n访问结果：Hello Nacos Discovery nacos-consumer\n\n\n\n# 关于 Nacos Starter 更多的配置项信息\n\n更多关于 spring-cloud-starter-alibaba-nacos-discovery 的 starter 配置项如下所示:\n\n配置项               KEY                                              默认值                          说明\n服务端地址             spring.cloud.nacos.discovery.server-addr         无                            Nacos Server 启动监听的ip地址和端口\n服务名               spring.cloud.nacos.discovery.service             ${spring.application.name}   给当前的服务命名\n服务分组              spring.cloud.nacos.discovery.group               DEFAULT_GROUP                设置服务所处的分组\n权重                spring.cloud.nacos.discovery.weight              1                            取值范围 1 到 100，数值越大，权重越大\n网卡名               spring.cloud.nacos.discovery.network-interface   无                            当IP未配置时，注册的IP为此网卡所对应的IP地址，如果此项也未配置，则默认取第一块网卡的地址\n注册的IP地址           spring.cloud.nacos.discovery.ip                  无                            优先级最高\n注册的端口             spring.cloud.nacos.discovery.port                -1                           默认情况下不用配置，会自动探测\n命名空间              spring.cloud.nacos.discovery.namespace           无                            常用场景之一是不同环境的注册的区分隔离，例如开发测试环境和生产环境的资源（如配置、服务）隔离等。\nAccessKey         spring.cloud.nacos.discovery.access-key          无                            当要上阿里云时，阿里云上面的一个云账号名\nSecretKey         spring.cloud.nacos.discovery.secret-key          无                            当要上阿里云时，阿里云上面的一个云账号密码\nMetadata          spring.cloud.nacos.discovery.metadata            无                            使用Map格式配置，用户可以根据自己的需要自定义一些和服务相关的元数据信息\n日志文件名             spring.cloud.nacos.discovery.log-name            无                            \n集群                spring.cloud.nacos.discovery.cluster-name        DEFAULT                      配置成Nacos集群名称\n接入点               spring.cloud.nacos.discovery.enpoint             UTF-8                        地域的某个服务的入口域名，通过此域名可以动态地拿到服务端地址\n是否集成Ribbon        ribbon.nacos.enabled                             true                         一般都设置成true即可\n是否开启Nacos Watch   spring.cloud.nacos.discovery.watch.enabled       true                         可以设置成false来关闭 watch',normalizedContent:'# nacos discovery\n\n\n# spring cloud alibaba nacos discovery\n\n该项目通过自动配置以及其他 spring 编程模型的习惯用法为 spring boot 应用程序在服务注册与发现方面提供和 nacos 的无缝集成。 通过一些简单的注解，您可以快速来注册一个服务，并使用经过双十一考验的 nacos 组件来作为大规模分布式系统的服务注册中心。\n\n\n# 服务注册发现: nacos discovery starter\n\n服务发现是微服务架构体系中最关键的组件之一。如果尝试着用手动的方式来给每一个客户端来配置所有服务提供者的服务列表是一件非常困难的事，而且也不利于 服务的动态扩缩容。nacos discovery starter 可以帮助您将服务自动注册到 nacos 服务端并且能够动态感知和刷新某个服务实例的服务列表。除此之外，nacos discovery starter 也将服务实例自身的一些元数据信息-例如 host，port,健康检查url，主页等-注册到 nacos 。nacos 的获取和启动方式可以参考 nacos 官网。\n\n# 如何引入 nacos discovery starter\n\n如果要在您的项目中使用 nacos 来实现服务发现，使用 group id 为 com.alibaba.cloud 和 artifact id 为 spring-cloud-starter-alibaba-nacos-discovery 的 starter。\n\n<dependency>\n    <groupid>com.alibaba.cloud</groupid>\n    <artifactid>spring-cloud-starter-alibaba-nacos-discovery</artifactid>\n</dependency>\n\n\n# 启动一个 provider 应用\n\n以下步骤向您展示了如何将一个服务注册到 nacos。\n\n 1. pom.xml的配置。一个完整的 pom.xml 配置如下所示：\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <groupid>open.source.test</groupid>\n    <artifactid>nacos-discovery-test</artifactid>\n    <version>1.0-snapshot</version>\n    <name>nacos-discovery-test</name>\n\n    <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>${spring.boot.version}</version>\n        <relativepath/>\n    </parent>\n\n    <properties>\n        <project.build.sourceencoding>utf-8</project.build.sourceencoding>\n        <project.reporting.outputencoding>utf-8</project.reporting.outputencoding>\n        <java.version>1.8</java.version>\n    </properties>\n\n    <dependencymanagement>\n        <dependencies>\n            <dependency>\n                <groupid>org.springframework.cloud</groupid>\n                <artifactid>spring-cloud-dependencies</artifactid>\n                <version>${spring.cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            <dependency>\n                <groupid>com.alibaba.cloud</groupid>\n                <artifactid>spring-cloud-alibaba-dependencies</artifactid>\n                <version>${spring.cloud.alibaba.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencymanagement>\n\n    <dependencies>\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-web</artifactid>\n        </dependency>\n\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-actuator</artifactid>\n        </dependency>\n\n        <dependency>\n            <groupid>com.alibaba.cloud</groupid>\n            <artifactid>spring-cloud-starter-alibaba-nacos-discovery</artifactid>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-maven-plugin</artifactid>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n 1. application.properties 配置。一些关于 nacos 基本的配置也必须在 application.properties(也可以是application.yaml)配置，如下所示： application.properties\n\nserver.port=8081\nspring.application.name=nacos-producer\nspring.cloud.nacos.discovery.server-addr=127.0.0.1:8848\nmanagement.endpoints.web.exposure.include=*\n\n\nnote   如果不想使用 nacos 作为您的服务注册与发现，可以将\n       spring.cloud.nacos.discovery.enabled 设置为 false。\n\n 1. 启动 provider 示例。如下所示：\n\n@springbootapplication\n@enablediscoveryclient\npublic class nacosproviderdemoapplication {\n\n    public static void main(string[] args) {\n        springapplication.run(nacosproducerdemoapplication.class, args);\n    }\n\n    @restcontroller\n    public class echocontroller {\n        @getmapping(value = "/echo/{string}")\n        public string echo(@pathvariable string string) {\n            return "hello nacos discovery " + string;\n        }\n    }\n}\n\n\n这个时候你就可以在 nacos的控制台上看到注册上来的服务信息了。\n\nnote   再启动 provider 应用之前 请先将 nacos 服务启动。具体启动方式可参考 nacos 官网。\n\n\n# 服务的 endpoint\n\nspring-cloud-starter-alibaba-nacos-discovery 在实现的时候提供了一个endpoint,endpoint的访问地址为 http://ip:port/actuator/nacos-discovery。 endpoint 的信息主要提供了两类:\n\n1、subscribe: 显示了当前有哪些服务订阅者\n2、nacosdiscoveryproperties: 显示了当前服务实例关于 nacos 的基础配置\n\n\n一个服务实例访问 endpoint 的信息如下所示：\n\n{\n  "subscribe": [\n    {\n      "jsonfromserver": "",\n      "name": "nacos-provider",\n      "clusters": "",\n      "cachemillis": 10000,\n      "hosts": [\n        {\n          "instanceid": "30.5.124.156#8081#default#nacos-provider",\n          "ip": "30.5.124.156",\n          "port": 8081,\n          "weight": 1.0,\n          "healthy": true,\n          "enabled": true,\n          "cluster": {\n            "servicename": null,\n            "name": null,\n            "healthchecker": {\n              "type": "tcp"\n            },\n            "defaultport": 80,\n            "defaultcheckport": 80,\n            "useipport4check": true,\n            "metadata": {\n\n            }\n          },\n          "service": null,\n          "metadata": {\n\n          }\n        }\n      ],\n      "lastreftime": 1541755293119,\n      "checksum": "e5a699c9201f5328241c178e804657e11541755293119",\n      "allips": false,\n      "key": "nacos-producer",\n      "valid": true\n    }\n  ],\n  "nacosdiscoveryproperties": {\n    "serveraddr": "127.0.0.1:8848",\n    "endpoint": "",\n    "namespace": "",\n    "logname": "",\n    "service": "nacos-provider",\n    "weight": 1.0,\n    "clustername": "default",\n    "metadata": {\n\n    },\n    "registerenabled": true,\n    "ip": "30.5.124.201",\n    "networkinterface": "",\n    "port": 8082,\n    "secure": false,\n    "accesskey": "",\n    "secretkey": ""\n  }\n}\n\n\n\n# 启动一个 consumer 应用\n\nconsumer 的应用可能还没像启动一个 provider 应用那么简单。因为在 consumer 端需要去调用 provider 端提供的rest 服务。例子中我们使用最原始的一种方式， 即显示的使用 loadbalanceclient 和 resttemolate 结合的方式来访问。 pom.xml 和 application.properties 的配置可以参考 1.2 小结。启动一个 consumer应用的示例代码如下所示：\n\nnote   通过带有负载均衡的resttemplate 和 feignclient 也是可以访问的。\n\n@springbootapplication\n@enablediscoveryclient\npublic class nacosconsumerapp {\n\n    @restcontroller\n    public class nacoscontroller{\n\n        @autowired\n        private loadbalancerclient loadbalancerclient;\n        @autowired\n        private resttemplate resttemplate;\n\n        @value("${spring.application.name}")\n        private string appname;\n\n        @getmapping("/echo/app-name")\n        public string echoappname(){\n            //使用 loadbalanceclient 和 resttemolate 结合的方式来访问\n            serviceinstance serviceinstance = loadbalancerclient.choose("nacos-provider");\n            string url = string.format("http://%s:%s/echo/%s",serviceinstance.gethost(),serviceinstance.getport(),appname);\n            system.out.println("request url:"+url);\n            return resttemplate.getforobject(url,string.class);\n        }\n\n    }\n\n    //实例化 resttemplate 实例\n    @bean\n    public resttemplate resttemplate(){\n\n        return new resttemplate();\n    }\n\n    public static void main(string[] args) {\n\n        springapplication.run(nacosconsumerapp.class,args);\n    }\n}\n\n\n这个例子中我们注入了一个 loadbalancerclient 的实例，并且手动的实例化一个 resttemplate，同时将 spring.application.name 的配置值 注入到应用中来， 目的是调用 provider 提供的服务时，希望将当前配置的应用名给显示出来。\n\nnote   在启动 consumer 应用之前请先将 nacos 服务启动好。具体启动方式可参考 nacos 官网。\n\n启动后，访问 consumer 提供出来的 http://ip:port/echo/app-name 接口。我这里测试启动的 port是 8082。访问结果如下所示：\n\n访问地址：http://127.0.0.1:8082/echo/app-name\n访问结果：hello nacos discovery nacos-consumer\n\n\n\n# 关于 nacos starter 更多的配置项信息\n\n更多关于 spring-cloud-starter-alibaba-nacos-discovery 的 starter 配置项如下所示:\n\n配置项               key                                              默认值                          说明\n服务端地址             spring.cloud.nacos.discovery.server-addr         无                            nacos server 启动监听的ip地址和端口\n服务名               spring.cloud.nacos.discovery.service             ${spring.application.name}   给当前的服务命名\n服务分组              spring.cloud.nacos.discovery.group               default_group                设置服务所处的分组\n权重                spring.cloud.nacos.discovery.weight              1                            取值范围 1 到 100，数值越大，权重越大\n网卡名               spring.cloud.nacos.discovery.network-interface   无                            当ip未配置时，注册的ip为此网卡所对应的ip地址，如果此项也未配置，则默认取第一块网卡的地址\n注册的ip地址           spring.cloud.nacos.discovery.ip                  无                            优先级最高\n注册的端口             spring.cloud.nacos.discovery.port                -1                           默认情况下不用配置，会自动探测\n命名空间              spring.cloud.nacos.discovery.namespace           无                            常用场景之一是不同环境的注册的区分隔离，例如开发测试环境和生产环境的资源（如配置、服务）隔离等。\naccesskey         spring.cloud.nacos.discovery.access-key          无                            当要上阿里云时，阿里云上面的一个云账号名\nsecretkey         spring.cloud.nacos.discovery.secret-key          无                            当要上阿里云时，阿里云上面的一个云账号密码\nmetadata          spring.cloud.nacos.discovery.metadata            无                            使用map格式配置，用户可以根据自己的需要自定义一些和服务相关的元数据信息\n日志文件名             spring.cloud.nacos.discovery.log-name            无                            \n集群                spring.cloud.nacos.discovery.cluster-name        default                      配置成nacos集群名称\n接入点               spring.cloud.nacos.discovery.enpoint             utf-8                        地域的某个服务的入口域名，通过此域名可以动态地拿到服务端地址\n是否集成ribbon        ribbon.nacos.enabled                             true                         一般都设置成true即可\n是否开启nacos watch   spring.cloud.nacos.discovery.watch.enabled       true                         可以设置成false来关闭 watch',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:47:02",lastUpdatedTimestamp:1644378422e3},{title:"Nacos config",frontmatter:{title:"Nacos config",date:"2022-02-09T11:45:40.000Z",permalink:"/pages/3a1e7d/"},regularPath:"/16.Spring%20Cloud%20Alibaba/01.Spring%20Cloud%20Alibaba/04.Nacos%20config.html",relativePath:"16.Spring Cloud Alibaba/01.Spring Cloud Alibaba/04.Nacos config.md",key:"v-19430f44",path:"/pages/3a1e7d/",headers:[{level:2,title:"Spring Cloud Alibaba Nacos Config",slug:"spring-cloud-alibaba-nacos-config",normalizedTitle:"spring cloud alibaba nacos config",charIndex:19},{level:3,title:"快速开始",slug:"快速开始",normalizedTitle:"快速开始",charIndex:424},{level:3,title:"基于 dataid 为 yaml 的文件扩展名配置方式",slug:"基于-dataid-为-yaml-的文件扩展名配置方式",normalizedTitle:"基于 dataid 为 yaml 的文件扩展名配置方式",charIndex:2407},{level:3,title:"支持配置的动态更新",slug:"支持配置的动态更新",normalizedTitle:"支持配置的动态更新",charIndex:3335},{level:3,title:"可支持profile粒度的配置",slug:"可支持profile粒度的配置",normalizedTitle:"可支持profile粒度的配置",charIndex:5159},{level:3,title:"支持自定义 namespace 的配置",slug:"支持自定义-namespace-的配置",normalizedTitle:"支持自定义 namespace 的配置",charIndex:7888},{level:3,title:"支持自定义 Group 的配置",slug:"支持自定义-group-的配置",normalizedTitle:"支持自定义 group 的配置",charIndex:8444},{level:3,title:"支持自定义扩展的 Data Id 配置",slug:"支持自定义扩展的-data-id-配置",normalizedTitle:"支持自定义扩展的 data id 配置",charIndex:8726},{level:3,title:"配置的优先级",slug:"配置的优先级",normalizedTitle:"配置的优先级",charIndex:10855},{level:3,title:"完全关闭配置",slug:"完全关闭配置",normalizedTitle:"完全关闭配置",charIndex:11182}],headersStr:"Spring Cloud Alibaba Nacos Config 快速开始 基于 dataid 为 yaml 的文件扩展名配置方式 支持配置的动态更新 可支持profile粒度的配置 支持自定义 namespace 的配置 支持自定义 Group 的配置 支持自定义扩展的 Data Id 配置 配置的优先级 完全关闭配置",content:'# Nacos config\n\n\n# Spring Cloud Alibaba Nacos Config\n\nNacos 提供用于存储配置和其他元数据的 key/value 存储，为分布式系统中的外部化配置提供服务器端和客户端支持。使用 Spring Cloud Alibaba Nacos Config，您可以在 Nacos Server 集中管理你 Spring Cloud 应用的外部属性配置。\n\nSpring Cloud Alibaba Nacos Config 是 Config Server 和 Client 的替代方案，客户端和服务器上的概念与 Spring Environment 和 PropertySource 有着一致的抽象，在特殊的 bootstrap 阶段，配置被加载到 Spring 环境中。当应用程序通过部署管道从开发到测试再到生产时，您可以管理这些环境之间的配置，并确保应用程序具有迁移时需要运行的所有内容。\n\n\n# 快速开始\n\n# Nacos 服务端初始化\n\n1、启动Nacos Server。启动方式可见 Nacos 官网\n\n2、启动好Nacos之后，在Nacos添加如下的配置：\n\nData ID:    nacos-config.properties\n\nGroup  :    DEFAULT_GROUP\n\n配置格式:    Properties\n\n配置内容：   user.name=nacos-config-properties\n            user.age=90\n\n\nNOTE   注意DATAID是以 PROPERTIES(默认的文件扩展名方式)为扩展名。\n\n# 客户端使用方式\n\n如果要在您的项目中使用 Nacos 来实现应用的外部化配置，使用 group ID 为 com.alibaba.cloud 和 artifact ID 为 spring-cloud-starter-alibaba-nacos-config 的 starter。\n\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>\n</dependency>\n\n\n现在就可以创建一个标准的 Spring Boot 应用。\n\n@SpringBootApplication\npublic class ProviderApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext applicationContext = SpringApplication.run(ProviderApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty("user.name");\n        String userAge = applicationContext.getEnvironment().getProperty("user.age");\n        System.err.println("user name :"+userName+"; age: "+userAge);\n    }\n}\n\n\n在运行此 Example 之前， 必须使用 bootstrap.properties 配置文件来配置Nacos Server 地址，例如：\n\nbootstrap.properties\n\nspring.application.name=nacos-config\nspring.cloud.nacos.config.server-addr=127.0.0.1:8848\n\n\nNOTE   注意当你使用域名的方式来访问 NACOS 时，SPRING.CLOUD.NACOS.CONFIG.SERVER-ADDR\n       配置的方式为 域名:PORT。 例如 NACOS 的域名为ABC.COM.NACOS，监听的端口为 80，则\n       SPRING.CLOUD.NACOS.CONFIG.SERVER-ADDR=ABC.COM.NACOS:80。 注意\n       80 端口不能省略。\n\n启动这个 Example，可以看到如下输出结果：\n\n2018-11-02 14:24:51.638  INFO 32700 --- [main] c.a.demo.provider.ProviderApplication    : Started ProviderApplication in 14.645 seconds (JVM running for 15.139)\nuser name :nacos-config-properties; age: 90\n2018-11-02 14:24:51.688  INFO 32700 --- [-127.0.0.1:8848] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@a8c5e74: startup date [Fri Nov 02 14:24:51 CST 2018]; root of context hierarchy\n2018-11\n\n\n\n# 基于 dataid 为 yaml 的文件扩展名配置方式\n\nspring-cloud-starter-alibaba-nacos-config 对于 yaml 格式也是完美支持的。这个时候只需要完成以下两步：\n\n1、在应用的 bootstrap.properties 配置文件中显示的声明 dataid 文件扩展名。如下所示\n\nbootstrap.properties\n\nspring.cloud.nacos.config.file-extension=yaml\n\n\n2、在 Nacos 的控制台新增一个dataid为yaml为扩展名的配置，如下所示：\n\nData ID:        nacos-config.yaml\n\nGroup  :        DEFAULT_GROUP\n\n配置格式:        YAML\n\n配置内容:        user.name: nacos-config-yaml\n                user.age: 68\n\n\n这两步完成后，重启测试程序，可以看到如下输出结果。\n\n2018-11-02 14:59:00.484  INFO 32928 --- [main] c.a.demo.provider.ProviderApplication:Started ProviderApplication in 14.183 seconds (JVM running for 14.671)\nuser name :nacos-config-yaml; age: 68\n2018-11-02 14:59:00.529  INFO 32928 --- [-127.0.0.1:8848] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@265a478e: startup date [Fri Nov 02 14:59:00 CST 2018]; root of context hierarchy\n\n\n\n# 支持配置的动态更新\n\nspring-cloud-starter-alibaba-nacos-config 也支持配置的动态更新，启动 Spring Boot 应用测试的代码如下：\n\n@SpringBootApplication\npublic class ProviderApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext applicationContext = SpringApplication.run(ProviderApplication.class, args);\n        while(true) {\n            //当动态配置刷新时，会更新到 Enviroment中，因此这里每隔一秒中从Enviroment中获取配置\n            String userName = applicationContext.getEnvironment().getProperty("user.name");\n            String userAge = applicationContext.getEnvironment().getProperty("user.age");\n            System.err.println("user name :" + userName + "; age: " + userAge);\n            TimeUnit.SECONDS.sleep(1);\n        }\n    }\n}\n\n\n如下所示，当变更user.name时，应用程序中能够获取到最新的值：\n\nuser name :nacos-config-yaml; age: 68\nuser name :nacos-config-yaml; age: 68\nuser name :nacos-config-yaml; age: 68\n2018-11-02 15:04:25.069  INFO 32957 --- [-127.0.0.1:8848] o.s.boot.SpringApplication               : Started application in 0.144 seconds (JVM running for 71.752)\n2018-11-02 15:04:25.070  INFO 32957 --- [-127.0.0.1:8848] s.c.a.AnnotationConfigApplicationContext : Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@10c89124: startup date [Fri Nov 02 15:04:25 CST 2018]; parent: org.springframework.context.annotation.AnnotationConfigApplicationContext@6520af7\n2018-11-02 15:04:25.071  INFO 32957 --- [-127.0.0.1:8848] s.c.a.AnnotationConfigApplicationContext : Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@6520af7: startup date [Fri Nov 02 15:04:24 CST 2018]; root of context hierarchy\n//从 Enviroment 中 读取到更改后的值\nuser name :nacos-config-yaml-update; age: 68\nuser name :nacos-config-yaml-update; age: 68\n\n\nNOTE   你可以通过配置 SPRING.CLOUD.NACOS.CONFIG.REFRESH.ENABLED=FALSE\n       来关闭动态刷新\n\n\n# 可支持profile粒度的配置\n\nspring-cloud-starter-alibaba-nacos-config 在加载配置的时候，不仅仅加载了以 dataid 为 ${spring.application.name}.${file-extension:properties} 为前缀的基础配置，还加载了dataid为 ${spring.application.name}-${profile}.${file-extension:properties} 的基础配置。在日常开发中如果遇到多套环境下的不同配置，可以通过Spring 提供的 ${spring.profiles.active} 这个配置项来配置。\n\nspring.profiles.active=develop\n\n\nNOTE   ${SPRING.PROFILES.ACTIVE} 当通过配置文件来指定时必须放在\n       BOOTSTRAP.PROPERTIES 文件中。\n\nNacos 上新增一个dataid为：nacos-config-develop.yaml的基础配置，如下所示：\n\nData ID:        nacos-config-develop.yaml\n\nGroup  :        DEFAULT_GROUP\n\n配置格式:        YAML\n\n配置内容:        current.env: develop-env\n\n\n启动 Spring Boot 应用测试的代码如下：\n\n@SpringBootApplication\npublic class ProviderApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext applicationContext = SpringApplication.run(ProviderApplication.class, args);\n        while(true) {\n            String userName = applicationContext.getEnvironment().getProperty("user.name");\n            String userAge = applicationContext.getEnvironment().getProperty("user.age");\n            //获取当前部署的环境\n            String currentEnv = applicationContext.getEnvironment().getProperty("current.env");\n            System.err.println("in "+currentEnv+" enviroment; "+"user name :" + userName + "; age: " + userAge);\n            TimeUnit.SECONDS.sleep(1);\n        }\n    }\n}\n\n\n启动后，可见控制台的输出结果：\n\nin develop-env enviroment; user name :nacos-config-yaml-update; age: 68\n2018-11-02 15:34:25.013  INFO 33014 --- [ Thread-11] ConfigServletWebServerApplicationContext : Closing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@6f1c29b7: startup date [Fri Nov 02 15:33:57 CST 2018]; parent: org.springframework.context.annotation.AnnotationConfigApplicationContext@63355449\n\n\n如果需要切换到生产环境，只需要更改 ${spring.profiles.active} 参数配置即可。如下所示：\n\nspring.profiles.active=product\n\n\n同时生产环境上 Nacos 需要添加对应 dataid 的基础配置。例如，在生成环境下的 Naocs 添加了dataid为：nacos-config-product.yaml的配置：\n\nData ID:        nacos-config-product.yaml\n\nGroup  :        DEFAULT_GROUP\n\n配置格式:        YAML\n\n配置内容:        current.env: product-env\n\n\n启动测试程序，输出结果如下：\n\nin product-env enviroment; user name :nacos-config-yaml-update; age: 68\n2018-11-02 15:42:14.628  INFO 33024 --- [Thread-11] ConfigServletWebServerApplicationContext : Closing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@6aa8e115: startup date [Fri Nov 02 15:42:03 CST 2018]; parent: org.springframework.context.annotation.AnnotationConfigApplicationContext@19bb07ed\n\n\nNOTE   此案例中我们通过 SPRING.PROFILES.ACTIVE=<PROFILENAME>\n       的方式写死在配置文件中，而在真正的项目实施过程中这个变量的值是需要不同环境而有不同的值。这个时候通常的做法是通过\n       -DSPRING.PROFILES.ACTIVE=<PROFILE> 参数指定其配置来达到环境间灵活的切换。\n\n\n# 支持自定义 namespace 的配置\n\n首先看一下 Nacos 的 Namespace 的概念， Nacos 概念\n\n> 用于进行租户粒度的配置隔离。不同的命名空间下，可以存在相同的 Group 或 Data ID 的配置。Namespace 的常用场景之一是不同环境的配置的区分隔离，例如开发测试环境和生产环境的资源（如配置、服务）隔离等。\n\n在没有明确指定 ${spring.cloud.nacos.config.namespace} 配置的情况下， 默认使用的是 Nacos 上 Public 这个namespae。如果需要使用自定义的命名空间，可以通过以下配置来实现：\n\nspring.cloud.nacos.config.namespace=b3404bc0-d7dc-4855-b519-570ed34b62d7\n\n\nNOTE   该配置必须放在 BOOTSTRAP.PROPERTIES 文件中。此外\n       SPRING.CLOUD.NACOS.CONFIG.NAMESPACE 的值是 NAMESPACE 对应的 ID，ID\n       值可以在 NACOS 的控制台获取。并且在添加配置时注意不要选择其他的\n       NAMESPAE，否则将会导致读取不到正确的配置。\n\n\n# 支持自定义 Group 的配置\n\n在没有明确指定 ${spring.cloud.nacos.config.group} 配置的情况下， 默认使用的是 DEFAULT_GROUP 。如果需要自定义自己的 Group，可以通过以下配置来实现：\n\nspring.cloud.nacos.config.group=DEVELOP_GROUP\n\n\nNOTE   该配置必须放在 BOOTSTRAP.PROPERTIES 文件中。并且在添加配置时 GROUP 的值一定要和\n       SPRING.CLOUD.NACOS.CONFIG.GROUP 的配置值一致。\n\n\n# 支持自定义扩展的 Data Id 配置\n\nSpring Cloud Alibaba Nacos Config 从 0.2.1 版本后，可支持自定义 Data Id 的配置。关于这部分详细的设计可参考 这里。 一个完整的配置案例如下所示：\n\nspring.application.name=opensource-service-provider\nspring.cloud.nacos.config.server-addr=127.0.0.1:8848\n\n# config external configuration\n# 1、Data Id 在默认的组 DEFAULT_GROUP,不支持配置的动态刷新\nspring.cloud.nacos.config.extension-configs[0].data-id=ext-config-common01.properties\n\n# 2、Data Id 不在默认的组，不支持动态刷新\nspring.cloud.nacos.config.extension-configs[1].data-id=ext-config-common02.properties\nspring.cloud.nacos.config.extension-configs[1].group=GLOBALE_GROUP\n\n# 3、Data Id 既不在默认的组，也支持动态刷新\nspring.cloud.nacos.config.extension-configs[2].data-id=ext-config-common03.properties\nspring.cloud.nacos.config.extension-configs[2].group=REFRESH_GROUP\nspring.cloud.nacos.config.extension-configs[2].refresh=true\n\n\n可以看到:\n\n * 通过 spring.cloud.nacos.config.extension-configs[n].data-id 的配置方式来支持多个 Data Id 的配置。\n * 通过 spring.cloud.nacos.config.extension-configs[n].group 的配置方式自定义 Data Id 所在的组，不明确配置的话，默认是 DEFAULT_GROUP。\n * 通过 spring.cloud.nacos.config.extension-configs[n].refresh 的配置方式来控制该 Data Id 在配置变更时，是否支持应用中可动态刷新， 感知到最新的配置值。默认是不支持的。\n\nNOTE   多个 DATA ID 同时配置时，他的优先级关系是\n       SPRING.CLOUD.NACOS.CONFIG.EXTENSION-CONFIGS[N].DATA-ID 其中 N\n       的值越大，优先级越高。\n\nNOTE   SPRING.CLOUD.NACOS.CONFIG.EXTENSION-CONFIGS[N].DATA-ID\n       的值必须带文件扩展名，文件扩展名既可支持 PROPERTIES，又可以支持 YAML/YML。 此时\n       SPRING.CLOUD.NACOS.CONFIG.FILE-EXTENSION 的配置对自定义扩展配置的 DATA\n       ID 文件扩展名没有影响。\n\n通过自定义扩展的 Data Id 配置，既可以解决多个应用间配置共享的问题，又可以支持一个应用有多个配置文件。\n\n为了更加清晰的在多个应用间配置共享的 Data Id ，你可以通过以下的方式来配置：\n\n# 配置支持共享的 Data Id\nspring.cloud.nacos.config.shared-configs[0].data-id=common.yaml\n\n# 配置 Data Id 所在分组，缺省默认 DEFAULT_GROUP\nspring.cloud.nacos.config.shared-configs[0].group=GROUP_APP1\n\n# 配置Data Id 在配置变更时，是否动态刷新，缺省默认 false\nspring.cloud.nacos.config.shared-configs[0].refresh=true\n\n\n可以看到：\n\n * 通过 spring.cloud.nacos.config.shared-configs[n].data-id 来支持多个共享 Data Id 的配置。\n * 通过 spring.cloud.nacos.config.shared-configs[n].group 来配置自定义 Data Id 所在的组，不明确配置的话，默认是 DEFAULT_GROUP。\n * 通过 spring.cloud.nacos.config.shared-configs[n].refresh 来控制该Data Id在配置变更时，是否支持应用中动态刷新，默认false。\n\n\n# 配置的优先级\n\nSpring Cloud Alibaba Nacos Config 目前提供了三种配置能力从 Nacos 拉取相关的配置。\n\n * A: 通过 spring.cloud.nacos.config.shared-configs[n].data-id 支持多个共享 Data Id 的配置\n * B: 通过 spring.cloud.nacos.config.extension-configs[n].data-id 的方式支持多个扩展 Data Id 的配置\n * C: 通过内部相关规则(应用名、应用名+ Profile )自动生成相关的 Data Id 配置\n\n当三种方式共同使用时，他们的一个优先级关系是:A < B < C\n\n\n# 完全关闭配置\n\n通过设置 spring.cloud.nacos.config.enabled = false 来完全关闭 Spring Cloud Nacos Config',normalizedContent:'# nacos config\n\n\n# spring cloud alibaba nacos config\n\nnacos 提供用于存储配置和其他元数据的 key/value 存储，为分布式系统中的外部化配置提供服务器端和客户端支持。使用 spring cloud alibaba nacos config，您可以在 nacos server 集中管理你 spring cloud 应用的外部属性配置。\n\nspring cloud alibaba nacos config 是 config server 和 client 的替代方案，客户端和服务器上的概念与 spring environment 和 propertysource 有着一致的抽象，在特殊的 bootstrap 阶段，配置被加载到 spring 环境中。当应用程序通过部署管道从开发到测试再到生产时，您可以管理这些环境之间的配置，并确保应用程序具有迁移时需要运行的所有内容。\n\n\n# 快速开始\n\n# nacos 服务端初始化\n\n1、启动nacos server。启动方式可见 nacos 官网\n\n2、启动好nacos之后，在nacos添加如下的配置：\n\ndata id:    nacos-config.properties\n\ngroup  :    default_group\n\n配置格式:    properties\n\n配置内容：   user.name=nacos-config-properties\n            user.age=90\n\n\nnote   注意dataid是以 properties(默认的文件扩展名方式)为扩展名。\n\n# 客户端使用方式\n\n如果要在您的项目中使用 nacos 来实现应用的外部化配置，使用 group id 为 com.alibaba.cloud 和 artifact id 为 spring-cloud-starter-alibaba-nacos-config 的 starter。\n\n<dependency>\n    <groupid>com.alibaba.cloud</groupid>\n    <artifactid>spring-cloud-starter-alibaba-nacos-config</artifactid>\n</dependency>\n\n\n现在就可以创建一个标准的 spring boot 应用。\n\n@springbootapplication\npublic class providerapplication {\n\n    public static void main(string[] args) {\n        configurableapplicationcontext applicationcontext = springapplication.run(providerapplication.class, args);\n        string username = applicationcontext.getenvironment().getproperty("user.name");\n        string userage = applicationcontext.getenvironment().getproperty("user.age");\n        system.err.println("user name :"+username+"; age: "+userage);\n    }\n}\n\n\n在运行此 example 之前， 必须使用 bootstrap.properties 配置文件来配置nacos server 地址，例如：\n\nbootstrap.properties\n\nspring.application.name=nacos-config\nspring.cloud.nacos.config.server-addr=127.0.0.1:8848\n\n\nnote   注意当你使用域名的方式来访问 nacos 时，spring.cloud.nacos.config.server-addr\n       配置的方式为 域名:port。 例如 nacos 的域名为abc.com.nacos，监听的端口为 80，则\n       spring.cloud.nacos.config.server-addr=abc.com.nacos:80。 注意\n       80 端口不能省略。\n\n启动这个 example，可以看到如下输出结果：\n\n2018-11-02 14:24:51.638  info 32700 --- [main] c.a.demo.provider.providerapplication    : started providerapplication in 14.645 seconds (jvm running for 15.139)\nuser name :nacos-config-properties; age: 90\n2018-11-02 14:24:51.688  info 32700 --- [-127.0.0.1:8848] s.c.a.annotationconfigapplicationcontext : refreshing org.springframework.context.annotation.annotationconfigapplicationcontext@a8c5e74: startup date [fri nov 02 14:24:51 cst 2018]; root of context hierarchy\n2018-11\n\n\n\n# 基于 dataid 为 yaml 的文件扩展名配置方式\n\nspring-cloud-starter-alibaba-nacos-config 对于 yaml 格式也是完美支持的。这个时候只需要完成以下两步：\n\n1、在应用的 bootstrap.properties 配置文件中显示的声明 dataid 文件扩展名。如下所示\n\nbootstrap.properties\n\nspring.cloud.nacos.config.file-extension=yaml\n\n\n2、在 nacos 的控制台新增一个dataid为yaml为扩展名的配置，如下所示：\n\ndata id:        nacos-config.yaml\n\ngroup  :        default_group\n\n配置格式:        yaml\n\n配置内容:        user.name: nacos-config-yaml\n                user.age: 68\n\n\n这两步完成后，重启测试程序，可以看到如下输出结果。\n\n2018-11-02 14:59:00.484  info 32928 --- [main] c.a.demo.provider.providerapplication:started providerapplication in 14.183 seconds (jvm running for 14.671)\nuser name :nacos-config-yaml; age: 68\n2018-11-02 14:59:00.529  info 32928 --- [-127.0.0.1:8848] s.c.a.annotationconfigapplicationcontext : refreshing org.springframework.context.annotation.annotationconfigapplicationcontext@265a478e: startup date [fri nov 02 14:59:00 cst 2018]; root of context hierarchy\n\n\n\n# 支持配置的动态更新\n\nspring-cloud-starter-alibaba-nacos-config 也支持配置的动态更新，启动 spring boot 应用测试的代码如下：\n\n@springbootapplication\npublic class providerapplication {\n\n    public static void main(string[] args) {\n        configurableapplicationcontext applicationcontext = springapplication.run(providerapplication.class, args);\n        while(true) {\n            //当动态配置刷新时，会更新到 enviroment中，因此这里每隔一秒中从enviroment中获取配置\n            string username = applicationcontext.getenvironment().getproperty("user.name");\n            string userage = applicationcontext.getenvironment().getproperty("user.age");\n            system.err.println("user name :" + username + "; age: " + userage);\n            timeunit.seconds.sleep(1);\n        }\n    }\n}\n\n\n如下所示，当变更user.name时，应用程序中能够获取到最新的值：\n\nuser name :nacos-config-yaml; age: 68\nuser name :nacos-config-yaml; age: 68\nuser name :nacos-config-yaml; age: 68\n2018-11-02 15:04:25.069  info 32957 --- [-127.0.0.1:8848] o.s.boot.springapplication               : started application in 0.144 seconds (jvm running for 71.752)\n2018-11-02 15:04:25.070  info 32957 --- [-127.0.0.1:8848] s.c.a.annotationconfigapplicationcontext : closing org.springframework.context.annotation.annotationconfigapplicationcontext@10c89124: startup date [fri nov 02 15:04:25 cst 2018]; parent: org.springframework.context.annotation.annotationconfigapplicationcontext@6520af7\n2018-11-02 15:04:25.071  info 32957 --- [-127.0.0.1:8848] s.c.a.annotationconfigapplicationcontext : closing org.springframework.context.annotation.annotationconfigapplicationcontext@6520af7: startup date [fri nov 02 15:04:24 cst 2018]; root of context hierarchy\n//从 enviroment 中 读取到更改后的值\nuser name :nacos-config-yaml-update; age: 68\nuser name :nacos-config-yaml-update; age: 68\n\n\nnote   你可以通过配置 spring.cloud.nacos.config.refresh.enabled=false\n       来关闭动态刷新\n\n\n# 可支持profile粒度的配置\n\nspring-cloud-starter-alibaba-nacos-config 在加载配置的时候，不仅仅加载了以 dataid 为 ${spring.application.name}.${file-extension:properties} 为前缀的基础配置，还加载了dataid为 ${spring.application.name}-${profile}.${file-extension:properties} 的基础配置。在日常开发中如果遇到多套环境下的不同配置，可以通过spring 提供的 ${spring.profiles.active} 这个配置项来配置。\n\nspring.profiles.active=develop\n\n\nnote   ${spring.profiles.active} 当通过配置文件来指定时必须放在\n       bootstrap.properties 文件中。\n\nnacos 上新增一个dataid为：nacos-config-develop.yaml的基础配置，如下所示：\n\ndata id:        nacos-config-develop.yaml\n\ngroup  :        default_group\n\n配置格式:        yaml\n\n配置内容:        current.env: develop-env\n\n\n启动 spring boot 应用测试的代码如下：\n\n@springbootapplication\npublic class providerapplication {\n\n    public static void main(string[] args) {\n        configurableapplicationcontext applicationcontext = springapplication.run(providerapplication.class, args);\n        while(true) {\n            string username = applicationcontext.getenvironment().getproperty("user.name");\n            string userage = applicationcontext.getenvironment().getproperty("user.age");\n            //获取当前部署的环境\n            string currentenv = applicationcontext.getenvironment().getproperty("current.env");\n            system.err.println("in "+currentenv+" enviroment; "+"user name :" + username + "; age: " + userage);\n            timeunit.seconds.sleep(1);\n        }\n    }\n}\n\n\n启动后，可见控制台的输出结果：\n\nin develop-env enviroment; user name :nacos-config-yaml-update; age: 68\n2018-11-02 15:34:25.013  info 33014 --- [ thread-11] configservletwebserverapplicationcontext : closing org.springframework.boot.web.servlet.context.annotationconfigservletwebserverapplicationcontext@6f1c29b7: startup date [fri nov 02 15:33:57 cst 2018]; parent: org.springframework.context.annotation.annotationconfigapplicationcontext@63355449\n\n\n如果需要切换到生产环境，只需要更改 ${spring.profiles.active} 参数配置即可。如下所示：\n\nspring.profiles.active=product\n\n\n同时生产环境上 nacos 需要添加对应 dataid 的基础配置。例如，在生成环境下的 naocs 添加了dataid为：nacos-config-product.yaml的配置：\n\ndata id:        nacos-config-product.yaml\n\ngroup  :        default_group\n\n配置格式:        yaml\n\n配置内容:        current.env: product-env\n\n\n启动测试程序，输出结果如下：\n\nin product-env enviroment; user name :nacos-config-yaml-update; age: 68\n2018-11-02 15:42:14.628  info 33024 --- [thread-11] configservletwebserverapplicationcontext : closing org.springframework.boot.web.servlet.context.annotationconfigservletwebserverapplicationcontext@6aa8e115: startup date [fri nov 02 15:42:03 cst 2018]; parent: org.springframework.context.annotation.annotationconfigapplicationcontext@19bb07ed\n\n\nnote   此案例中我们通过 spring.profiles.active=<profilename>\n       的方式写死在配置文件中，而在真正的项目实施过程中这个变量的值是需要不同环境而有不同的值。这个时候通常的做法是通过\n       -dspring.profiles.active=<profile> 参数指定其配置来达到环境间灵活的切换。\n\n\n# 支持自定义 namespace 的配置\n\n首先看一下 nacos 的 namespace 的概念， nacos 概念\n\n> 用于进行租户粒度的配置隔离。不同的命名空间下，可以存在相同的 group 或 data id 的配置。namespace 的常用场景之一是不同环境的配置的区分隔离，例如开发测试环境和生产环境的资源（如配置、服务）隔离等。\n\n在没有明确指定 ${spring.cloud.nacos.config.namespace} 配置的情况下， 默认使用的是 nacos 上 public 这个namespae。如果需要使用自定义的命名空间，可以通过以下配置来实现：\n\nspring.cloud.nacos.config.namespace=b3404bc0-d7dc-4855-b519-570ed34b62d7\n\n\nnote   该配置必须放在 bootstrap.properties 文件中。此外\n       spring.cloud.nacos.config.namespace 的值是 namespace 对应的 id，id\n       值可以在 nacos 的控制台获取。并且在添加配置时注意不要选择其他的\n       namespae，否则将会导致读取不到正确的配置。\n\n\n# 支持自定义 group 的配置\n\n在没有明确指定 ${spring.cloud.nacos.config.group} 配置的情况下， 默认使用的是 default_group 。如果需要自定义自己的 group，可以通过以下配置来实现：\n\nspring.cloud.nacos.config.group=develop_group\n\n\nnote   该配置必须放在 bootstrap.properties 文件中。并且在添加配置时 group 的值一定要和\n       spring.cloud.nacos.config.group 的配置值一致。\n\n\n# 支持自定义扩展的 data id 配置\n\nspring cloud alibaba nacos config 从 0.2.1 版本后，可支持自定义 data id 的配置。关于这部分详细的设计可参考 这里。 一个完整的配置案例如下所示：\n\nspring.application.name=opensource-service-provider\nspring.cloud.nacos.config.server-addr=127.0.0.1:8848\n\n# config external configuration\n# 1、data id 在默认的组 default_group,不支持配置的动态刷新\nspring.cloud.nacos.config.extension-configs[0].data-id=ext-config-common01.properties\n\n# 2、data id 不在默认的组，不支持动态刷新\nspring.cloud.nacos.config.extension-configs[1].data-id=ext-config-common02.properties\nspring.cloud.nacos.config.extension-configs[1].group=globale_group\n\n# 3、data id 既不在默认的组，也支持动态刷新\nspring.cloud.nacos.config.extension-configs[2].data-id=ext-config-common03.properties\nspring.cloud.nacos.config.extension-configs[2].group=refresh_group\nspring.cloud.nacos.config.extension-configs[2].refresh=true\n\n\n可以看到:\n\n * 通过 spring.cloud.nacos.config.extension-configs[n].data-id 的配置方式来支持多个 data id 的配置。\n * 通过 spring.cloud.nacos.config.extension-configs[n].group 的配置方式自定义 data id 所在的组，不明确配置的话，默认是 default_group。\n * 通过 spring.cloud.nacos.config.extension-configs[n].refresh 的配置方式来控制该 data id 在配置变更时，是否支持应用中可动态刷新， 感知到最新的配置值。默认是不支持的。\n\nnote   多个 data id 同时配置时，他的优先级关系是\n       spring.cloud.nacos.config.extension-configs[n].data-id 其中 n\n       的值越大，优先级越高。\n\nnote   spring.cloud.nacos.config.extension-configs[n].data-id\n       的值必须带文件扩展名，文件扩展名既可支持 properties，又可以支持 yaml/yml。 此时\n       spring.cloud.nacos.config.file-extension 的配置对自定义扩展配置的 data\n       id 文件扩展名没有影响。\n\n通过自定义扩展的 data id 配置，既可以解决多个应用间配置共享的问题，又可以支持一个应用有多个配置文件。\n\n为了更加清晰的在多个应用间配置共享的 data id ，你可以通过以下的方式来配置：\n\n# 配置支持共享的 data id\nspring.cloud.nacos.config.shared-configs[0].data-id=common.yaml\n\n# 配置 data id 所在分组，缺省默认 default_group\nspring.cloud.nacos.config.shared-configs[0].group=group_app1\n\n# 配置data id 在配置变更时，是否动态刷新，缺省默认 false\nspring.cloud.nacos.config.shared-configs[0].refresh=true\n\n\n可以看到：\n\n * 通过 spring.cloud.nacos.config.shared-configs[n].data-id 来支持多个共享 data id 的配置。\n * 通过 spring.cloud.nacos.config.shared-configs[n].group 来配置自定义 data id 所在的组，不明确配置的话，默认是 default_group。\n * 通过 spring.cloud.nacos.config.shared-configs[n].refresh 来控制该data id在配置变更时，是否支持应用中动态刷新，默认false。\n\n\n# 配置的优先级\n\nspring cloud alibaba nacos config 目前提供了三种配置能力从 nacos 拉取相关的配置。\n\n * a: 通过 spring.cloud.nacos.config.shared-configs[n].data-id 支持多个共享 data id 的配置\n * b: 通过 spring.cloud.nacos.config.extension-configs[n].data-id 的方式支持多个扩展 data id 的配置\n * c: 通过内部相关规则(应用名、应用名+ profile )自动生成相关的 data id 配置\n\n当三种方式共同使用时，他们的一个优先级关系是:a < b < c\n\n\n# 完全关闭配置\n\n通过设置 spring.cloud.nacos.config.enabled = false 来完全关闭 spring cloud nacos config',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:47:02",lastUpdatedTimestamp:1644378422e3},{title:"什么是 Docker",frontmatter:{title:"什么是 Docker",date:"2022-02-08T16:52:21.000Z",permalink:"/pages/555c55/"},regularPath:"/17.Docker/01.Docker/01.%E4%BB%80%E4%B9%88%E6%98%AF%20Docker.html",relativePath:"17.Docker/01.Docker/01.什么是 Docker.md",key:"v-5b9e5448",path:"/pages/555c55/",headersStr:null,content:"# 什么是 Docker\n\nDocker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 Apache 2.0 授权协议开源，主要项目代码在 GitHub 上进行维护。Docker 项目后来还加入了 Linux 基金会，并成立推动 开放容器联盟（OCI）。\n\nDocker 自开源后受到广泛的关注和讨论，至今其 GitHub 项目已经超过 4 万 6 千个星标和一万多个 fork。甚至由于 Docker 项目的火爆，在 2013 年底，dotCloud 公司决定改名为 Docker。Docker 最初是在 Ubuntu 12.04 上开发实现的；Red Hat 则从 RHEL 6.5 开始对 Docker 进行支持；Google 也在其 PaaS 产品中广泛应用 Docker。\n\nDocker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 AUFS 类的 Union FS 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd。\n\nDocker 在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护。使得 Docker 技术比虚拟机技术更为轻便、快捷。\n\n下面的图片比较了 Docker 和传统虚拟化方式的不同之处。传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。\n\n\n\n",normalizedContent:"# 什么是 docker\n\ndocker 最初是 dotcloud 公司创始人 solomon hykes 在法国期间发起的一个公司内部项目，它是基于 dotcloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 apache 2.0 授权协议开源，主要项目代码在 github 上进行维护。docker 项目后来还加入了 linux 基金会，并成立推动 开放容器联盟（oci）。\n\ndocker 自开源后受到广泛的关注和讨论，至今其 github 项目已经超过 4 万 6 千个星标和一万多个 fork。甚至由于 docker 项目的火爆，在 2013 年底，dotcloud 公司决定改名为 docker。docker 最初是在 ubuntu 12.04 上开发实现的；red hat 则从 rhel 6.5 开始对 docker 进行支持；google 也在其 paas 产品中广泛应用 docker。\n\ndocker 使用 google 公司推出的 go 语言 进行开发实现，基于 linux 内核的 cgroup，namespace，以及 aufs 类的 union fs 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 lxc，从 0.7 版本以后开始去除 lxc，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runc 和 containerd。\n\ndocker 在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护。使得 docker 技术比虚拟机技术更为轻便、快捷。\n\n下面的图片比较了 docker 和传统虚拟化方式的不同之处。传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。\n\n\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"为什么要使用 Docker",frontmatter:{title:"为什么要使用 Docker",date:"2022-02-08T16:52:21.000Z",permalink:"/pages/d1d9db/"},regularPath:"/17.Docker/01.Docker/02.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8%20Docker.html",relativePath:"17.Docker/01.Docker/02.为什么要使用 Docker.md",key:"v-3058349d",path:"/pages/d1d9db/",headers:[{level:2,title:"更高效的利用系统资源",slug:"更高效的利用系统资源",normalizedTitle:"更高效的利用系统资源",charIndex:61},{level:2,title:"更快速的启动时间",slug:"更快速的启动时间",normalizedTitle:"更快速的启动时间",charIndex:198},{level:2,title:"一致的运行环境",slug:"一致的运行环境",normalizedTitle:"一致的运行环境",charIndex:311},{level:2,title:"持续交付和部署",slug:"持续交付和部署",normalizedTitle:"持续交付和部署",charIndex:457},{level:2,title:"更轻松的迁移",slug:"更轻松的迁移",normalizedTitle:"更轻松的迁移",charIndex:778},{level:2,title:"更轻松的维护和扩展",slug:"更轻松的维护和扩展",normalizedTitle:"更轻松的维护和扩展",charIndex:938},{level:2,title:"对比传统虚拟机总结",slug:"对比传统虚拟机总结",normalizedTitle:"对比传统虚拟机总结",charIndex:1111}],headersStr:"更高效的利用系统资源 更快速的启动时间 一致的运行环境 持续交付和部署 更轻松的迁移 更轻松的维护和扩展 对比传统虚拟机总结",content:"# 为什么要使用 Docker\n\n作为一种新兴的虚拟化方式，Docker 跟传统的虚拟化方式相比具有众多的优势。\n\n\n# 更高效的利用系统资源\n\n由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，Docker 对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。\n\n\n# 更快速的启动时间\n\n传统的虚拟机技术启动应用服务往往需要数分钟，而 Docker 容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。\n\n\n# 一致的运行环境\n\n开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 「这段代码在我机器上没问题啊」 这类问题。\n\n\n# 持续交付和部署\n\n对开发和运维（DevOps）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。\n\n使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 Dockerfile 来进行镜像构建，并结合 持续集成(Continuous Integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合 持续部署(Continuous Delivery/Deployment) 系统进行自动部署。\n\n而且使用 Dockerfile 使镜像构建透明化，不仅仅开发团队可以理解应用运行环境，也方便运维团队理解应用运行所需条件，帮助更好的生产环境中部署该镜像。\n\n\n# 更轻松的迁移\n\n由于 Docker 确保了执行环境的一致性，使得应用的迁移更加容易。Docker 可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，甚至是笔记本，其运行结果是一致的。因此用户可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。\n\n\n# 更轻松的维护和扩展\n\nDocker 使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单。此外，Docker 团队同各个开源项目团队一起维护了一大批高质量的 官方镜像，既可以直接在生产环境使用，又可以作为基础进一步定制，大大的降低了应用服务的镜像制作成本。\n\n\n# 对比传统虚拟机总结\n\n特性      容器          虚拟机\n启动      秒级          分钟级\n硬盘使用    一般为 MB      一般为 GB\n性能      接近原生        弱于\n系统支持量   单机支持上千个容器   一般几十个",normalizedContent:"# 为什么要使用 docker\n\n作为一种新兴的虚拟化方式，docker 跟传统的虚拟化方式相比具有众多的优势。\n\n\n# 更高效的利用系统资源\n\n由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，docker 对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。\n\n\n# 更快速的启动时间\n\n传统的虚拟机技术启动应用服务往往需要数分钟，而 docker 容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。\n\n\n# 一致的运行环境\n\n开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 「这段代码在我机器上没问题啊」 这类问题。\n\n\n# 持续交付和部署\n\n对开发和运维（devops）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。\n\n使用 docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 dockerfile 来进行镜像构建，并结合 持续集成(continuous integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合 持续部署(continuous delivery/deployment) 系统进行自动部署。\n\n而且使用 dockerfile 使镜像构建透明化，不仅仅开发团队可以理解应用运行环境，也方便运维团队理解应用运行所需条件，帮助更好的生产环境中部署该镜像。\n\n\n# 更轻松的迁移\n\n由于 docker 确保了执行环境的一致性，使得应用的迁移更加容易。docker 可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，甚至是笔记本，其运行结果是一致的。因此用户可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。\n\n\n# 更轻松的维护和扩展\n\ndocker 使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单。此外，docker 团队同各个开源项目团队一起维护了一大批高质量的 官方镜像，既可以直接在生产环境使用，又可以作为基础进一步定制，大大的降低了应用服务的镜像制作成本。\n\n\n# 对比传统虚拟机总结\n\n特性      容器          虚拟机\n启动      秒级          分钟级\n硬盘使用    一般为 mb      一般为 gb\n性能      接近原生        弱于\n系统支持量   单机支持上千个容器   一般几十个",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 基本概念",frontmatter:{title:"Docker 基本概念",date:"2022-02-08T17:06:01.000Z",permalink:"/pages/ad1da1/"},regularPath:"/17.Docker/02.Docker%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/01.Docker%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.html",relativePath:"17.Docker/02.Docker 基本概念/01.Docker 基本概念.md",key:"v-741ac0fb",path:"/pages/ad1da1/",headersStr:null,content:"# Docker 基本概念\n\nDocker 包括三个基本概念\n\n * 镜像（Image）\n * 容器（Container）\n * 仓库（Repository）\n\n理解了这三个概念，就理解了 Docker 的整个生命周期。",normalizedContent:"# docker 基本概念\n\ndocker 包括三个基本概念\n\n * 镜像（image）\n * 容器（container）\n * 仓库（repository）\n\n理解了这三个概念，就理解了 docker 的整个生命周期。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 引擎",frontmatter:{title:"Docker 引擎",date:"2022-02-08T17:06:01.000Z",permalink:"/pages/77a885/"},regularPath:"/17.Docker/02.Docker%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/02.Docker%20%E5%BC%95%E6%93%8E.html",relativePath:"17.Docker/02.Docker 基本概念/02.Docker 引擎.md",key:"v-0dc6bf26",path:"/pages/77a885/",headersStr:null,content:"# Docker 引擎\n\nDocker 引擎是一个包含以下主要组件的客户端服务器应用程序。\n\n * 一种服务器，它是一种称为守护进程并且长时间运行的程序。\n * REST API用于指定程序可以用来与守护进程通信的接口，并指示它做什么。\n * 一个有命令行界面 (CLI) 工具的客户端。\n\nDocker 引擎组件的流程如下图所示：\n\n",normalizedContent:"# docker 引擎\n\ndocker 引擎是一个包含以下主要组件的客户端服务器应用程序。\n\n * 一种服务器，它是一种称为守护进程并且长时间运行的程序。\n * rest api用于指定程序可以用来与守护进程通信的接口，并指示它做什么。\n * 一个有命令行界面 (cli) 工具的客户端。\n\ndocker 引擎组件的流程如下图所示：\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Docker 系统架构",frontmatter:{title:"Docker 系统架构",date:"2022-02-08T17:06:01.000Z",permalink:"/pages/f566e4/"},regularPath:"/17.Docker/02.Docker%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/03.Docker%20%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84.html",relativePath:"17.Docker/02.Docker 基本概念/03.Docker 系统架构.md",key:"v-17b7bcd2",path:"/pages/f566e4/",headersStr:null,content:"# Docker 系统架构\n\nDocker 使用客户端-服务器 (C/S) 架构模式，使用远程 API 来管理和创建 Docker 容器。\n\nDocker 容器通过 Docker 镜像来创建。\n\n容器与镜像的关系类似于面向对象编程中的对象与类。\n\nDOCKER   面向对象\n容器       对象\n镜像       类\n\n\n\n标题               说明\n镜像(Images)       Docker 镜像是用于创建 Docker 容器的模板。\n容器(Container)    容器是独立运行的一个或一组应用。\n客户端(Client)      Docker 客户端通过命令行或者其他工具使用 Docker API 与 Docker 的守护进程通信。\n主机(Host)         一个物理或者虚拟的机器用于执行 Docker 守护进程和容器。\n仓库(Registry)     Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。Docker Hub 提供了庞大的镜像集合供使用。\nDocker Machine   Docker\n                 Machine是一个简化Docker安装的命令行工具，通过一个简单的命令行即可在相应的平台上安装Docker，比如VirtualBox、\n                 Digital Ocean、Microsoft Azure。",normalizedContent:"# docker 系统架构\n\ndocker 使用客户端-服务器 (c/s) 架构模式，使用远程 api 来管理和创建 docker 容器。\n\ndocker 容器通过 docker 镜像来创建。\n\n容器与镜像的关系类似于面向对象编程中的对象与类。\n\ndocker   面向对象\n容器       对象\n镜像       类\n\n\n\n标题               说明\n镜像(images)       docker 镜像是用于创建 docker 容器的模板。\n容器(container)    容器是独立运行的一个或一组应用。\n客户端(client)      docker 客户端通过命令行或者其他工具使用 docker api 与 docker 的守护进程通信。\n主机(host)         一个物理或者虚拟的机器用于执行 docker 守护进程和容器。\n仓库(registry)     docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。docker hub 提供了庞大的镜像集合供使用。\ndocker machine   docker\n                 machine是一个简化docker安装的命令行工具，通过一个简单的命令行即可在相应的平台上安装docker，比如virtualbox、\n                 digital ocean、microsoft azure。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Docker 镜像",frontmatter:{title:"Docker 镜像",date:"2022-02-08T17:06:01.000Z",permalink:"/pages/546146/"},regularPath:"/17.Docker/02.Docker%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/04.Docker%20%E9%95%9C%E5%83%8F.html",relativePath:"17.Docker/02.Docker 基本概念/04.Docker 镜像.md",key:"v-397e4290",path:"/pages/546146/",headers:[{level:2,title:"分层存储",slug:"分层存储",normalizedTitle:"分层存储",charIndex:287}],headersStr:"分层存储",content:"# Docker 镜像\n\n我们都知道，操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而 Docker 镜像（Image），就相当于是一个 root 文件系统。比如官方镜像 ubuntu:16.04 就包含了完整的一套 Ubuntu 16.04 最小系统的 root 文件系统。\n\nDocker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。\n\n\n# 分层存储\n\n因为镜像包含操作系统完整的 root 文件系统，其体积往往是庞大的，因此在 Docker 设计时，就充分利用 Union FS 的技术，将其设计为分层存储的架构。所以严格来说，镜像并非是像一个 ISO 那样的打包文件，镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。\n\n镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。\n\n分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。\n\n关于镜像构建，将会在后续相关章节中做进一步的讲解。",normalizedContent:"# docker 镜像\n\n我们都知道，操作系统分为内核和用户空间。对于 linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而 docker 镜像（image），就相当于是一个 root 文件系统。比如官方镜像 ubuntu:16.04 就包含了完整的一套 ubuntu 16.04 最小系统的 root 文件系统。\n\ndocker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。\n\n\n# 分层存储\n\n因为镜像包含操作系统完整的 root 文件系统，其体积往往是庞大的，因此在 docker 设计时，就充分利用 union fs 的技术，将其设计为分层存储的架构。所以严格来说，镜像并非是像一个 iso 那样的打包文件，镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。\n\n镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。\n\n分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。\n\n关于镜像构建，将会在后续相关章节中做进一步的讲解。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 容器",frontmatter:{title:"Docker 容器",date:"2022-02-08T17:06:01.000Z",permalink:"/pages/c9e8a8/"},regularPath:"/17.Docker/02.Docker%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/05.Docker%20%E5%AE%B9%E5%99%A8.html",relativePath:"17.Docker/02.Docker 基本概念/05.Docker 容器.md",key:"v-4b0f9931",path:"/pages/c9e8a8/",headersStr:null,content:"# Docker 容器\n\n镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。\n\n容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。也因为这种隔离的特性，很多人初学 Docker 时常常会混淆容器和虚拟机。\n\n前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。\n\n容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。\n\n按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。\n\n数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。",normalizedContent:"# docker 容器\n\n镜像（image）和容器（container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。\n\n容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 id 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。也因为这种隔离的特性，很多人初学 docker 时常常会混淆容器和虚拟机。\n\n前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。\n\n容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。\n\n按照 docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。\n\n数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 仓库",frontmatter:{title:"Docker 仓库",date:"2022-02-08T17:06:01.000Z",permalink:"/pages/f00cd0/"},regularPath:"/17.Docker/02.Docker%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/06.Docker%20%E4%BB%93%E5%BA%93.html",relativePath:"17.Docker/02.Docker 基本概念/06.Docker 仓库.md",key:"v-2046e738",path:"/pages/f00cd0/",headers:[{level:2,title:"公有 Docker Registry",slug:"公有-docker-registry",normalizedTitle:"公有 docker registry",charIndex:565},{level:2,title:"私有 Docker Registry",slug:"私有-docker-registry",normalizedTitle:"私有 docker registry",charIndex:1104}],headersStr:"公有 Docker Registry 私有 Docker Registry",content:"# Docker 仓库\n\n镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。\n\n一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。\n\n通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 <仓库名>:<标签> 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。\n\n以 Ubuntu 镜像 为例，ubuntu 是仓库的名字，其内包含有不同的版本标签，如，14.04, 16.04。我们可以通过 ubuntu:14.04，或者 ubuntu:16.04 来具体指定所需哪个版本的镜像。如果忽略了标签，比如 ubuntu，那将视为 ubuntu:latest。\n\n仓库名经常以 两段式路径 形式出现，比如 jwilder/nginx-proxy，前者往往意味着 Docker Registry 多用户环境下的用户名，后者则往往是对应的软件名。但这并非绝对，取决于所使用的具体 Docker Registry 的软件或服务。\n\n\n# 公有 Docker Registry\n\nDocker Registry 公开服务是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。\n\n最常使用的 Registry 公开服务是官方的 Docker Hub，这也是默认的 Registry，并拥有大量的高质量的官方镜像。除此以外，还有 CoreOS 的 Quay.io，CoreOS 相关的镜像存储在这里；Google 的 Google Container Registry，Kubernetes 的镜像使用的就是这个服务。\n\n由于某些原因，在国内访问这些服务可能会比较慢。国内的一些云服务商提供了针对 Docker Hub 的镜像服务（Registry Mirror），这些镜像服务被称为加速器。常见的有 阿里云加速器、DaoCloud 加速器 等。使用加速器会直接从国内的地址下载 Docker Hub 的镜像，比直接从 Docker Hub 下载速度会提高很多。\n\n国内也有一些云服务商提供类似于 Docker Hub 的公开服务。比如 时速云镜像仓库、网易云镜像服务、DaoCloud 镜像市场、阿里云镜像库 等。\n\n\n# 私有 Docker Registry\n\n除了使用公开服务外，用户还可以在本地搭建私有 Docker Registry。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。\n\n开源的 Docker Registry 镜像只提供了 Docker Registry API 的服务端实现，足以支持 docker 命令，不影响使用。但不包含图形界面，以及镜像维护、用户管理、访问控制等高级功能。在官方的商业化版本 Docker Trusted Registry 中，提供了这些高级功能。\n\n除了官方的 Docker Registry 外，还有第三方软件实现了 Docker Registry API，甚至提供了用户界面以及一些高级功能。比如，VMWare Harbor 和 Sonatype Nexus。",normalizedContent:"# docker 仓库\n\n镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，docker registry 就是这样的服务。\n\n一个 docker registry 中可以包含多个仓库（repository）；每个仓库可以包含多个标签（tag）；每个标签对应一个镜像。\n\n通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 <仓库名>:<标签> 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。\n\n以 ubuntu 镜像 为例，ubuntu 是仓库的名字，其内包含有不同的版本标签，如，14.04, 16.04。我们可以通过 ubuntu:14.04，或者 ubuntu:16.04 来具体指定所需哪个版本的镜像。如果忽略了标签，比如 ubuntu，那将视为 ubuntu:latest。\n\n仓库名经常以 两段式路径 形式出现，比如 jwilder/nginx-proxy，前者往往意味着 docker registry 多用户环境下的用户名，后者则往往是对应的软件名。但这并非绝对，取决于所使用的具体 docker registry 的软件或服务。\n\n\n# 公有 docker registry\n\ndocker registry 公开服务是开放给用户使用、允许用户管理镜像的 registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。\n\n最常使用的 registry 公开服务是官方的 docker hub，这也是默认的 registry，并拥有大量的高质量的官方镜像。除此以外，还有 coreos 的 quay.io，coreos 相关的镜像存储在这里；google 的 google container registry，kubernetes 的镜像使用的就是这个服务。\n\n由于某些原因，在国内访问这些服务可能会比较慢。国内的一些云服务商提供了针对 docker hub 的镜像服务（registry mirror），这些镜像服务被称为加速器。常见的有 阿里云加速器、daocloud 加速器 等。使用加速器会直接从国内的地址下载 docker hub 的镜像，比直接从 docker hub 下载速度会提高很多。\n\n国内也有一些云服务商提供类似于 docker hub 的公开服务。比如 时速云镜像仓库、网易云镜像服务、daocloud 镜像市场、阿里云镜像库 等。\n\n\n# 私有 docker registry\n\n除了使用公开服务外，用户还可以在本地搭建私有 docker registry。docker 官方提供了 docker registry 镜像，可以直接使用做为私有 registry 服务。\n\n开源的 docker registry 镜像只提供了 docker registry api 的服务端实现，足以支持 docker 命令，不影响使用。但不包含图形界面，以及镜像维护、用户管理、访问控制等高级功能。在官方的商业化版本 docker trusted registry 中，提供了这些高级功能。\n\n除了官方的 docker registry 外，还有第三方软件实现了 docker registry api，甚至提供了用户界面以及一些高级功能。比如，vmware harbor 和 sonatype nexus。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"安装 Docker",frontmatter:{title:"安装 Docker",date:"2022-02-08T17:17:46.000Z",permalink:"/pages/11ef44/"},regularPath:"/17.Docker/03.%E5%AE%89%E8%A3%85%20Docker/01.%E5%AE%89%E8%A3%85%20Docker.html",relativePath:"17.Docker/03.安装 Docker/01.安装 Docker.md",key:"v-05980e1b",path:"/pages/11ef44/",headersStr:null,content:"# 安装 Docker\n\nDocker 在 1.13 版本之后，从 2017 年的 3 月 1 日开始，版本命名规则变为如下：\n\n项目          说明\n版本格式        YY.MM\nStable 版本   每个季度发行\nEdge 版本     每个月发行\n\n同时 Docker 划分为 CE 和 EE。CE 即社区版（免费，支持周期三个月），EE 即企业版，强调安全，付费使用。\n\nDocker CE 每月发布一个 Edge 版本 (17.03, 17.04, 17.05...)，每三个月发布一个 Stable 版本 (17.03, 17.06, 17.09...)，Docker EE 和 Stable 版本号保持一致，但每个版本提供一年维护。\n\n官方网站上有各种环境下的 安装指南，这里主要介绍 Docker CE 在 Linux 、Windows 10 (PC) 和 macOS 上的安装。",normalizedContent:"# 安装 docker\n\ndocker 在 1.13 版本之后，从 2017 年的 3 月 1 日开始，版本命名规则变为如下：\n\n项目          说明\n版本格式        yy.mm\nstable 版本   每个季度发行\nedge 版本     每个月发行\n\n同时 docker 划分为 ce 和 ee。ce 即社区版（免费，支持周期三个月），ee 即企业版，强调安全，付费使用。\n\ndocker ce 每月发布一个 edge 版本 (17.03, 17.04, 17.05...)，每三个月发布一个 stable 版本 (17.03, 17.06, 17.09...)，docker ee 和 stable 版本号保持一致，但每个版本提供一年维护。\n\n官方网站上有各种环境下的 安装指南，这里主要介绍 docker ce 在 linux 、windows 10 (pc) 和 macos 上的安装。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Ubuntu 安装 Docker",frontmatter:{title:"Ubuntu 安装 Docker",date:"2022-02-08T17:17:46.000Z",permalink:"/pages/748872/"},regularPath:"/17.Docker/03.%E5%AE%89%E8%A3%85%20Docker/02.Ubuntu%20%E5%AE%89%E8%A3%85%20Docker.html",relativePath:"17.Docker/03.安装 Docker/02.Ubuntu 安装 Docker.md",key:"v-4637e644",path:"/pages/748872/",headers:[{level:2,title:"准备工作",slug:"准备工作",normalizedTitle:"准备工作",charIndex:74},{level:3,title:"系统要求",slug:"系统要求",normalizedTitle:"系统要求",charIndex:83},{level:3,title:"关闭swap",slug:"关闭swap",normalizedTitle:"关闭swap",charIndex:335},{level:3,title:"卸载旧版本",slug:"卸载旧版本",normalizedTitle:"卸载旧版本",charIndex:686},{level:3,title:"支持的存储驱动程序",slug:"支持的存储驱动程序",normalizedTitle:"支持的存储驱动程序",charIndex:828},{level:2,title:"使用存储库安装",slug:"使用存储库安装",normalizedTitle:"使用存储库安装",charIndex:899},{level:3,title:"设置存储库",slug:"设置存储库",normalizedTitle:"设置存储库",charIndex:969},{level:3,title:"更新并安装 Docker CE",slug:"更新并安装-docker-ce",normalizedTitle:"更新并安装 docker ce",charIndex:1689},{level:2,title:"使用脚本自动安装",slug:"使用脚本自动安装",normalizedTitle:"使用脚本自动安装",charIndex:1933},{level:2,title:"启动 Docker CE",slug:"启动-docker-ce",normalizedTitle:"启动 docker ce",charIndex:2142},{level:2,title:"建立 docker 用户组",slug:"建立-docker-用户组",normalizedTitle:"建立 docker 用户组",charIndex:2218},{level:2,title:"测试 Docker 是否安装正确",slug:"测试-docker-是否安装正确",normalizedTitle:"测试 docker 是否安装正确",charIndex:2518},{level:2,title:"镜像加速",slug:"镜像加速",normalizedTitle:"镜像加速",charIndex:3650},{level:2,title:"常见问题",slug:"常见问题",normalizedTitle:"常见问题",charIndex:3715},{level:2,title:"卸载 Docker Engine",slug:"卸载-docker-engine",normalizedTitle:"卸载 docker engine",charIndex:3976},{level:2,title:"参考文档",slug:"参考文档",normalizedTitle:"参考文档",charIndex:4367}],headersStr:"准备工作 系统要求 关闭swap 卸载旧版本 支持的存储驱动程序 使用存储库安装 设置存储库 更新并安装 Docker CE 使用脚本自动安装 启动 Docker CE 建立 docker 用户组 测试 Docker 是否安装正确 镜像加速 常见问题 卸载 Docker Engine 参考文档",content:'# Ubuntu 安装 Docker\n\n注意\n\n切勿在没有配置 Docker APT 源的情况下直接使用 apt 命令安装 Docker.\n\n\n# 准备工作\n\n\n# 系统要求\n\nDocker CE 支持以下版本的 Ubuntu 操作系统：\n\n * Ubuntu Focal 20.04 (LTS)\n * Ubuntu Bionic 18.04 (LTS)\n\nDocker CE 可以安装在 64 位的 x86 平台或 ARM 平台上。Ubuntu 发行版中，LTS（Long-Term-Support）长期支持版本，会获得 5 年的升级维护支持，这样的版本会更稳定，因此在生产环境中推荐使用 LTS 版本,当前最新的 LTS 版本为 Ubuntu 20.04。\n\n\n# 关闭swap\n\nswapoff -a\nyes | cp /etc/fstab /etc/fstab_bak\ncat /etc/fstab_bak |grep -v swap > /etc/fstab\n\n\n使用free -h命令检查配置是否生效\n\n\n\n\n\n \n\n\nfree -h   \n              total        used        free      shared  buff/cache   available\nMem:           1.8G        142M        1.5G        8.5M        122M        1.5G\nSwap:            0B          0B          0B\n\n\n\n# 卸载旧版本\n\n旧版本的 Docker 称为 docker，docker.io 或者 docker-engine，使用以下命令卸载旧版本：\n\nsudo apt-get remove docker docker-engine docker.io containerd runc\n\n\n\n# 支持的存储驱动程序\n\nUbuntu 16.04 + 上的 Docker CE 默认使用 overlay2 存储层驱动,无需手动配置。\n\n\n# 使用存储库安装\n\n在新主机上首次安装Docker引擎之前，需要设置Docker存储库。之后，您可以从存储库安装和更新Docker。\n\n\n# 设置存储库\n\nStep 1 : 更新apt程序包索引并安装程序包，以允许apt通过HTTPS使用存储库：\n\nsudo apt-get update\n\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n\n\nStep 2 : 添加阿里云的官方GPG密钥：\n\ncurl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n\n\nStep 3 : 验证 GPG 证书\n\nsudo apt-key fingerprint 0EBFCD88\n\npub   rsa4096 2017-02-22 [SCEA]\n      9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88\nuid           [ unknown] Docker Release (CE deb) <docker@docker.com>\nsub   rsa4096 2017-02-22 [S]\n\n\nStep 4 : 写入软件源信息\n\nsudo add-apt-repository \\\n   "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable"\n\n\n\n# 更新并安装 Docker CE\n\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n\n\n提示\n\n以上命令会添加稳定版本的 Docker CE APT 镜像源，如果需要最新或者测试版本的 Docker CE 请将 stable 改为 edge 或者 test。从 Docker 17.06 开始，edge test 版本的 APT 镜像源也会包含稳定版本的 Docker。\n\n\n# 使用脚本自动安装\n\n在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，Ubuntu 系统上可以使用这套脚本安装：\n\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n\n\n执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的 Edge 版本安装在系统中。\n\n\n# 启动 Docker CE\n\nsudo systemctl enable docker\nsudo systemctl start docker\n\n\n\n# 建立 docker 用户组\n\n默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。\n\n建立 docker 组：\n\nsudo groupadd docker\n\n\n将当前用户加入 docker 组：\n\nsudo usermod -aG docker $USER\n\n\n退出当前终端并重新登录，进行如下测试。\n\n\n# 测试 Docker 是否安装正确\n\ndocker run hello-world\n\nUnable to find image \'hello-world:latest\' locally\nlatest: Pulling from library/hello-world\nca4f61b1923c: Pull complete\nDigest: sha256:be0cd392e45be79ffeffa6b05338b98ebb16c87b255f48e297ec7f98e123905c\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://cloud.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/engine/userguide/\n\n\n若能正常输出以上信息，则说明安装成功。\n\n\n# 镜像加速\n\n鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，强烈建议安装 Docker 之后配置 国内镜像加速。\n\n\n# 常见问题\n\ndocker info指令报若下错误：WARNING: No memory limit support 或 WARNING: No swap limit support\n\n解决方法：\n\n1、打开/etc/default/grub文件，添加如下内容：\n\nvim /etc/default/grub\n# 修改\nGRUB_CMDLINE_LINUX="cgroup_enable=memory swapaccount=1"\n\n\n2、更新grub\n\nupdate-grub\n\n\n3、重启系统\n\nreboot\n\n\n\n# 卸载 Docker Engine\n\n 1. Uninstall the Docker Engine, CLI, and Containerd packages:\n\nsudo apt-get purge docker-ce docker-ce-cli containerd.io\n\n\n 2. Images, containers, volumes, or customized configuration files on your host are not automatically removed. To delete all images, containers, and volumes:\n\nsudo rm -rf /var/lib/docker\n\n\nYou must delete any edited configuration files manually.\n\n\n# 参考文档\n\n * Docker 官方 Ubuntu 安装文档',normalizedContent:'# ubuntu 安装 docker\n\n注意\n\n切勿在没有配置 docker apt 源的情况下直接使用 apt 命令安装 docker.\n\n\n# 准备工作\n\n\n# 系统要求\n\ndocker ce 支持以下版本的 ubuntu 操作系统：\n\n * ubuntu focal 20.04 (lts)\n * ubuntu bionic 18.04 (lts)\n\ndocker ce 可以安装在 64 位的 x86 平台或 arm 平台上。ubuntu 发行版中，lts（long-term-support）长期支持版本，会获得 5 年的升级维护支持，这样的版本会更稳定，因此在生产环境中推荐使用 lts 版本,当前最新的 lts 版本为 ubuntu 20.04。\n\n\n# 关闭swap\n\nswapoff -a\nyes | cp /etc/fstab /etc/fstab_bak\ncat /etc/fstab_bak |grep -v swap > /etc/fstab\n\n\n使用free -h命令检查配置是否生效\n\n\n\n\n\n \n\n\nfree -h   \n              total        used        free      shared  buff/cache   available\nmem:           1.8g        142m        1.5g        8.5m        122m        1.5g\nswap:            0b          0b          0b\n\n\n\n# 卸载旧版本\n\n旧版本的 docker 称为 docker，docker.io 或者 docker-engine，使用以下命令卸载旧版本：\n\nsudo apt-get remove docker docker-engine docker.io containerd runc\n\n\n\n# 支持的存储驱动程序\n\nubuntu 16.04 + 上的 docker ce 默认使用 overlay2 存储层驱动,无需手动配置。\n\n\n# 使用存储库安装\n\n在新主机上首次安装docker引擎之前，需要设置docker存储库。之后，您可以从存储库安装和更新docker。\n\n\n# 设置存储库\n\nstep 1 : 更新apt程序包索引并安装程序包，以允许apt通过https使用存储库：\n\nsudo apt-get update\n\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n\n\nstep 2 : 添加阿里云的官方gpg密钥：\n\ncurl -fssl http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n\n\nstep 3 : 验证 gpg 证书\n\nsudo apt-key fingerprint 0ebfcd88\n\npub   rsa4096 2017-02-22 [scea]\n      9dc8 5822 9fc7 dd38 854a  e2d8 8d81 803c 0ebf cd88\nuid           [ unknown] docker release (ce deb) <docker@docker.com>\nsub   rsa4096 2017-02-22 [s]\n\n\nstep 4 : 写入软件源信息\n\nsudo add-apt-repository \\\n   "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable"\n\n\n\n# 更新并安装 docker ce\n\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n\n\n提示\n\n以上命令会添加稳定版本的 docker ce apt 镜像源，如果需要最新或者测试版本的 docker ce 请将 stable 改为 edge 或者 test。从 docker 17.06 开始，edge test 版本的 apt 镜像源也会包含稳定版本的 docker。\n\n\n# 使用脚本自动安装\n\n在测试或开发环境中 docker 官方为了简化安装流程，提供了一套便捷的安装脚本，ubuntu 系统上可以使用这套脚本安装：\n\ncurl -fssl https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n\n\n执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 docker ce 的 edge 版本安装在系统中。\n\n\n# 启动 docker ce\n\nsudo systemctl enable docker\nsudo systemctl start docker\n\n\n\n# 建立 docker 用户组\n\n默认情况下，docker 命令会使用 unix socket 与 docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 docker 引擎的 unix socket。出于安全考虑，一般 linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。\n\n建立 docker 组：\n\nsudo groupadd docker\n\n\n将当前用户加入 docker 组：\n\nsudo usermod -ag docker $user\n\n\n退出当前终端并重新登录，进行如下测试。\n\n\n# 测试 docker 是否安装正确\n\ndocker run hello-world\n\nunable to find image \'hello-world:latest\' locally\nlatest: pulling from library/hello-world\nca4f61b1923c: pull complete\ndigest: sha256:be0cd392e45be79ffeffa6b05338b98ebb16c87b255f48e297ec7f98e123905c\nstatus: downloaded newer image for hello-world:latest\n\nhello from docker!\nthis message shows that your installation appears to be working correctly.\n\nto generate this message, docker took the following steps:\n 1. the docker client contacted the docker daemon.\n 2. the docker daemon pulled the "hello-world" image from the docker hub.\n    (amd64)\n 3. the docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. the docker daemon streamed that output to the docker client, which sent it\n    to your terminal.\n\nto try something more ambitious, you can run an ubuntu container with:\n docker run -it ubuntu bash\n\nshare images, automate workflows, and more with a free docker id:\n https://cloud.docker.com/\n\nfor more examples and ideas, visit:\n https://docs.docker.com/engine/userguide/\n\n\n若能正常输出以上信息，则说明安装成功。\n\n\n# 镜像加速\n\n鉴于国内网络问题，后续拉取 docker 镜像十分缓慢，强烈建议安装 docker 之后配置 国内镜像加速。\n\n\n# 常见问题\n\ndocker info指令报若下错误：warning: no memory limit support 或 warning: no swap limit support\n\n解决方法：\n\n1、打开/etc/default/grub文件，添加如下内容：\n\nvim /etc/default/grub\n# 修改\ngrub_cmdline_linux="cgroup_enable=memory swapaccount=1"\n\n\n2、更新grub\n\nupdate-grub\n\n\n3、重启系统\n\nreboot\n\n\n\n# 卸载 docker engine\n\n 1. uninstall the docker engine, cli, and containerd packages:\n\nsudo apt-get purge docker-ce docker-ce-cli containerd.io\n\n\n 2. images, containers, volumes, or customized configuration files on your host are not automatically removed. to delete all images, containers, and volumes:\n\nsudo rm -rf /var/lib/docker\n\n\nyou must delete any edited configuration files manually.\n\n\n# 参考文档\n\n * docker 官方 ubuntu 安装文档',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"CentOS 安装 Docker",frontmatter:{title:"CentOS 安装 Docker",date:"2022-02-08T17:17:46.000Z",permalink:"/pages/102284/"},regularPath:"/17.Docker/03.%E5%AE%89%E8%A3%85%20Docker/03.CentOS%20%E5%AE%89%E8%A3%85%20Docker.html",relativePath:"17.Docker/03.安装 Docker/03.CentOS 安装 Docker.md",key:"v-5b6a58d6",path:"/pages/102284/",headers:[{level:2,title:"前提条件",slug:"前提条件",normalizedTitle:"前提条件",charIndex:74},{level:3,title:"操作系统要求",slug:"操作系统要求",normalizedTitle:"操作系统要求",charIndex:83},{level:3,title:"卸载旧版本",slug:"卸载旧版本",normalizedTitle:"卸载旧版本",charIndex:227},{level:2,title:"使用 yum 安装",slug:"使用-yum-安装",normalizedTitle:"使用 yum 安装",charIndex:573},{level:3,title:"设置存储库",slug:"设置存储库",normalizedTitle:"设置存储库",charIndex:660},{level:3,title:"安装 Docker Engine-Community",slug:"安装-docker-engine-community",normalizedTitle:"安装 docker engine-community",charIndex:1323},{level:2,title:"启动 Docker CE",slug:"启动-docker-ce",normalizedTitle:"启动 docker ce",charIndex:1704},{level:2,title:"开机自启动",slug:"开机自启动",normalizedTitle:"开机自启动",charIndex:1751},{level:2,title:"建立 docker 用户组",slug:"建立-docker-用户组",normalizedTitle:"建立 docker 用户组",charIndex:1792},{level:2,title:"测试 Docker 是否安装正确",slug:"测试-docker-是否安装正确",normalizedTitle:"测试 docker 是否安装正确",charIndex:2092},{level:2,title:"镜像加速",slug:"镜像加速",normalizedTitle:"镜像加速",charIndex:3224},{level:2,title:"添加内核参数",slug:"添加内核参数",normalizedTitle:"添加内核参数",charIndex:3289},{level:2,title:"参考文档",slug:"参考文档",normalizedTitle:"参考文档",charIndex:3617}],headersStr:"前提条件 操作系统要求 卸载旧版本 使用 yum 安装 设置存储库 安装 Docker Engine-Community 启动 Docker CE 开机自启动 建立 docker 用户组 测试 Docker 是否安装正确 镜像加速 添加内核参数 参考文档",content:"# CentOS 安装 Docker\n\n注意\n\n切勿在没有配置 Docker YUM 源的情况下直接使用 yum 命令安装 Docker.\n\n\n# 前提条件\n\n\n# 操作系统要求\n\n要安装Docker Engine-Community，您需要一个CentOS 7的维护版本。不支持或未测试存档版本。该centos-extras库必须启用。默认情况下，此存储库是启用的，但是如果已禁用它，则需要 重新启用它。overlay2建议使用存储驱动程序。\n\n\n# 卸载旧版本\n\n旧版本的 Docker 称为 docker 或者 docker-engine，使用以下命令卸载旧版本：\n\nsudo yum remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-engine\n\n\n\n# 使用 yum 安装\n\n在新主机上首次安装Docker Engine-Community之前，需要设置Docker存储库。之后，您可以从存储库安装和更新Docker。\n\n\n# 设置存储库\n\n1.安装所需的软件包。yum-utils提供了yum-config-manager 效用，并device-mapper-persistent-data和lvm2由需要devicemapper存储驱动程序。\n\nsudo yum install -y yum-utils \\\n           device-mapper-persistent-data \\\n           lvm2\n\n\n2.使用以下命令来设置稳定的存储库。\n\n提示\n\n鉴于国内网络问题，强烈建议使用国内源，官方源请在注释中查看。\n\n# 阿里源\nsudo yum-config-manager \\\n    --add-repo \\\n    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n\n\n# 官方源\n# sudo yum-config-manager \\\n#     --add-repo \\\n#     https://download.docker.com/linux/centos/docker-ce.repo    \n\n\n如果需要最新版本的 Docker CE 请使用以下命令：\n\nsudo yum-config-manager --enable docker-ce-edge\n\n\n如果需要测试版本的 Docker CE 请使用以下命令：\n\nsudo yum-config-manager --enable docker-ce-test\n\n\n\n# 安装 Docker Engine-Community\n\n安装最新版本的Docker Engine-Community和containerd，或者转到下一步安装特定版本：\n\nsudo yum install docker-ce docker-ce-cli containerd.io\n\n\n如果提示您接受GPG密钥，请验证指纹是否匹配 060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35，如果是，则接受它。\n\n注意\n\n有多个Docker存储库吗？ 如果您启用了多个Docker存储库，则在未在yum installor或yum update命令中指定版本的情况下进行安装或更新将始终安装可能的最高版本，这可能不适合您的稳定性需求。\n\nDocker已安装但尚未启动。docker创建该组，但没有用户添加到该组。\n\n\n# 启动 Docker CE\n\nsudo systemctl start docker\n\n\n\n# 开机自启动\n\nsudo systemctl enable docker\n\n\n\n# 建立 docker 用户组\n\n默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。\n\n建立 docker 组：\n\nsudo groupadd docker\n\n\n将当前用户加入 docker 组：\n\nsudo usermod -aG docker $USER\n\n\n退出当前终端并重新登录，进行如下测试。\n\n\n# 测试 Docker 是否安装正确\n\ndocker run hello-world\n\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nca4f61b1923c: Pull complete\nDigest: sha256:be0cd392e45be79ffeffa6b05338b98ebb16c87b255f48e297ec7f98e123905c\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://cloud.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/engine/userguide/\n\n\n若能正常输出以上信息，则说明安装成功。\n\n\n# 镜像加速\n\n鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，强烈建议安装 Docker 之后配置 国内镜像加速。\n\n\n# 添加内核参数\n\n默认配置下，如果在 CentOS 使用 Docker CE 看到下面的这些警告信息：\n\nWARNING: bridge-nf-call-iptables is disabled\nWARNING: bridge-nf-call-ip6tables is disabled\n\n\n请添加内核配置参数以启用这些功能。\n\nsudo tee -a /etc/sysctl.conf <<-EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\n\n\n然后重新加载 sysctl.conf 即可\n\nsudo sysctl -p\n\n\n\n# 参考文档\n\n * Docker 官方 CentOS 安装文档。",normalizedContent:"# centos 安装 docker\n\n注意\n\n切勿在没有配置 docker yum 源的情况下直接使用 yum 命令安装 docker.\n\n\n# 前提条件\n\n\n# 操作系统要求\n\n要安装docker engine-community，您需要一个centos 7的维护版本。不支持或未测试存档版本。该centos-extras库必须启用。默认情况下，此存储库是启用的，但是如果已禁用它，则需要 重新启用它。overlay2建议使用存储驱动程序。\n\n\n# 卸载旧版本\n\n旧版本的 docker 称为 docker 或者 docker-engine，使用以下命令卸载旧版本：\n\nsudo yum remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-engine\n\n\n\n# 使用 yum 安装\n\n在新主机上首次安装docker engine-community之前，需要设置docker存储库。之后，您可以从存储库安装和更新docker。\n\n\n# 设置存储库\n\n1.安装所需的软件包。yum-utils提供了yum-config-manager 效用，并device-mapper-persistent-data和lvm2由需要devicemapper存储驱动程序。\n\nsudo yum install -y yum-utils \\\n           device-mapper-persistent-data \\\n           lvm2\n\n\n2.使用以下命令来设置稳定的存储库。\n\n提示\n\n鉴于国内网络问题，强烈建议使用国内源，官方源请在注释中查看。\n\n# 阿里源\nsudo yum-config-manager \\\n    --add-repo \\\n    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n\n\n# 官方源\n# sudo yum-config-manager \\\n#     --add-repo \\\n#     https://download.docker.com/linux/centos/docker-ce.repo    \n\n\n如果需要最新版本的 docker ce 请使用以下命令：\n\nsudo yum-config-manager --enable docker-ce-edge\n\n\n如果需要测试版本的 docker ce 请使用以下命令：\n\nsudo yum-config-manager --enable docker-ce-test\n\n\n\n# 安装 docker engine-community\n\n安装最新版本的docker engine-community和containerd，或者转到下一步安装特定版本：\n\nsudo yum install docker-ce docker-ce-cli containerd.io\n\n\n如果提示您接受gpg密钥，请验证指纹是否匹配 060a 61c5 1b55 8a7f 742b 77aa c52f eb6b 621e 9f35，如果是，则接受它。\n\n注意\n\n有多个docker存储库吗？ 如果您启用了多个docker存储库，则在未在yum installor或yum update命令中指定版本的情况下进行安装或更新将始终安装可能的最高版本，这可能不适合您的稳定性需求。\n\ndocker已安装但尚未启动。docker创建该组，但没有用户添加到该组。\n\n\n# 启动 docker ce\n\nsudo systemctl start docker\n\n\n\n# 开机自启动\n\nsudo systemctl enable docker\n\n\n\n# 建立 docker 用户组\n\n默认情况下，docker 命令会使用 unix socket 与 docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 docker 引擎的 unix socket。出于安全考虑，一般 linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。\n\n建立 docker 组：\n\nsudo groupadd docker\n\n\n将当前用户加入 docker 组：\n\nsudo usermod -ag docker $user\n\n\n退出当前终端并重新登录，进行如下测试。\n\n\n# 测试 docker 是否安装正确\n\ndocker run hello-world\n\nunable to find image 'hello-world:latest' locally\nlatest: pulling from library/hello-world\nca4f61b1923c: pull complete\ndigest: sha256:be0cd392e45be79ffeffa6b05338b98ebb16c87b255f48e297ec7f98e123905c\nstatus: downloaded newer image for hello-world:latest\n\nhello from docker!\nthis message shows that your installation appears to be working correctly.\n\nto generate this message, docker took the following steps:\n 1. the docker client contacted the docker daemon.\n 2. the docker daemon pulled the \"hello-world\" image from the docker hub.\n    (amd64)\n 3. the docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. the docker daemon streamed that output to the docker client, which sent it\n    to your terminal.\n\nto try something more ambitious, you can run an ubuntu container with:\n docker run -it ubuntu bash\n\nshare images, automate workflows, and more with a free docker id:\n https://cloud.docker.com/\n\nfor more examples and ideas, visit:\n https://docs.docker.com/engine/userguide/\n\n\n若能正常输出以上信息，则说明安装成功。\n\n\n# 镜像加速\n\n鉴于国内网络问题，后续拉取 docker 镜像十分缓慢，强烈建议安装 docker 之后配置 国内镜像加速。\n\n\n# 添加内核参数\n\n默认配置下，如果在 centos 使用 docker ce 看到下面的这些警告信息：\n\nwarning: bridge-nf-call-iptables is disabled\nwarning: bridge-nf-call-ip6tables is disabled\n\n\n请添加内核配置参数以启用这些功能。\n\nsudo tee -a /etc/sysctl.conf <<-eof\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\neof\n\n\n然后重新加载 sysctl.conf 即可\n\nsudo sysctl -p\n\n\n\n# 参考文档\n\n * docker 官方 centos 安装文档。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"macOS 安装 Docker",frontmatter:{title:"macOS 安装 Docker",date:"2022-02-08T17:17:46.000Z",permalink:"/pages/847488/"},regularPath:"/17.Docker/03.%E5%AE%89%E8%A3%85%20Docker/04.macOS%20%E5%AE%89%E8%A3%85%20Docker.html",relativePath:"17.Docker/03.安装 Docker/04.macOS 安装 Docker.md",key:"v-a1271344",path:"/pages/847488/",headers:[{level:2,title:"系统要求",slug:"系统要求",normalizedTitle:"系统要求",charIndex:22},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:8},{level:3,title:"使用 Homebrew 安装",slug:"使用-homebrew-安装",normalizedTitle:"使用 homebrew 安装",charIndex:116},{level:3,title:"手动下载安装",slug:"手动下载安装",normalizedTitle:"手动下载安装",charIndex:231},{level:2,title:"运行",slug:"运行",normalizedTitle:"运行",charIndex:389},{level:2,title:"镜像加速",slug:"镜像加速",normalizedTitle:"镜像加速",charIndex:1035}],headersStr:"系统要求 安装 使用 Homebrew 安装 手动下载安装 运行 镜像加速",content:'# macOS 安装 Docker\n\n\n# 系统要求\n\nDocker for Mac 要求系统最低为 macOS 10.10.3 Yosemite。如果系统不满足需求，可以安装 Docker Toolbox。\n\n\n# 安装\n\n\n# 使用 Homebrew 安装\n\nHomebrew 的 Cask 已经支持 Docker for Mac，因此可以很方便的使用 Homebrew Cask 来进行安装：\n\nbrew cask install docker\n\n\n\n# 手动下载安装\n\n如果需要手动下载，请点击以下链接下载 Stable 或 Edge 版本的 Docker for Mac。\n\n如同 macOS 其它软件一样，安装也非常简单，双击下载的 .dmg 文件，然后将那只叫 Moby 的鲸鱼图标拖拽到 Application 文件夹即可（其间需要输入用户密码）。\n\n\n\n\n# 运行\n\n从应用中找到 Docker 图标并点击运行。\n\n\n\n运行之后，会在右上角菜单栏看到多了一个鲸鱼图标，这个图标表明了 Docker 的运行状态。\n\n\n\n第一次点击图标，可能会看到这个安装成功的界面，点击 "Got it!" 可以关闭这个窗口。\n\n\n\n以后每次点击鲸鱼图标会弹出操作菜单。\n\n\n\n启动终端后，通过命令可以检查安装后的 Docker 版本。\n\ndocker --version\nDocker version 17.10.0-ce, build f4ffd25\ndocker-compose --version\ndocker-compose version 1.17.0-rc1, build a0f95af\ndocker-machine --version\ndocker-machine version 0.13.0, build 9ba6da9\n\n\n如果 docker version、docker info 都正常的话，可以尝试运行一个 Nginx 服务器：\n\ndocker run -d -p 80:80 --name webserver nginx\n\n\n服务运行后，可以访问 http://localhost，如果看到了 "Welcome to nginx!"，就说明 Docker for Mac 安装成功了。\n\n\n\n要停止 Nginx 服务器并删除执行下面的命令：\n\ndocker stop webserver\ndocker rm webserver\n\n\n\n# 镜像加速\n\n鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，强烈建议安装 Docker 之后配置 国内镜像加速。',normalizedContent:'# macos 安装 docker\n\n\n# 系统要求\n\ndocker for mac 要求系统最低为 macos 10.10.3 yosemite。如果系统不满足需求，可以安装 docker toolbox。\n\n\n# 安装\n\n\n# 使用 homebrew 安装\n\nhomebrew 的 cask 已经支持 docker for mac，因此可以很方便的使用 homebrew cask 来进行安装：\n\nbrew cask install docker\n\n\n\n# 手动下载安装\n\n如果需要手动下载，请点击以下链接下载 stable 或 edge 版本的 docker for mac。\n\n如同 macos 其它软件一样，安装也非常简单，双击下载的 .dmg 文件，然后将那只叫 moby 的鲸鱼图标拖拽到 application 文件夹即可（其间需要输入用户密码）。\n\n\n\n\n# 运行\n\n从应用中找到 docker 图标并点击运行。\n\n\n\n运行之后，会在右上角菜单栏看到多了一个鲸鱼图标，这个图标表明了 docker 的运行状态。\n\n\n\n第一次点击图标，可能会看到这个安装成功的界面，点击 "got it!" 可以关闭这个窗口。\n\n\n\n以后每次点击鲸鱼图标会弹出操作菜单。\n\n\n\n启动终端后，通过命令可以检查安装后的 docker 版本。\n\ndocker --version\ndocker version 17.10.0-ce, build f4ffd25\ndocker-compose --version\ndocker-compose version 1.17.0-rc1, build a0f95af\ndocker-machine --version\ndocker-machine version 0.13.0, build 9ba6da9\n\n\n如果 docker version、docker info 都正常的话，可以尝试运行一个 nginx 服务器：\n\ndocker run -d -p 80:80 --name webserver nginx\n\n\n服务运行后，可以访问 http://localhost，如果看到了 "welcome to nginx!"，就说明 docker for mac 安装成功了。\n\n\n\n要停止 nginx 服务器并删除执行下面的命令：\n\ndocker stop webserver\ndocker rm webserver\n\n\n\n# 镜像加速\n\n鉴于国内网络问题，后续拉取 docker 镜像十分缓慢，强烈建议安装 docker 之后配置 国内镜像加速。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Windows 安装 Docker",frontmatter:{title:"Windows 安装 Docker",date:"2022-02-08T17:17:46.000Z",permalink:"/pages/c4f755/"},regularPath:"/17.Docker/03.%E5%AE%89%E8%A3%85%20Docker/05.Windows%20%E5%AE%89%E8%A3%85%20Docker.html",relativePath:"17.Docker/03.安装 Docker/05.Windows 安装 Docker.md",key:"v-79b29984",path:"/pages/c4f755/",headers:[{level:2,title:"系统要求",slug:"系统要求",normalizedTitle:"系统要求",charIndex:24},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:10},{level:2,title:"运行",slug:"运行",normalizedTitle:"运行",charIndex:197},{level:2,title:"镜像加速",slug:"镜像加速",normalizedTitle:"镜像加速",charIndex:331}],headersStr:"系统要求 安装 运行 镜像加速",content:"# Windows 安装 Docker\n\n\n# 系统要求\n\nDocker for Windows 支持 64 位版本的 Windows 10 Pro，且必须开启 Hyper-V。\n\n\n# 安装\n\n点击以下链接下载 Stable 或 Edge 版本的 Docker for Windows。\n\n下载好之后双击 Docker for Windows Installer.exe 开始安装。\n\n\n# 运行\n\n在 Windows 搜索栏输入 Docker 点击 Docker for Windows 开始运行。\n\n\n\nDocker CE 启动之后会在 Windows 任务栏出现鲸鱼图标。\n\n\n\n等待片刻，点击 Got it 开始使用 Docker CE。\n\n\n\n\n# 镜像加速\n\n鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，强烈建议安装 Docker 之后配置 国内镜像加速。",normalizedContent:"# windows 安装 docker\n\n\n# 系统要求\n\ndocker for windows 支持 64 位版本的 windows 10 pro，且必须开启 hyper-v。\n\n\n# 安装\n\n点击以下链接下载 stable 或 edge 版本的 docker for windows。\n\n下载好之后双击 docker for windows installer.exe 开始安装。\n\n\n# 运行\n\n在 windows 搜索栏输入 docker 点击 docker for windows 开始运行。\n\n\n\ndocker ce 启动之后会在 windows 任务栏出现鲸鱼图标。\n\n\n\n等待片刻，点击 got it 开始使用 docker ce。\n\n\n\n\n# 镜像加速\n\n鉴于国内网络问题，后续拉取 docker 镜像十分缓慢，强烈建议安装 docker 之后配置 国内镜像加速。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Docker 镜像加速器",frontmatter:{title:"Docker 镜像加速器",date:"2022-02-08T17:17:46.000Z",permalink:"/pages/87062e/"},regularPath:"/17.Docker/03.%E5%AE%89%E8%A3%85%20Docker/06.Docker%20%E9%95%9C%E5%83%8F%E5%8A%A0%E9%80%9F%E5%99%A8.html",relativePath:"17.Docker/03.安装 Docker/06.Docker 镜像加速器.md",key:"v-782329dc",path:"/pages/87062e/",headers:[{level:2,title:"Ubuntu 18.04+、Debian 8+、CentOS 7",slug:"ubuntu-18-04-、debian-8-、centos-7",normalizedTitle:"ubuntu 18.04+、debian 8+、centos 7",charIndex:166},{level:2,title:"Windows 10",slug:"windows-10",normalizedTitle:"windows 10",charIndex:852},{level:2,title:"macOS",slug:"macos",normalizedTitle:"macos",charIndex:1046},{level:2,title:"检查加速器是否生效",slug:"检查加速器是否生效",normalizedTitle:"检查加速器是否生效",charIndex:1243}],headersStr:"Ubuntu 18.04+、Debian 8+、CentOS 7 Windows 10 macOS 检查加速器是否生效",content:'# Docker 镜像加速器\n\n国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务，例如：\n\n * Docker官方中国\n * 网易云\n * 百度云\n * 阿里云\n * DaoCloud\n\n我们以 Docker 官方加速器为例进行介绍。\n\n\n# Ubuntu 18.04+、Debian 8+、CentOS 7\n\n对于使用 systemd 的系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件）\n\n官方加速\n\n{\n  "registry-mirrors": [\n     "https://registry.docker-cn.com"\n  ]\n}\n\n\n阿里云加速\n\n{\n  "registry-mirrors": [\n     "https://registry.cn-beijing.aliyuncs.com"\n  ]\n}\n\n\n百度云加速\n\n{\n  "registry-mirrors": [\n     "https://mirror.baidubce.com"\n  ]\n}\n\n\n163加速\n\n{\n  "registry-mirrors": [\n     "https://hub-mirror.c.163.com"\n  ]\n}\n\n\n其他\n\n{\n  "registry-mirrors": [\n     "https://dockerhub.azk8s.cn",\n     "https://reg-mirror.qiniu.com",\n     "https://registry.docker-cn.com"\n  ]\n}\n\n\n注意\n\n一定要保证该文件符合 json 规范，否则 Docker 将不能启动。\n\n之后重新启动服务。\n\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n\n\n\n# Windows 10\n\n对于使用 Windows 10 的系统，在系统右下角托盘 Docker 图标内右键菜单选择 Settings，打开配置窗口后左侧导航菜单选择 Daemon。在 Registry mirrors 一栏中填写加速器地址 https://registry.docker-cn.com，之后点击 Apply 保存后 Docker 就会重启并应用配置的镜像地址了。\n\n\n# macOS\n\n对于使用 macOS 的用户，在任务栏点击 Docker for mac 应用图标 -> Perferences... -> Daemon -> Registry mirrors。在列表中填写加速器地址 https://registry.docker-cn.com。修改完成之后，点击 Apply & Restart 按钮，Docker 就会重启并应用配置的镜像地址了。\n\n\n# 检查加速器是否生效\n\n配置加速器之后，如果拉取镜像仍然十分缓慢，请手动检查加速器配置是否生效，在命令行执行 docker info，如果从结果中看到了如下内容，说明配置成功。\n\nRegistry Mirrors:\n https://registry.docker-cn.com/\n',normalizedContent:'# docker 镜像加速器\n\n国内从 docker hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。docker 官方和国内很多云服务商都提供了国内加速器服务，例如：\n\n * docker官方中国\n * 网易云\n * 百度云\n * 阿里云\n * daocloud\n\n我们以 docker 官方加速器为例进行介绍。\n\n\n# ubuntu 18.04+、debian 8+、centos 7\n\n对于使用 systemd 的系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件）\n\n官方加速\n\n{\n  "registry-mirrors": [\n     "https://registry.docker-cn.com"\n  ]\n}\n\n\n阿里云加速\n\n{\n  "registry-mirrors": [\n     "https://registry.cn-beijing.aliyuncs.com"\n  ]\n}\n\n\n百度云加速\n\n{\n  "registry-mirrors": [\n     "https://mirror.baidubce.com"\n  ]\n}\n\n\n163加速\n\n{\n  "registry-mirrors": [\n     "https://hub-mirror.c.163.com"\n  ]\n}\n\n\n其他\n\n{\n  "registry-mirrors": [\n     "https://dockerhub.azk8s.cn",\n     "https://reg-mirror.qiniu.com",\n     "https://registry.docker-cn.com"\n  ]\n}\n\n\n注意\n\n一定要保证该文件符合 json 规范，否则 docker 将不能启动。\n\n之后重新启动服务。\n\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n\n\n\n# windows 10\n\n对于使用 windows 10 的系统，在系统右下角托盘 docker 图标内右键菜单选择 settings，打开配置窗口后左侧导航菜单选择 daemon。在 registry mirrors 一栏中填写加速器地址 https://registry.docker-cn.com，之后点击 apply 保存后 docker 就会重启并应用配置的镜像地址了。\n\n\n# macos\n\n对于使用 macos 的用户，在任务栏点击 docker for mac 应用图标 -> perferences... -> daemon -> registry mirrors。在列表中填写加速器地址 https://registry.docker-cn.com。修改完成之后，点击 apply & restart 按钮，docker 就会重启并应用配置的镜像地址了。\n\n\n# 检查加速器是否生效\n\n配置加速器之后，如果拉取镜像仍然十分缓慢，请手动检查加速器配置是否生效，在命令行执行 docker info，如果从结果中看到了如下内容，说明配置成功。\n\nregistry mirrors:\n https://registry.docker-cn.com/\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"使用 Docker 镜像",frontmatter:{title:"使用 Docker 镜像",date:"2022-02-08T17:58:04.000Z",permalink:"/pages/13e1d1/"},regularPath:"/17.Docker/04.Docker%20%E9%95%9C%E5%83%8F/01.%E4%BD%BF%E7%94%A8%20Docker%20%E9%95%9C%E5%83%8F.html",relativePath:"17.Docker/04.Docker 镜像/01.使用 Docker 镜像.md",key:"v-36dcd140",path:"/pages/13e1d1/",headersStr:null,content:"# 使用 Docker 镜像\n\n在之前的介绍中，我们知道镜像是 Docker 的三大组件之一。\n\nDocker 运行容器前需要本地存在对应的镜像，如果本地不存在该镜像，Docker 会从镜像仓库下载该镜像。\n\n本章将介绍更多关于镜像的内容，包括：\n\n * 从仓库获取镜像；\n * 管理本地主机上的镜像；\n * 介绍镜像实现的基本原理。",normalizedContent:"# 使用 docker 镜像\n\n在之前的介绍中，我们知道镜像是 docker 的三大组件之一。\n\ndocker 运行容器前需要本地存在对应的镜像，如果本地不存在该镜像，docker 会从镜像仓库下载该镜像。\n\n本章将介绍更多关于镜像的内容，包括：\n\n * 从仓库获取镜像；\n * 管理本地主机上的镜像；\n * 介绍镜像实现的基本原理。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 获取镜像",frontmatter:{title:"Docker 获取镜像",date:"2022-02-08T17:58:04.000Z",permalink:"/pages/840a1a/"},regularPath:"/17.Docker/04.Docker%20%E9%95%9C%E5%83%8F/02.Docker%20%E8%8E%B7%E5%8F%96%E9%95%9C%E5%83%8F.html",relativePath:"17.Docker/04.Docker 镜像/02.Docker 获取镜像.md",key:"v-02185a76",path:"/pages/840a1a/",headers:[{level:2,title:"运行",slug:"运行",normalizedTitle:"运行",charIndex:1105}],headersStr:"运行",content:'# Docker 获取镜像\n\n之前提到过，Docker Hub 上有大量的高质量的镜像可以用，这里我们就说一下怎么获取这些镜像。\n\n从 Docker 镜像仓库获取镜像的命令是 docker pull。其命令格式为：\n\ndocker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]\n\n\n具体的选项可以通过 docker pull --help 命令看到，这里我们说一下镜像名称的格式。\n\n * Docker 镜像仓库地址：地址的格式一般是 <域名/IP>[:端口号]。默认地址是 Docker Hub。\n * 仓库名：如之前所说，这里的仓库名是两段式名称，即 <用户名>/<软件名>。对于 Docker Hub，如果不给出用户名，则默认为 library，也就是官方镜像。\n\n比如：\n\ndocker pull ubuntu:16.04\n16.04: Pulling from library/ubuntu\nbf5d46315322: Pull complete\n9f13e0ac480c: Pull complete\ne8988b5b3097: Pull complete\n40af181810e7: Pull complete\ne6f7c7e5c03e: Pull complete\nDigest: sha256:147913621d9cdea08853f6ba9116c2e27a3ceffecf3b492983ae97c3d643fbbe\nStatus: Downloaded newer image for ubuntu:16.04\n\n\n上面的命令中没有给出 Docker 镜像仓库地址，因此将会从 Docker Hub 获取镜像。而镜像名称是 ubuntu:16.04，因此将会获取官方镜像 library/ubuntu 仓库中标签为 16.04 的镜像。\n\n从下载过程中可以看到我们之前提及的分层存储的概念，镜像是由多层存储所构成。下载也是一层层的去下载，并非单一文件。下载过程中给出了每一层的 ID 的前 12 位。并且下载结束后，给出该镜像完整的 sha256 的摘要，以确保下载一致性。\n\n在使用上面命令的时候，你可能会发现，你所看到的层 ID 以及 sha256 的摘要和这里的不一样。这是因为官方镜像是一直在维护的，有任何新的 bug，或者版本更新，都会进行修复再以原来的标签发布，这样可以确保任何使用这个标签的用户可以获得更安全、更稳定的镜像。\n\n如果从 Docker Hub 下载镜像非常缓慢，可以参照 镜像加速器 一节配置加速器。\n\n\n# 运行\n\n有了镜像后，我们就能够以这个镜像为基础启动并运行一个容器。以上面的 ubuntu:16.04 为例，如果我们打算启动里面的 bash 并且进行交互式操作的话，可以执行下面的命令。\n\ndocker run -it --rm \\\n    ubuntu:16.04 \\\n    bash\n\nroot@e7009c6ce357:/# cat /etc/os-release\nNAME="Ubuntu"\nVERSION="16.04.4 LTS, Trusty Tahr"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME="Ubuntu 16.04.4 LTS"\nVERSION_ID="16.04"\nHOME_URL="http://www.ubuntu.com/"\nSUPPORT_URL="http://help.ubuntu.com/"\nBUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"\n\n\ndocker run 就是运行容器的命令，我们这里简要的说明一下上面用到的参数。\n\n * -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。\n * --rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 --rm 可以避免浪费空间。\n * ubuntu:16.04：这是指用 ubuntu:16.04 镜像为基础来启动容器。\n * bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 bash。\n\n进入容器后，我们可以在 Shell 下操作，执行任何所需的命令。这里，我们执行了 cat /etc/os-release，这是 Linux 常用的查看当前系统版本的命令，从返回的结果可以看到容器内是 Ubuntu 16.04.4 LTS 系统。\n\n最后我们通过 exit 退出了这个容器。',normalizedContent:'# docker 获取镜像\n\n之前提到过，docker hub 上有大量的高质量的镜像可以用，这里我们就说一下怎么获取这些镜像。\n\n从 docker 镜像仓库获取镜像的命令是 docker pull。其命令格式为：\n\ndocker pull [选项] [docker registry 地址[:端口号]/]仓库名[:标签]\n\n\n具体的选项可以通过 docker pull --help 命令看到，这里我们说一下镜像名称的格式。\n\n * docker 镜像仓库地址：地址的格式一般是 <域名/ip>[:端口号]。默认地址是 docker hub。\n * 仓库名：如之前所说，这里的仓库名是两段式名称，即 <用户名>/<软件名>。对于 docker hub，如果不给出用户名，则默认为 library，也就是官方镜像。\n\n比如：\n\ndocker pull ubuntu:16.04\n16.04: pulling from library/ubuntu\nbf5d46315322: pull complete\n9f13e0ac480c: pull complete\ne8988b5b3097: pull complete\n40af181810e7: pull complete\ne6f7c7e5c03e: pull complete\ndigest: sha256:147913621d9cdea08853f6ba9116c2e27a3ceffecf3b492983ae97c3d643fbbe\nstatus: downloaded newer image for ubuntu:16.04\n\n\n上面的命令中没有给出 docker 镜像仓库地址，因此将会从 docker hub 获取镜像。而镜像名称是 ubuntu:16.04，因此将会获取官方镜像 library/ubuntu 仓库中标签为 16.04 的镜像。\n\n从下载过程中可以看到我们之前提及的分层存储的概念，镜像是由多层存储所构成。下载也是一层层的去下载，并非单一文件。下载过程中给出了每一层的 id 的前 12 位。并且下载结束后，给出该镜像完整的 sha256 的摘要，以确保下载一致性。\n\n在使用上面命令的时候，你可能会发现，你所看到的层 id 以及 sha256 的摘要和这里的不一样。这是因为官方镜像是一直在维护的，有任何新的 bug，或者版本更新，都会进行修复再以原来的标签发布，这样可以确保任何使用这个标签的用户可以获得更安全、更稳定的镜像。\n\n如果从 docker hub 下载镜像非常缓慢，可以参照 镜像加速器 一节配置加速器。\n\n\n# 运行\n\n有了镜像后，我们就能够以这个镜像为基础启动并运行一个容器。以上面的 ubuntu:16.04 为例，如果我们打算启动里面的 bash 并且进行交互式操作的话，可以执行下面的命令。\n\ndocker run -it --rm \\\n    ubuntu:16.04 \\\n    bash\n\nroot@e7009c6ce357:/# cat /etc/os-release\nname="ubuntu"\nversion="16.04.4 lts, trusty tahr"\nid=ubuntu\nid_like=debian\npretty_name="ubuntu 16.04.4 lts"\nversion_id="16.04"\nhome_url="http://www.ubuntu.com/"\nsupport_url="http://help.ubuntu.com/"\nbug_report_url="http://bugs.launchpad.net/ubuntu/"\n\n\ndocker run 就是运行容器的命令，我们这里简要的说明一下上面用到的参数。\n\n * -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。\n * --rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 --rm 可以避免浪费空间。\n * ubuntu:16.04：这是指用 ubuntu:16.04 镜像为基础来启动容器。\n * bash：放在镜像名后的是命令，这里我们希望有个交互式 shell，因此用的是 bash。\n\n进入容器后，我们可以在 shell 下操作，执行任何所需的命令。这里，我们执行了 cat /etc/os-release，这是 linux 常用的查看当前系统版本的命令，从返回的结果可以看到容器内是 ubuntu 16.04.4 lts 系统。\n\n最后我们通过 exit 退出了这个容器。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 列出镜像",frontmatter:{title:"Docker 列出镜像",date:"2022-02-08T17:58:04.000Z",permalink:"/pages/be83bd/"},regularPath:"/17.Docker/04.Docker%20%E9%95%9C%E5%83%8F/03.Docker%20%E5%88%97%E5%87%BA%E9%95%9C%E5%83%8F.html",relativePath:"17.Docker/04.Docker 镜像/03.Docker 列出镜像.md",key:"v-38a7db08",path:"/pages/be83bd/",headers:[{level:2,title:"镜像体积",slug:"镜像体积",normalizedTitle:"镜像体积",charIndex:940},{level:2,title:"虚悬镜像",slug:"虚悬镜像",normalizedTitle:"虚悬镜像",charIndex:1933},{level:2,title:"中间层镜像",slug:"中间层镜像",normalizedTitle:"中间层镜像",charIndex:2617},{level:2,title:"列出部分镜像",slug:"列出部分镜像",normalizedTitle:"列出部分镜像",charIndex:2975},{level:2,title:"以特定格式显示",slug:"以特定格式显示",normalizedTitle:"以特定格式显示",charIndex:4240}],headersStr:"镜像体积 虚悬镜像 中间层镜像 列出部分镜像 以特定格式显示",content:'# Docker 列出镜像\n\n要想列出已经下载下来的镜像，可以使用 docker image ls 命令。\n\ndocker image ls\nREPOSITORY           TAG                 IMAGE ID            CREATED             SIZE\nredis                latest              5f515359c7f8        5 days ago          183 MB\nnginx                latest              05a60462f8ba        5 days ago          181 MB\nmongo                3.2                 fe9198c04d62        5 days ago          342 MB\n<none>               <none>              00285df0df87        5 days ago          342 MB\nubuntu               16.04               f753707788c5        4 weeks ago         127 MB\nubuntu               latest              f753707788c5        4 weeks ago         127 MB\nubuntu               14.04               1e0c3dd64ccd        4 weeks ago         188 MB\n\n\n列表包含了 仓库名、标签、镜像 ID、创建时间 以及 所占用的空间。\n\n其中仓库名、标签在之前的基础概念章节已经介绍过了。镜像 ID 则是镜像的唯一标识，一个镜像可以对应多个标签。因此，在上面的例子中，我们可以看到 ubuntu:16.04 和 ubuntu:latest 拥有相同的 ID，因为它们对应的是同一个镜像。\n\n\n# 镜像体积\n\n如果仔细观察，会注意到，这里标识的所占用空间和在 Docker Hub 上看到的镜像大小不同。比如，ubuntu:16.04 镜像大小，在这里是 127 MB，但是在 Docker Hub 显示的却是 50 MB。这是因为 Docker Hub 中显示的体积是压缩后的体积。在镜像下载和上传过程中镜像是保持着压缩状态的，因此 Docker Hub 所显示的大小是网络传输中更关心的流量大小。而 docker image ls 显示的是镜像下载到本地后，展开的大小，准确说，是展开后的各层所占空间的总和，因为镜像到本地后，查看空间的时候，更关心的是本地磁盘空间占用的大小。\n\n另外一个需要注意的问题是，docker image ls 列表中的镜像体积总和并非是所有镜像实际硬盘消耗。由于 Docker 镜像是多层存储结构，并且可以继承、复用，因此不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层。由于 Docker 使用 Union FS，相同的层只需要保存一份即可，因此实际镜像硬盘占用空间很可能要比这个列表镜像大小的总和要小的多。\n\n你可以通过以下命令来便捷的查看镜像、容器、数据卷所占用的空间。\n\ndocker system df\n\nTYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE\nImages              24                  0                   1.992GB             1.992GB (100%)\nContainers          1                   0                   62.82MB             62.82MB (100%)\nLocal Volumes       9                   0                   652.2MB             652.2MB (100%)\nBuild Cache                                                 0B                  0B\n\n\n\n# 虚悬镜像\n\n上面的镜像列表中，还可以看到一个特殊的镜像，这个镜像既没有仓库名，也没有标签，均为 ``。：\n\n<none>               <none>              00285df0df87        5 days ago          342 MB\n\n\n这个镜像原本是有镜像名和标签的，原来为 mongo:3.2，随着官方镜像维护，发布了新版本后，重新 docker pull mongo:3.2 时，mongo:3.2 这个镜像名被转移到了新下载的镜像身上，而旧的镜像上的这个名称则被取消，从而成为了 。除了 `docker pull` 可能导致这种情况，`docker build` 也同样可以导致这种现象。由于新旧镜像同名，旧镜像名称被取消，从而出现仓库名、标签均为 的镜像。这类无标签镜像也被称为 虚悬镜像(dangling image) ，可以用下面的命令专门显示这类镜像：\n\ndocker image ls -f dangling=true\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n<none>              <none>              00285df0df87        5 days ago          342 MB\n\n\n一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除。\n\ndocker image prune\n\n\n\n# 中间层镜像\n\n为了加速镜像构建、重复利用资源，Docker 会利用 中间层镜像。所以在使用一段时间后，可能会看到一些依赖的中间层镜像。默认的 docker image ls 列表中只会显示顶层镜像，如果希望显示包括中间层镜像在内的所有镜像的话，需要加 -a 参数。\n\ndocker image ls -a\n\n\n这样会看到很多无标签的镜像，与之前的虚悬镜像不同，这些无标签的镜像很多都是中间层镜像，是其它镜像所依赖的镜像。这些无标签镜像不应该删除，否则会导致上层镜像因为依赖丢失而出错。实际上，这些镜像也没必要删除，因为之前说过，相同的层只会存一遍，而这些镜像是别的镜像的依赖，因此并不会因为它们被列出来而多存了一份，无论如何你也会需要它们。只要删除那些依赖它们的镜像后，这些依赖的中间层镜像也会被连带删除。\n\n\n# 列出部分镜像\n\n不加任何参数的情况下，docker image ls 会列出所有顶级镜像，但是有时候我们只希望列出部分镜像。docker image ls 有好几个参数可以帮助做到这个事情。\n\n根据仓库名列出镜像\n\ndocker image ls ubuntu\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nubuntu              16.04               f753707788c5        4 weeks ago         127 MB\nubuntu              latest              f753707788c5        4 weeks ago         127 MB\nubuntu              14.04               1e0c3dd64ccd        4 weeks ago         188 MB\n\n\n列出特定的某个镜像，也就是说指定仓库名和标签\n\ndocker image ls ubuntu:16.04\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nubuntu              16.04               f753707788c5        4 weeks ago         127 MB\n\n\n除此以外，docker image ls 还支持强大的过滤器参数 --filter，或者简写 -f。之前我们已经看到了使用过滤器来列出虚悬镜像的用法，它还有更多的用法。比如，我们希望看到在 mongo:3.2 之后建立的镜像，可以用下面的命令：\n\ndocker image ls -f since=mongo:3.2\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nredis               latest              5f515359c7f8        5 days ago          183 MB\nnginx               latest              05a60462f8ba        5 days ago          181 MB\n\n\n想查看某个位置之前的镜像也可以，只需要把 since 换成 before 即可。\n\n此外，如果镜像构建时，定义了 LABEL，还可以通过 LABEL 来过滤。\n\ndocker image ls -f label=com.example.version=0.1\n...\n\n\n\n# 以特定格式显示\n\n默认情况下，docker image ls 会输出一个完整的表格，但是我们并非所有时候都会需要这些内容。比如，刚才删除虚悬镜像的时候，我们需要利用 docker image ls 把所有的虚悬镜像的 ID 列出来，然后才可以交给 docker image rm 命令作为参数来删除指定的这些镜像，这个时候就用到了 -q 参数。\n\ndocker image ls -q\n5f515359c7f8\n05a60462f8ba\nfe9198c04d62\n00285df0df87\nf753707788c5\nf753707788c5\n1e0c3dd64ccd\n\n\n--filter 配合 -q 产生出指定范围的 ID 列表，然后送给另一个 docker 命令作为参数，从而针对这组实体成批的进行某种操作的做法在 Docker 命令行使用过程中非常常见，不仅仅是镜像，将来我们会在各个命令中看到这类搭配以完成很强大的功能。因此每次在文档看到过滤器后，可以多注意一下它们的用法。\n\n另外一些时候，我们可能只是对表格的结构不满意，希望自己组织列；或者不希望有标题，这样方便其它程序解析结果等，这就用到了 Go 的模板语法。\n\n比如，下面的命令会直接列出镜像结果，并且只包含镜像ID和仓库名：\n\ndocker image ls --format "{{.ID}}: {{.Repository}}"\n5f515359c7f8: redis\n05a60462f8ba: nginx\nfe9198c04d62: mongo\n00285df0df87: <none>\nf753707788c5: ubuntu\nf753707788c5: ubuntu\n1e0c3dd64ccd: ubuntu\n\n\n或者打算以表格等距显示，并且有标题行，和默认一样，不过自己定义列：\n\ndocker image ls --format "table {{.ID}}\\t{{.Repository}}\\t{{.Tag}}"\nIMAGE ID            REPOSITORY          TAG\n5f515359c7f8        redis               latest\n05a60462f8ba        nginx               latest\nfe9198c04d62        mongo               3.2\n00285df0df87        <none>              <none>\nf753707788c5        ubuntu              16.04\nf753707788c5        ubuntu              latest\n1e0c3dd64ccd        ubuntu              14.04\n',normalizedContent:'# docker 列出镜像\n\n要想列出已经下载下来的镜像，可以使用 docker image ls 命令。\n\ndocker image ls\nrepository           tag                 image id            created             size\nredis                latest              5f515359c7f8        5 days ago          183 mb\nnginx                latest              05a60462f8ba        5 days ago          181 mb\nmongo                3.2                 fe9198c04d62        5 days ago          342 mb\n<none>               <none>              00285df0df87        5 days ago          342 mb\nubuntu               16.04               f753707788c5        4 weeks ago         127 mb\nubuntu               latest              f753707788c5        4 weeks ago         127 mb\nubuntu               14.04               1e0c3dd64ccd        4 weeks ago         188 mb\n\n\n列表包含了 仓库名、标签、镜像 id、创建时间 以及 所占用的空间。\n\n其中仓库名、标签在之前的基础概念章节已经介绍过了。镜像 id 则是镜像的唯一标识，一个镜像可以对应多个标签。因此，在上面的例子中，我们可以看到 ubuntu:16.04 和 ubuntu:latest 拥有相同的 id，因为它们对应的是同一个镜像。\n\n\n# 镜像体积\n\n如果仔细观察，会注意到，这里标识的所占用空间和在 docker hub 上看到的镜像大小不同。比如，ubuntu:16.04 镜像大小，在这里是 127 mb，但是在 docker hub 显示的却是 50 mb。这是因为 docker hub 中显示的体积是压缩后的体积。在镜像下载和上传过程中镜像是保持着压缩状态的，因此 docker hub 所显示的大小是网络传输中更关心的流量大小。而 docker image ls 显示的是镜像下载到本地后，展开的大小，准确说，是展开后的各层所占空间的总和，因为镜像到本地后，查看空间的时候，更关心的是本地磁盘空间占用的大小。\n\n另外一个需要注意的问题是，docker image ls 列表中的镜像体积总和并非是所有镜像实际硬盘消耗。由于 docker 镜像是多层存储结构，并且可以继承、复用，因此不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层。由于 docker 使用 union fs，相同的层只需要保存一份即可，因此实际镜像硬盘占用空间很可能要比这个列表镜像大小的总和要小的多。\n\n你可以通过以下命令来便捷的查看镜像、容器、数据卷所占用的空间。\n\ndocker system df\n\ntype                total               active              size                reclaimable\nimages              24                  0                   1.992gb             1.992gb (100%)\ncontainers          1                   0                   62.82mb             62.82mb (100%)\nlocal volumes       9                   0                   652.2mb             652.2mb (100%)\nbuild cache                                                 0b                  0b\n\n\n\n# 虚悬镜像\n\n上面的镜像列表中，还可以看到一个特殊的镜像，这个镜像既没有仓库名，也没有标签，均为 ``。：\n\n<none>               <none>              00285df0df87        5 days ago          342 mb\n\n\n这个镜像原本是有镜像名和标签的，原来为 mongo:3.2，随着官方镜像维护，发布了新版本后，重新 docker pull mongo:3.2 时，mongo:3.2 这个镜像名被转移到了新下载的镜像身上，而旧的镜像上的这个名称则被取消，从而成为了 。除了 `docker pull` 可能导致这种情况，`docker build` 也同样可以导致这种现象。由于新旧镜像同名，旧镜像名称被取消，从而出现仓库名、标签均为 的镜像。这类无标签镜像也被称为 虚悬镜像(dangling image) ，可以用下面的命令专门显示这类镜像：\n\ndocker image ls -f dangling=true\nrepository          tag                 image id            created             size\n<none>              <none>              00285df0df87        5 days ago          342 mb\n\n\n一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除。\n\ndocker image prune\n\n\n\n# 中间层镜像\n\n为了加速镜像构建、重复利用资源，docker 会利用 中间层镜像。所以在使用一段时间后，可能会看到一些依赖的中间层镜像。默认的 docker image ls 列表中只会显示顶层镜像，如果希望显示包括中间层镜像在内的所有镜像的话，需要加 -a 参数。\n\ndocker image ls -a\n\n\n这样会看到很多无标签的镜像，与之前的虚悬镜像不同，这些无标签的镜像很多都是中间层镜像，是其它镜像所依赖的镜像。这些无标签镜像不应该删除，否则会导致上层镜像因为依赖丢失而出错。实际上，这些镜像也没必要删除，因为之前说过，相同的层只会存一遍，而这些镜像是别的镜像的依赖，因此并不会因为它们被列出来而多存了一份，无论如何你也会需要它们。只要删除那些依赖它们的镜像后，这些依赖的中间层镜像也会被连带删除。\n\n\n# 列出部分镜像\n\n不加任何参数的情况下，docker image ls 会列出所有顶级镜像，但是有时候我们只希望列出部分镜像。docker image ls 有好几个参数可以帮助做到这个事情。\n\n根据仓库名列出镜像\n\ndocker image ls ubuntu\nrepository          tag                 image id            created             size\nubuntu              16.04               f753707788c5        4 weeks ago         127 mb\nubuntu              latest              f753707788c5        4 weeks ago         127 mb\nubuntu              14.04               1e0c3dd64ccd        4 weeks ago         188 mb\n\n\n列出特定的某个镜像，也就是说指定仓库名和标签\n\ndocker image ls ubuntu:16.04\nrepository          tag                 image id            created             size\nubuntu              16.04               f753707788c5        4 weeks ago         127 mb\n\n\n除此以外，docker image ls 还支持强大的过滤器参数 --filter，或者简写 -f。之前我们已经看到了使用过滤器来列出虚悬镜像的用法，它还有更多的用法。比如，我们希望看到在 mongo:3.2 之后建立的镜像，可以用下面的命令：\n\ndocker image ls -f since=mongo:3.2\nrepository          tag                 image id            created             size\nredis               latest              5f515359c7f8        5 days ago          183 mb\nnginx               latest              05a60462f8ba        5 days ago          181 mb\n\n\n想查看某个位置之前的镜像也可以，只需要把 since 换成 before 即可。\n\n此外，如果镜像构建时，定义了 label，还可以通过 label 来过滤。\n\ndocker image ls -f label=com.example.version=0.1\n...\n\n\n\n# 以特定格式显示\n\n默认情况下，docker image ls 会输出一个完整的表格，但是我们并非所有时候都会需要这些内容。比如，刚才删除虚悬镜像的时候，我们需要利用 docker image ls 把所有的虚悬镜像的 id 列出来，然后才可以交给 docker image rm 命令作为参数来删除指定的这些镜像，这个时候就用到了 -q 参数。\n\ndocker image ls -q\n5f515359c7f8\n05a60462f8ba\nfe9198c04d62\n00285df0df87\nf753707788c5\nf753707788c5\n1e0c3dd64ccd\n\n\n--filter 配合 -q 产生出指定范围的 id 列表，然后送给另一个 docker 命令作为参数，从而针对这组实体成批的进行某种操作的做法在 docker 命令行使用过程中非常常见，不仅仅是镜像，将来我们会在各个命令中看到这类搭配以完成很强大的功能。因此每次在文档看到过滤器后，可以多注意一下它们的用法。\n\n另外一些时候，我们可能只是对表格的结构不满意，希望自己组织列；或者不希望有标题，这样方便其它程序解析结果等，这就用到了 go 的模板语法。\n\n比如，下面的命令会直接列出镜像结果，并且只包含镜像id和仓库名：\n\ndocker image ls --format "{{.id}}: {{.repository}}"\n5f515359c7f8: redis\n05a60462f8ba: nginx\nfe9198c04d62: mongo\n00285df0df87: <none>\nf753707788c5: ubuntu\nf753707788c5: ubuntu\n1e0c3dd64ccd: ubuntu\n\n\n或者打算以表格等距显示，并且有标题行，和默认一样，不过自己定义列：\n\ndocker image ls --format "table {{.id}}\\t{{.repository}}\\t{{.tag}}"\nimage id            repository          tag\n5f515359c7f8        redis               latest\n05a60462f8ba        nginx               latest\nfe9198c04d62        mongo               3.2\n00285df0df87        <none>              <none>\nf753707788c5        ubuntu              16.04\nf753707788c5        ubuntu              latest\n1e0c3dd64ccd        ubuntu              14.04\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 删除本地镜像",frontmatter:{title:"Docker 删除本地镜像",date:"2022-02-08T17:58:04.000Z",permalink:"/pages/058894/"},regularPath:"/17.Docker/04.Docker%20%E9%95%9C%E5%83%8F/04.Docker%20%E5%88%A0%E9%99%A4%E6%9C%AC%E5%9C%B0%E9%95%9C%E5%83%8F.html",relativePath:"17.Docker/04.Docker 镜像/04.Docker 删除本地镜像.md",key:"v-1bf741b0",path:"/pages/058894/",headers:[{level:2,title:"用 ID、镜像名、摘要删除镜像",slug:"用-id、镜像名、摘要删除镜像",normalizedTitle:"用 id、镜像名、摘要删除镜像",charIndex:103},{level:2,title:"Untagged 和 Deleted",slug:"untagged-和-deleted",normalizedTitle:"untagged 和 deleted",charIndex:2370},{level:2,title:"用 docker image ls 命令来配合",slug:"用-docker-image-ls-命令来配合",normalizedTitle:"用 docker image ls 命令来配合",charIndex:3129},{level:2,title:"Docker 批量删除虚悬镜像",slug:"docker-批量删除虚悬镜像",normalizedTitle:"docker 批量删除虚悬镜像",charIndex:3473},{level:2,title:"CentOS/RHEL 的用户需要注意的事项",slug:"centos-rhel-的用户需要注意的事项",normalizedTitle:"centos/rhel 的用户需要注意的事项",charIndex:3673}],headersStr:"用 ID、镜像名、摘要删除镜像 Untagged 和 Deleted 用 docker image ls 命令来配合 Docker 批量删除虚悬镜像 CentOS/RHEL 的用户需要注意的事项",content:"# Docker 删除本地镜像\n\n如果要删除本地的镜像，可以使用 docker image rm 命令，其格式为：\n\ndocker image rm [选项] <镜像1> [<镜像2> ...]\n\n\n\n# 用 ID、镜像名、摘要删除镜像\n\n其中，<镜像> 可以是 镜像短 ID、镜像长 ID、镜像名 或者 镜像摘要。\n\n比如我们有这么一些镜像：\n\ndocker image ls\nREPOSITORY                  TAG                 IMAGE ID            CREATED             SIZE\ncentos                      latest              0584b3d2cf6d        3 weeks ago         196.5 MB\nredis                       alpine              501ad78535f0        3 weeks ago         21.03 MB\ndocker                      latest              cf693ec9b5c7        3 weeks ago         105.1 MB\nnginx                       latest              e43d811ce2f4        5 weeks ago         181.5 MB\n\n\n我们可以用镜像的完整 ID，也称为 长 ID，来删除镜像。使用脚本的时候可能会用长 ID，但是人工输入就太累了，所以更多的时候是用 短 ID 来删除镜像。docker image ls 默认列出的就已经是短 ID 了，一般取前3个字符以上，只要足够区分于别的镜像就可以了。\n\n比如这里，如果我们要删除 redis:alpine 镜像，可以执行：\n\ndocker image rm 501\nUntagged: redis:alpine\nUntagged: redis@sha256:f1ed3708f538b537eb9c2a7dd50dc90a706f7debd7e1196c9264edeea521a86d\nDeleted: sha256:501ad78535f015d88872e13fa87a828425117e3d28075d0c117932b05bf189b7\nDeleted: sha256:96167737e29ca8e9d74982ef2a0dda76ed7b430da55e321c071f0dbff8c2899b\nDeleted: sha256:32770d1dcf835f192cafd6b9263b7b597a1778a403a109e2cc2ee866f74adf23\nDeleted: sha256:127227698ad74a5846ff5153475e03439d96d4b1c7f2a449c7a826ef74a2d2fa\nDeleted: sha256:1333ecc582459bac54e1437335c0816bc17634e131ea0cc48daa27d32c75eab3\nDeleted: sha256:4fc455b921edf9c4aea207c51ab39b10b06540c8b4825ba57b3feed1668fa7c7\n\n\n我们也可以用镜像名，也就是 <仓库名>:<标签>，来删除镜像。\n\ndocker image rm centos\nUntagged: centos:latest\nUntagged: centos@sha256:b2f9d1c0ff5f87a4743104d099a3d561002ac500db1b9bfa02a783a46e0d366c\nDeleted: sha256:0584b3d2cf6d235ee310cf14b54667d889887b838d3f3d3033acd70fc3c48b8a\nDeleted: sha256:97ca462ad9eeae25941546209454496e1d66749d53dfa2ee32bf1faabd239d38\n\n\n当然，更精确的是使用 镜像摘要 删除镜像。\n\ndocker image ls --digests\nREPOSITORY                  TAG                 DIGEST                                                                    IMAGE ID            CREATED             SIZE\nnode                        slim                sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228   6e0c4c8e3913        3 weeks ago         214 MB\n\ndocker image rm node@sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228\nUntagged: node@sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228\n\n\n\n# Untagged 和 Deleted\n\n如果观察上面这几个命令的运行输出信息的话，你会注意到删除行为分为两类，一类是 Untagged，另一类是 Deleted。我们之前介绍过，镜像的唯一标识是其 ID 和摘要，而一个镜像可以有多个标签。\n\n因此当我们使用上面命令删除镜像的时候，实际上是在要求删除某个标签的镜像。所以首先需要做的是将满足我们要求的所有镜像标签都取消，这就是我们看到的 Untagged 的信息。因为一个镜像可以对应多个标签，因此当我们删除了所指定的标签后，可能还有别的标签指向了这个镜像，如果是这种情况，那么 Delete 行为就不会发生。所以并非所有的 docker image rm 都会产生删除镜像的行为，有可能仅仅是取消了某个标签而已。\n\n当该镜像所有的标签都被取消了，该镜像很可能会失去了存在的意义，因此会触发删除行为。镜像是多层存储结构，因此在删除的时候也是从上层向基础层方向依次进行判断删除。镜像的多层结构让镜像复用变动非常容易，因此很有可能某个其它镜像正依赖于当前镜像的某一层。这种情况，依旧不会触发删除该层的行为。直到没有任何层依赖当前层时，才会真实的删除当前层。这就是为什么，有时候会奇怪，为什么明明没有别的标签指向这个镜像，但是它还是存在的原因，也是为什么有时候会发现所删除的层数和自己 docker pull 看到的层数不一样的源。\n\n除了镜像依赖以外，还需要注意的是容器对镜像的依赖。如果有用这个镜像启动的容器存在（即使容器没有运行），那么同样不可以删除这个镜像。之前讲过，容器是以镜像为基础，再加一层容器存储层，组成这样的多层存储结构去运行的。因此该镜像如果被这个容器所依赖的，那么删除必然会导致故障。如果这些容器是不需要的，应该先将它们删除，然后再来删除镜像。\n\n\n# 用 docker image ls 命令来配合\n\n像其它可以承接多个实体的命令一样，可以使用 docker image ls -q 来配合使用 docker image rm，这样可以成批的删除希望删除的镜像。我们在“镜像列表”章节介绍过很多过滤镜像列表的方式都可以拿过来使用。\n\n比如，我们需要删除所有仓库名为 redis 的镜像：\n\ndocker image rm $(docker image ls -q redis)\n\n\n或者删除所有在 mongo:3.2 之前的镜像：\n\ndocker image rm $(docker image ls -q -f before=mongo:3.2)\n\n\n充分利用你的想象力和 Linux 命令行的强大，你可以完成很多非常赞的功能。\n\n\n# Docker 批量删除虚悬镜像\n\n在我们构建镜像的过程中，常常需要使用build命令根据Dockerfile进行构建镜像，并且会build很多次，镜像名字也是相同的，那么就会出来下面这种情况\n\n有那么多的虚悬镜像，对于我这种强迫症的人来说，看着就腻歪，所以可以使用下面的命令清除这些虚悬镜像\n\ndocker rmi $(docker images -q -f dangling=true)\n\n\n\n# CentOS/RHEL 的用户需要注意的事项\n\n在 Ubuntu/Debian 上有 UnionFS 可以使用，如 aufs 或者 overlay2，而 CentOS 和 RHEL 的内核中没有相关驱动。因此对于这类系统，一般使用 devicemapper 驱动利用 LVM 的一些机制来模拟分层存储。这样的做法除了性能比较差外，稳定性一般也不好，而且配置相对复杂。Docker 安装在 CentOS/RHEL 上后，会默认选择 devicemapper，但是为了简化配置，其 devicemapper 是跑在一个稀疏文件模拟的块设备上，也被称为 loop-lvm。这样的选择是因为不需要额外配置就可以运行 Docker，这是自动配置唯一能做到的事情。但是 loop-lvm 的做法非常不好，其稳定性、性能更差，无论是日志还是 docker info 中都会看到警告信息。官方文档有明确的文章讲解了如何配置块设备给 devicemapper 驱动做存储层的做法，这类做法也被称为配置 direct-lvm。\n\n除了前面说到的问题外，devicemapper + loop-lvm 还有一个缺陷，因为它是稀疏文件，所以它会不断增长。用户在使用过程中会注意到 /var/lib/docker/devicemapper/devicemapper/data 不断增长，而且无法控制。很多人会希望删除镜像或者可以解决这个问题，结果发现效果并不明显。原因就是这个稀疏文件的空间释放后基本不进行垃圾回收的问题。因此往往会出现即使删除了文件内容，空间却无法回收，随着使用这个稀疏文件一直在不断增长。\n\n所以对于 CentOS/RHEL 的用户来说，在没有办法使用 UnionFS 的情况下，一定要配置 direct-lvm 给 devicemapper，无论是为了性能、稳定性还是空间利用率。\n\n或许有人注意到了 CentOS 7 中存在被 backports 回来的 overlay 驱动，不过 CentOS 里的这个驱动达不到生产环境使用的稳定程度，所以不推荐使用。",normalizedContent:"# docker 删除本地镜像\n\n如果要删除本地的镜像，可以使用 docker image rm 命令，其格式为：\n\ndocker image rm [选项] <镜像1> [<镜像2> ...]\n\n\n\n# 用 id、镜像名、摘要删除镜像\n\n其中，<镜像> 可以是 镜像短 id、镜像长 id、镜像名 或者 镜像摘要。\n\n比如我们有这么一些镜像：\n\ndocker image ls\nrepository                  tag                 image id            created             size\ncentos                      latest              0584b3d2cf6d        3 weeks ago         196.5 mb\nredis                       alpine              501ad78535f0        3 weeks ago         21.03 mb\ndocker                      latest              cf693ec9b5c7        3 weeks ago         105.1 mb\nnginx                       latest              e43d811ce2f4        5 weeks ago         181.5 mb\n\n\n我们可以用镜像的完整 id，也称为 长 id，来删除镜像。使用脚本的时候可能会用长 id，但是人工输入就太累了，所以更多的时候是用 短 id 来删除镜像。docker image ls 默认列出的就已经是短 id 了，一般取前3个字符以上，只要足够区分于别的镜像就可以了。\n\n比如这里，如果我们要删除 redis:alpine 镜像，可以执行：\n\ndocker image rm 501\nuntagged: redis:alpine\nuntagged: redis@sha256:f1ed3708f538b537eb9c2a7dd50dc90a706f7debd7e1196c9264edeea521a86d\ndeleted: sha256:501ad78535f015d88872e13fa87a828425117e3d28075d0c117932b05bf189b7\ndeleted: sha256:96167737e29ca8e9d74982ef2a0dda76ed7b430da55e321c071f0dbff8c2899b\ndeleted: sha256:32770d1dcf835f192cafd6b9263b7b597a1778a403a109e2cc2ee866f74adf23\ndeleted: sha256:127227698ad74a5846ff5153475e03439d96d4b1c7f2a449c7a826ef74a2d2fa\ndeleted: sha256:1333ecc582459bac54e1437335c0816bc17634e131ea0cc48daa27d32c75eab3\ndeleted: sha256:4fc455b921edf9c4aea207c51ab39b10b06540c8b4825ba57b3feed1668fa7c7\n\n\n我们也可以用镜像名，也就是 <仓库名>:<标签>，来删除镜像。\n\ndocker image rm centos\nuntagged: centos:latest\nuntagged: centos@sha256:b2f9d1c0ff5f87a4743104d099a3d561002ac500db1b9bfa02a783a46e0d366c\ndeleted: sha256:0584b3d2cf6d235ee310cf14b54667d889887b838d3f3d3033acd70fc3c48b8a\ndeleted: sha256:97ca462ad9eeae25941546209454496e1d66749d53dfa2ee32bf1faabd239d38\n\n\n当然，更精确的是使用 镜像摘要 删除镜像。\n\ndocker image ls --digests\nrepository                  tag                 digest                                                                    image id            created             size\nnode                        slim                sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228   6e0c4c8e3913        3 weeks ago         214 mb\n\ndocker image rm node@sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228\nuntagged: node@sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228\n\n\n\n# untagged 和 deleted\n\n如果观察上面这几个命令的运行输出信息的话，你会注意到删除行为分为两类，一类是 untagged，另一类是 deleted。我们之前介绍过，镜像的唯一标识是其 id 和摘要，而一个镜像可以有多个标签。\n\n因此当我们使用上面命令删除镜像的时候，实际上是在要求删除某个标签的镜像。所以首先需要做的是将满足我们要求的所有镜像标签都取消，这就是我们看到的 untagged 的信息。因为一个镜像可以对应多个标签，因此当我们删除了所指定的标签后，可能还有别的标签指向了这个镜像，如果是这种情况，那么 delete 行为就不会发生。所以并非所有的 docker image rm 都会产生删除镜像的行为，有可能仅仅是取消了某个标签而已。\n\n当该镜像所有的标签都被取消了，该镜像很可能会失去了存在的意义，因此会触发删除行为。镜像是多层存储结构，因此在删除的时候也是从上层向基础层方向依次进行判断删除。镜像的多层结构让镜像复用变动非常容易，因此很有可能某个其它镜像正依赖于当前镜像的某一层。这种情况，依旧不会触发删除该层的行为。直到没有任何层依赖当前层时，才会真实的删除当前层。这就是为什么，有时候会奇怪，为什么明明没有别的标签指向这个镜像，但是它还是存在的原因，也是为什么有时候会发现所删除的层数和自己 docker pull 看到的层数不一样的源。\n\n除了镜像依赖以外，还需要注意的是容器对镜像的依赖。如果有用这个镜像启动的容器存在（即使容器没有运行），那么同样不可以删除这个镜像。之前讲过，容器是以镜像为基础，再加一层容器存储层，组成这样的多层存储结构去运行的。因此该镜像如果被这个容器所依赖的，那么删除必然会导致故障。如果这些容器是不需要的，应该先将它们删除，然后再来删除镜像。\n\n\n# 用 docker image ls 命令来配合\n\n像其它可以承接多个实体的命令一样，可以使用 docker image ls -q 来配合使用 docker image rm，这样可以成批的删除希望删除的镜像。我们在“镜像列表”章节介绍过很多过滤镜像列表的方式都可以拿过来使用。\n\n比如，我们需要删除所有仓库名为 redis 的镜像：\n\ndocker image rm $(docker image ls -q redis)\n\n\n或者删除所有在 mongo:3.2 之前的镜像：\n\ndocker image rm $(docker image ls -q -f before=mongo:3.2)\n\n\n充分利用你的想象力和 linux 命令行的强大，你可以完成很多非常赞的功能。\n\n\n# docker 批量删除虚悬镜像\n\n在我们构建镜像的过程中，常常需要使用build命令根据dockerfile进行构建镜像，并且会build很多次，镜像名字也是相同的，那么就会出来下面这种情况\n\n有那么多的虚悬镜像，对于我这种强迫症的人来说，看着就腻歪，所以可以使用下面的命令清除这些虚悬镜像\n\ndocker rmi $(docker images -q -f dangling=true)\n\n\n\n# centos/rhel 的用户需要注意的事项\n\n在 ubuntu/debian 上有 unionfs 可以使用，如 aufs 或者 overlay2，而 centos 和 rhel 的内核中没有相关驱动。因此对于这类系统，一般使用 devicemapper 驱动利用 lvm 的一些机制来模拟分层存储。这样的做法除了性能比较差外，稳定性一般也不好，而且配置相对复杂。docker 安装在 centos/rhel 上后，会默认选择 devicemapper，但是为了简化配置，其 devicemapper 是跑在一个稀疏文件模拟的块设备上，也被称为 loop-lvm。这样的选择是因为不需要额外配置就可以运行 docker，这是自动配置唯一能做到的事情。但是 loop-lvm 的做法非常不好，其稳定性、性能更差，无论是日志还是 docker info 中都会看到警告信息。官方文档有明确的文章讲解了如何配置块设备给 devicemapper 驱动做存储层的做法，这类做法也被称为配置 direct-lvm。\n\n除了前面说到的问题外，devicemapper + loop-lvm 还有一个缺陷，因为它是稀疏文件，所以它会不断增长。用户在使用过程中会注意到 /var/lib/docker/devicemapper/devicemapper/data 不断增长，而且无法控制。很多人会希望删除镜像或者可以解决这个问题，结果发现效果并不明显。原因就是这个稀疏文件的空间释放后基本不进行垃圾回收的问题。因此往往会出现即使删除了文件内容，空间却无法回收，随着使用这个稀疏文件一直在不断增长。\n\n所以对于 centos/rhel 的用户来说，在没有办法使用 unionfs 的情况下，一定要配置 direct-lvm 给 devicemapper，无论是为了性能、稳定性还是空间利用率。\n\n或许有人注意到了 centos 7 中存在被 backports 回来的 overlay 驱动，不过 centos 里的这个驱动达不到生产环境使用的稳定程度，所以不推荐使用。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 镜像导入导出",frontmatter:{title:"Docker 镜像导入导出",date:"2022-02-08T17:58:04.000Z",permalink:"/pages/79f5f4/"},regularPath:"/17.Docker/04.Docker%20%E9%95%9C%E5%83%8F/05.Docker%20%E9%95%9C%E5%83%8F%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA.html",relativePath:"17.Docker/04.Docker 镜像/05.Docker 镜像导入导出.md",key:"v-74089f43",path:"/pages/79f5f4/",headers:[{level:2,title:"导入导出命令介绍",slug:"导入导出命令介绍",normalizedTitle:"导入导出命令介绍",charIndex:102},{level:3,title:"save",slug:"save",normalizedTitle:"save",charIndex:132},{level:3,title:"load",slug:"load",normalizedTitle:"load",charIndex:137},{level:3,title:"export",slug:"export",normalizedTitle:"export",charIndex:118},{level:3,title:"import",slug:"import",normalizedTitle:"import",charIndex:125},{level:2,title:"区别",slug:"区别",normalizedTitle:"区别",charIndex:2168},{level:2,title:"建议",slug:"建议",normalizedTitle:"建议",charIndex:4696}],headersStr:"导入导出命令介绍 save load export import 区别 建议",content:'# Docker 镜像导入导出\n\n本文介绍Docker镜像的导入导出，用于迁移、备份、升级等场景，准备环境如下：\n\n * Ubuntu 18.04.5 LTS\n * Docker 19.03.8\n\n\n# 导入导出命令介绍\n\n涉及的命令有export、import、save、load\n\n\n# save\n\n * 命令 docker save [options] images [images...]\n   \n   docker save --help\n   \n   Usage:  docker save [OPTIONS] IMAGE [IMAGE...]\n   \n   Save one or more images to a tar archive (streamed to STDOUT by default)\n   \n   Options:\n     -o, --output string   Write to a file, instead of STDOUT\n   \n\n * 示例 docker save -o nginx.tar nginx:latest 或 docker save > nginx.tar nginx:latest 其中-o和>表示输出到文件，nginx.tar为目标文件，nginx:latest是源镜像名（name:tag）\n\n\n# load\n\n * 命令 docker load [options]\n   \n   docker load --help\n   \n   Usage:  docker load [OPTIONS]\n   \n   Load an image from a tar archive or STDIN\n   \n   Options:\n     -i, --input string   Read from tar archive file, instead of STDIN\n     -q, --quiet          Suppress the load output\n   \n\n * 示例 docker load -i nginx.tar 或 docker load < nginx.tar 其中-i和<表示从文件输入。会成功导入镜像及相关元数据，包括tag信息\n\n\n# export\n\n * 命令 docker export [options] container\n   \n   docker load --help\n   \n   Usage:  docker load [OPTIONS]\n   \n   Load an image from a tar archive or STDIN\n   \n   Options:\n     -i, --input string   Read from tar archive file, instead of STDIN\n     -q, --quiet          Suppress the load output\n   docker export --help\n   \n   Usage:  docker export [OPTIONS] CONTAINER\n   \n   Export a container\'s filesystem as a tar archive\n   \n   Options:\n     -o, --output string   Write to a file, instead of STDOUT\n   \n\n * 示例 docker export -o nginx-test.tar nginx-test 其中-o表示输出到文件，nginx-test.tar为目标文件，nginx-test是源容器名（name）\n\n\n# import\n\n * 命令 docker import [options] file|URL|- [REPOSITORY[:TAG]]\n   \n   docker import --help\n   \n   Usage:  docker import [OPTIONS] file|URL|- [REPOSITORY[:TAG]]\n   \n   Import the contents from a tarball to create a filesystem image\n   \n   Options:\n     -c, --change list       Apply Dockerfile instruction to the created image\n     -m, --message string    Set commit message for imported image\n         --platform string   Set platform if server is multi-platform capable\n   \n\n * 示例 docker import nginx-test.tar nginx:imp 或 cat nginx-test.tar | docker import - nginx:imp\n\n\n# 区别\n\n * export命令导出的tar文件略小于save命令导出的\n   \n   ll -al\n   total 802252\n   drwxrwxr-x 2 heyuqiang heyuqiang      4096 Aug 22 14:50 ./\n   drwxrwxr-x 4 heyuqiang heyuqiang      4096 Aug 22 14:06 ../\n   -rw------- 1 heyuqiang heyuqiang 136842240 Aug 22 14:45 nginx.tar\n   -rw------- 1 heyuqiang heyuqiang 135116800 Aug 22 14:50 nginx-test.tar\n   \n\n * export命令是从容器（container）中导出tar文件，而save命令则是从镜像（images）中导出\n * 基于第二点，export导出的文件再import回去时，无法保留镜像所有历史（即每一层layer信息，不熟悉的可以去看Dockerfile），不能进行回滚操作；而save是依据镜像来的，所以导入时可以完整保留下每一层layer信息。如下图所示，nginx:latest是save导出load导入的，nginx:imp是export导出import导入的。\n   \n   docker history nginx:latest \n   IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n   4bb46517cac3        8 days ago          /bin/sh -c #(nop)  CMD ["nginx" "-g" "daemon…   0B                  \n   <missing>           8 days ago          /bin/sh -c #(nop)  STOPSIGNAL SIGTERM           0B                  \n   <missing>           8 days ago          /bin/sh -c #(nop)  EXPOSE 80                    0B                  \n   <missing>           8 days ago          /bin/sh -c #(nop)  ENTRYPOINT ["/docker-entr…   0B                  \n   <missing>           8 days ago          /bin/sh -c #(nop) COPY file:0fd5fca330dcd6a7…   1.04kB              \n   <missing>           8 days ago          /bin/sh -c #(nop) COPY file:1d0a4127e78a26c1…   1.96kB              \n   <missing>           8 days ago          /bin/sh -c #(nop) COPY file:e7e183879c35719c…   1.2kB               \n   <missing>           8 days ago          /bin/sh -c set -x     && addgroup --system -…   63.4MB              \n   <missing>           8 days ago          /bin/sh -c #(nop)  ENV PKG_RELEASE=1~buster     0B                  \n   <missing>           8 days ago          /bin/sh -c #(nop)  ENV NJS_VERSION=0.4.3        0B                  \n   <missing>           8 days ago          /bin/sh -c #(nop)  ENV NGINX_VERSION=1.19.2     0B                  \n   <missing>           2 weeks ago         /bin/sh -c #(nop)  LABEL maintainer=NGINX Do…   0B                  \n   <missing>           2 weeks ago         /bin/sh -c #(nop)  CMD ["bash"]                 0B                  \n   <missing>           2 weeks ago         /bin/sh -c #(nop) ADD file:3af3091e7d2bb40bc…   69.2MB              \n   docker history nginx:imp\n   IMAGE               CREATED             CREATED BY          SIZE                COMMENT\n   b6a847fedf23        47 seconds ago                          131MB               Imported from -\n   \n\n\n# 建议\n\n可以依据具体使用场景来选择命令\n\n * 若是只想备份images，使用save、load即可\n * 若是在启动容器后，容器内容有变化，需要备份，则使用export、import',normalizedContent:'# docker 镜像导入导出\n\n本文介绍docker镜像的导入导出，用于迁移、备份、升级等场景，准备环境如下：\n\n * ubuntu 18.04.5 lts\n * docker 19.03.8\n\n\n# 导入导出命令介绍\n\n涉及的命令有export、import、save、load\n\n\n# save\n\n * 命令 docker save [options] images [images...]\n   \n   docker save --help\n   \n   usage:  docker save [options] image [image...]\n   \n   save one or more images to a tar archive (streamed to stdout by default)\n   \n   options:\n     -o, --output string   write to a file, instead of stdout\n   \n\n * 示例 docker save -o nginx.tar nginx:latest 或 docker save > nginx.tar nginx:latest 其中-o和>表示输出到文件，nginx.tar为目标文件，nginx:latest是源镜像名（name:tag）\n\n\n# load\n\n * 命令 docker load [options]\n   \n   docker load --help\n   \n   usage:  docker load [options]\n   \n   load an image from a tar archive or stdin\n   \n   options:\n     -i, --input string   read from tar archive file, instead of stdin\n     -q, --quiet          suppress the load output\n   \n\n * 示例 docker load -i nginx.tar 或 docker load < nginx.tar 其中-i和<表示从文件输入。会成功导入镜像及相关元数据，包括tag信息\n\n\n# export\n\n * 命令 docker export [options] container\n   \n   docker load --help\n   \n   usage:  docker load [options]\n   \n   load an image from a tar archive or stdin\n   \n   options:\n     -i, --input string   read from tar archive file, instead of stdin\n     -q, --quiet          suppress the load output\n   docker export --help\n   \n   usage:  docker export [options] container\n   \n   export a container\'s filesystem as a tar archive\n   \n   options:\n     -o, --output string   write to a file, instead of stdout\n   \n\n * 示例 docker export -o nginx-test.tar nginx-test 其中-o表示输出到文件，nginx-test.tar为目标文件，nginx-test是源容器名（name）\n\n\n# import\n\n * 命令 docker import [options] file|url|- [repository[:tag]]\n   \n   docker import --help\n   \n   usage:  docker import [options] file|url|- [repository[:tag]]\n   \n   import the contents from a tarball to create a filesystem image\n   \n   options:\n     -c, --change list       apply dockerfile instruction to the created image\n     -m, --message string    set commit message for imported image\n         --platform string   set platform if server is multi-platform capable\n   \n\n * 示例 docker import nginx-test.tar nginx:imp 或 cat nginx-test.tar | docker import - nginx:imp\n\n\n# 区别\n\n * export命令导出的tar文件略小于save命令导出的\n   \n   ll -al\n   total 802252\n   drwxrwxr-x 2 heyuqiang heyuqiang      4096 aug 22 14:50 ./\n   drwxrwxr-x 4 heyuqiang heyuqiang      4096 aug 22 14:06 ../\n   -rw------- 1 heyuqiang heyuqiang 136842240 aug 22 14:45 nginx.tar\n   -rw------- 1 heyuqiang heyuqiang 135116800 aug 22 14:50 nginx-test.tar\n   \n\n * export命令是从容器（container）中导出tar文件，而save命令则是从镜像（images）中导出\n * 基于第二点，export导出的文件再import回去时，无法保留镜像所有历史（即每一层layer信息，不熟悉的可以去看dockerfile），不能进行回滚操作；而save是依据镜像来的，所以导入时可以完整保留下每一层layer信息。如下图所示，nginx:latest是save导出load导入的，nginx:imp是export导出import导入的。\n   \n   docker history nginx:latest \n   image               created             created by                                      size                comment\n   4bb46517cac3        8 days ago          /bin/sh -c #(nop)  cmd ["nginx" "-g" "daemon…   0b                  \n   <missing>           8 days ago          /bin/sh -c #(nop)  stopsignal sigterm           0b                  \n   <missing>           8 days ago          /bin/sh -c #(nop)  expose 80                    0b                  \n   <missing>           8 days ago          /bin/sh -c #(nop)  entrypoint ["/docker-entr…   0b                  \n   <missing>           8 days ago          /bin/sh -c #(nop) copy file:0fd5fca330dcd6a7…   1.04kb              \n   <missing>           8 days ago          /bin/sh -c #(nop) copy file:1d0a4127e78a26c1…   1.96kb              \n   <missing>           8 days ago          /bin/sh -c #(nop) copy file:e7e183879c35719c…   1.2kb               \n   <missing>           8 days ago          /bin/sh -c set -x     && addgroup --system -…   63.4mb              \n   <missing>           8 days ago          /bin/sh -c #(nop)  env pkg_release=1~buster     0b                  \n   <missing>           8 days ago          /bin/sh -c #(nop)  env njs_version=0.4.3        0b                  \n   <missing>           8 days ago          /bin/sh -c #(nop)  env nginx_version=1.19.2     0b                  \n   <missing>           2 weeks ago         /bin/sh -c #(nop)  label maintainer=nginx do…   0b                  \n   <missing>           2 weeks ago         /bin/sh -c #(nop)  cmd ["bash"]                 0b                  \n   <missing>           2 weeks ago         /bin/sh -c #(nop) add file:3af3091e7d2bb40bc…   69.2mb              \n   docker history nginx:imp\n   image               created             created by          size                comment\n   b6a847fedf23        47 seconds ago                          131mb               imported from -\n   \n\n\n# 建议\n\n可以依据具体使用场景来选择命令\n\n * 若是只想备份images，使用save、load即可\n * 若是在启动容器后，容器内容有变化，需要备份，则使用export、import',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"利用 commit 理解镜像构成",frontmatter:{title:"利用 commit 理解镜像构成",date:"2022-02-08T17:58:04.000Z",permalink:"/pages/134f38/"},regularPath:"/17.Docker/04.Docker%20%E9%95%9C%E5%83%8F/06.%E5%88%A9%E7%94%A8%20commit%20%E7%90%86%E8%A7%A3%E9%95%9C%E5%83%8F%E6%9E%84%E6%88%90.html",relativePath:"17.Docker/04.Docker 镜像/06.利用 commit 理解镜像构成.md",key:"v-16c13813",path:"/pages/134f38/",headers:[{level:2,title:"慎用 docker commit",slug:"慎用-docker-commit",normalizedTitle:"慎用 docker commit",charIndex:3876}],headersStr:"慎用 docker commit",content:'# 利用 commit 理解镜像构成\n\n注意： docker commit 命令除了学习之外，还有一些特殊的应用场合，比如被入侵后保存现场等。但是，不要使用 docker commit 定制镜像，定制镜像应该使用 Dockerfile 来完成。如果你想要定制镜像请查看下一小节。\n\n镜像是容器的基础，每次执行 docker run 的时候都会指定哪个镜像作为容器运行的基础。在之前的例子中，我们所使用的都是来自于 Docker Hub 的镜像。直接使用这些镜像是可以满足一定的需求，而当这些镜像无法直接满足需求时，我们就需要定制这些镜像。接下来的几节就将讲解如何定制镜像。\n\n回顾一下之前我们学到的知识，镜像是多层存储，每一层是在前一层的基础上进行的修改；而容器同样也是多层存储，是在以镜像为基础层，在其基础上加一层作为容器运行时的存储层。\n\n现在让我们以定制一个 Web 服务器为例子，来讲解镜像是如何构建的。\n\ndocker run --name webserver -d -p 80:80 nginx\n\n\n这条命令会用 nginx 镜像启动一个容器，命名为 webserver，并且映射了 80 端口，这样我们可以用浏览器去访问这个 nginx 服务器。\n\n如果是在 Linux 本机运行的 Docker，或者如果使用的是 Docker for Mac、Docker for Windows，那么可以直接访问：http://localhost；如果使用的是 Docker Toolbox，或者是在虚拟机、云服务器上安装的 Docker，则需要将 localhost 换为虚拟机地址或者实际云服务器地址。\n\n直接用浏览器访问的话，我们会看到默认的 Nginx 欢迎页面。\n\n\n\n现在，假设我们非常不喜欢这个欢迎页面，我们希望改成欢迎 Docker 的文字，我们可以使用 docker exec 命令进入容器，修改其内容。\n\ndocker exec -it webserver bash\nroot@3729b97e8226:/# echo \'<h1>Hello, Docker!</h1>\' > /usr/share/nginx/html/index.html\nroot@3729b97e8226:/# exit\nexit\n\n\n我们以交互式终端方式进入 webserver 容器，并执行了 bash 命令，也就是获得一个可操作的 Shell。\n\n然后，我们用 Hello, Docker! 覆盖了 /usr/share/nginx/html/index.html 的内容。\n\n现在我们再刷新浏览器的话，会发现内容被改变了。\n\n\n\n我们修改了容器的文件，也就是改动了容器的存储层。我们可以通过 docker diff 命令看到具体的改动。\n\ndocker diff webserver\nC /root\nA /root/.bash_history\nC /run\nC /usr\nC /usr/share\nC /usr/share/nginx\nC /usr/share/nginx/html\nC /usr/share/nginx/html/index.html\nC /var\nC /var/cache\nC /var/cache/nginx\nA /var/cache/nginx/client_temp\nA /var/cache/nginx/fastcgi_temp\nA /var/cache/nginx/proxy_temp\nA /var/cache/nginx/scgi_temp\nA /var/cache/nginx/uwsgi_temp\n\n\n现在我们定制好了变化，我们希望能将其保存下来形成镜像。\n\n要知道，当我们运行一个容器的时候（如果不使用卷的话），我们做的任何文件修改都会被记录于容器存储层里。而 Docker 提供了一个 docker commit 命令，可以将容器的存储层保存下来成为镜像。换句话说，就是在原有镜像的基础上，再叠加上容器的存储层，并构成新的镜像。以后我们运行这个新镜像的时候，就会拥有原有容器最后的文件变化。\n\ndocker commit 的语法格式为：\n\ndocker commit [选项] <容器ID或容器名> [<仓库名>[:<标签>]]\n\n\n我们可以用下面的命令将容器保存为镜像：\n\ndocker commit \\\n    --author "Tao Wang <twang2218@gmail.com>" \\\n    --message "修改了默认网页" \\\n    webserver \\\n    nginx:v2\nsha256:07e33465974800ce65751acc279adc6ed2dc5ed4e0838f8b86f0c87aa1795214\n\n\n其中 --author 是指定修改的作者，而 --message 则是记录本次修改的内容。这点和 git 版本控制相似，不过这里这些信息可以省略留空。\n\n我们可以在 docker image ls 中看到这个新定制的镜像：\n\ndocker image ls nginx\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nnginx               v2                  07e334659748        9 seconds ago       181.5 MB\nnginx               1.11                05a60462f8ba        12 days ago         181.5 MB\nnginx               latest              e43d811ce2f4        4 weeks ago         181.5 MB```\n\n我们还可以用 `docker history` 具体查看镜像内的历史记录，如果比较 `nginx:latest` 的历史记录，我们会发现新增了我们刚刚提交的这一层。\n\n```bash\ndocker history nginx:v2\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n07e334659748        54 seconds ago      nginx -g daemon off;                            95 B                修改了默认网页\ne43d811ce2f4        4 weeks ago         /bin/sh -c #(nop)  CMD ["nginx" "-g" "daemon    0 B\n<missing>           4 weeks ago         /bin/sh -c #(nop)  EXPOSE 443/tcp 80/tcp        0 B\n<missing>           4 weeks ago         /bin/sh -c ln -sf /dev/stdout /var/log/nginx/   22 B\n<missing>           4 weeks ago         /bin/sh -c apt-key adv --keyserver hkp://pgp.   58.46 MB\n<missing>           4 weeks ago         /bin/sh -c #(nop)  ENV NGINX_VERSION=1.11.5-1   0 B\n<missing>           4 weeks ago         /bin/sh -c #(nop)  MAINTAINER NGINX Docker Ma   0 B\n<missing>           4 weeks ago         /bin/sh -c #(nop)  CMD ["/bin/bash"]            0 B\n<missing>           4 weeks ago         /bin/sh -c #(nop) ADD file:23aa4f893e3288698c   123 MB\n\n\n新的镜像定制好后，我们可以来运行这个镜像。\n\ndocker run --name web2 -d -p 81:80 nginx:v2\n\n\n这里我们命名为新的服务为 web2，并且映射到 81 端口。如果是 Docker for Mac/Windows 或 Linux 桌面的话，我们就可以直接访问 http://localhost:81 看到结果，其内容应该和之前修改后的 webserver 一样。\n\n至此，我们第一次完成了定制镜像，使用的是 docker commit 命令，手动操作给旧的镜像添加了新的一层，形成新的镜像，对镜像多层存储应该有了更直观的感觉。\n\n\n# 慎用 docker commit\n\n使用 docker commit 命令虽然可以比较直观的帮助理解镜像分层存储的概念，但是实际环境中并不会这样使用。\n\n首先，如果仔细观察之前的 docker diff webserver 的结果，你会发现除了真正想要修改的 /usr/share/nginx/html/index.html 文件外，由于命令的执行，还有很多文件被改动或添加了。这还仅仅是最简单的操作，如果是安装软件包、编译构建，那会有大量的无关内容被添加进来，如果不小心清理，将会导致镜像极为臃肿。\n\n此外，使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。而且，即使是这个制作镜像的人，过一段时间后也无法记清具体在操作的。虽然 docker diff 或许可以告诉得到一些线索，但是远远不到可以确保生成一致镜像的地步。这种黑箱镜像的维护工作是非常痛苦的。\n\n而且，回顾之前提及的镜像所使用的分层存储的概念，除当前层外，之前的每一层都是不会发生改变的，换句话说，任何修改的结果仅仅是在当前层进行标记、添加、修改，而不会改动上一层。如果使用 docker commit 制作镜像，以及后期修改的话，每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像，即使根本无法访问到。这会让镜像更加臃肿。',normalizedContent:'# 利用 commit 理解镜像构成\n\n注意： docker commit 命令除了学习之外，还有一些特殊的应用场合，比如被入侵后保存现场等。但是，不要使用 docker commit 定制镜像，定制镜像应该使用 dockerfile 来完成。如果你想要定制镜像请查看下一小节。\n\n镜像是容器的基础，每次执行 docker run 的时候都会指定哪个镜像作为容器运行的基础。在之前的例子中，我们所使用的都是来自于 docker hub 的镜像。直接使用这些镜像是可以满足一定的需求，而当这些镜像无法直接满足需求时，我们就需要定制这些镜像。接下来的几节就将讲解如何定制镜像。\n\n回顾一下之前我们学到的知识，镜像是多层存储，每一层是在前一层的基础上进行的修改；而容器同样也是多层存储，是在以镜像为基础层，在其基础上加一层作为容器运行时的存储层。\n\n现在让我们以定制一个 web 服务器为例子，来讲解镜像是如何构建的。\n\ndocker run --name webserver -d -p 80:80 nginx\n\n\n这条命令会用 nginx 镜像启动一个容器，命名为 webserver，并且映射了 80 端口，这样我们可以用浏览器去访问这个 nginx 服务器。\n\n如果是在 linux 本机运行的 docker，或者如果使用的是 docker for mac、docker for windows，那么可以直接访问：http://localhost；如果使用的是 docker toolbox，或者是在虚拟机、云服务器上安装的 docker，则需要将 localhost 换为虚拟机地址或者实际云服务器地址。\n\n直接用浏览器访问的话，我们会看到默认的 nginx 欢迎页面。\n\n\n\n现在，假设我们非常不喜欢这个欢迎页面，我们希望改成欢迎 docker 的文字，我们可以使用 docker exec 命令进入容器，修改其内容。\n\ndocker exec -it webserver bash\nroot@3729b97e8226:/# echo \'<h1>hello, docker!</h1>\' > /usr/share/nginx/html/index.html\nroot@3729b97e8226:/# exit\nexit\n\n\n我们以交互式终端方式进入 webserver 容器，并执行了 bash 命令，也就是获得一个可操作的 shell。\n\n然后，我们用 hello, docker! 覆盖了 /usr/share/nginx/html/index.html 的内容。\n\n现在我们再刷新浏览器的话，会发现内容被改变了。\n\n\n\n我们修改了容器的文件，也就是改动了容器的存储层。我们可以通过 docker diff 命令看到具体的改动。\n\ndocker diff webserver\nc /root\na /root/.bash_history\nc /run\nc /usr\nc /usr/share\nc /usr/share/nginx\nc /usr/share/nginx/html\nc /usr/share/nginx/html/index.html\nc /var\nc /var/cache\nc /var/cache/nginx\na /var/cache/nginx/client_temp\na /var/cache/nginx/fastcgi_temp\na /var/cache/nginx/proxy_temp\na /var/cache/nginx/scgi_temp\na /var/cache/nginx/uwsgi_temp\n\n\n现在我们定制好了变化，我们希望能将其保存下来形成镜像。\n\n要知道，当我们运行一个容器的时候（如果不使用卷的话），我们做的任何文件修改都会被记录于容器存储层里。而 docker 提供了一个 docker commit 命令，可以将容器的存储层保存下来成为镜像。换句话说，就是在原有镜像的基础上，再叠加上容器的存储层，并构成新的镜像。以后我们运行这个新镜像的时候，就会拥有原有容器最后的文件变化。\n\ndocker commit 的语法格式为：\n\ndocker commit [选项] <容器id或容器名> [<仓库名>[:<标签>]]\n\n\n我们可以用下面的命令将容器保存为镜像：\n\ndocker commit \\\n    --author "tao wang <twang2218@gmail.com>" \\\n    --message "修改了默认网页" \\\n    webserver \\\n    nginx:v2\nsha256:07e33465974800ce65751acc279adc6ed2dc5ed4e0838f8b86f0c87aa1795214\n\n\n其中 --author 是指定修改的作者，而 --message 则是记录本次修改的内容。这点和 git 版本控制相似，不过这里这些信息可以省略留空。\n\n我们可以在 docker image ls 中看到这个新定制的镜像：\n\ndocker image ls nginx\nrepository          tag                 image id            created             size\nnginx               v2                  07e334659748        9 seconds ago       181.5 mb\nnginx               1.11                05a60462f8ba        12 days ago         181.5 mb\nnginx               latest              e43d811ce2f4        4 weeks ago         181.5 mb```\n\n我们还可以用 `docker history` 具体查看镜像内的历史记录，如果比较 `nginx:latest` 的历史记录，我们会发现新增了我们刚刚提交的这一层。\n\n```bash\ndocker history nginx:v2\nimage               created             created by                                      size                comment\n07e334659748        54 seconds ago      nginx -g daemon off;                            95 b                修改了默认网页\ne43d811ce2f4        4 weeks ago         /bin/sh -c #(nop)  cmd ["nginx" "-g" "daemon    0 b\n<missing>           4 weeks ago         /bin/sh -c #(nop)  expose 443/tcp 80/tcp        0 b\n<missing>           4 weeks ago         /bin/sh -c ln -sf /dev/stdout /var/log/nginx/   22 b\n<missing>           4 weeks ago         /bin/sh -c apt-key adv --keyserver hkp://pgp.   58.46 mb\n<missing>           4 weeks ago         /bin/sh -c #(nop)  env nginx_version=1.11.5-1   0 b\n<missing>           4 weeks ago         /bin/sh -c #(nop)  maintainer nginx docker ma   0 b\n<missing>           4 weeks ago         /bin/sh -c #(nop)  cmd ["/bin/bash"]            0 b\n<missing>           4 weeks ago         /bin/sh -c #(nop) add file:23aa4f893e3288698c   123 mb\n\n\n新的镜像定制好后，我们可以来运行这个镜像。\n\ndocker run --name web2 -d -p 81:80 nginx:v2\n\n\n这里我们命名为新的服务为 web2，并且映射到 81 端口。如果是 docker for mac/windows 或 linux 桌面的话，我们就可以直接访问 http://localhost:81 看到结果，其内容应该和之前修改后的 webserver 一样。\n\n至此，我们第一次完成了定制镜像，使用的是 docker commit 命令，手动操作给旧的镜像添加了新的一层，形成新的镜像，对镜像多层存储应该有了更直观的感觉。\n\n\n# 慎用 docker commit\n\n使用 docker commit 命令虽然可以比较直观的帮助理解镜像分层存储的概念，但是实际环境中并不会这样使用。\n\n首先，如果仔细观察之前的 docker diff webserver 的结果，你会发现除了真正想要修改的 /usr/share/nginx/html/index.html 文件外，由于命令的执行，还有很多文件被改动或添加了。这还仅仅是最简单的操作，如果是安装软件包、编译构建，那会有大量的无关内容被添加进来，如果不小心清理，将会导致镜像极为臃肿。\n\n此外，使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。而且，即使是这个制作镜像的人，过一段时间后也无法记清具体在操作的。虽然 docker diff 或许可以告诉得到一些线索，但是远远不到可以确保生成一致镜像的地步。这种黑箱镜像的维护工作是非常痛苦的。\n\n而且，回顾之前提及的镜像所使用的分层存储的概念，除当前层外，之前的每一层都是不会发生改变的，换句话说，任何修改的结果仅仅是在当前层进行标记、添加、修改，而不会改动上一层。如果使用 docker commit 制作镜像，以及后期修改的话，每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像，即使根本无法访问到。这会让镜像更加臃肿。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"使用 Dockerfile 定制镜像",frontmatter:{title:"使用 Dockerfile 定制镜像",date:"2022-02-08T17:58:04.000Z",permalink:"/pages/ffea77/"},regularPath:"/17.Docker/04.Docker%20%E9%95%9C%E5%83%8F/07.%E4%BD%BF%E7%94%A8%20Dockerfile%20%E5%AE%9A%E5%88%B6%E9%95%9C%E5%83%8F.html",relativePath:"17.Docker/04.Docker 镜像/07.使用 Dockerfile 定制镜像.md",key:"v-65ae1b58",path:"/pages/ffea77/",headers:[{level:2,title:"Dockerfile 定制镜像",slug:"dockerfile-定制镜像",normalizedTitle:"dockerfile 定制镜像",charIndex:5},{level:2,title:"FROM 指定基础镜像",slug:"from-指定基础镜像",normalizedTitle:"from 指定基础镜像",charIndex:543},{level:2,title:"RUN 执行命令",slug:"run-执行命令",normalizedTitle:"run 执行命令",charIndex:1331},{level:2,title:"构建镜像",slug:"构建镜像",normalizedTitle:"构建镜像",charIndex:3381},{level:2,title:"镜像构建上下文（Context）",slug:"镜像构建上下文-context",normalizedTitle:"镜像构建上下文（context）",charIndex:4107},{level:2,title:"其它 docker build 的用法",slug:"其它-docker-build-的用法",normalizedTitle:"其它 docker build 的用法",charIndex:6059},{level:3,title:"直接用 Git repo 进行构建",slug:"直接用-git-repo-进行构建",normalizedTitle:"直接用 git repo 进行构建",charIndex:6083},{level:3,title:"用给定的 tar 压缩包构建",slug:"用给定的-tar-压缩包构建",normalizedTitle:"用给定的 tar 压缩包构建",charIndex:6600},{level:3,title:"[#](https://wowox.com/zh/docs-docker/Docker-使用-Dockerfile-定制镜像.html#从标准输入中读取-dockerfile-进行构建)从标准输入中读取 Dockerfile 进行构建",slug:"从标准输入中读取-dockerfile-进行构建",normalizedTitle:'<a href="https://wowox.com/zh/docs-docker/docker-%e4%bd%bf%e7%94%a8-dockerfile-%e5%ae%9a%e5%88%b6%e9%95%9c%e5%83%8f.html#%e4%bb%8e%e6%a0%87%e5%87%86%e8%be%93%e5%85%a5%e4%b8%ad%e8%af%bb%e5%8f%96-dockerfile-%e8%bf%9b%e8%a1%8c%e6%9e%84%e5%bb%ba" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>从标准输入中读取 dockerfile 进行构建',charIndex:null},{level:3,title:"从标准输入中读取上下文压缩包进行构建",slug:"从标准输入中读取上下文压缩包进行构建",normalizedTitle:"从标准输入中读取上下文压缩包进行构建",charIndex:6950}],headersStr:"Dockerfile 定制镜像 FROM 指定基础镜像 RUN 执行命令 构建镜像 镜像构建上下文（Context） 其它 docker build 的用法 直接用 Git repo 进行构建 用给定的 tar 压缩包构建 [#](https://wowox.com/zh/docs-docker/Docker-使用-Dockerfile-定制镜像.html#从标准输入中读取-dockerfile-进行构建)从标准输入中读取 Dockerfile 进行构建 从标准输入中读取上下文压缩包进行构建",content:'# 使用 Dockerfile 定制镜像\n\n\n# Dockerfile 定制镜像\n\n从刚才的 docker commit 的学习中，我们可以了解到，镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。\n\nDockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。\n\n还以之前定制 nginx 镜像为例，这次我们使用 Dockerfile 来定制。\n\n在一个空白目录中，建立一个文本文件，并命名为 Dockerfile：\n\nmkdir mynginx\ncd mynginx\ntouch Dockerfile\n\n\n其内容为：\n\nFROM nginx\nRUN echo \'<h1>Hello, Docker!</h1>\' > /usr/share/nginx/html/index.html\n\n\n这个 Dockerfile 很简单，一共就两行。涉及到了两条指令，FROM 和 RUN。\n\n\n# FROM 指定基础镜像\n\n所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个 nginx 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 FROM 就是指定基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。\n\n在 Docker Store 上有非常多的高质量的官方镜像，有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat 等；也有一些方便开发、构建、运行各种语言应用的镜像，如 node、openjdk、python、ruby、golang 等。可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。\n\n如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如 ubuntu、debian、centos、fedora、alpine 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。\n\n除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。\n\nFROM scratch\n...\n\n\n如果你以 scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。\n\n不以任何系统为基础，直接将可执行文件复制进镜像的做法并不罕见，比如 swarm、coreos/etcd。对于 Linux 下静态编译的程序来说，并不需要有操作系统提供运行时支持，所需的一切库都已经在可执行文件里了，因此直接 FROM scratch 会让镜像体积更加小巧。使用 Go 语言 开发的应用很多会使用这种方式来制作镜像，这也是为什么有人认为 Go 是特别适合容器微服务架构的语言的原因之一。\n\n\n# RUN 执行命令\n\nRUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN 指令在定制镜像时是最常用的指令之一。其格式有两种：\n\n * shell 格式：RUN <命令>，就像直接在命令行中输入的命令一样。刚才写的 Dockerfile 中的 RUN 指令就是这种格式。\n\nRUN echo \'<h1>Hello, Docker!</h1>\' > /usr/share/nginx/html/index.html\n\n\n * exec 格式：RUN ["可执行文件", "参数1", "参数2"]，这更像是函数调用中的格式。\n\n既然 RUN 就像 Shell 脚本一样可以执行命令，那么我们是否就可以像 Shell 脚本一样把每个命令对应一个 RUN 呢？比如这样：\n\nFROM debian:jessie\n\nRUN apt-get update\nRUN apt-get install -y gcc libc6-dev make\nRUN wget -O redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz"\nRUN mkdir -p /usr/src/redis\nRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1\nRUN make -C /usr/src/redis\nRUN make -C /usr/src/redis install\n\n\n之前说过，Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。\n\n而上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 Docker 的人常犯的一个错误。\n\nUnion FS 是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。\n\n上面的 Dockerfile 正确的写法应该是这样：\n\nFROM debian:jessie\n\nRUN buildDeps=\'gcc libc6-dev make\' \\\n    && apt-get update \\\n    && apt-get install -y $buildDeps \\\n    && wget -O redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz" \\\n    && mkdir -p /usr/src/redis \\\n    && tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\\n    && make -C /usr/src/redis \\\n    && make -C /usr/src/redis install \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && rm redis.tar.gz \\\n    && rm -r /usr/src/redis \\\n    && apt-get purge -y --auto-remove $buildDeps\n\n\n首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个 RUN 对一一对应不同的命令，而是仅仅使用一个 RUN 指令，并使用 && 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。\n\n并且，这里为了格式化还进行了换行。Dockerfile 支持 Shell 类的行尾添加 \\ 的命令换行方式，以及行首 # 进行注释的格式。良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。\n\n此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。\n\n很多人初学 Docker 制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。\n\n\n# 构建镜像\n\n好了，让我们再回到之前定制的 nginx 镜像的 Dockerfile 来。现在我们明白了这个 Dockerfile 的内容，那么让我们来构建这个镜像吧。\n\n在 Dockerfile 文件所在目录执行：\n\ndocker build -t nginx:v3 .\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM nginx\n ---\x3e e43d811ce2f4\nStep 2 : RUN echo \'<h1>Hello, Docker!</h1>\' > /usr/share/nginx/html/index.html\n ---\x3e Running in 9cdc27646c7b\n ---\x3e 44aa4490ce2c\nRemoving intermediate container 9cdc27646c7b\nSuccessfully built 44aa4490ce2c\n\n\n从命令的输出结果中，我们可以清晰的看到镜像的构建过程。在 Step 2 中，如同我们之前所说的那样，RUN 指令启动了一个容器 9cdc27646c7b，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c，随后删除了所用到的这个容器 9cdc27646c7b。\n\n这里我们使用了 docker build 命令进行镜像构建。其格式为：\n\ndocker build [选项] <上下文路径/URL/->\n\n\n在这里我们指定了最终镜像的名称 -t nginx:v3，构建成功后，我们可以像之前运行 nginx:v2 那样来运行这个镜像，其结果会和 nginx:v2 一样。\n\n\n# 镜像构建上下文（Context）\n\n如果注意，会看到 docker build 命令最后有一个 .。. 表示当前目录，而 Dockerfile 就在当前目录，因此不少初学者以为这个路径是在指定 Dockerfile 所在路径，这么理解其实是不准确的。如果对应上面的命令格式，你可能会发现，这是在指定上下文路径。那么什么是上下文呢？\n\n首先我们要理解 docker build 的工作原理。Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API 与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。也因为这种 C/S 设计，让我们操作远程服务器的 Docker 引擎变得轻而易举。\n\n当我们进行镜像构建的时候，并非所有定制都会通过 RUN 指令完成，经常会需要将一些本地文件复制进镜像，比如通过 COPY 指令、ADD 指令等。而 docker build 命令构建镜像，其实并非在本地构建，而是在服务端，也就是 Docker 引擎中构建的。那么在这种客户端/服务端的架构中，如何才能让服务端获得本地文件呢？\n\n这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。\n\n如果在 Dockerfile 中这么写：\n\nCOPY ./package.json /app/\n\n\n这并不是要复制执行 docker build 命令所在的目录下的 package.json，也不是复制 Dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的 package.json。\n\n因此，COPY 这类指令中的源文件的路径都是相对路径。这也是初学者经常会问的为什么 COPY ../package.json /app 或者 COPY /opt/xxxx /app 无法工作的原因，因为这些路径已经超出了上下文的范围，Docker 引擎无法获得这些位置的文件。如果真的需要那些文件，应该将它们复制到上下文目录中去。\n\n现在就可以理解刚才的命令 docker build -t nginx:v3 . 中的这个 .，实际上是在指定上下文的目录，docker build 命令会将该目录下的内容打包交给 Docker 引擎以帮助构建镜像。\n\n如果观察 docker build 输出，我们其实已经看到了这个发送上下文的过程：\n\ndocker build -t nginx:v3 .\nSending build context to Docker daemon 2.048 kB\n...\n\n\n理解构建上下文对于镜像构建是很重要的，避免犯一些不应该的错误。比如有些初学者在发现 COPY /opt/xxxx /app 不工作后，于是干脆将 Dockerfile 放到了硬盘根目录去构建，结果发现 docker build 执行后，在发送一个几十 GB 的东西，极为缓慢而且很容易构建失败。那是因为这种做法是在让 docker build 打包整个硬盘，这显然是使用错误。\n\n一般来说，应该会将 Dockerfile 置于一个空目录下，或者项目根目录下。如果该目录下没有所需文件，那么应该把所需文件复制一份过来。如果目录下有些东西确实不希望构建时传给 Docker 引擎，那么可以用 .gitignore 一样的语法写一个 .dockerignore，该文件是用于剔除不需要作为上下文传递给 Docker 引擎的。\n\n那么为什么会有人误以为 . 是指定 Dockerfile 所在目录呢？这是因为在默认情况下，如果不额外指定 Dockerfile 的话，会将上下文目录下的名为 Dockerfile 的文件作为 Dockerfile。\n\n这只是默认行为，实际上 Dockerfile 的文件名并不要求必须为 Dockerfile，而且并不要求必须位于上下文目录中，比如可以用 -f ../Dockerfile.php 参数指定某个文件作为 Dockerfile。\n\n当然，一般大家习惯性的会使用默认的文件名 Dockerfile，以及会将其置于镜像构建上下文目录中。\n\n\n# 其它 docker build 的用法\n\n\n# 直接用 Git repo 进行构建\n\n或许你已经注意到了，docker build 还支持从 URL 构建，比如可以直接从 Git repo 中构建：\n\ndocker build https://github.com/twang2218/gitlab-ce-zh.git#:8.14\ndocker build https://github.com/twang2218/gitlab-ce-zh.git\\#:8.14\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM gitlab/gitlab-ce:8.14.0-ce.0\n8.14.0-ce.0: Pulling from gitlab/gitlab-ce\naed15891ba52: Already exists\n773ae8583d14: Already exists\n...\n\n\n这行命令指定了构建所需的 Git repo，并且指定默认的 master 分支，构建目录为 /8.14/，然后 Docker 就会自己去 git clone 这个项目、切换到指定分支、并进入到指定目录后开始构建。\n\n\n# 用给定的 tar 压缩包构建\n\ndocker build http://server/context.tar.gz\n\n\n如果所给出的 URL 不是个 Git repo，而是个 tar 压缩包，那么 Docker 引擎会下载这个包，并自动解压缩，以其作为上下文，开始构建。\n\n\n# #从标准输入中读取 Dockerfile 进行构建\n\ndocker build - < Dockerfile\n\n\n或\n\ncat Dockerfile | docker build -\n\n\n如果标准输入传入的是文本文件，则将其视为 Dockerfile，并开始构建。这种形式由于直接从标准输入中读取 Dockerfile 的内容，它没有上下文，因此不可以像其他方法那样可以将本地文件 COPY 进镜像之类的事情。\n\n\n# 从标准输入中读取上下文压缩包进行构建\n\ndocker build - < context.tar.gz\n\n\n如果发现标准输入的文件格式是 gzip、bzip2 以及 xz 的话，将会使其为上下文压缩包，直接将其展开，将里面视为上下文，并开始构建。',normalizedContent:'# 使用 dockerfile 定制镜像\n\n\n# dockerfile 定制镜像\n\n从刚才的 docker commit 的学习中，我们可以了解到，镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 dockerfile。\n\ndockerfile 是一个文本文件，其内包含了一条条的指令(instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。\n\n还以之前定制 nginx 镜像为例，这次我们使用 dockerfile 来定制。\n\n在一个空白目录中，建立一个文本文件，并命名为 dockerfile：\n\nmkdir mynginx\ncd mynginx\ntouch dockerfile\n\n\n其内容为：\n\nfrom nginx\nrun echo \'<h1>hello, docker!</h1>\' > /usr/share/nginx/html/index.html\n\n\n这个 dockerfile 很简单，一共就两行。涉及到了两条指令，from 和 run。\n\n\n# from 指定基础镜像\n\n所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个 nginx 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 from 就是指定基础镜像，因此一个 dockerfile 中 from 是必备的指令，并且必须是第一条指令。\n\n在 docker store 上有非常多的高质量的官方镜像，有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat 等；也有一些方便开发、构建、运行各种语言应用的镜像，如 node、openjdk、python、ruby、golang 等。可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。\n\n如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如 ubuntu、debian、centos、fedora、alpine 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。\n\n除了选择现有镜像为基础镜像外，docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。\n\nfrom scratch\n...\n\n\n如果你以 scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。\n\n不以任何系统为基础，直接将可执行文件复制进镜像的做法并不罕见，比如 swarm、coreos/etcd。对于 linux 下静态编译的程序来说，并不需要有操作系统提供运行时支持，所需的一切库都已经在可执行文件里了，因此直接 from scratch 会让镜像体积更加小巧。使用 go 语言 开发的应用很多会使用这种方式来制作镜像，这也是为什么有人认为 go 是特别适合容器微服务架构的语言的原因之一。\n\n\n# run 执行命令\n\nrun 指令是用来执行命令行命令的。由于命令行的强大能力，run 指令在定制镜像时是最常用的指令之一。其格式有两种：\n\n * shell 格式：run <命令>，就像直接在命令行中输入的命令一样。刚才写的 dockerfile 中的 run 指令就是这种格式。\n\nrun echo \'<h1>hello, docker!</h1>\' > /usr/share/nginx/html/index.html\n\n\n * exec 格式：run ["可执行文件", "参数1", "参数2"]，这更像是函数调用中的格式。\n\n既然 run 就像 shell 脚本一样可以执行命令，那么我们是否就可以像 shell 脚本一样把每个命令对应一个 run 呢？比如这样：\n\nfrom debian:jessie\n\nrun apt-get update\nrun apt-get install -y gcc libc6-dev make\nrun wget -o redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz"\nrun mkdir -p /usr/src/redis\nrun tar -xzf redis.tar.gz -c /usr/src/redis --strip-components=1\nrun make -c /usr/src/redis\nrun make -c /usr/src/redis install\n\n\n之前说过，dockerfile 中每一个指令都会建立一层，run 也不例外。每一个 run 的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。\n\n而上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 docker 的人常犯的一个错误。\n\nunion fs 是有最大层数限制的，比如 aufs，曾经是最大不得超过 42 层，现在是不得超过 127 层。\n\n上面的 dockerfile 正确的写法应该是这样：\n\nfrom debian:jessie\n\nrun builddeps=\'gcc libc6-dev make\' \\\n    && apt-get update \\\n    && apt-get install -y $builddeps \\\n    && wget -o redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz" \\\n    && mkdir -p /usr/src/redis \\\n    && tar -xzf redis.tar.gz -c /usr/src/redis --strip-components=1 \\\n    && make -c /usr/src/redis \\\n    && make -c /usr/src/redis install \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && rm redis.tar.gz \\\n    && rm -r /usr/src/redis \\\n    && apt-get purge -y --auto-remove $builddeps\n\n\n首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个 run 对一一对应不同的命令，而是仅仅使用一个 run 指令，并使用 && 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 dockerfile 的时候，要经常提醒自己，这并不是在写 shell 脚本，而是在定义每一层该如何构建。\n\n并且，这里为了格式化还进行了换行。dockerfile 支持 shell 类的行尾添加 \\ 的命令换行方式，以及行首 # 进行注释的格式。良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。\n\n此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。\n\n很多人初学 docker 制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。\n\n\n# 构建镜像\n\n好了，让我们再回到之前定制的 nginx 镜像的 dockerfile 来。现在我们明白了这个 dockerfile 的内容，那么让我们来构建这个镜像吧。\n\n在 dockerfile 文件所在目录执行：\n\ndocker build -t nginx:v3 .\nsending build context to docker daemon 2.048 kb\nstep 1 : from nginx\n ---\x3e e43d811ce2f4\nstep 2 : run echo \'<h1>hello, docker!</h1>\' > /usr/share/nginx/html/index.html\n ---\x3e running in 9cdc27646c7b\n ---\x3e 44aa4490ce2c\nremoving intermediate container 9cdc27646c7b\nsuccessfully built 44aa4490ce2c\n\n\n从命令的输出结果中，我们可以清晰的看到镜像的构建过程。在 step 2 中，如同我们之前所说的那样，run 指令启动了一个容器 9cdc27646c7b，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c，随后删除了所用到的这个容器 9cdc27646c7b。\n\n这里我们使用了 docker build 命令进行镜像构建。其格式为：\n\ndocker build [选项] <上下文路径/url/->\n\n\n在这里我们指定了最终镜像的名称 -t nginx:v3，构建成功后，我们可以像之前运行 nginx:v2 那样来运行这个镜像，其结果会和 nginx:v2 一样。\n\n\n# 镜像构建上下文（context）\n\n如果注意，会看到 docker build 命令最后有一个 .。. 表示当前目录，而 dockerfile 就在当前目录，因此不少初学者以为这个路径是在指定 dockerfile 所在路径，这么理解其实是不准确的。如果对应上面的命令格式，你可能会发现，这是在指定上下文路径。那么什么是上下文呢？\n\n首先我们要理解 docker build 的工作原理。docker 在运行时分为 docker 引擎（也就是服务端守护进程）和客户端工具。docker 的引擎提供了一组 rest api，被称为 docker remote api，而如 docker 命令这样的客户端工具，则是通过这组 api 与 docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（docker 引擎）完成。也因为这种 c/s 设计，让我们操作远程服务器的 docker 引擎变得轻而易举。\n\n当我们进行镜像构建的时候，并非所有定制都会通过 run 指令完成，经常会需要将一些本地文件复制进镜像，比如通过 copy 指令、add 指令等。而 docker build 命令构建镜像，其实并非在本地构建，而是在服务端，也就是 docker 引擎中构建的。那么在这种客户端/服务端的架构中，如何才能让服务端获得本地文件呢？\n\n这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 docker 引擎。这样 docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。\n\n如果在 dockerfile 中这么写：\n\ncopy ./package.json /app/\n\n\n这并不是要复制执行 docker build 命令所在的目录下的 package.json，也不是复制 dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的 package.json。\n\n因此，copy 这类指令中的源文件的路径都是相对路径。这也是初学者经常会问的为什么 copy ../package.json /app 或者 copy /opt/xxxx /app 无法工作的原因，因为这些路径已经超出了上下文的范围，docker 引擎无法获得这些位置的文件。如果真的需要那些文件，应该将它们复制到上下文目录中去。\n\n现在就可以理解刚才的命令 docker build -t nginx:v3 . 中的这个 .，实际上是在指定上下文的目录，docker build 命令会将该目录下的内容打包交给 docker 引擎以帮助构建镜像。\n\n如果观察 docker build 输出，我们其实已经看到了这个发送上下文的过程：\n\ndocker build -t nginx:v3 .\nsending build context to docker daemon 2.048 kb\n...\n\n\n理解构建上下文对于镜像构建是很重要的，避免犯一些不应该的错误。比如有些初学者在发现 copy /opt/xxxx /app 不工作后，于是干脆将 dockerfile 放到了硬盘根目录去构建，结果发现 docker build 执行后，在发送一个几十 gb 的东西，极为缓慢而且很容易构建失败。那是因为这种做法是在让 docker build 打包整个硬盘，这显然是使用错误。\n\n一般来说，应该会将 dockerfile 置于一个空目录下，或者项目根目录下。如果该目录下没有所需文件，那么应该把所需文件复制一份过来。如果目录下有些东西确实不希望构建时传给 docker 引擎，那么可以用 .gitignore 一样的语法写一个 .dockerignore，该文件是用于剔除不需要作为上下文传递给 docker 引擎的。\n\n那么为什么会有人误以为 . 是指定 dockerfile 所在目录呢？这是因为在默认情况下，如果不额外指定 dockerfile 的话，会将上下文目录下的名为 dockerfile 的文件作为 dockerfile。\n\n这只是默认行为，实际上 dockerfile 的文件名并不要求必须为 dockerfile，而且并不要求必须位于上下文目录中，比如可以用 -f ../dockerfile.php 参数指定某个文件作为 dockerfile。\n\n当然，一般大家习惯性的会使用默认的文件名 dockerfile，以及会将其置于镜像构建上下文目录中。\n\n\n# 其它 docker build 的用法\n\n\n# 直接用 git repo 进行构建\n\n或许你已经注意到了，docker build 还支持从 url 构建，比如可以直接从 git repo 中构建：\n\ndocker build https://github.com/twang2218/gitlab-ce-zh.git#:8.14\ndocker build https://github.com/twang2218/gitlab-ce-zh.git\\#:8.14\nsending build context to docker daemon 2.048 kb\nstep 1 : from gitlab/gitlab-ce:8.14.0-ce.0\n8.14.0-ce.0: pulling from gitlab/gitlab-ce\naed15891ba52: already exists\n773ae8583d14: already exists\n...\n\n\n这行命令指定了构建所需的 git repo，并且指定默认的 master 分支，构建目录为 /8.14/，然后 docker 就会自己去 git clone 这个项目、切换到指定分支、并进入到指定目录后开始构建。\n\n\n# 用给定的 tar 压缩包构建\n\ndocker build http://server/context.tar.gz\n\n\n如果所给出的 url 不是个 git repo，而是个 tar 压缩包，那么 docker 引擎会下载这个包，并自动解压缩，以其作为上下文，开始构建。\n\n\n# #从标准输入中读取 dockerfile 进行构建\n\ndocker build - < dockerfile\n\n\n或\n\ncat dockerfile | docker build -\n\n\n如果标准输入传入的是文本文件，则将其视为 dockerfile，并开始构建。这种形式由于直接从标准输入中读取 dockerfile 的内容，它没有上下文，因此不可以像其他方法那样可以将本地文件 copy 进镜像之类的事情。\n\n\n# 从标准输入中读取上下文压缩包进行构建\n\ndocker build - < context.tar.gz\n\n\n如果发现标准输入的文件格式是 gzip、bzip2 以及 xz 的话，将会使其为上下文压缩包，直接将其展开，将里面视为上下文，并开始构建。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Dockerfile 指令详解",frontmatter:{title:"Dockerfile 指令详解",date:"2022-02-08T17:58:04.000Z",permalink:"/pages/6d2add/"},regularPath:"/17.Docker/04.Docker%20%E9%95%9C%E5%83%8F/08.Dockerfile%20%E6%8C%87%E4%BB%A4%E8%AF%A6%E8%A7%A3.html",relativePath:"17.Docker/04.Docker 镜像/08.Dockerfile 指令详解.md",key:"v-92e32acc",path:"/pages/6d2add/",headers:[{level:2,title:"COPY 复制文件",slug:"copy-复制文件",normalizedTitle:"copy 复制文件",charIndex:100},{level:2,title:"ADD 更高级的复制文件",slug:"add-更高级的复制文件",normalizedTitle:"add 更高级的复制文件",charIndex:600},{level:2,title:"CMD 容器启动命令",slug:"cmd-容器启动命令",normalizedTitle:"cmd 容器启动命令",charIndex:1403},{level:2,title:"ENTRYPOINT 入口点",slug:"entrypoint-入口点",normalizedTitle:"entrypoint 入口点",charIndex:2772},{level:3,title:"场景一：让镜像变成像命令一样使用",slug:"场景一-让镜像变成像命令一样使用",normalizedTitle:"场景一：让镜像变成像命令一样使用",charIndex:3124},{level:3,title:"场景二：应用运行前的准备工作",slug:"场景二-应用运行前的准备工作",normalizedTitle:"场景二：应用运行前的准备工作",charIndex:4858},{level:2,title:"ENV 设置环境变量",slug:"env-设置环境变量",normalizedTitle:"env 设置环境变量",charIndex:5788},{level:2,title:"ARG 构建参数",slug:"arg-构建参数",normalizedTitle:"arg 构建参数",charIndex:6938},{level:2,title:"VOLUME 定义匿名卷",slug:"volume-定义匿名卷",normalizedTitle:"volume 定义匿名卷",charIndex:7440},{level:2,title:"EXPOSE 暴露端口",slug:"expose-暴露端口",normalizedTitle:"expose 暴露端口",charIndex:7926},{level:2,title:"WORKDIR 指定工作目录",slug:"workdir-指定工作目录",normalizedTitle:"workdir 指定工作目录",charIndex:8534},{level:2,title:"USER 指定当前用户",slug:"user-指定当前用户",normalizedTitle:"user 指定当前用户",charIndex:9171},{level:2,title:"HEALTHCHECK 健康检查",slug:"healthcheck-健康检查",normalizedTitle:"healthcheck 健康检查",charIndex:9887},{level:2,title:"ONBUILD 为他人作嫁衣",slug:"onbuild-为他人作嫁衣",normalizedTitle:"onbuild 为他人作嫁衣",charIndex:13126},{level:2,title:"参考文档",slug:"参考文档",normalizedTitle:"参考文档",charIndex:15039}],headersStr:"COPY 复制文件 ADD 更高级的复制文件 CMD 容器启动命令 ENTRYPOINT 入口点 场景一：让镜像变成像命令一样使用 场景二：应用运行前的准备工作 ENV 设置环境变量 ARG 构建参数 VOLUME 定义匿名卷 EXPOSE 暴露端口 WORKDIR 指定工作目录 USER 指定当前用户 HEALTHCHECK 健康检查 ONBUILD 为他人作嫁衣 参考文档",content:'# Dockerfile 指令详解\n\n我们已经介绍了 FROM，RUN，还提及了 COPY, ADD，其实 Dockerfile 功能很强大，它提供了十多个指令。下面我们继续讲解其他的指令。\n\n\n# COPY 复制文件\n\n格式：\n\n * COPY <源路径>... <目标路径>\n * COPY ["<源路径1>",... "<目标路径>"]\n\n和 RUN 指令一样，也有两种格式，一种类似于命令行，一种类似于函数调用。\n\nCOPY 指令将从构建上下文目录中 <源路径> 的文件/目录复制到新的一层的镜像内的 <目标路径> 位置。比如：\n\nCOPY package.json /usr/src/app/\n\n\n<源路径> 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则，如：\n\nCOPY hom* /mydir/\nCOPY hom?.txt /mydir/\n\n\n<目标路径> 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 WORKDIR 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。\n\n此外，还需要注意一点，使用 COPY 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 Git 进行管理的时候。\n\n\n# ADD 更高级的复制文件\n\nADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。\n\n比如 <源路径> 可以是一个 URL，这种情况下，Docker 引擎会试图去下载这个链接的文件放到 <目标路径> 去。下载后的文件权限自动设置为 600，如果这并不是想要的权限，那么还需要增加额外的一层 RUN 进行权限调整，另外，如果下载的是个压缩包，需要解压缩，也一样还需要额外的一层 RUN 指令进行解压缩。所以不如直接使用 RUN 指令，然后使用 wget 或者 curl 工具下载，处理权限、解压缩、然后清理无用文件更合理。因此，这个功能其实并不实用，而且不推荐使用。\n\n如果 <源路径> 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 <目标路径> 去。\n\n在某些情况下，这个自动解压缩的功能非常有用，比如官方镜像 ubuntu 中：\n\nFROM scratch\nADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz /\n...\n\n\n但在某些情况下，如果我们真的是希望复制个压缩文件进去，而不解压缩，这时就不可以使用 ADD 命令了。\n\n在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。\n\n另外需要注意的是，ADD 指令会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。\n\n因此在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则，所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD。\n\n\n# CMD 容器启动命令\n\nCMD 指令的格式和 RUN 相似，也是两种格式：\n\n * shell 格式：CMD <命令>\n * exec 格式：CMD ["可执行文件", "参数1", "参数2"...]\n * 参数列表格式：CMD ["参数1", "参数2"...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。\n\n之前介绍容器的时候曾经说过，Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。CMD 指令就是用于指定默认的容器主进程的启动命令的。\n\n在运行时可以指定新的命令来替代镜像设置中的这个默认命令，比如，ubuntu 镜像默认的 CMD 是 /bin/bash，如果我们直接 docker run -it ubuntu 的话，会直接进入 bash。我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release。这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了，输出了系统版本信息。\n\n在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 "，而不要使用单引号。\n\n如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如：\n\nCMD echo $HOME\n\n\n在实际执行中，会将其变更为：\n\nCMD [ "sh", "-c", "echo $HOME" ]\n\n\n这就是为什么我们可以使用环境变量的原因，因为这些环境变量会被 shell 进行解析处理。\n\n提到 CMD 就不得不提容器中应用在前台执行和后台执行的问题。这是初学者常出现的一个混淆。\n\nDocker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 upstart/systemd 去启动后台服务，容器内没有后台服务的概念。\n\n一些初学者将 CMD 写为：\n\nCMD service nginx start\n\n\n然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念，没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。\n\n对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。\n\n而使用 service nginx start 命令，则是希望 upstart 来以后台守护进程形式启动 nginx 服务。而刚才说了 CMD service nginx start 会被理解为 CMD [ "sh", "-c", "service nginx start"]，因此主进程实际上是 sh。那么当 service nginx start 命令结束后，sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。\n\n正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如：\n\nCMD ["nginx", "-g", "daemon off;"]\n\n\n\n# ENTRYPOINT 入口点\n\nENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式。\n\nENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 --entrypoint 来指定。\n\n当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为：\n\n<ENTRYPOINT> "<CMD>"\n\n\n那么有了 CMD 后，为什么还要有 ENTRYPOINT 呢？这种 "" 有什么好处么？让我们来看几个场景。\n\n\n# 场景一：让镜像变成像命令一样使用\n\n假设我们需要一个得知自己当前公网 IP 的镜像，那么可以先用 CMD 来实现：\n\nFROM ubuntu:16.04\nRUN apt-get update \\\n    && apt-get install -y curl \\\n    && rm -rf /var/lib/apt/lists/*\nCMD [ "curl", "-s", "http://ip.cn" ]\n\n\n假如我们使用 docker build -t myip . 来构建镜像的话，如果我们需要查询当前公网 IP，只需要执行：\n\ndocker run myip\n当前 IP：61.148.226.66 来自：北京市 联通\n\n\n嗯，这么看起来好像可以直接把镜像当做命令使用了，不过命令总有参数，如果我们希望加参数呢？比如从上面的 CMD 中可以看到实质的命令是 curl，那么如果我们希望显示 HTTP 头信息，就需要加上 -i 参数。那么我们可以直接加 -i 参数给 docker run myip 么？\n\ndocker run myip -i\ndocker: Error response from daemon: invalid header field value "oci runtime error: container_linux.go:247: starting container process caused \\"exec: \\\\\\"-i\\\\\\": executable file not found in $PATH\\"\\n".\n\n\n我们可以看到可执行文件找不到的报错，executable file not found。之前我们说过，跟在镜像名后面的是 command，运行时会替换 CMD 的默认值。因此这里的 -i 替换了原来的 CMD，而不是添加在原来的 curl -s http://ip.cn 后面。而 -i 根本不是命令，所以自然找不到。\n\n那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令：\n\ndocker run myip curl -s http://ip.cn -i\n\n\n这显然不是很好的解决方案，而使用 ENTRYPOINT 就可以解决这个问题。现在我们重新用 ENTRYPOINT 来实现这个镜像：\n\nFROM ubuntu:16.04\nRUN apt-get update \\\n    && apt-get install -y curl \\\n    && rm -rf /var/lib/apt/lists/*\nENTRYPOINT [ "curl", "-s", "http://ip.cn" ]\n\n\n这次我们再来尝试直接使用 docker run myip -i：\n\ndocker run myip\n当前 IP：61.148.226.66 来自：北京市 联通\n\ndocker run myip -i\nHTTP/1.1 200 OK\nServer: nginx/1.8.0\nDate: Tue, 22 Nov 2016 05:12:40 GMT\nContent-Type: text/html; charset=UTF-8\nVary: Accept-Encoding\nX-Powered-By: PHP/5.6.24-1~dotdeb+7.1\nX-Cache: MISS from cache-2\nX-Cache-Lookup: MISS from cache-2:80\nX-Cache: MISS from proxy-2_6\nTransfer-Encoding: chunked\nVia: 1.1 cache-2:80, 1.1 proxy-2_6:8006\nConnection: keep-alive\n\n当前 IP：61.148.226.66 来自：北京市 联通\n\n\n可以看到，这次成功了。这是因为当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给 ENTRYPOINT，而这里 -i 就是新的 CMD，因此会作为参数传给 curl，从而达到了我们预期的效果。\n\n\n# 场景二：应用运行前的准备工作\n\n启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。\n\n比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。\n\n此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。\n\n这些准备工作是和容器 CMD 无关的，无论 CMD 为什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 ``）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的：\n\nFROM alpine:3.4\n...\nRUN addgroup -S redis && adduser -S -G redis redis\n...\nENTRYPOINT ["docker-entrypoint.sh"]\n\nEXPOSE 6379\nCMD [ "redis-server" ]\n\n\n可以看到其中为了 redis 服务创建了 redis 用户，并在最后指定了 ENTRYPOINT 为 docker-entrypoint.sh 脚本。\n\n#!/bin/sh\n...\n# allow the container to be started with `--user`\nif [ "$1" = \'redis-server\' -a "$(id -u)" = \'0\' ]; then\n\tchown -R redis .\n\texec su-exec redis "$0" "$@"\nfi\n\nexec "$@"\n\n\n该脚本的内容就是根据 CMD 的内容来判断，如果是 redis-server 的话，则切换到 redis 用户身份启动服务器，否则依旧使用 root 身份执行。比如：\n\ndocker run -it redis id\nuid=0(root) gid=0(root) groups=0(root)\n\n\n\n# ENV 设置环境变量\n\n格式有两种：\n\n * ENV\n * ENV = =...\n\n这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 RUN，还是运行时的应用，都可以直接使用这里定义的环境变量。\n\nENV VERSION=1.0 DEBUG=on \\\n    NAME="Happy Feet"\n\n\n这个例子中演示了如何换行，以及对含有空格的值用双引号括起来的办法，这和 Shell 下的行为是一致的。\n\n定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。比如在官方 node 镜像 Dockerfile 中，就有类似这样的代码：\n\nENV NODE_VERSION 7.2.0\n\nRUN curl -SLO "https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz" \\\n  && curl -SLO "https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc" \\\n  && gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\\n  && grep " node-v$NODE_VERSION-linux-x64.tar.xz\\$" SHASUMS256.txt | sha256sum -c - \\\n  && tar -xJf "node-v$NODE_VERSION-linux-x64.tar.xz" -C /usr/local --strip-components=1 \\\n  && rm "node-v$NODE_VERSION-linux-x64.tar.xz" SHASUMS256.txt.asc SHASUMS256.txt \\\n  && ln -s /usr/local/bin/node /usr/local/bin/nodejs\n\n\n在这里先定义了环境变量 NODE_VERSION，其后的 RUN 这层里，多次使用 $NODE_VERSION 来进行操作定制。可以看到，将来升级镜像构建版本的时候，只需要更新 7.2.0 即可，Dockerfile 构建维护变得更轻松了。\n\n下列指令可以支持环境变量展开： ADD、COPY、ENV、EXPOSE、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD。\n\n可以从这个指令列表里感觉到，环境变量可以使用的地方很多，很强大。通过环境变量，我们可以让一份 Dockerfile 制作更多的镜像，只需使用不同的环境变量即可。\n\n\n# ARG 构建参数\n\n格式：ARG <参数名>[=<默认值>]\n\n构建参数和 ENV 的效果一样，都是设置环境变量。所不同的是，ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。\n\nDockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg <参数名>=<值> 来覆盖。\n\n在 1.13 之前的版本，要求 --build-arg 中的参数名，必须在 Dockerfile 中用 ARG 定义过了，换句话说，就是 --build-arg 指定的参数，必须在 Dockerfile 中使用了。如果对应参数没有被使用，则会报错退出构建。从 1.13 开始，这种严格的限制被放开，不再报错退出，而是显示警告信息，并继续构建。这对于使用 CI 系统，用同样的构建流程构建不同的 Dockerfile 的时候比较有帮助，避免构建命令必须根据每个 Dockerfile 的内容修改。\n\n\n# VOLUME 定义匿名卷\n\n格式为：\n\n * VOLUME ["<路径1>", "<路径2>"...]\n * VOLUME <路径>\n\n之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，后面的章节我们会进一步介绍 Docker 卷的概念。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。\n\nVOLUME /data\n\n\n这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。比如：\n\ndocker run -d -v mydata:/data xxxx\n\n\n在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。\n\n\n# EXPOSE 暴露端口\n\n格式为 EXPOSE <端口1> [<端口2>...]。\n\nEXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。\n\n此外，在早期 Docker 版本中还有一个特殊的用处。以前所有容器都运行于默认桥接网络中，因此所有容器互相之间都可以直接访问，这样存在一定的安全性问题。于是有了一个 Docker 引擎参数 --icc=false，当指定该参数后，容器间将默认无法互访，除非互相间使用了 --links 参数的容器才可以互通，并且只有镜像中 EXPOSE 所声明的端口才可以被访问。这个 --icc=false 的用法，在引入了 docker network 后已经基本不用了，通过自定义网络可以很轻松的实现容器间的互联与隔离。\n\n要将 EXPOSE 和在运行时使用 -p <宿主端口>:<容器端口> 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。\n\n\n# WORKDIR 指定工作目录\n\n格式为 WORKDIR <工作目录路径>。\n\n使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。\n\n之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误：\n\nRUN cd /app\nRUN echo "hello" > world.txt\n\n\n如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello。原因其实很简单，在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令；而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。\n\n之前说过每一个 RUN 都是启动一个容器、执行命令、然后提交存储层文件变更。第一层 RUN cd /app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。\n\n因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。\n\n\n# USER 指定当前用户\n\n格式：USER <用户名>\n\nUSER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。\n\n当然，和 WORKDIR 一样，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。\n\nRUN groupadd -r redis && useradd -r -g redis redis\nUSER redis\nRUN [ "redis-server" ]\n\n\n如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su 或者 sudo，这些都需要比较麻烦的配置，而且在 TTY 缺失的环境下经常出错。建议使用 gosu。\n\n# 建立 redis 用户，并使用 gosu 换另一个用户执行命令\nRUN groupadd -r redis && useradd -r -g redis redis\n# 下载 gosu\nRUN wget -O /usr/local/bin/gosu "https://github.com/tianon/gosu/releases/download/1.7/gosu-amd64" \\\n    && chmod +x /usr/local/bin/gosu \\\n    && gosu nobody true\n# 设置 CMD，并以另外的用户执行\nCMD [ "exec", "gosu", "redis", "redis-server" ]\n\n\n\n# HEALTHCHECK 健康检查\n\n格式：\n\n * HEALTHCHECK [选项] CMD <命令>：设置检查容器健康状况的命令\n * HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令\n\nHEALTHCHECK 指令是告诉 Docker 应该如何进行判断容器的状态是否正常，这是 Docker 1.12 引入的新指令。\n\n在没有 HEALTHCHECK 指令前，Docker 引擎只可以通过容器内主进程是否退出来判断容器是否状态异常。很多情况下这没问题，但是如果程序进入死锁状态，或者死循环状态，应用进程并不退出，但是该容器已经无法提供服务了。在 1.12 以前，Docker 不会检测到容器的这种状态，从而不会重新调度，导致可能会有部分容器已经无法提供服务了却还在接受用户请求。\n\n而自 1.12 之后，Docker 提供了 HEALTHCHECK 指令，通过该指令指定一行命令，用这行命令来判断容器主进程的服务状态是否还正常，从而比较真实的反应容器实际状态。\n\n当在一个镜像指定了 HEALTHCHECK 指令后，用其启动容器，初始状态会为 starting，在 HEALTHCHECK 指令检查成功后变为 healthy，如果连续一定次数失败，则会变为 unhealthy。\n\nHEALTHCHECK 支持下列选项：\n\n * --interval=<间隔>：两次健康检查的间隔，默认为 30 秒；\n * --timeout=<时长>：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒；\n * --retries=<次数>：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。\n\n和 CMD, ENTRYPOINT 一样，HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。\n\n在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：0：成功；1：失败；2：保留，不要使用这个值。\n\n假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl 来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写：\n\nFROM nginx\nRUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*\nHEALTHCHECK --interval=5s --timeout=3s \\\n  CMD curl -fs http://localhost/ || exit 1\n\n\n这里我们设置了每 5 秒检查一次（这里为了试验所以间隔非常短，实际应该相对较长），如果健康检查命令超过 3 秒没响应就视为失败，并且使用 curl -fs http://localhost/ || exit 1 作为健康检查命令。\n\n使用 docker build 来构建这个镜像：\n\ndocker build -t myweb:v1 .\n\n\n构建好了后，我们启动一个容器：\n\ndocker run -d --name web -p 80:80 myweb:v1\n\n\n当运行该镜像后，可以通过 docker container ls 看到最初的状态为 (health: starting)：\n\ndocker container ls\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                            PORTS               NAMES\n03e28eb00bd0        myweb:v1            "nginx -g \'daemon off"   3 seconds ago       Up 2 seconds (health: starting)   80/tcp, 443/tcp     web\n\n\n在等待几秒钟后，再次 docker container ls，就会看到健康状态变化为了 (healthy)：\n\ndocker container ls\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                    PORTS               NAMES\n03e28eb00bd0        myweb:v1            "nginx -g \'daemon off"   18 seconds ago      Up 16 seconds (healthy)   80/tcp, 443/tcp     web\n\n\n如果健康检查连续失败超过了重试次数，状态就会变为 (unhealthy)。\n\n为了帮助排障，健康检查命令的输出（包括 stdout 以及 stderr）都会被存储于健康状态里，可以用 docker inspect 来查看。\n\ndocker inspect --format \'{{json .State.Health}}\' web | python -m json.tool\n{\n    "FailingStreak": 0,\n    "Log": [\n        {\n            "End": "2016-11-25T14:35:37.940957051Z",\n            "ExitCode": 0,\n            "Output": "<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Welcome to nginx!</title>\\n<style>\\n    body {\\n        width: 35em;\\n        margin: 0 auto;\\n        font-family: Tahoma, Verdana, Arial, sans-serif;\\n    }\\n</style>\\n</head>\\n<body>\\n<h1>Welcome to nginx!</h1>\\n<p>If you see this page, the nginx web server is successfully installed and\\nworking. Further configuration is required.</p>\\n\\n<p>For online documentation and support please refer to\\n<a href=\\"http://nginx.org/\\">nginx.org</a>.<br/>\\nCommercial support is available at\\n<a href=\\"http://nginx.com/\\">nginx.com</a>.</p>\\n\\n<p><em>Thank you for using nginx.</em></p>\\n</body>\\n</html>\\n",\n            "Start": "2016-11-25T14:35:37.780192565Z"\n        }\n    ],\n    "Status": "healthy"\n}\n\n\n\n# ONBUILD 为他人作嫁衣\n\n格式：ONBUILD <其它指令>。\n\nONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN, COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。\n\nDockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 ONBUILD 是为了帮助别人定制自己而准备的。\n\n假设我们要制作 Node.js 所写的应用的镜像。我们都知道 Node.js 使用 npm 进行包管理，所有依赖、配置、启动信息等会放到 package.json 文件里。在拿到程序代码后，需要先进行 npm install 才可以获得所有需要的依赖。然后就可以通过 npm start 来启动应用。因此，一般来说会这样写 Dockerfile：\n\nFROM node:slim\nRUN mkdir /app\nWORKDIR /app\nCOPY ./package.json /app\nRUN [ "npm", "install" ]\nCOPY . /app/\nCMD [ "npm", "start" ]\n\n\n把这个 Dockerfile 放到 Node.js 项目的根目录，构建好镜像后，就可以直接拿来启动容器运行。但是如果我们还有第二个 Node.js 项目也差不多呢？好吧，那就再把这个 Dockerfile 复制到第二个项目里。那如果有第三个项目呢？再复制么？文件的副本越多，版本控制就越困难，让我们继续看这样的场景维护的问题。\n\n如果第一个 Node.js 项目在开发过程中，发现这个 Dockerfile 里存在问题，比如敲错字了、或者需要安装额外的包，然后开发人员修复了这个 Dockerfile，再次构建，问题解决。\b第一个项目没问题了，但是第二个项目呢？虽然最初 Dockerfile 是复制、粘贴自第一个项目的，但是并不会因为第一个项目修复了他们的 Dockerfile，而第二个项目的 Dockerfile 就会被自动修复。\n\n那么我们可不可以做一个基础镜像，然后各个项目使用这个基础镜像呢？这样基础镜像更新，各个项目不用同步 Dockerfile 的变化，重新构建后就继承了基础镜像的更新？好吧，可以，让我们看看这样的结果。那么上面的这个 Dockerfile 就会变为：\n\nFROM node:slim\nRUN mkdir /app\nWORKDIR /app\nCMD [ "npm", "start" ]\n\n\n这里我们把项目相关的构建指令拿出来，放到子项目里去。假设这个基础镜像的名字为 my-node 的话，各个项目内的自己的 Dockerfile 就变为：\n\nFROM my-node\nCOPY ./package.json /app\nRUN [ "npm", "install" ]\nCOPY . /app/\n\n\n基础镜像变化后，各个项目都用这个 Dockerfile 重新构建镜像，会继承基础镜像的更新。\n\n那么，问题解决了么？没有。准确说，只解决了一半。如果这个 Dockerfile 里面有些东西需要调整呢？比如 npm install 都需要加一些参数，那怎么办？这一行 RUN 是不可能放入基础镜像的，因为涉及到了当前项目的 ./package.json，难道又要一个个修改么？所以说，这样制作基础镜像，只解决了原来的 Dockerfile 的前4条指令的变化问题，而后面三条指令的变化则完全没办法处理。\n\nONBUILD 可以解决这个问题。让我们用 ONBUILD 重新写一下基础镜像的 Dockerfile:\n\nFROM node:slim\nRUN mkdir /app\nWORKDIR /app\nONBUILD COPY ./package.json /app\nONBUILD RUN [ "npm", "install" ]\nONBUILD COPY . /app/\nCMD [ "npm", "start" ]\n\n\n这次我们回到原始的 Dockerfile，但是这次将项目相关的指令加上 ONBUILD，这样在构建基础镜像的时候，这三行并不会被执行。然后各个项目的 Dockerfile 就变成了简单地：\n\nFROM my-node\n\n\n是的，只有这么一行。当在各个项目目录中，用这个只有一行的 Dockerfile 构建镜像时，之前基础镜像的那三行 ONBUILD 就会开始执行，成功的将当前项目的代码复制进镜像、并且针对本项目执行 npm install，生成应用镜像。\n\n\n# 参考文档\n\n * Dockerfie 官方文档\n * Dockerfile 最佳实践文档\n * Docker 官方镜像 Dockerfile',normalizedContent:'# dockerfile 指令详解\n\n我们已经介绍了 from，run，还提及了 copy, add，其实 dockerfile 功能很强大，它提供了十多个指令。下面我们继续讲解其他的指令。\n\n\n# copy 复制文件\n\n格式：\n\n * copy <源路径>... <目标路径>\n * copy ["<源路径1>",... "<目标路径>"]\n\n和 run 指令一样，也有两种格式，一种类似于命令行，一种类似于函数调用。\n\ncopy 指令将从构建上下文目录中 <源路径> 的文件/目录复制到新的一层的镜像内的 <目标路径> 位置。比如：\n\ncopy package.json /usr/src/app/\n\n\n<源路径> 可以是多个，甚至可以是通配符，其通配符规则要满足 go 的 filepath.match 规则，如：\n\ncopy hom* /mydir/\ncopy hom?.txt /mydir/\n\n\n<目标路径> 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 workdir 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。\n\n此外，还需要注意一点，使用 copy 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 git 进行管理的时候。\n\n\n# add 更高级的复制文件\n\nadd 指令和 copy 的格式和性质基本一致。但是在 copy 基础上增加了一些功能。\n\n比如 <源路径> 可以是一个 url，这种情况下，docker 引擎会试图去下载这个链接的文件放到 <目标路径> 去。下载后的文件权限自动设置为 600，如果这并不是想要的权限，那么还需要增加额外的一层 run 进行权限调整，另外，如果下载的是个压缩包，需要解压缩，也一样还需要额外的一层 run 指令进行解压缩。所以不如直接使用 run 指令，然后使用 wget 或者 curl 工具下载，处理权限、解压缩、然后清理无用文件更合理。因此，这个功能其实并不实用，而且不推荐使用。\n\n如果 <源路径> 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，add 指令将会自动解压缩这个压缩文件到 <目标路径> 去。\n\n在某些情况下，这个自动解压缩的功能非常有用，比如官方镜像 ubuntu 中：\n\nfrom scratch\nadd ubuntu-xenial-core-cloudimg-amd64-root.tar.gz /\n...\n\n\n但在某些情况下，如果我们真的是希望复制个压缩文件进去，而不解压缩，这时就不可以使用 add 命令了。\n\n在 docker 官方的 dockerfile 最佳实践文档 中要求，尽可能的使用 copy，因为 copy 的语义很明确，就是复制文件而已，而 add 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 add 的场合，就是所提及的需要自动解压缩的场合。\n\n另外需要注意的是，add 指令会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。\n\n因此在 copy 和 add 指令中选择的时候，可以遵循这样的原则，所有的文件复制均使用 copy 指令，仅在需要自动解压缩的场合使用 add。\n\n\n# cmd 容器启动命令\n\ncmd 指令的格式和 run 相似，也是两种格式：\n\n * shell 格式：cmd <命令>\n * exec 格式：cmd ["可执行文件", "参数1", "参数2"...]\n * 参数列表格式：cmd ["参数1", "参数2"...]。在指定了 entrypoint 指令后，用 cmd 指定具体的参数。\n\n之前介绍容器的时候曾经说过，docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。cmd 指令就是用于指定默认的容器主进程的启动命令的。\n\n在运行时可以指定新的命令来替代镜像设置中的这个默认命令，比如，ubuntu 镜像默认的 cmd 是 /bin/bash，如果我们直接 docker run -it ubuntu 的话，会直接进入 bash。我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release。这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了，输出了系统版本信息。\n\n在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 json 数组，因此一定要使用双引号 "，而不要使用单引号。\n\n如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如：\n\ncmd echo $home\n\n\n在实际执行中，会将其变更为：\n\ncmd [ "sh", "-c", "echo $home" ]\n\n\n这就是为什么我们可以使用环境变量的原因，因为这些环境变量会被 shell 进行解析处理。\n\n提到 cmd 就不得不提容器中应用在前台执行和后台执行的问题。这是初学者常出现的一个混淆。\n\ndocker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 upstart/systemd 去启动后台服务，容器内没有后台服务的概念。\n\n一些初学者将 cmd 写为：\n\ncmd service nginx start\n\n\n然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念，没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。\n\n对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。\n\n而使用 service nginx start 命令，则是希望 upstart 来以后台守护进程形式启动 nginx 服务。而刚才说了 cmd service nginx start 会被理解为 cmd [ "sh", "-c", "service nginx start"]，因此主进程实际上是 sh。那么当 service nginx start 命令结束后，sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。\n\n正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如：\n\ncmd ["nginx", "-g", "daemon off;"]\n\n\n\n# entrypoint 入口点\n\nentrypoint 的格式和 run 指令格式一样，分为 exec 格式和 shell 格式。\n\nentrypoint 的目的和 cmd 一样，都是在指定容器启动程序及参数。entrypoint 在运行时也可以替代，不过比 cmd 要略显繁琐，需要通过 docker run 的参数 --entrypoint 来指定。\n\n当指定了 entrypoint 后，cmd 的含义就发生了改变，不再是直接的运行其命令，而是将 cmd 的内容作为参数传给 entrypoint 指令，换句话说实际执行时，将变为：\n\n<entrypoint> "<cmd>"\n\n\n那么有了 cmd 后，为什么还要有 entrypoint 呢？这种 "" 有什么好处么？让我们来看几个场景。\n\n\n# 场景一：让镜像变成像命令一样使用\n\n假设我们需要一个得知自己当前公网 ip 的镜像，那么可以先用 cmd 来实现：\n\nfrom ubuntu:16.04\nrun apt-get update \\\n    && apt-get install -y curl \\\n    && rm -rf /var/lib/apt/lists/*\ncmd [ "curl", "-s", "http://ip.cn" ]\n\n\n假如我们使用 docker build -t myip . 来构建镜像的话，如果我们需要查询当前公网 ip，只需要执行：\n\ndocker run myip\n当前 ip：61.148.226.66 来自：北京市 联通\n\n\n嗯，这么看起来好像可以直接把镜像当做命令使用了，不过命令总有参数，如果我们希望加参数呢？比如从上面的 cmd 中可以看到实质的命令是 curl，那么如果我们希望显示 http 头信息，就需要加上 -i 参数。那么我们可以直接加 -i 参数给 docker run myip 么？\n\ndocker run myip -i\ndocker: error response from daemon: invalid header field value "oci runtime error: container_linux.go:247: starting container process caused \\"exec: \\\\\\"-i\\\\\\": executable file not found in $path\\"\\n".\n\n\n我们可以看到可执行文件找不到的报错，executable file not found。之前我们说过，跟在镜像名后面的是 command，运行时会替换 cmd 的默认值。因此这里的 -i 替换了原来的 cmd，而不是添加在原来的 curl -s http://ip.cn 后面。而 -i 根本不是命令，所以自然找不到。\n\n那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令：\n\ndocker run myip curl -s http://ip.cn -i\n\n\n这显然不是很好的解决方案，而使用 entrypoint 就可以解决这个问题。现在我们重新用 entrypoint 来实现这个镜像：\n\nfrom ubuntu:16.04\nrun apt-get update \\\n    && apt-get install -y curl \\\n    && rm -rf /var/lib/apt/lists/*\nentrypoint [ "curl", "-s", "http://ip.cn" ]\n\n\n这次我们再来尝试直接使用 docker run myip -i：\n\ndocker run myip\n当前 ip：61.148.226.66 来自：北京市 联通\n\ndocker run myip -i\nhttp/1.1 200 ok\nserver: nginx/1.8.0\ndate: tue, 22 nov 2016 05:12:40 gmt\ncontent-type: text/html; charset=utf-8\nvary: accept-encoding\nx-powered-by: php/5.6.24-1~dotdeb+7.1\nx-cache: miss from cache-2\nx-cache-lookup: miss from cache-2:80\nx-cache: miss from proxy-2_6\ntransfer-encoding: chunked\nvia: 1.1 cache-2:80, 1.1 proxy-2_6:8006\nconnection: keep-alive\n\n当前 ip：61.148.226.66 来自：北京市 联通\n\n\n可以看到，这次成功了。这是因为当存在 entrypoint 后，cmd 的内容将会作为参数传给 entrypoint，而这里 -i 就是新的 cmd，因此会作为参数传给 curl，从而达到了我们预期的效果。\n\n\n# 场景二：应用运行前的准备工作\n\n启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。\n\n比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。\n\n此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。\n\n这些准备工作是和容器 cmd 无关的，无论 cmd 为什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 entrypoint 中去执行，而这个脚本会将接到的参数（也就是 ``）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的：\n\nfrom alpine:3.4\n...\nrun addgroup -s redis && adduser -s -g redis redis\n...\nentrypoint ["docker-entrypoint.sh"]\n\nexpose 6379\ncmd [ "redis-server" ]\n\n\n可以看到其中为了 redis 服务创建了 redis 用户，并在最后指定了 entrypoint 为 docker-entrypoint.sh 脚本。\n\n#!/bin/sh\n...\n# allow the container to be started with `--user`\nif [ "$1" = \'redis-server\' -a "$(id -u)" = \'0\' ]; then\n\tchown -r redis .\n\texec su-exec redis "$0" "$@"\nfi\n\nexec "$@"\n\n\n该脚本的内容就是根据 cmd 的内容来判断，如果是 redis-server 的话，则切换到 redis 用户身份启动服务器，否则依旧使用 root 身份执行。比如：\n\ndocker run -it redis id\nuid=0(root) gid=0(root) groups=0(root)\n\n\n\n# env 设置环境变量\n\n格式有两种：\n\n * env\n * env = =...\n\n这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 run，还是运行时的应用，都可以直接使用这里定义的环境变量。\n\nenv version=1.0 debug=on \\\n    name="happy feet"\n\n\n这个例子中演示了如何换行，以及对含有空格的值用双引号括起来的办法，这和 shell 下的行为是一致的。\n\n定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。比如在官方 node 镜像 dockerfile 中，就有类似这样的代码：\n\nenv node_version 7.2.0\n\nrun curl -slo "https://nodejs.org/dist/v$node_version/node-v$node_version-linux-x64.tar.xz" \\\n  && curl -slo "https://nodejs.org/dist/v$node_version/shasums256.txt.asc" \\\n  && gpg --batch --decrypt --output shasums256.txt shasums256.txt.asc \\\n  && grep " node-v$node_version-linux-x64.tar.xz\\$" shasums256.txt | sha256sum -c - \\\n  && tar -xjf "node-v$node_version-linux-x64.tar.xz" -c /usr/local --strip-components=1 \\\n  && rm "node-v$node_version-linux-x64.tar.xz" shasums256.txt.asc shasums256.txt \\\n  && ln -s /usr/local/bin/node /usr/local/bin/nodejs\n\n\n在这里先定义了环境变量 node_version，其后的 run 这层里，多次使用 $node_version 来进行操作定制。可以看到，将来升级镜像构建版本的时候，只需要更新 7.2.0 即可，dockerfile 构建维护变得更轻松了。\n\n下列指令可以支持环境变量展开： add、copy、env、expose、label、user、workdir、volume、stopsignal、onbuild。\n\n可以从这个指令列表里感觉到，环境变量可以使用的地方很多，很强大。通过环境变量，我们可以让一份 dockerfile 制作更多的镜像，只需使用不同的环境变量即可。\n\n\n# arg 构建参数\n\n格式：arg <参数名>[=<默认值>]\n\n构建参数和 env 的效果一样，都是设置环境变量。所不同的是，arg 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 arg 保存密码之类的信息，因为 docker history 还是可以看到所有值的。\n\ndockerfile 中的 arg 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg <参数名>=<值> 来覆盖。\n\n在 1.13 之前的版本，要求 --build-arg 中的参数名，必须在 dockerfile 中用 arg 定义过了，换句话说，就是 --build-arg 指定的参数，必须在 dockerfile 中使用了。如果对应参数没有被使用，则会报错退出构建。从 1.13 开始，这种严格的限制被放开，不再报错退出，而是显示警告信息，并继续构建。这对于使用 ci 系统，用同样的构建流程构建不同的 dockerfile 的时候比较有帮助，避免构建命令必须根据每个 dockerfile 的内容修改。\n\n\n# volume 定义匿名卷\n\n格式为：\n\n * volume ["<路径1>", "<路径2>"...]\n * volume <路径>\n\n之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，后面的章节我们会进一步介绍 docker 卷的概念。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。\n\nvolume /data\n\n\n这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。比如：\n\ndocker run -d -v mydata:/data xxxx\n\n\n在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 dockerfile 中定义的匿名卷的挂载配置。\n\n\n# expose 暴露端口\n\n格式为 expose <端口1> [<端口2>...]。\n\nexpose 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。在 dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -p 时，会自动随机映射 expose 的端口。\n\n此外，在早期 docker 版本中还有一个特殊的用处。以前所有容器都运行于默认桥接网络中，因此所有容器互相之间都可以直接访问，这样存在一定的安全性问题。于是有了一个 docker 引擎参数 --icc=false，当指定该参数后，容器间将默认无法互访，除非互相间使用了 --links 参数的容器才可以互通，并且只有镜像中 expose 所声明的端口才可以被访问。这个 --icc=false 的用法，在引入了 docker network 后已经基本不用了，通过自定义网络可以很轻松的实现容器间的互联与隔离。\n\n要将 expose 和在运行时使用 -p <宿主端口>:<容器端口> 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 expose 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。\n\n\n# workdir 指定工作目录\n\n格式为 workdir <工作目录路径>。\n\n使用 workdir 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，workdir 会帮你建立目录。\n\n之前提到一些初学者常犯的错误是把 dockerfile 等同于 shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误：\n\nrun cd /app\nrun echo "hello" > world.txt\n\n\n如果将这个 dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello。原因其实很简单，在 shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令；而在 dockerfile 中，这两行 run 命令的执行环境根本不同，是两个完全不同的容器。这就是对 dockerfile 构建分层存储的概念不了解所导致的错误。\n\n之前说过每一个 run 都是启动一个容器、执行命令、然后提交存储层文件变更。第一层 run cd /app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。\n\n因此如果需要改变以后各层的工作目录的位置，那么应该使用 workdir 指令。\n\n\n# user 指定当前用户\n\n格式：user <用户名>\n\nuser 指令和 workdir 相似，都是改变环境状态并影响以后的层。workdir 是改变工作目录，user 则是改变之后层的执行 run, cmd 以及 entrypoint 这类命令的身份。\n\n当然，和 workdir 一样，user 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。\n\nrun groupadd -r redis && useradd -r -g redis redis\nuser redis\nrun [ "redis-server" ]\n\n\n如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su 或者 sudo，这些都需要比较麻烦的配置，而且在 tty 缺失的环境下经常出错。建议使用 gosu。\n\n# 建立 redis 用户，并使用 gosu 换另一个用户执行命令\nrun groupadd -r redis && useradd -r -g redis redis\n# 下载 gosu\nrun wget -o /usr/local/bin/gosu "https://github.com/tianon/gosu/releases/download/1.7/gosu-amd64" \\\n    && chmod +x /usr/local/bin/gosu \\\n    && gosu nobody true\n# 设置 cmd，并以另外的用户执行\ncmd [ "exec", "gosu", "redis", "redis-server" ]\n\n\n\n# healthcheck 健康检查\n\n格式：\n\n * healthcheck [选项] cmd <命令>：设置检查容器健康状况的命令\n * healthcheck none：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令\n\nhealthcheck 指令是告诉 docker 应该如何进行判断容器的状态是否正常，这是 docker 1.12 引入的新指令。\n\n在没有 healthcheck 指令前，docker 引擎只可以通过容器内主进程是否退出来判断容器是否状态异常。很多情况下这没问题，但是如果程序进入死锁状态，或者死循环状态，应用进程并不退出，但是该容器已经无法提供服务了。在 1.12 以前，docker 不会检测到容器的这种状态，从而不会重新调度，导致可能会有部分容器已经无法提供服务了却还在接受用户请求。\n\n而自 1.12 之后，docker 提供了 healthcheck 指令，通过该指令指定一行命令，用这行命令来判断容器主进程的服务状态是否还正常，从而比较真实的反应容器实际状态。\n\n当在一个镜像指定了 healthcheck 指令后，用其启动容器，初始状态会为 starting，在 healthcheck 指令检查成功后变为 healthy，如果连续一定次数失败，则会变为 unhealthy。\n\nhealthcheck 支持下列选项：\n\n * --interval=<间隔>：两次健康检查的间隔，默认为 30 秒；\n * --timeout=<时长>：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒；\n * --retries=<次数>：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。\n\n和 cmd, entrypoint 一样，healthcheck 只可以出现一次，如果写了多个，只有最后一个生效。\n\n在 healthcheck [选项] cmd 后面的命令，格式和 entrypoint 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：0：成功；1：失败；2：保留，不要使用这个值。\n\n假设我们有个镜像是个最简单的 web 服务，我们希望增加健康检查来判断其 web 服务是否在正常工作，我们可以用 curl 来帮助判断，其 dockerfile 的 healthcheck 可以这么写：\n\nfrom nginx\nrun apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*\nhealthcheck --interval=5s --timeout=3s \\\n  cmd curl -fs http://localhost/ || exit 1\n\n\n这里我们设置了每 5 秒检查一次（这里为了试验所以间隔非常短，实际应该相对较长），如果健康检查命令超过 3 秒没响应就视为失败，并且使用 curl -fs http://localhost/ || exit 1 作为健康检查命令。\n\n使用 docker build 来构建这个镜像：\n\ndocker build -t myweb:v1 .\n\n\n构建好了后，我们启动一个容器：\n\ndocker run -d --name web -p 80:80 myweb:v1\n\n\n当运行该镜像后，可以通过 docker container ls 看到最初的状态为 (health: starting)：\n\ndocker container ls\ncontainer id        image               command                  created             status                            ports               names\n03e28eb00bd0        myweb:v1            "nginx -g \'daemon off"   3 seconds ago       up 2 seconds (health: starting)   80/tcp, 443/tcp     web\n\n\n在等待几秒钟后，再次 docker container ls，就会看到健康状态变化为了 (healthy)：\n\ndocker container ls\ncontainer id        image               command                  created             status                    ports               names\n03e28eb00bd0        myweb:v1            "nginx -g \'daemon off"   18 seconds ago      up 16 seconds (healthy)   80/tcp, 443/tcp     web\n\n\n如果健康检查连续失败超过了重试次数，状态就会变为 (unhealthy)。\n\n为了帮助排障，健康检查命令的输出（包括 stdout 以及 stderr）都会被存储于健康状态里，可以用 docker inspect 来查看。\n\ndocker inspect --format \'{{json .state.health}}\' web | python -m json.tool\n{\n    "failingstreak": 0,\n    "log": [\n        {\n            "end": "2016-11-25t14:35:37.940957051z",\n            "exitcode": 0,\n            "output": "<!doctype html>\\n<html>\\n<head>\\n<title>welcome to nginx!</title>\\n<style>\\n    body {\\n        width: 35em;\\n        margin: 0 auto;\\n        font-family: tahoma, verdana, arial, sans-serif;\\n    }\\n</style>\\n</head>\\n<body>\\n<h1>welcome to nginx!</h1>\\n<p>if you see this page, the nginx web server is successfully installed and\\nworking. further configuration is required.</p>\\n\\n<p>for online documentation and support please refer to\\n<a href=\\"http://nginx.org/\\">nginx.org</a>.<br/>\\ncommercial support is available at\\n<a href=\\"http://nginx.com/\\">nginx.com</a>.</p>\\n\\n<p><em>thank you for using nginx.</em></p>\\n</body>\\n</html>\\n",\n            "start": "2016-11-25t14:35:37.780192565z"\n        }\n    ],\n    "status": "healthy"\n}\n\n\n\n# onbuild 为他人作嫁衣\n\n格式：onbuild <其它指令>。\n\nonbuild 是一个特殊的指令，它后面跟的是其它指令，比如 run, copy 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。\n\ndockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 onbuild 是为了帮助别人定制自己而准备的。\n\n假设我们要制作 node.js 所写的应用的镜像。我们都知道 node.js 使用 npm 进行包管理，所有依赖、配置、启动信息等会放到 package.json 文件里。在拿到程序代码后，需要先进行 npm install 才可以获得所有需要的依赖。然后就可以通过 npm start 来启动应用。因此，一般来说会这样写 dockerfile：\n\nfrom node:slim\nrun mkdir /app\nworkdir /app\ncopy ./package.json /app\nrun [ "npm", "install" ]\ncopy . /app/\ncmd [ "npm", "start" ]\n\n\n把这个 dockerfile 放到 node.js 项目的根目录，构建好镜像后，就可以直接拿来启动容器运行。但是如果我们还有第二个 node.js 项目也差不多呢？好吧，那就再把这个 dockerfile 复制到第二个项目里。那如果有第三个项目呢？再复制么？文件的副本越多，版本控制就越困难，让我们继续看这样的场景维护的问题。\n\n如果第一个 node.js 项目在开发过程中，发现这个 dockerfile 里存在问题，比如敲错字了、或者需要安装额外的包，然后开发人员修复了这个 dockerfile，再次构建，问题解决。\b第一个项目没问题了，但是第二个项目呢？虽然最初 dockerfile 是复制、粘贴自第一个项目的，但是并不会因为第一个项目修复了他们的 dockerfile，而第二个项目的 dockerfile 就会被自动修复。\n\n那么我们可不可以做一个基础镜像，然后各个项目使用这个基础镜像呢？这样基础镜像更新，各个项目不用同步 dockerfile 的变化，重新构建后就继承了基础镜像的更新？好吧，可以，让我们看看这样的结果。那么上面的这个 dockerfile 就会变为：\n\nfrom node:slim\nrun mkdir /app\nworkdir /app\ncmd [ "npm", "start" ]\n\n\n这里我们把项目相关的构建指令拿出来，放到子项目里去。假设这个基础镜像的名字为 my-node 的话，各个项目内的自己的 dockerfile 就变为：\n\nfrom my-node\ncopy ./package.json /app\nrun [ "npm", "install" ]\ncopy . /app/\n\n\n基础镜像变化后，各个项目都用这个 dockerfile 重新构建镜像，会继承基础镜像的更新。\n\n那么，问题解决了么？没有。准确说，只解决了一半。如果这个 dockerfile 里面有些东西需要调整呢？比如 npm install 都需要加一些参数，那怎么办？这一行 run 是不可能放入基础镜像的，因为涉及到了当前项目的 ./package.json，难道又要一个个修改么？所以说，这样制作基础镜像，只解决了原来的 dockerfile 的前4条指令的变化问题，而后面三条指令的变化则完全没办法处理。\n\nonbuild 可以解决这个问题。让我们用 onbuild 重新写一下基础镜像的 dockerfile:\n\nfrom node:slim\nrun mkdir /app\nworkdir /app\nonbuild copy ./package.json /app\nonbuild run [ "npm", "install" ]\nonbuild copy . /app/\ncmd [ "npm", "start" ]\n\n\n这次我们回到原始的 dockerfile，但是这次将项目相关的指令加上 onbuild，这样在构建基础镜像的时候，这三行并不会被执行。然后各个项目的 dockerfile 就变成了简单地：\n\nfrom my-node\n\n\n是的，只有这么一行。当在各个项目目录中，用这个只有一行的 dockerfile 构建镜像时，之前基础镜像的那三行 onbuild 就会开始执行，成功的将当前项目的代码复制进镜像、并且针对本项目执行 npm install，生成应用镜像。\n\n\n# 参考文档\n\n * dockerfie 官方文档\n * dockerfile 最佳实践文档\n * docker 官方镜像 dockerfile',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Dockerfile 多阶段构建",frontmatter:{title:"Dockerfile 多阶段构建",date:"2022-02-08T17:58:04.000Z",permalink:"/pages/f99491/"},regularPath:"/17.Docker/04.Docker%20%E9%95%9C%E5%83%8F/09.Dockerfile%20%E5%A4%9A%E9%98%B6%E6%AE%B5%E6%9E%84%E5%BB%BA.html",relativePath:"17.Docker/04.Docker 镜像/09.Dockerfile 多阶段构建.md",key:"v-12b8a377",path:"/pages/f99491/",headers:[{level:2,title:"之前的做法",slug:"之前的做法",normalizedTitle:"之前的做法",charIndex:23},{level:3,title:"全部放入一个 Dockerfile",slug:"全部放入一个-dockerfile",normalizedTitle:"全部放入一个 dockerfile",charIndex:81},{level:3,title:"分散到多个 Dockerfile",slug:"分散到多个-dockerfile",normalizedTitle:"分散到多个 dockerfile",charIndex:770},{level:2,title:"使用多阶段构建",slug:"使用多阶段构建",normalizedTitle:"使用多阶段构建",charIndex:1934},{level:3,title:"只构建某一阶段的镜像",slug:"只构建某一阶段的镜像",normalizedTitle:"只构建某一阶段的镜像",charIndex:2835},{level:3,title:"构建时从其他镜像复制文件",slug:"构建时从其他镜像复制文件",normalizedTitle:"构建时从其他镜像复制文件",charIndex:3036}],headersStr:"之前的做法 全部放入一个 Dockerfile 分散到多个 Dockerfile 使用多阶段构建 只构建某一阶段的镜像 构建时从其他镜像复制文件",content:'# Dockerfile 多阶段构建\n\n\n# 之前的做法\n\n在 Docker 17.05 版本之前，我们构建 Docker 镜像时，通常会采用两种方式：\n\n\n# 全部放入一个 Dockerfile\n\n一种方式是将所有的构建过程编包含在一个 Dockerfile 中，包括项目及其依赖库的编译、测试、打包等流程，这里可能会带来的一些问题：\n\n * Dockerfile 特别长，可维护性降低\n * 镜像层次多，镜像体积较大，部署时间变长\n * 源代码存在泄露的风险\n\n例如\n\n编写 app.go 文件，该程序输出 Hello World!\n\npackage main  \n\nimport "fmt"  \n\nfunc main(){  \n    fmt.Printf("Hello World!");\n}\n\n\n编写 Dockerfile.one 文件\n\nFROM golang:1.9-alpine\n\nRUN apk --no-cache add git ca-certificates\n\nWORKDIR /go/src/github.com/go/helloworld/\n\nCOPY app.go .\n\nRUN go get -d -v github.com/go-sql-driver/mysql \\\n  && CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . \\\n  && cp /go/src/github.com/go/helloworld/app /root\n\nWORKDIR /root/\n\nCMD ["./app"]\n\n\n构建镜像\n\ndocker build -t go/helloworld:1 -f Dockerfile.one .\n\n\n\n# 分散到多个 Dockerfile\n\n另一种方式，就是我们事先在一个 Dockerfile 将项目及其依赖库编译测试打包好后，再将其拷贝到运行环境中，这种方式需要我们编写两个 Dockerfile 和一些编译脚本才能将其两个阶段自动整合起来，这种方式虽然可以很好地规避第一种方式存在的风险，但明显部署过程较复杂。\n\n例如\n\n编写 Dockerfile.build 文件\n\nFROM golang:1.9-alpine\n\nRUN apk --no-cache add git\n\nWORKDIR /go/src/github.com/go/helloworld\n\nCOPY app.go .\n\nRUN go get -d -v github.com/go-sql-driver/mysql \\\n  && CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\n\n编写 Dockerfile.copy 文件\n\nFROM alpine:latest\n\nRUN apk --no-cache add ca-certificates\n\nWORKDIR /root/\n\nCOPY app .\n\nCMD ["./app"]\n\n\n新建 build.sh\n\n#!/bin/sh\necho Building go/helloworld:build\n\ndocker build -t go/helloworld:build . -f Dockerfile.build\n\ndocker create --name extract go/helloworld:build\ndocker cp extract:/go/src/github.com/go/helloworld/app ./app\ndocker rm -f extract\n\necho Building go/helloworld:2\n\ndocker build --no-cache -t go/helloworld:2 . -f Dockerfile.copy\nrm ./app\n\n\n现在运行脚本即可构建镜像\n\nchmod +x build.sh\n\n./build.sh\n\n\n对比两种方式生成的镜像大小\n\ndocker image ls\n\nREPOSITORY      TAG    IMAGE ID        CREATED         SIZE\ngo/helloworld   2      f7cf3465432c    22 seconds ago  6.47MB\ngo/helloworld   1      f55d3e16affc    2 minutes ago   295MB\n\n\n\n# 使用多阶段构建\n\n为解决以上问题，Docker v17.05 开始支持多阶段构建 (multistage builds)。使用多阶段构建我们就可以很容易解决前面提到的问题，并且只需要编写一个 Dockerfile：\n\n例如\n\n编写 Dockerfile 文件\n\nFROM golang:1.9-alpine as builder\n\nRUN apk --no-cache add git\n\nWORKDIR /go/src/github.com/go/helloworld/\n\nRUN go get -d -v github.com/go-sql-driver/mysql\n\nCOPY app.go .\n\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest as prod\n\nRUN apk --no-cache add ca-certificates\n\nWORKDIR /root/\n\nCOPY --from=0 /go/src/github.com/go/helloworld/app .\n\nCMD ["./app"]  \n\n\n构建镜像\n\ndocker build -t go/helloworld:3 .\n\n\n对比三个镜像大小\n\ndocker image ls\n\nREPOSITORY        TAG   IMAGE ID         CREATED            SIZE\ngo/helloworld     3     d6911ed9c846     7 seconds ago      6.47MB\ngo/helloworld     2     f7cf3465432c     22 seconds ago     6.47MB\ngo/helloworld     1     f55d3e16affc     2 minutes ago      295MB\n\n\n很明显使用多阶段构建的镜像体积小，同时也完美解决了上边提到的问题。\n\n\n# 只构建某一阶段的镜像\n\n我们可以使用 as 来为某一阶段命名，例如\n\nFROM golang:1.9-alpine as builder\n\n\n例如当我们只想构建 builder 阶段的镜像时，我们可以在使用 docker build 命令时加上 --target 参数即可\n\ndocker build --target builder -t username/imagename:tag .\n\n\n\n# 构建时从其他镜像复制文件\n\n上面例子中我们使用 COPY --from=0 /go/src/github.com/go/helloworld/app . 从上一阶段的镜像中复制文件，我们也可以复制任意镜像中的文件。\n\nCOPY --from=nginx:latest /etc/nginx/nginx.conf /nginx.conf\n',normalizedContent:'# dockerfile 多阶段构建\n\n\n# 之前的做法\n\n在 docker 17.05 版本之前，我们构建 docker 镜像时，通常会采用两种方式：\n\n\n# 全部放入一个 dockerfile\n\n一种方式是将所有的构建过程编包含在一个 dockerfile 中，包括项目及其依赖库的编译、测试、打包等流程，这里可能会带来的一些问题：\n\n * dockerfile 特别长，可维护性降低\n * 镜像层次多，镜像体积较大，部署时间变长\n * 源代码存在泄露的风险\n\n例如\n\n编写 app.go 文件，该程序输出 hello world!\n\npackage main  \n\nimport "fmt"  \n\nfunc main(){  \n    fmt.printf("hello world!");\n}\n\n\n编写 dockerfile.one 文件\n\nfrom golang:1.9-alpine\n\nrun apk --no-cache add git ca-certificates\n\nworkdir /go/src/github.com/go/helloworld/\n\ncopy app.go .\n\nrun go get -d -v github.com/go-sql-driver/mysql \\\n  && cgo_enabled=0 goos=linux go build -a -installsuffix cgo -o app . \\\n  && cp /go/src/github.com/go/helloworld/app /root\n\nworkdir /root/\n\ncmd ["./app"]\n\n\n构建镜像\n\ndocker build -t go/helloworld:1 -f dockerfile.one .\n\n\n\n# 分散到多个 dockerfile\n\n另一种方式，就是我们事先在一个 dockerfile 将项目及其依赖库编译测试打包好后，再将其拷贝到运行环境中，这种方式需要我们编写两个 dockerfile 和一些编译脚本才能将其两个阶段自动整合起来，这种方式虽然可以很好地规避第一种方式存在的风险，但明显部署过程较复杂。\n\n例如\n\n编写 dockerfile.build 文件\n\nfrom golang:1.9-alpine\n\nrun apk --no-cache add git\n\nworkdir /go/src/github.com/go/helloworld\n\ncopy app.go .\n\nrun go get -d -v github.com/go-sql-driver/mysql \\\n  && cgo_enabled=0 goos=linux go build -a -installsuffix cgo -o app .\n\n\n编写 dockerfile.copy 文件\n\nfrom alpine:latest\n\nrun apk --no-cache add ca-certificates\n\nworkdir /root/\n\ncopy app .\n\ncmd ["./app"]\n\n\n新建 build.sh\n\n#!/bin/sh\necho building go/helloworld:build\n\ndocker build -t go/helloworld:build . -f dockerfile.build\n\ndocker create --name extract go/helloworld:build\ndocker cp extract:/go/src/github.com/go/helloworld/app ./app\ndocker rm -f extract\n\necho building go/helloworld:2\n\ndocker build --no-cache -t go/helloworld:2 . -f dockerfile.copy\nrm ./app\n\n\n现在运行脚本即可构建镜像\n\nchmod +x build.sh\n\n./build.sh\n\n\n对比两种方式生成的镜像大小\n\ndocker image ls\n\nrepository      tag    image id        created         size\ngo/helloworld   2      f7cf3465432c    22 seconds ago  6.47mb\ngo/helloworld   1      f55d3e16affc    2 minutes ago   295mb\n\n\n\n# 使用多阶段构建\n\n为解决以上问题，docker v17.05 开始支持多阶段构建 (multistage builds)。使用多阶段构建我们就可以很容易解决前面提到的问题，并且只需要编写一个 dockerfile：\n\n例如\n\n编写 dockerfile 文件\n\nfrom golang:1.9-alpine as builder\n\nrun apk --no-cache add git\n\nworkdir /go/src/github.com/go/helloworld/\n\nrun go get -d -v github.com/go-sql-driver/mysql\n\ncopy app.go .\n\nrun cgo_enabled=0 goos=linux go build -a -installsuffix cgo -o app .\n\nfrom alpine:latest as prod\n\nrun apk --no-cache add ca-certificates\n\nworkdir /root/\n\ncopy --from=0 /go/src/github.com/go/helloworld/app .\n\ncmd ["./app"]  \n\n\n构建镜像\n\ndocker build -t go/helloworld:3 .\n\n\n对比三个镜像大小\n\ndocker image ls\n\nrepository        tag   image id         created            size\ngo/helloworld     3     d6911ed9c846     7 seconds ago      6.47mb\ngo/helloworld     2     f7cf3465432c     22 seconds ago     6.47mb\ngo/helloworld     1     f55d3e16affc     2 minutes ago      295mb\n\n\n很明显使用多阶段构建的镜像体积小，同时也完美解决了上边提到的问题。\n\n\n# 只构建某一阶段的镜像\n\n我们可以使用 as 来为某一阶段命名，例如\n\nfrom golang:1.9-alpine as builder\n\n\n例如当我们只想构建 builder 阶段的镜像时，我们可以在使用 docker build 命令时加上 --target 参数即可\n\ndocker build --target builder -t username/imagename:tag .\n\n\n\n# 构建时从其他镜像复制文件\n\n上面例子中我们使用 copy --from=0 /go/src/github.com/go/helloworld/app . 从上一阶段的镜像中复制文件，我们也可以复制任意镜像中的文件。\n\ncopy --from=nginx:latest /etc/nginx/nginx.conf /nginx.conf\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"镜像的实现原理",frontmatter:{title:"镜像的实现原理",date:"2022-02-08T17:58:04.000Z",permalink:"/pages/3937d4/"},regularPath:"/17.Docker/04.Docker%20%E9%95%9C%E5%83%8F/10.%E9%95%9C%E5%83%8F%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86.html",relativePath:"17.Docker/04.Docker 镜像/10.镜像的实现原理.md",key:"v-dd62df30",path:"/pages/3937d4/",headersStr:null,content:"# 镜像的实现原理\n\nDocker 镜像是怎么实现增量的修改和维护的？\n\n每个镜像都由很多层次构成，Docker 使用 Union FS 将这些不同的层结合到一个镜像中去。\n\n通常 Union FS 有两个用途, 一方面可以实现不借助 LVM、RAID 将多个 disk 挂到同一个目录下,另一个更常用的就是将一个只读的分支和一个可写的分支联合在一起，Live CD 正是基于此方法可以允许在镜像不变的基础上允许用户在其上进行一些写操作。\n\nDocker 在 AUFS 上构建的容器也是利用了类似的原理。",normalizedContent:"# 镜像的实现原理\n\ndocker 镜像是怎么实现增量的修改和维护的？\n\n每个镜像都由很多层次构成，docker 使用 union fs 将这些不同的层结合到一个镜像中去。\n\n通常 union fs 有两个用途, 一方面可以实现不借助 lvm、raid 将多个 disk 挂到同一个目录下,另一个更常用的就是将一个只读的分支和一个可写的分支联合在一起，live cd 正是基于此方法可以允许在镜像不变的基础上允许用户在其上进行一些写操作。\n\ndocker 在 aufs 上构建的容器也是利用了类似的原理。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"操作 Docker 容器",frontmatter:{title:"操作 Docker 容器",date:"2022-02-09T09:11:46.000Z",permalink:"/pages/7e034a/"},regularPath:"/17.Docker/05.Docker%20%E5%AE%B9%E5%99%A8/01.%E6%93%8D%E4%BD%9C%20Docker%20%E5%AE%B9%E5%99%A8.html",relativePath:"17.Docker/05.Docker 容器/01.操作 Docker 容器.md",key:"v-1247ca23",path:"/pages/7e034a/",headersStr:null,content:"# 操作 Docker 容器\n\n容器是 Docker 又一核心概念。\n\n简单的说，容器是独立运行的一个或一组应用，以及它们的运行态环境。对应的，虚拟机可以理解为模拟运行的一整套操作系统（提供了运行态环境和其他系统环境）和跑在上面的应用。\n\n本章将具体介绍如何来管理一个容器，包括创建、启动和停止等。",normalizedContent:"# 操作 docker 容器\n\n容器是 docker 又一核心概念。\n\n简单的说，容器是独立运行的一个或一组应用，以及它们的运行态环境。对应的，虚拟机可以理解为模拟运行的一整套操作系统（提供了运行态环境和其他系统环境）和跑在上面的应用。\n\n本章将具体介绍如何来管理一个容器，包括创建、启动和停止等。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 启动容器",frontmatter:{title:"Docker 启动容器",date:"2022-02-09T09:11:46.000Z",permalink:"/pages/f5a508/"},regularPath:"/17.Docker/05.Docker%20%E5%AE%B9%E5%99%A8/02.Docker%20%E5%90%AF%E5%8A%A8%E5%AE%B9%E5%99%A8.html",relativePath:"17.Docker/05.Docker 容器/02.Docker 启动容器.md",key:"v-39ac2c18",path:"/pages/f5a508/",headers:[{level:2,title:"新建并启动",slug:"新建并启动",normalizedTitle:"新建并启动",charIndex:117},{level:2,title:"启动已终止容器",slug:"启动已终止容器",normalizedTitle:"启动已终止容器",charIndex:824}],headersStr:"新建并启动 启动已终止容器",content:"# Docker 启动容器\n\n启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。\n\n因为 Docker 的容器实在太轻量级了，很多时候用户都是随时删除和新创建容器。\n\n\n# 新建并启动\n\n所需要的命令主要为 docker run。\n\n例如，下面的命令输出一个 “Hello World”，之后终止容器。\n\ndocker run ubuntu:14.04 /bin/echo 'Hello world'\nHello world\n\n\n这跟在本地直接执行 /bin/echo 'hello world' 几乎感觉不出任何区别。\n\n下面的命令则启动一个 bash 终端，允许用户进行交互。\n\ndocker run -t -i ubuntu:14.04 /bin/bash\nroot@af8bae53bdd3:/#\n\n\n其中，-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开。\n\n在交互模式下，用户可以通过所创建的终端来输入命令，例如\n\nroot@af8bae53bdd3:/# pwd\n/\nroot@af8bae53bdd3:/# ls\nbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var\n\n\n当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括：\n\n * 检查本地是否存在指定的镜像，不存在就从公有仓库下载\n * 利用镜像创建并启动一个容器\n * 分配一个文件系统，并在只读的镜像层外面挂载一层可读写层\n * 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去\n * 从地址池配置一个 ip 地址给容器\n * 执行用户指定的应用程序\n * 执行完毕后容器被终止\n\n\n# 启动已终止容器\n\n可以利用 docker container start 命令，直接将一个已经终止的容器启动运行。\n\n容器的核心为所执行的应用程序，所需要的资源都是应用程序运行所必需的。除此之外，并没有其它的资源。可以在伪终端中利用 ps 或 top 来查看进程信息。\n\nroot@ba267838cc1b:/# ps\n  PID TTY          TIME CMD\n    1 ?        00:00:00 bash\n   11 ?        00:00:00 ps\n\n\n可见，容器中仅运行了指定的 bash 应用。这种特点使得 Docker 对资源的利用率极高，是货真价实的轻量级虚拟化。",normalizedContent:"# docker 启动容器\n\n启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。\n\n因为 docker 的容器实在太轻量级了，很多时候用户都是随时删除和新创建容器。\n\n\n# 新建并启动\n\n所需要的命令主要为 docker run。\n\n例如，下面的命令输出一个 “hello world”，之后终止容器。\n\ndocker run ubuntu:14.04 /bin/echo 'hello world'\nhello world\n\n\n这跟在本地直接执行 /bin/echo 'hello world' 几乎感觉不出任何区别。\n\n下面的命令则启动一个 bash 终端，允许用户进行交互。\n\ndocker run -t -i ubuntu:14.04 /bin/bash\nroot@af8bae53bdd3:/#\n\n\n其中，-t 选项让docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开。\n\n在交互模式下，用户可以通过所创建的终端来输入命令，例如\n\nroot@af8bae53bdd3:/# pwd\n/\nroot@af8bae53bdd3:/# ls\nbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var\n\n\n当利用 docker run 来创建容器时，docker 在后台运行的标准操作包括：\n\n * 检查本地是否存在指定的镜像，不存在就从公有仓库下载\n * 利用镜像创建并启动一个容器\n * 分配一个文件系统，并在只读的镜像层外面挂载一层可读写层\n * 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去\n * 从地址池配置一个 ip 地址给容器\n * 执行用户指定的应用程序\n * 执行完毕后容器被终止\n\n\n# 启动已终止容器\n\n可以利用 docker container start 命令，直接将一个已经终止的容器启动运行。\n\n容器的核心为所执行的应用程序，所需要的资源都是应用程序运行所必需的。除此之外，并没有其它的资源。可以在伪终端中利用 ps 或 top 来查看进程信息。\n\nroot@ba267838cc1b:/# ps\n  pid tty          time cmd\n    1 ?        00:00:00 bash\n   11 ?        00:00:00 ps\n\n\n可见，容器中仅运行了指定的 bash 应用。这种特点使得 docker 对资源的利用率极高，是货真价实的轻量级虚拟化。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 守护态运行",frontmatter:{title:"Docker 守护态运行",date:"2022-02-09T09:11:46.000Z",permalink:"/pages/4c0f60/"},regularPath:"/17.Docker/05.Docker%20%E5%AE%B9%E5%99%A8/03.Docker%20%E5%AE%88%E6%8A%A4%E6%80%81%E8%BF%90%E8%A1%8C.html",relativePath:"17.Docker/05.Docker 容器/03.Docker 守护态运行.md",key:"v-80330662",path:"/pages/4c0f60/",headersStr:null,content:'# Docker 守护态运行\n\n更多的时候，需要让 Docker 在后台运行而不是直接把执行命令的结果输出在当前宿主机下。此时，可以通过添加 -d 参数来实现。\n\n下面举两个例子来说明一下。\n\n如果不使用 -d 参数运行容器。\n\ndocker run ubuntu:17.10 /bin/sh -c "while true; do echo hello world; sleep 1; done"\nhello world\nhello world\nhello world\nhello world\n\n\n容器会把输出的结果 (STDOUT) 打印到宿主机上面\n\n如果使用了 -d 参数运行容器。\n\ndocker run -d ubuntu:17.10 /bin/sh -c "while true; do echo hello world; sleep 1; done"\n77b2dc01fe0f3f1265df143181e7b9af5e05279a884f4776ee75350ea9d8017a\n\n\n此时容器会在后台运行并不会把输出的结果 (STDOUT) 打印到宿主机上面(输出结果可以用 docker logs 查看)。\n\n注： 容器是否会长久运行，是和 docker run 指定的命令有关，和 -d 参数无关。\n\n使用 -d 参数启动后会返回一个唯一的 id，也可以通过 docker container ls 命令来查看容器信息。\n\ndocker container ls\nCONTAINER ID  IMAGE         COMMAND               CREATED        STATUS       PORTS NAMES\n77b2dc01fe0f  ubuntu:17.10  /bin/sh -c \'while tr  2 minutes ago  Up 1 minute        agitated_wright\n\n\n要获取容器的输出信息，可以通过 docker container logs 命令。\n\ndocker container logs [container ID or NAMES]\nhello world\nhello world\nhello world\n. . .\n',normalizedContent:'# docker 守护态运行\n\n更多的时候，需要让 docker 在后台运行而不是直接把执行命令的结果输出在当前宿主机下。此时，可以通过添加 -d 参数来实现。\n\n下面举两个例子来说明一下。\n\n如果不使用 -d 参数运行容器。\n\ndocker run ubuntu:17.10 /bin/sh -c "while true; do echo hello world; sleep 1; done"\nhello world\nhello world\nhello world\nhello world\n\n\n容器会把输出的结果 (stdout) 打印到宿主机上面\n\n如果使用了 -d 参数运行容器。\n\ndocker run -d ubuntu:17.10 /bin/sh -c "while true; do echo hello world; sleep 1; done"\n77b2dc01fe0f3f1265df143181e7b9af5e05279a884f4776ee75350ea9d8017a\n\n\n此时容器会在后台运行并不会把输出的结果 (stdout) 打印到宿主机上面(输出结果可以用 docker logs 查看)。\n\n注： 容器是否会长久运行，是和 docker run 指定的命令有关，和 -d 参数无关。\n\n使用 -d 参数启动后会返回一个唯一的 id，也可以通过 docker container ls 命令来查看容器信息。\n\ndocker container ls\ncontainer id  image         command               created        status       ports names\n77b2dc01fe0f  ubuntu:17.10  /bin/sh -c \'while tr  2 minutes ago  up 1 minute        agitated_wright\n\n\n要获取容器的输出信息，可以通过 docker container logs 命令。\n\ndocker container logs [container id or names]\nhello world\nhello world\nhello world\n. . .\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 终止容器",frontmatter:{title:"Docker 终止容器",date:"2022-02-09T09:11:46.000Z",permalink:"/pages/a777e4/"},regularPath:"/17.Docker/05.Docker%20%E5%AE%B9%E5%99%A8/04.Docker%20%E7%BB%88%E6%AD%A2%E5%AE%B9%E5%99%A8.html",relativePath:"17.Docker/05.Docker 容器/04.Docker 终止容器.md",key:"v-73993688",path:"/pages/a777e4/",headersStr:null,content:'# Docker 终止容器\n\n可以使用 docker container stop 来终止一个运行中的容器。\n\n此外，当 Docker 容器中指定的应用终结时，容器也自动终止。\n\n例如对于上一章节中只启动了一个终端的容器，用户通过 exit 命令或 Ctrl+d 来退出终端时，所创建的容器立刻终止。\n\n终止状态的容器可以用 docker container ls -a 命令看到。例如\n\ndocker container ls -a\nCONTAINER ID        IMAGE                    COMMAND                CREATED             STATUS                          PORTS               NAMES\nba267838cc1b        ubuntu:14.04             "/bin/bash"            30 minutes ago      Exited (0) About a minute ago                       trusting_newton\n98e5efa7d997        training/webapp:latest   "python app.py"        About an hour ago   Exited (0) 34 minutes ago                           backstabbing_pike\n\n\n处于终止状态的容器，可以通过 docker container start 命令来重新启动。\n\n此外，docker container restart 命令会将一个运行态的容器终止，然后再重新启动它。',normalizedContent:'# docker 终止容器\n\n可以使用 docker container stop 来终止一个运行中的容器。\n\n此外，当 docker 容器中指定的应用终结时，容器也自动终止。\n\n例如对于上一章节中只启动了一个终端的容器，用户通过 exit 命令或 ctrl+d 来退出终端时，所创建的容器立刻终止。\n\n终止状态的容器可以用 docker container ls -a 命令看到。例如\n\ndocker container ls -a\ncontainer id        image                    command                created             status                          ports               names\nba267838cc1b        ubuntu:14.04             "/bin/bash"            30 minutes ago      exited (0) about a minute ago                       trusting_newton\n98e5efa7d997        training/webapp:latest   "python app.py"        about an hour ago   exited (0) 34 minutes ago                           backstabbing_pike\n\n\n处于终止状态的容器，可以通过 docker container start 命令来重新启动。\n\n此外，docker container restart 命令会将一个运行态的容器终止，然后再重新启动它。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 进入容器",frontmatter:{title:"Docker 进入容器",date:"2022-02-09T09:11:46.000Z",permalink:"/pages/9efa8d/"},regularPath:"/17.Docker/05.Docker%20%E5%AE%B9%E5%99%A8/05.Docker%20%E8%BF%9B%E5%85%A5%E5%AE%B9%E5%99%A8.html",relativePath:"17.Docker/05.Docker 容器/05.Docker 进入容器.md",key:"v-570536b6",path:"/pages/9efa8d/",headers:[{level:2,title:"attach 命令",slug:"attach-命令",normalizedTitle:"attach 命令",charIndex:66},{level:2,title:"exec 命令",slug:"exec-命令",normalizedTitle:"exec 命令",charIndex:84},{level:3,title:"-i -t 参数",slug:"i-t-参数",normalizedTitle:"-i -t 参数",charIndex:647}],headersStr:"attach 命令 exec 命令 -i -t 参数",content:'# Docker 进入容器\n\n在使用 -d 参数时，容器启动后会进入后台。\n\n某些时候需要进入容器进行操作，包括使用 docker attach 命令或 docker exec 命令，推荐大家使用 docker exec 命令，原因会在下面说明。\n\n\n# attach 命令\n\ndocker attach 是 Docker 自带的命令。下面示例如何使用该命令。\n\ndocker run -dit ubuntu\n243c32535da7d142fb0e6df616a3c3ada0b8ab417937c853a9e1c251f499f550\n\ndocker container ls\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n243c32535da7        ubuntu:latest       "/bin/bash"         18 seconds ago      Up 17 seconds                           nostalgic_hypatia\n\ndocker attach 243c\nroot@243c32535da7:/#\n\n\n注意： 如果从这个 stdin 中 exit，会导致容器的停止。\n\n\n# exec 命令\n\n\n# -i -t 参数\n\ndocker exec 后边可以跟多个参数，这里主要说明 -i -t 参数。\n\n只用 -i 参数时，由于没有分配伪终端，界面没有我们熟悉的 Linux 命令提示符，但命令执行结果仍然可以返回。\n\n当 -i -t 参数一起使用时，则可以看到我们熟悉的 Linux 命令提示符。\n\ndocker run -dit ubuntu\n69d137adef7a8a689cbcb059e94da5489d3cddd240ff675c640c8d96e84fe1f6\n\ndocker container ls\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n69d137adef7a        ubuntu:latest       "/bin/bash"         18 seconds ago      Up 17 seconds                           zealous_swirles\n\ndocker exec -i 69d1 bash\nls\nbin\nboot\ndev\n...\n\ndocker exec -it 69d1 bash\nroot@69d137adef7a:/#\n\n\n如果从这个 stdin 中 exit，不会导致容器的停止。这就是为什么推荐大家使用 docker exec 的原因。\n\n更多参数说明请使用 docker exec --help 查看。',normalizedContent:'# docker 进入容器\n\n在使用 -d 参数时，容器启动后会进入后台。\n\n某些时候需要进入容器进行操作，包括使用 docker attach 命令或 docker exec 命令，推荐大家使用 docker exec 命令，原因会在下面说明。\n\n\n# attach 命令\n\ndocker attach 是 docker 自带的命令。下面示例如何使用该命令。\n\ndocker run -dit ubuntu\n243c32535da7d142fb0e6df616a3c3ada0b8ab417937c853a9e1c251f499f550\n\ndocker container ls\ncontainer id        image               command             created             status              ports               names\n243c32535da7        ubuntu:latest       "/bin/bash"         18 seconds ago      up 17 seconds                           nostalgic_hypatia\n\ndocker attach 243c\nroot@243c32535da7:/#\n\n\n注意： 如果从这个 stdin 中 exit，会导致容器的停止。\n\n\n# exec 命令\n\n\n# -i -t 参数\n\ndocker exec 后边可以跟多个参数，这里主要说明 -i -t 参数。\n\n只用 -i 参数时，由于没有分配伪终端，界面没有我们熟悉的 linux 命令提示符，但命令执行结果仍然可以返回。\n\n当 -i -t 参数一起使用时，则可以看到我们熟悉的 linux 命令提示符。\n\ndocker run -dit ubuntu\n69d137adef7a8a689cbcb059e94da5489d3cddd240ff675c640c8d96e84fe1f6\n\ndocker container ls\ncontainer id        image               command             created             status              ports               names\n69d137adef7a        ubuntu:latest       "/bin/bash"         18 seconds ago      up 17 seconds                           zealous_swirles\n\ndocker exec -i 69d1 bash\nls\nbin\nboot\ndev\n...\n\ndocker exec -it 69d1 bash\nroot@69d137adef7a:/#\n\n\n如果从这个 stdin 中 exit，不会导致容器的停止。这就是为什么推荐大家使用 docker exec 的原因。\n\n更多参数说明请使用 docker exec --help 查看。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 导出和导入容器",frontmatter:{title:"Docker 导出和导入容器",date:"2022-02-09T09:11:46.000Z",permalink:"/pages/d3a79f/"},regularPath:"/17.Docker/05.Docker%20%E5%AE%B9%E5%99%A8/06.Docker%20%E5%AF%BC%E5%87%BA%E5%92%8C%E5%AF%BC%E5%85%A5%E5%AE%B9%E5%99%A8.html",relativePath:"17.Docker/05.Docker 容器/06.Docker 导出和导入容器.md",key:"v-045a323e",path:"/pages/d3a79f/",headers:[{level:2,title:"导出容器",slug:"导出容器",normalizedTitle:"导出容器",charIndex:21},{level:2,title:"导入容器快照",slug:"导入容器快照",normalizedTitle:"导入容器快照",charIndex:411}],headersStr:"导出容器 导入容器快照",content:'# Docker 导出和导入容器\n\n\n# 导出容器\n\n如果要导出本地某个容器，可以使用 docker export 命令。\n\ndocker container ls -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                    PORTS               NAMES\n7691a814370e        ubuntu:14.04        "/bin/bash"         36 hours ago        Exited (0) 21 hours ago                       test\ndocker export 7691a814370e > ubuntu.tar\n\n\n这样将导出容器快照到本地文件。\n\n\n# 导入容器快照\n\n可以使用 docker import 从容器快照文件中再导入为镜像，例如\n\ncat ubuntu.tar | docker import - test/ubuntu:v1.0\ndocker image ls\nREPOSITORY          TAG                 IMAGE ID            CREATED              VIRTUAL SIZE\ntest/ubuntu         v1.0                9d37a6082e97        About a minute ago   171.3 MB\n\n\n此外，也可以通过指定 URL 或者某个目录来导入，例如\n\ndocker import http://example.com/exampleimage.tgz example/imagerepo\n\n\n注：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。',normalizedContent:'# docker 导出和导入容器\n\n\n# 导出容器\n\n如果要导出本地某个容器，可以使用 docker export 命令。\n\ndocker container ls -a\ncontainer id        image               command             created             status                    ports               names\n7691a814370e        ubuntu:14.04        "/bin/bash"         36 hours ago        exited (0) 21 hours ago                       test\ndocker export 7691a814370e > ubuntu.tar\n\n\n这样将导出容器快照到本地文件。\n\n\n# 导入容器快照\n\n可以使用 docker import 从容器快照文件中再导入为镜像，例如\n\ncat ubuntu.tar | docker import - test/ubuntu:v1.0\ndocker image ls\nrepository          tag                 image id            created              virtual size\ntest/ubuntu         v1.0                9d37a6082e97        about a minute ago   171.3 mb\n\n\n此外，也可以通过指定 url 或者某个目录来导入，例如\n\ndocker import http://example.com/exampleimage.tgz example/imagerepo\n\n\n注：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 删除容器",frontmatter:{title:"Docker 删除容器",date:"2022-02-09T09:11:46.000Z",permalink:"/pages/8b2f36/"},regularPath:"/17.Docker/05.Docker%20%E5%AE%B9%E5%99%A8/07.Docker%20%E5%88%A0%E9%99%A4%E5%AE%B9%E5%99%A8.html",relativePath:"17.Docker/05.Docker 容器/07.Docker 删除容器.md",key:"v-ae2ed992",path:"/pages/8b2f36/",headers:[{level:2,title:"清理所有处于终止状态的容器",slug:"清理所有处于终止状态的容器",normalizedTitle:"清理所有处于终止状态的容器",charIndex:169}],headersStr:"清理所有处于终止状态的容器",content:"# Docker 删除容器\n\n可以使用 docker container rm 来删除一个处于终止状态的容器。例如\n\ndocker container rm  trusting_newton\ntrusting_newton\n\n\n如果要删除一个运行中的容器，可以添加 -f 参数。Docker 会发送 SIGKILL 信号给容器。\n\n\n# 清理所有处于终止状态的容器\n\n用 docker container ls -a 命令可以查看所有已经创建的包括终止状态的容器，如果数量太多要一个个删除可能会很麻烦，用下面的命令可以清理掉所有处于终止状态的容器。\n\ndocker container prune\n",normalizedContent:"# docker 删除容器\n\n可以使用 docker container rm 来删除一个处于终止状态的容器。例如\n\ndocker container rm  trusting_newton\ntrusting_newton\n\n\n如果要删除一个运行中的容器，可以添加 -f 参数。docker 会发送 sigkill 信号给容器。\n\n\n# 清理所有处于终止状态的容器\n\n用 docker container ls -a 命令可以查看所有已经创建的包括终止状态的容器，如果数量太多要一个个删除可能会很麻烦，用下面的命令可以清理掉所有处于终止状态的容器。\n\ndocker container prune\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"访问 Docker 仓库",frontmatter:{title:"访问 Docker 仓库",date:"2022-02-09T09:17:56.000Z",permalink:"/pages/c0886b/"},regularPath:"/17.Docker/06.%E8%AE%BF%E9%97%AE%20Docker%20%E4%BB%93%E5%BA%93/01.%E8%AE%BF%E9%97%AE%20Docker%20%E4%BB%93%E5%BA%93.html",relativePath:"17.Docker/06.访问 Docker 仓库/01.访问 Docker 仓库.md",key:"v-059385f2",path:"/pages/c0886b/",headersStr:null,content:"# 访问 Docker 仓库\n\n仓库（Repository）是集中存放镜像的地方。\n\n一个容易混淆的概念是注册服务器（Registry）。实际上注册服务器是管理仓库的具体服务器，每个服务器上可以有多个仓库，而每个仓库下面有多个镜像。从这方面来说，仓库可以被认为是一个具体的项目或目录。例如对于仓库地址 dl.dockerpool.com/ubuntu 来说，dl.dockerpool.com 是注册服务器地址，ubuntu 是仓库名。\n\n大部分时候，并不需要严格区分这两者的概念。",normalizedContent:"# 访问 docker 仓库\n\n仓库（repository）是集中存放镜像的地方。\n\n一个容易混淆的概念是注册服务器（registry）。实际上注册服务器是管理仓库的具体服务器，每个服务器上可以有多个仓库，而每个仓库下面有多个镜像。从这方面来说，仓库可以被认为是一个具体的项目或目录。例如对于仓库地址 dl.dockerpool.com/ubuntu 来说，dl.dockerpool.com 是注册服务器地址，ubuntu 是仓库名。\n\n大部分时候，并不需要严格区分这两者的概念。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker Hub",frontmatter:{title:"Docker Hub",date:"2022-02-09T09:17:56.000Z",permalink:"/pages/ebce48/"},regularPath:"/17.Docker/06.%E8%AE%BF%E9%97%AE%20Docker%20%E4%BB%93%E5%BA%93/02.Docker%20Hub.html",relativePath:"17.Docker/06.访问 Docker 仓库/02.Docker Hub.md",key:"v-687cd7b8",path:"/pages/ebce48/",headers:[{level:2,title:"注册",slug:"注册",normalizedTitle:"注册",charIndex:109},{level:2,title:"登录",slug:"登录",normalizedTitle:"登录",charIndex:165},{level:2,title:"拉取镜像",slug:"拉取镜像",normalizedTitle:"拉取镜像",charIndex:258},{level:2,title:"推送镜像",slug:"推送镜像",normalizedTitle:"推送镜像",charIndex:1648},{level:2,title:"自动创建",slug:"自动创建",normalizedTitle:"自动创建",charIndex:1131}],headersStr:"注册 登录 拉取镜像 推送镜像 自动创建",content:"# Docker Hub\n\n目前 Docker 官方维护了一个公共仓库 Docker Hub，其中已经包括了数量超过 15,000 的镜像。大部分需求都可以通过在 Docker Hub 中直接下载镜像来实现。\n\n\n# 注册\n\n你可以在 https://cloud.docker.com 免费注册一个 Docker 账号。\n\n\n# 登录\n\n可以通过执行 docker login 命令交互式的输入用户名及密码来完成在命令行界面登录 Docker Hub。\n\n你可以通过 docker logout 退出登录。\n\n\n# 拉取镜像\n\n你可以通过 docker search 命令来查找官方仓库中的镜像，并利用 docker pull 命令来将它下载到本地。\n\n例如以 centos 为关键词进行搜索：\n\ndocker search centos\nNAME                                            DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\ncentos                                          The official build of CentOS.                   465       [OK]\ntianon/centos                                   CentOS 5 and 6, created using rinse instea...   28\nblalor/centos                                   Bare-bones base CentOS 6.5 image                6                    [OK]\nsaltstack/centos-6-minimal                                                                      6                    [OK]\ntutum/centos-6.4                                DEPRECATED. Use tutum/centos:6.4 instead. ...   5                    [OK]\n\n\n可以看到返回了很多包含关键字的镜像，其中包括镜像名字、描述、收藏数（表示该镜像的受关注程度）、是否官方创建、是否自动创建。\n\n官方的镜像说明是官方项目组创建和维护的，automated 资源允许用户验证镜像的来源和内容。\n\n根据是否是官方提供，可将镜像资源分为两类。\n\n一种是类似 centos 这样的镜像，被称为基础镜像或根镜像。这些基础镜像由 Docker 公司创建、验证、支持、提供。这样的镜像往往使用单个单词作为名字。\n\n还有一种类型，比如 tianon/centos 镜像，它是由 Docker 的用户创建并维护的，往往带有用户名称前缀。可以通过前缀 username/ 来指定使用某个用户提供的镜像，比如 tianon 用户。\n\n另外，在查找的时候通过 --filter=stars=N 参数可以指定仅显示收藏数量为 N 以上的镜像。\n\n下载官方 centos 镜像到本地。\n\ndocker pull centos\nPulling repository centos\n0b443ba03958: Download complete\n539c0211cd76: Download complete\n511136ea3c5a: Download complete\n7064731afe90: Download complete\n\n\n\n# 推送镜像\n\n用户也可以在登录后通过 docker push 命令来将自己的镜像推送到 Docker Hub。\n\n以下命令中的 username 请替换为你的 Docker 账号用户名。\n\ndocker tag ubuntu:17.10 username/ubuntu:17.10\n\ndocker image ls\n\nREPOSITORY                                               TAG                    IMAGE ID            CREATED             SIZE\nubuntu                                                   17.10                  275d79972a86        6 days ago          94.6MB\nusername/ubuntu                                          17.10                  275d79972a86        6 days ago          94.6MB\n\ndocker push username/ubuntu:17.10\n\ndocker search username\n\nNAME                      DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED\nusername/ubuntu\n\n\n\n# 自动创建\n\n自动创建（Automated Builds）功能对于需要经常升级镜像内程序来说，十分方便。\n\n有时候，用户创建了镜像，安装了某个软件，如果软件发布新版本则需要手动更新镜像。\n\n而自动创建允许用户通过 Docker Hub 指定跟踪一个目标网站（目前支持 GitHub 或 BitBucket）上的项目，一旦项目发生新的提交或者创建新的标签（tag），Docker Hub 会自动构建镜像并推送到 Docker Hub 中。\n\n要配置自动创建，包括如下的步骤：\n\n * 创建并登录 Docker Hub，以及目标网站；\n * 在目标网站中连接帐户到 Docker Hub；\n * 在 Docker Hub 中 配置一个自动创建；\n * 选取一个目标网站中的项目（需要含 Dockerfile）和分支；\n * 指定 Dockerfile 的位置，并提交创建。\n\n之后，可以在 Docker Hub 的 自动创建页面 中跟踪每次创建的状态。",normalizedContent:"# docker hub\n\n目前 docker 官方维护了一个公共仓库 docker hub，其中已经包括了数量超过 15,000 的镜像。大部分需求都可以通过在 docker hub 中直接下载镜像来实现。\n\n\n# 注册\n\n你可以在 https://cloud.docker.com 免费注册一个 docker 账号。\n\n\n# 登录\n\n可以通过执行 docker login 命令交互式的输入用户名及密码来完成在命令行界面登录 docker hub。\n\n你可以通过 docker logout 退出登录。\n\n\n# 拉取镜像\n\n你可以通过 docker search 命令来查找官方仓库中的镜像，并利用 docker pull 命令来将它下载到本地。\n\n例如以 centos 为关键词进行搜索：\n\ndocker search centos\nname                                            description                                     stars     official   automated\ncentos                                          the official build of centos.                   465       [ok]\ntianon/centos                                   centos 5 and 6, created using rinse instea...   28\nblalor/centos                                   bare-bones base centos 6.5 image                6                    [ok]\nsaltstack/centos-6-minimal                                                                      6                    [ok]\ntutum/centos-6.4                                deprecated. use tutum/centos:6.4 instead. ...   5                    [ok]\n\n\n可以看到返回了很多包含关键字的镜像，其中包括镜像名字、描述、收藏数（表示该镜像的受关注程度）、是否官方创建、是否自动创建。\n\n官方的镜像说明是官方项目组创建和维护的，automated 资源允许用户验证镜像的来源和内容。\n\n根据是否是官方提供，可将镜像资源分为两类。\n\n一种是类似 centos 这样的镜像，被称为基础镜像或根镜像。这些基础镜像由 docker 公司创建、验证、支持、提供。这样的镜像往往使用单个单词作为名字。\n\n还有一种类型，比如 tianon/centos 镜像，它是由 docker 的用户创建并维护的，往往带有用户名称前缀。可以通过前缀 username/ 来指定使用某个用户提供的镜像，比如 tianon 用户。\n\n另外，在查找的时候通过 --filter=stars=n 参数可以指定仅显示收藏数量为 n 以上的镜像。\n\n下载官方 centos 镜像到本地。\n\ndocker pull centos\npulling repository centos\n0b443ba03958: download complete\n539c0211cd76: download complete\n511136ea3c5a: download complete\n7064731afe90: download complete\n\n\n\n# 推送镜像\n\n用户也可以在登录后通过 docker push 命令来将自己的镜像推送到 docker hub。\n\n以下命令中的 username 请替换为你的 docker 账号用户名。\n\ndocker tag ubuntu:17.10 username/ubuntu:17.10\n\ndocker image ls\n\nrepository                                               tag                    image id            created             size\nubuntu                                                   17.10                  275d79972a86        6 days ago          94.6mb\nusername/ubuntu                                          17.10                  275d79972a86        6 days ago          94.6mb\n\ndocker push username/ubuntu:17.10\n\ndocker search username\n\nname                      description                                     stars               official            automated\nusername/ubuntu\n\n\n\n# 自动创建\n\n自动创建（automated builds）功能对于需要经常升级镜像内程序来说，十分方便。\n\n有时候，用户创建了镜像，安装了某个软件，如果软件发布新版本则需要手动更新镜像。\n\n而自动创建允许用户通过 docker hub 指定跟踪一个目标网站（目前支持 github 或 bitbucket）上的项目，一旦项目发生新的提交或者创建新的标签（tag），docker hub 会自动构建镜像并推送到 docker hub 中。\n\n要配置自动创建，包括如下的步骤：\n\n * 创建并登录 docker hub，以及目标网站；\n * 在目标网站中连接帐户到 docker hub；\n * 在 docker hub 中 配置一个自动创建；\n * 选取一个目标网站中的项目（需要含 dockerfile）和分支；\n * 指定 dockerfile 的位置，并提交创建。\n\n之后，可以在 docker hub 的 自动创建页面 中跟踪每次创建的状态。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 私有仓库",frontmatter:{title:"Docker 私有仓库",date:"2022-02-09T09:17:56.000Z",permalink:"/pages/92e266/"},regularPath:"/17.Docker/06.%E8%AE%BF%E9%97%AE%20Docker%20%E4%BB%93%E5%BA%93/03.Docker%20%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93.html",relativePath:"17.Docker/06.访问 Docker 仓库/03.Docker 私有仓库.md",key:"v-f8655a0e",path:"/pages/92e266/",headers:[{level:2,title:"安装运行 docker-registry",slug:"安装运行-docker-registry",normalizedTitle:"安装运行 docker-registry",charIndex:155},{level:3,title:"容器运行",slug:"容器运行",normalizedTitle:"容器运行",charIndex:180},{level:2,title:"在私有仓库上传、搜索、下载镜像",slug:"在私有仓库上传、搜索、下载镜像",normalizedTitle:"在私有仓库上传、搜索、下载镜像",charIndex:520},{level:2,title:"注意事项",slug:"注意事项",normalizedTitle:"注意事项",charIndex:2446},{level:3,title:"Ubuntu 16.04+, Debian 8+, centos 7",slug:"ubuntu-16-04-debian-8-centos-7",normalizedTitle:"ubuntu 16.04+, debian 8+, centos 7",charIndex:2660},{level:2,title:"其他",slug:"其他",normalizedTitle:"其他",charIndex:2490}],headersStr:"安装运行 docker-registry 容器运行 在私有仓库上传、搜索、下载镜像 注意事项 Ubuntu 16.04+, Debian 8+, centos 7 其他",content:'# Docker 私有仓库\n\n有时候使用 Docker Hub 这样的公共仓库可能不方便，用户可以创建一个本地仓库供私人使用。\n\n本节介绍如何使用本地仓库。\n\ndocker-registry 是官方提供的工具，可以用于构建私有的镜像仓库。本文内容基于 docker-registry v2.x 版本。\n\n\n# 安装运行 docker-registry\n\n\n# 容器运行\n\n你可以通过获取官方 registry 镜像来运行。\n\ndocker run -d -p 5000:5000 --restart=always --name registry registry\n\n\n这将使用官方的 registry 镜像来启动私有仓库。默认情况下，仓库会被创建在容器的 /var/lib/registry 目录下。你可以通过 -v 参数来将镜像文件存放在本地的指定路径。例如下面的例子将上传的镜像放到本地的 /opt/data/registry 目录。\n\ndocker run -d \\\n    -p 5000:5000 \\\n    -v /opt/data/registry:/var/lib/registry \\\n    registry\n\n\n\n# 在私有仓库上传、搜索、下载镜像\n\n创建好私有仓库之后，就可以使用 docker tag 来标记一个镜像，然后推送它到仓库。例如私有仓库地址为 127.0.0.1:5000。\n\n先在本机查看已有的镜像。\n\ndocker image ls\nREPOSITORY                        TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nubuntu                            latest              ba5877dc9bec        6 weeks ago         192.7 MB\n\n\n使用 docker tag 将 ubuntu:latest 这个镜像标记为 127.0.0.1:5000/ubuntu:latest。\n\n格式为 docker tag IMAGE[:TAG] [REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG]。\n\ndocker tag ubuntu:latest 127.0.0.1:5000/ubuntu:latest\ndocker image ls\nREPOSITORY                        TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nubuntu                            latest              ba5877dc9bec        6 weeks ago         192.7 MB\n127.0.0.1:5000/ubuntu:latest      latest              ba5877dc9bec        6 weeks ago         192.7 MB\n\n\n使用 docker push 上传标记的镜像。\n\ndocker push 127.0.0.1:5000/ubuntu:latest\nThe push refers to repository [127.0.0.1:5000/ubuntu]\n373a30c24545: Pushed\na9148f5200b0: Pushed\ncdd3de0940ab: Pushed\nfc56279bbb33: Pushed\nb38367233d37: Pushed\n2aebd096e0e2: Pushed\nlatest: digest: sha256:fe4277621f10b5026266932ddf760f5a756d2facd505a94d2da12f4f52f71f5a size: 1568\n\n\n用 curl 查看仓库中的镜像。\n\ncurl 127.0.0.1:5000/v2/_catalog\n{"repositories":["ubuntu"]}\n\n\n这里可以看到 {"repositories":["ubuntu"]}，表明镜像已经被成功上传了。\n\n先删除已有镜像，再尝试从私有仓库中下载这个镜像。\n\ndocker image rm 127.0.0.1:5000/ubuntu:latest\n\ndocker pull 127.0.0.1:5000/ubuntu:latest\nPulling repository 127.0.0.1:5000/ubuntu:latest\nba5877dc9bec: Download complete\n511136ea3c5a: Download complete\n9bad880da3d2: Download complete\n25f11f5fb0cb: Download complete\nebc34468f71d: Download complete\n2318d26665ef: Download complete\n\ndocker image ls\nREPOSITORY                         TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\n127.0.0.1:5000/ubuntu:latest       latest              ba5877dc9bec        6 weeks ago         192.7 MB\n\n\n\n# 注意事项\n\n如果你不想使用 127.0.0.1:5000 作为仓库地址，比如想让本网段的其他主机也能把镜像推送到私有仓库。你就得把例如 192.168.199.100:5000 这样的内网地址作为私有仓库地址，这时你会发现无法成功推送镜像。\n\n这是因为 Docker 默认不允许非 HTTPS 方式推送镜像。我们可以通过 Docker 的配置选项来取消这个限制，或者查看下一节配置能够通过 HTTPS 访问的私有仓库。\n\n\n# Ubuntu 16.04+, Debian 8+, centos 7\n\n对于使用 systemd 的系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件）\n\n{\n  "registry-mirrors": [\n    "https://registry.docker-cn.com"\n  ],\n  "insecure-registries": [\n    "192.168.199.100:5000"\n  ]\n}\n\n\n> 注意：该文件必须符合 json 规范，否则 Docker 将不能启动。\n\n\n# 其他\n\n对于 Docker for Windows 、 Docker for Mac 在设置中编辑 daemon.json 增加和上边一样的字符串即可。',normalizedContent:'# docker 私有仓库\n\n有时候使用 docker hub 这样的公共仓库可能不方便，用户可以创建一个本地仓库供私人使用。\n\n本节介绍如何使用本地仓库。\n\ndocker-registry 是官方提供的工具，可以用于构建私有的镜像仓库。本文内容基于 docker-registry v2.x 版本。\n\n\n# 安装运行 docker-registry\n\n\n# 容器运行\n\n你可以通过获取官方 registry 镜像来运行。\n\ndocker run -d -p 5000:5000 --restart=always --name registry registry\n\n\n这将使用官方的 registry 镜像来启动私有仓库。默认情况下，仓库会被创建在容器的 /var/lib/registry 目录下。你可以通过 -v 参数来将镜像文件存放在本地的指定路径。例如下面的例子将上传的镜像放到本地的 /opt/data/registry 目录。\n\ndocker run -d \\\n    -p 5000:5000 \\\n    -v /opt/data/registry:/var/lib/registry \\\n    registry\n\n\n\n# 在私有仓库上传、搜索、下载镜像\n\n创建好私有仓库之后，就可以使用 docker tag 来标记一个镜像，然后推送它到仓库。例如私有仓库地址为 127.0.0.1:5000。\n\n先在本机查看已有的镜像。\n\ndocker image ls\nrepository                        tag                 image id            created             virtual size\nubuntu                            latest              ba5877dc9bec        6 weeks ago         192.7 mb\n\n\n使用 docker tag 将 ubuntu:latest 这个镜像标记为 127.0.0.1:5000/ubuntu:latest。\n\n格式为 docker tag image[:tag] [registry_host[:registry_port]/]repository[:tag]。\n\ndocker tag ubuntu:latest 127.0.0.1:5000/ubuntu:latest\ndocker image ls\nrepository                        tag                 image id            created             virtual size\nubuntu                            latest              ba5877dc9bec        6 weeks ago         192.7 mb\n127.0.0.1:5000/ubuntu:latest      latest              ba5877dc9bec        6 weeks ago         192.7 mb\n\n\n使用 docker push 上传标记的镜像。\n\ndocker push 127.0.0.1:5000/ubuntu:latest\nthe push refers to repository [127.0.0.1:5000/ubuntu]\n373a30c24545: pushed\na9148f5200b0: pushed\ncdd3de0940ab: pushed\nfc56279bbb33: pushed\nb38367233d37: pushed\n2aebd096e0e2: pushed\nlatest: digest: sha256:fe4277621f10b5026266932ddf760f5a756d2facd505a94d2da12f4f52f71f5a size: 1568\n\n\n用 curl 查看仓库中的镜像。\n\ncurl 127.0.0.1:5000/v2/_catalog\n{"repositories":["ubuntu"]}\n\n\n这里可以看到 {"repositories":["ubuntu"]}，表明镜像已经被成功上传了。\n\n先删除已有镜像，再尝试从私有仓库中下载这个镜像。\n\ndocker image rm 127.0.0.1:5000/ubuntu:latest\n\ndocker pull 127.0.0.1:5000/ubuntu:latest\npulling repository 127.0.0.1:5000/ubuntu:latest\nba5877dc9bec: download complete\n511136ea3c5a: download complete\n9bad880da3d2: download complete\n25f11f5fb0cb: download complete\nebc34468f71d: download complete\n2318d26665ef: download complete\n\ndocker image ls\nrepository                         tag                 image id            created             virtual size\n127.0.0.1:5000/ubuntu:latest       latest              ba5877dc9bec        6 weeks ago         192.7 mb\n\n\n\n# 注意事项\n\n如果你不想使用 127.0.0.1:5000 作为仓库地址，比如想让本网段的其他主机也能把镜像推送到私有仓库。你就得把例如 192.168.199.100:5000 这样的内网地址作为私有仓库地址，这时你会发现无法成功推送镜像。\n\n这是因为 docker 默认不允许非 https 方式推送镜像。我们可以通过 docker 的配置选项来取消这个限制，或者查看下一节配置能够通过 https 访问的私有仓库。\n\n\n# ubuntu 16.04+, debian 8+, centos 7\n\n对于使用 systemd 的系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件）\n\n{\n  "registry-mirrors": [\n    "https://registry.docker-cn.com"\n  ],\n  "insecure-registries": [\n    "192.168.199.100:5000"\n  ]\n}\n\n\n> 注意：该文件必须符合 json 规范，否则 docker 将不能启动。\n\n\n# 其他\n\n对于 docker for windows 、 docker for mac 在设置中编辑 daemon.json 增加和上边一样的字符串即可。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 私有仓库高级配置",frontmatter:{title:"Docker 私有仓库高级配置",date:"2022-02-09T09:17:56.000Z",permalink:"/pages/141d21/"},regularPath:"/17.Docker/06.%E8%AE%BF%E9%97%AE%20Docker%20%E4%BB%93%E5%BA%93/04.Docker%20%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE.html",relativePath:"17.Docker/06.访问 Docker 仓库/04.Docker 私有仓库高级配置.md",key:"v-093f317f",path:"/pages/141d21/",headers:[{level:2,title:"准备站点证书",slug:"准备站点证书",normalizedTitle:"准备站点证书",charIndex:115},{level:2,title:"配置私有仓库",slug:"配置私有仓库",normalizedTitle:"配置私有仓库",charIndex:1878},{level:2,title:"生成 http 认证文件",slug:"生成-http-认证文件",normalizedTitle:"生成 http 认证文件",charIndex:2658},{level:2,title:"编辑 docker-compose.yml",slug:"编辑-docker-compose-yml",normalizedTitle:"编辑 docker-compose.yml",charIndex:2840},{level:2,title:"修改 hosts",slug:"修改-hosts",normalizedTitle:"修改 hosts",charIndex:3065},{level:2,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:3123},{level:2,title:"测试私有仓库功能",slug:"测试私有仓库功能",normalizedTitle:"测试私有仓库功能",charIndex:3198},{level:2,title:"注意事项",slug:"注意事项",normalizedTitle:"注意事项",charIndex:3679}],headersStr:"准备站点证书 配置私有仓库 生成 http 认证文件 编辑 docker-compose.yml 修改 hosts 启动 测试私有仓库功能 注意事项",content:'# Docker 私有仓库高级配置\n\n上一节我们搭建了一个具有基础功能的私有仓库，本小节我们来使用 Docker Compose 搭建一个拥有权限认证、TLS 的私有仓库。\n\n新建一个文件夹，以下步骤均在该文件夹中进行。\n\n\n# 准备站点证书\n\n如果你拥有一个域名，国内各大云服务商均提供免费的站点证书。你也可以使用 openssl 自行签发证书。\n\n这里假设我们将要搭建的私有仓库地址为 docker.domain.com，下面我们介绍使用 openssl 自行签发 docker.domain.com 的站点 SSL 证书。\n\n第一步创建 CA 私钥。\n\nopenssl genrsa -out "root-ca.key" 4096\n\n\n第二步利用私钥创建 CA 根证书请求文件。\n\nopenssl req \\\n          -new -key "root-ca.key" \\\n          -out "root-ca.csr" -sha256 \\\n          -subj \'/C=CN/ST=Shanxi/L=Datong/O=Your Company Name/CN=Your Company Name Docker Registry CA\'\n\n\n> 以上命令中 -subj 参数里的 /C 表示国家，如 CN；/ST 表示省；/L 表示城市或者地区；/O 表示组织名；/CN 通用名称。\n\n第三步配置 CA 根证书，新建 root-ca.cnf。\n\n[root_ca]\nbasicConstraints = critical,CA:TRUE,pathlen:1\nkeyUsage = critical, nonRepudiation, cRLSign, keyCertSign\nsubjectKeyIdentifier=hash\n\n\n第四步签发根证书。\n\nopenssl x509 -req  -days 3650  -in "root-ca.csr" \\\n               -signkey "root-ca.key" -sha256 -out "root-ca.crt" \\\n               -extfile "root-ca.cnf" -extensions \\\n               root_ca\n\n\n第五步生成站点 SSL 私钥。\n\nopenssl genrsa -out "docker.domain.com.key" 4096\n\n\n第六步使用私钥生成证书请求文件。\n\nopenssl req -new -key "docker.domain.com.key" -out "site.csr" -sha256 \\\n          -subj \'/C=CN/ST=Shanxi/L=Datong/O=Your Company Name/CN=docker.domain.com\'\n\n\n第七步配置证书，新建 site.cnf 文件。\n\n[server]\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints = critical,CA:FALSE\nextendedKeyUsage=serverAuth\nkeyUsage = critical, digitalSignature, keyEncipherment\nsubjectAltName = DNS:docker.domain.com, IP:127.0.0.1\nsubjectKeyIdentifier=hash\n\n\n第八步签署站点 SSL 证书。\n\nopenssl x509 -req -days 750 -in "site.csr" -sha256 \\\n    -CA "root-ca.crt" -CAkey "root-ca.key"  -CAcreateserial \\\n    -out "docker.domain.com.crt" -extfile "site.cnf" -extensions server\n\n\n这样已经拥有了 docker.domain.com 的网站 SSL 私钥 docker.domain.com.key 和 SSL 证书 docker.domain.com.crt。\n\n新建 ssl 文件夹并将 docker.domain.com.key docker.domain.com.crt 这两个文件移入，删除其他文件。\n\n\n# 配置私有仓库\n\n私有仓库默认的配置文件位于 /etc/docker/registry/config.yml，我们先在本地编辑 config.yml，之后挂载到容器中。\n\nversion: 0.1\nlog:\n  accesslog:\n    disabled: true\n  level: debug\n  formatter: text\n  fields:\n    service: registry\n    environment: staging\nstorage:\n  delete:\n    enabled: true\n  cache:\n    blobdescriptor: inmemory\n  filesystem:\n    rootdirectory: /var/lib/registry\nauth:\n  htpasswd:\n    realm: basic-realm\n    path: /etc/docker/registry/auth/nginx.htpasswd\nhttp:\n  addr: :443\n  host: https://docker.domain.com\n  headers:\n    X-Content-Type-Options: [nosniff]\n  http2:\n    disabled: false\n  tls:\n    certificate: /etc/docker/registry/ssl/docker.domain.com.crt\n    key: /etc/docker/registry/ssl/docker.domain.com.key\nhealth:\n  storagedriver:\n    enabled: true\n    interval: 10s\nthreshold: 3\n\n\n\n# 生成 http 认证文件\n\nmkdir auth\n\ndocker run --rm \\\n    --entrypoint htpasswd \\\n    registry \\\n    -Bbn username password > auth/nginx.htpasswd\n\n\n> 将上面的 username password 替换为你自己的用户名和密码。\n\n\n# 编辑 docker-compose.yml\n\nversion: \'3\'\n\nservices:\n  registry:\n    image: registry\n    ports:\n      - "443:443"\n    volumes:\n      - ./:/etc/docker/registry\n      - registry-data:/var/lib/registry\n\nvolumes:\n  registry-data:\n\n\n\n# 修改 hosts\n\n编辑 /etc/hosts\n\ndocker.domain.com 127.0.0.1\n\n\n\n# 启动\n\ndocker-compose up -d\n\n\n这样我们就搭建好了一个具有权限认证、TLS 的私有仓库，接下来我们测试其功能是否正常。\n\n\n# 测试私有仓库功能\n\n登录到私有仓库。\n\ndocker login docker.domain.com\n\n\n尝试推送、拉取镜像。\n\ndocker pull ubuntu:17.10\n\ndocker tag ubuntu:17.10 docker.domain.com/username/ubuntu:17.10\n\ndocker push docker.domain.com/username/ubuntu:17.10\n\ndocker image rm docker.domain.com/username/ubuntu:17.10\n\ndocker pull docker.domain.com/username/ubuntu:17.10\n\n\n如果我们退出登录，尝试推送镜像。\n\ndocker logout docker.domain.com\n\ndocker push docker.domain.com/username/ubuntu:17.10\n\nno basic auth credentials\n\n\n发现会提示没有登录，不能将镜像推送到私有仓库中。\n\n\n# 注意事项\n\n如果你本机占用了 443 端口，你可以配置 Nginx 代理，这里不再赘述。',normalizedContent:'# docker 私有仓库高级配置\n\n上一节我们搭建了一个具有基础功能的私有仓库，本小节我们来使用 docker compose 搭建一个拥有权限认证、tls 的私有仓库。\n\n新建一个文件夹，以下步骤均在该文件夹中进行。\n\n\n# 准备站点证书\n\n如果你拥有一个域名，国内各大云服务商均提供免费的站点证书。你也可以使用 openssl 自行签发证书。\n\n这里假设我们将要搭建的私有仓库地址为 docker.domain.com，下面我们介绍使用 openssl 自行签发 docker.domain.com 的站点 ssl 证书。\n\n第一步创建 ca 私钥。\n\nopenssl genrsa -out "root-ca.key" 4096\n\n\n第二步利用私钥创建 ca 根证书请求文件。\n\nopenssl req \\\n          -new -key "root-ca.key" \\\n          -out "root-ca.csr" -sha256 \\\n          -subj \'/c=cn/st=shanxi/l=datong/o=your company name/cn=your company name docker registry ca\'\n\n\n> 以上命令中 -subj 参数里的 /c 表示国家，如 cn；/st 表示省；/l 表示城市或者地区；/o 表示组织名；/cn 通用名称。\n\n第三步配置 ca 根证书，新建 root-ca.cnf。\n\n[root_ca]\nbasicconstraints = critical,ca:true,pathlen:1\nkeyusage = critical, nonrepudiation, crlsign, keycertsign\nsubjectkeyidentifier=hash\n\n\n第四步签发根证书。\n\nopenssl x509 -req  -days 3650  -in "root-ca.csr" \\\n               -signkey "root-ca.key" -sha256 -out "root-ca.crt" \\\n               -extfile "root-ca.cnf" -extensions \\\n               root_ca\n\n\n第五步生成站点 ssl 私钥。\n\nopenssl genrsa -out "docker.domain.com.key" 4096\n\n\n第六步使用私钥生成证书请求文件。\n\nopenssl req -new -key "docker.domain.com.key" -out "site.csr" -sha256 \\\n          -subj \'/c=cn/st=shanxi/l=datong/o=your company name/cn=docker.domain.com\'\n\n\n第七步配置证书，新建 site.cnf 文件。\n\n[server]\nauthoritykeyidentifier=keyid,issuer\nbasicconstraints = critical,ca:false\nextendedkeyusage=serverauth\nkeyusage = critical, digitalsignature, keyencipherment\nsubjectaltname = dns:docker.domain.com, ip:127.0.0.1\nsubjectkeyidentifier=hash\n\n\n第八步签署站点 ssl 证书。\n\nopenssl x509 -req -days 750 -in "site.csr" -sha256 \\\n    -ca "root-ca.crt" -cakey "root-ca.key"  -cacreateserial \\\n    -out "docker.domain.com.crt" -extfile "site.cnf" -extensions server\n\n\n这样已经拥有了 docker.domain.com 的网站 ssl 私钥 docker.domain.com.key 和 ssl 证书 docker.domain.com.crt。\n\n新建 ssl 文件夹并将 docker.domain.com.key docker.domain.com.crt 这两个文件移入，删除其他文件。\n\n\n# 配置私有仓库\n\n私有仓库默认的配置文件位于 /etc/docker/registry/config.yml，我们先在本地编辑 config.yml，之后挂载到容器中。\n\nversion: 0.1\nlog:\n  accesslog:\n    disabled: true\n  level: debug\n  formatter: text\n  fields:\n    service: registry\n    environment: staging\nstorage:\n  delete:\n    enabled: true\n  cache:\n    blobdescriptor: inmemory\n  filesystem:\n    rootdirectory: /var/lib/registry\nauth:\n  htpasswd:\n    realm: basic-realm\n    path: /etc/docker/registry/auth/nginx.htpasswd\nhttp:\n  addr: :443\n  host: https://docker.domain.com\n  headers:\n    x-content-type-options: [nosniff]\n  http2:\n    disabled: false\n  tls:\n    certificate: /etc/docker/registry/ssl/docker.domain.com.crt\n    key: /etc/docker/registry/ssl/docker.domain.com.key\nhealth:\n  storagedriver:\n    enabled: true\n    interval: 10s\nthreshold: 3\n\n\n\n# 生成 http 认证文件\n\nmkdir auth\n\ndocker run --rm \\\n    --entrypoint htpasswd \\\n    registry \\\n    -bbn username password > auth/nginx.htpasswd\n\n\n> 将上面的 username password 替换为你自己的用户名和密码。\n\n\n# 编辑 docker-compose.yml\n\nversion: \'3\'\n\nservices:\n  registry:\n    image: registry\n    ports:\n      - "443:443"\n    volumes:\n      - ./:/etc/docker/registry\n      - registry-data:/var/lib/registry\n\nvolumes:\n  registry-data:\n\n\n\n# 修改 hosts\n\n编辑 /etc/hosts\n\ndocker.domain.com 127.0.0.1\n\n\n\n# 启动\n\ndocker-compose up -d\n\n\n这样我们就搭建好了一个具有权限认证、tls 的私有仓库，接下来我们测试其功能是否正常。\n\n\n# 测试私有仓库功能\n\n登录到私有仓库。\n\ndocker login docker.domain.com\n\n\n尝试推送、拉取镜像。\n\ndocker pull ubuntu:17.10\n\ndocker tag ubuntu:17.10 docker.domain.com/username/ubuntu:17.10\n\ndocker push docker.domain.com/username/ubuntu:17.10\n\ndocker image rm docker.domain.com/username/ubuntu:17.10\n\ndocker pull docker.domain.com/username/ubuntu:17.10\n\n\n如果我们退出登录，尝试推送镜像。\n\ndocker logout docker.domain.com\n\ndocker push docker.domain.com/username/ubuntu:17.10\n\nno basic auth credentials\n\n\n发现会提示没有登录，不能将镜像推送到私有仓库中。\n\n\n# 注意事项\n\n如果你本机占用了 443 端口，你可以配置 nginx 代理，这里不再赘述。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 数据管理",frontmatter:{title:"Docker 数据管理",date:"2022-02-09T09:20:42.000Z",permalink:"/pages/bcfae7/"},regularPath:"/17.Docker/07.Docker%20%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/01.Docker%20%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86.html",relativePath:"17.Docker/07.Docker 数据管理/01.Docker 数据管理.md",key:"v-9766780a",path:"/pages/bcfae7/",headersStr:null,content:"# Docker 数据管理\n\n这一章介绍如何在 Docker 内部以及容器之间管理数据，在容器中管理数据主要有两种方式：\n\n * 数据卷（Volumes）\n * 挂载主机目录 (Bind mounts)",normalizedContent:"# docker 数据管理\n\n这一章介绍如何在 docker 内部以及容器之间管理数据，在容器中管理数据主要有两种方式：\n\n * 数据卷（volumes）\n * 挂载主机目录 (bind mounts)",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker 数据卷",frontmatter:{title:"Docker 数据卷",date:"2022-02-09T09:20:42.000Z",permalink:"/pages/5f0882/"},regularPath:"/17.Docker/07.Docker%20%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/02.Docker%20%E6%95%B0%E6%8D%AE%E5%8D%B7.html",relativePath:"17.Docker/07.Docker 数据管理/02.Docker 数据卷.md",key:"v-59a33950",path:"/pages/5f0882/",headers:[{level:2,title:"选择 -v 还是 -–mount 参数",slug:"选择-v-还是-mount-参数",normalizedTitle:"选择 -v 还是 -–mount 参数",charIndex:223},{level:2,title:"创建一个数据卷",slug:"创建一个数据卷",normalizedTitle:"创建一个数据卷",charIndex:333},{level:2,title:"启动一个挂载数据卷的容器",slug:"启动一个挂载数据卷的容器",normalizedTitle:"启动一个挂载数据卷的容器",charIndex:691},{level:2,title:"查看数据卷的具体信息",slug:"查看数据卷的具体信息",normalizedTitle:"查看数据卷的具体信息",charIndex:976},{level:2,title:"删除数据卷",slug:"删除数据卷",normalizedTitle:"删除数据卷",charIndex:1326}],headersStr:"选择 -v 还是 -–mount 参数 创建一个数据卷 启动一个挂载数据卷的容器 查看数据卷的具体信息 删除数据卷",content:'# Docker 数据卷\n\n数据卷 是一个可供一个或多个容器使用的特殊目录，它绕过 UFS，可以提供很多有用的特性：\n\n * 数据卷 可以在容器之间共享和重用\n * 对 数据卷 的修改会立马生效\n * 对 数据卷 的更新，不会影响镜像\n * 数据卷 默认会一直存在，即使容器被删除\n\n> 注意：数据卷 的使用，类似于 Linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会隐藏掉，能显示看的是挂载的 数据卷。\n\n\n# 选择 -v 还是 -–mount 参数\n\nDocker 新用户应该选择 --mount 参数，经验丰富的 Docker 使用者对 -v 或者 --volume 已经很熟悉了，但是推荐使用 --mount 参数。\n\n\n# 创建一个数据卷\n\ndocker volume create my-vol\n\n\n查看所有的 数据卷\n\ndocker volume ls\n\nlocal               my-vol\n\n\n在主机里使用以下命令可以查看指定 数据卷 的信息\n\ndocker volume inspect my-vol\n[\n    {\n        "Driver": "local",\n        "Labels": {},\n        "Mountpoint": "/var/lib/docker/volumes/my-vol/_data",\n        "Name": "my-vol",\n        "Options": {},\n        "Scope": "local"\n    }\n]\n\n\n\n# 启动一个挂载数据卷的容器\n\n在用 docker run 命令的时候，使用 --mount 标记来将 数据卷 挂载到容器里。在一次 docker run 中可以挂载多个 数据卷。\n\n下面创建一个名为 web 的容器，并加载一个 数据卷 到容器的 /webapp 目录。\n\ndocker run -d -P \\\n    --name web \\\n    # -v my-vol:/wepapp \\\n    --mount source=my-vol,target=/webapp \\\n    training/webapp \\\n    python app.py\n\n\n\n# 查看数据卷的具体信息\n\n在主机里使用以下命令可以查看 web 容器的信息\n\ndocker inspect web\n\n\n数据卷 信息在 "Mounts" Key 下面\n\n"Mounts": [\n    {\n        "Type": "volume",\n        "Name": "my-vol",\n        "Source": "/var/lib/docker/volumes/my-vol/_data",\n        "Destination": "/app",\n        "Driver": "local",\n        "Mode": "",\n        "RW": true,\n        "Propagation": ""\n    }\n],\n\n\n\n# 删除数据卷\n\ndocker volume rm my-vol\n\n\n数据卷 是被设计用来持久化数据的，它的生命周期独立于容器，Docker 不会在容器被删除后自动删除 数据卷，并且也不存在垃圾回收这样的机制来处理没有任何容器引用的 数据卷。如果需要在删除容器的同时移除数据卷。可以在删除容器的时候使用 docker rm -v 这个命令。\n\n无主的数据卷可能会占据很多空间，要清理请使用以下命令\n\ndocker volume prune\n',normalizedContent:'# docker 数据卷\n\n数据卷 是一个可供一个或多个容器使用的特殊目录，它绕过 ufs，可以提供很多有用的特性：\n\n * 数据卷 可以在容器之间共享和重用\n * 对 数据卷 的修改会立马生效\n * 对 数据卷 的更新，不会影响镜像\n * 数据卷 默认会一直存在，即使容器被删除\n\n> 注意：数据卷 的使用，类似于 linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会隐藏掉，能显示看的是挂载的 数据卷。\n\n\n# 选择 -v 还是 -–mount 参数\n\ndocker 新用户应该选择 --mount 参数，经验丰富的 docker 使用者对 -v 或者 --volume 已经很熟悉了，但是推荐使用 --mount 参数。\n\n\n# 创建一个数据卷\n\ndocker volume create my-vol\n\n\n查看所有的 数据卷\n\ndocker volume ls\n\nlocal               my-vol\n\n\n在主机里使用以下命令可以查看指定 数据卷 的信息\n\ndocker volume inspect my-vol\n[\n    {\n        "driver": "local",\n        "labels": {},\n        "mountpoint": "/var/lib/docker/volumes/my-vol/_data",\n        "name": "my-vol",\n        "options": {},\n        "scope": "local"\n    }\n]\n\n\n\n# 启动一个挂载数据卷的容器\n\n在用 docker run 命令的时候，使用 --mount 标记来将 数据卷 挂载到容器里。在一次 docker run 中可以挂载多个 数据卷。\n\n下面创建一个名为 web 的容器，并加载一个 数据卷 到容器的 /webapp 目录。\n\ndocker run -d -p \\\n    --name web \\\n    # -v my-vol:/wepapp \\\n    --mount source=my-vol,target=/webapp \\\n    training/webapp \\\n    python app.py\n\n\n\n# 查看数据卷的具体信息\n\n在主机里使用以下命令可以查看 web 容器的信息\n\ndocker inspect web\n\n\n数据卷 信息在 "mounts" key 下面\n\n"mounts": [\n    {\n        "type": "volume",\n        "name": "my-vol",\n        "source": "/var/lib/docker/volumes/my-vol/_data",\n        "destination": "/app",\n        "driver": "local",\n        "mode": "",\n        "rw": true,\n        "propagation": ""\n    }\n],\n\n\n\n# 删除数据卷\n\ndocker volume rm my-vol\n\n\n数据卷 是被设计用来持久化数据的，它的生命周期独立于容器，docker 不会在容器被删除后自动删除 数据卷，并且也不存在垃圾回收这样的机制来处理没有任何容器引用的 数据卷。如果需要在删除容器的同时移除数据卷。可以在删除容器的时候使用 docker rm -v 这个命令。\n\n无主的数据卷可能会占据很多空间，要清理请使用以下命令\n\ndocker volume prune\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"监听主机目录",frontmatter:{title:"监听主机目录",date:"2022-02-09T09:20:42.000Z",permalink:"/pages/565ee4/"},regularPath:"/17.Docker/07.Docker%20%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/03.%E7%9B%91%E5%90%AC%E4%B8%BB%E6%9C%BA%E7%9B%AE%E5%BD%95.html",relativePath:"17.Docker/07.Docker 数据管理/03.监听主机目录.md",key:"v-87ec7d3a",path:"/pages/565ee4/",headers:[{level:2,title:"-v 还是 -–mount 参数",slug:"v-还是-mount-参数",normalizedTitle:"-v 还是 -–mount 参数",charIndex:13},{level:3,title:"挂载一个主机目录作为数据卷",slug:"挂载一个主机目录作为数据卷",normalizedTitle:"挂载一个主机目录作为数据卷",charIndex:120},{level:3,title:"查看数据卷的具体信息",slug:"查看数据卷的具体信息",normalizedTitle:"查看数据卷的具体信息",charIndex:903},{level:3,title:"挂载一个本地主机文件作为数据卷",slug:"挂载一个本地主机文件作为数据卷",normalizedTitle:"挂载一个本地主机文件作为数据卷",charIndex:1194}],headersStr:"-v 还是 -–mount 参数 挂载一个主机目录作为数据卷 查看数据卷的具体信息 挂载一个本地主机文件作为数据卷",content:'# 监听主机目录\n\n\n# -v 还是 -–mount 参数\n\nDocker 新用户应该选择 --mount 参数，经验丰富的 Docker 使用者对 -v 或者 --volume 已经很熟悉了，但是推荐使用 --mount 参数。\n\n\n# 挂载一个主机目录作为数据卷\n\n使用 --mount 标记可以指定挂载一个本地主机的目录到容器中去。\n\ndocker run -d -P \\\n    --name web \\\n    # -v /src/webapp:/opt/webapp \\\n    --mount type=bind,source=/src/webapp,target=/opt/webapp \\\n    training/webapp \\\n    python app.py\n\n\n上面的命令加载主机的 /src/webapp 目录到容器的 /opt/webapp目录。这个功能在进行测试的时候十分方便，比如用户可以放置一些程序到本地目录中，来查看容器是否正常工作。本地目录的路径必须是绝对路径，以前使用 -v 参数时如果本地目录不存在 Docker 会自动为你创建一个文件夹，现在使用 --mount 参数时如果本地目录不存在，Docker 会报错。\n\nDocker 挂载主机目录的默认权限是 读写，用户也可以通过增加 readonly 指定为 只读。\n\ndocker run -d -P \\\n    --name web \\\n    # -v /src/webapp:/opt/webapp:ro \\\n    --mount type=bind,source=/src/webapp,target=/opt/webapp,readonly \\\n    training/webapp \\\n    python app.py\n\n\n加了 readonly 之后，就挂载为 只读 了。如果你在容器内 /opt/webapp 目录新建文件，会显示如下错误\n\n/opt/webapp # touch new.txt\ntouch: new.txt: Read-only file system\n\n\n\n# 查看数据卷的具体信息\n\n在主机里使用以下命令可以查看 web 容器的信息\n\ndocker inspect web\n\n\n挂载主机目录 的配置信息在 "Mounts" Key 下面\n\n"Mounts": [\n    {\n        "Type": "bind",\n        "Source": "/src/webapp",\n        "Destination": "/opt/webapp",\n        "Mode": "",\n        "RW": true,\n        "Propagation": "rprivate"\n    }\n],\n\n\n\n# 挂载一个本地主机文件作为数据卷\n\n--mount 标记也可以从主机挂载单个文件到容器中\n\ndocker run --rm -it \\\n   # -v $HOME/.bash_history:/root/.bash_history \\\n   --mount type=bind,source=$HOME/.bash_history,target=/root/.bash_history \\\n   ubuntu:17.10 \\\n   bash\n\nroot@2affd44b4667:/# history\n1  ls\n2  diskutil list   \n\n\n这样就可以记录在容器输入过的命令了。',normalizedContent:'# 监听主机目录\n\n\n# -v 还是 -–mount 参数\n\ndocker 新用户应该选择 --mount 参数，经验丰富的 docker 使用者对 -v 或者 --volume 已经很熟悉了，但是推荐使用 --mount 参数。\n\n\n# 挂载一个主机目录作为数据卷\n\n使用 --mount 标记可以指定挂载一个本地主机的目录到容器中去。\n\ndocker run -d -p \\\n    --name web \\\n    # -v /src/webapp:/opt/webapp \\\n    --mount type=bind,source=/src/webapp,target=/opt/webapp \\\n    training/webapp \\\n    python app.py\n\n\n上面的命令加载主机的 /src/webapp 目录到容器的 /opt/webapp目录。这个功能在进行测试的时候十分方便，比如用户可以放置一些程序到本地目录中，来查看容器是否正常工作。本地目录的路径必须是绝对路径，以前使用 -v 参数时如果本地目录不存在 docker 会自动为你创建一个文件夹，现在使用 --mount 参数时如果本地目录不存在，docker 会报错。\n\ndocker 挂载主机目录的默认权限是 读写，用户也可以通过增加 readonly 指定为 只读。\n\ndocker run -d -p \\\n    --name web \\\n    # -v /src/webapp:/opt/webapp:ro \\\n    --mount type=bind,source=/src/webapp,target=/opt/webapp,readonly \\\n    training/webapp \\\n    python app.py\n\n\n加了 readonly 之后，就挂载为 只读 了。如果你在容器内 /opt/webapp 目录新建文件，会显示如下错误\n\n/opt/webapp # touch new.txt\ntouch: new.txt: read-only file system\n\n\n\n# 查看数据卷的具体信息\n\n在主机里使用以下命令可以查看 web 容器的信息\n\ndocker inspect web\n\n\n挂载主机目录 的配置信息在 "mounts" key 下面\n\n"mounts": [\n    {\n        "type": "bind",\n        "source": "/src/webapp",\n        "destination": "/opt/webapp",\n        "mode": "",\n        "rw": true,\n        "propagation": "rprivate"\n    }\n],\n\n\n\n# 挂载一个本地主机文件作为数据卷\n\n--mount 标记也可以从主机挂载单个文件到容器中\n\ndocker run --rm -it \\\n   # -v $home/.bash_history:/root/.bash_history \\\n   --mount type=bind,source=$home/.bash_history,target=/root/.bash_history \\\n   ubuntu:17.10 \\\n   bash\n\nroot@2affd44b4667:/# history\n1  ls\n2  diskutil list   \n\n\n这样就可以记录在容器输入过的命令了。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"什么是 Docker Compose",frontmatter:{title:"什么是 Docker Compose",date:"2022-02-09T09:23:30.000Z",permalink:"/pages/e17a65/"},regularPath:"/17.Docker/08.Docker%20%E4%B8%89%E5%89%91%E5%AE%A2%20Compose/01.%E4%BB%80%E4%B9%88%E6%98%AF%20Docker%20Compose.html",relativePath:"17.Docker/08.Docker 三剑客 Compose/01.什么是 Docker Compose.md",key:"v-046481ec",path:"/pages/e17a65/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:88}],headersStr:"概述",content:"# 什么是 Docker Compose\n\nDocker Compose 是 Docker 官方编排（Orchestration）项目之一，负责快速的部署分布式应用。\n\n\n# 概述\n\nCompose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。从功能上看，跟 OpenStack 中的 Heat 十分类似。\n\n其代码目前在 https://github.com/docker/compose 上开源。\n\nCompose 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」，其前身是开源项目 Fig。\n\n通过第一部分中的介绍，我们知道使用一个 Dockerfile 模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。\n\nCompose 恰好满足了这样的需求。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。\n\nCompose 中有两个重要的概念：\n\n * 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n * 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。\n\nCompose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n\nCompose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。",normalizedContent:"# 什么是 docker compose\n\ndocker compose 是 docker 官方编排（orchestration）项目之一，负责快速的部署分布式应用。\n\n\n# 概述\n\ncompose 项目是 docker 官方的开源项目，负责实现对 docker 容器集群的快速编排。从功能上看，跟 openstack 中的 heat 十分类似。\n\n其代码目前在 https://github.com/docker/compose 上开源。\n\ncompose 定位是 「定义和运行多个 docker 容器的应用（defining and running multi-container docker applications）」，其前身是开源项目 fig。\n\n通过第一部分中的介绍，我们知道使用一个 dockerfile 模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 web 项目，除了 web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。\n\ncompose 恰好满足了这样的需求。它允许用户通过一个单独的 docker-compose.yml 模板文件（yaml 格式）来定义一组相关联的应用容器为一个项目（project）。\n\ncompose 中有两个重要的概念：\n\n * 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n * 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。\n\ncompose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n\ncompose 项目由 python 编写，实现上调用了 docker 服务提供的 api 来对容器进行管理。因此，只要所操作的平台支持 docker api，就可以在其上利用 compose 来进行编排管理。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker Compose 安装与卸载",frontmatter:{title:"Docker Compose 安装与卸载",date:"2022-02-09T09:23:30.000Z",permalink:"/pages/f7fbae/"},regularPath:"/17.Docker/08.Docker%20%E4%B8%89%E5%89%91%E5%AE%A2%20Compose/02.Docker%20Compose%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%8D%B8%E8%BD%BD.html",relativePath:"17.Docker/08.Docker 三剑客 Compose/02.Docker Compose 安装与卸载.md",key:"v-098439d0",path:"/pages/f7fbae/",headers:[{level:2,title:"二进制包",slug:"二进制包",normalizedTitle:"二进制包",charIndex:371},{level:2,title:"PIP 安装",slug:"pip-安装",normalizedTitle:"pip 安装",charIndex:755},{level:2,title:"Bash命令补全",slug:"bash命令补全",normalizedTitle:"bash命令补全",charIndex:1212},{level:2,title:"Zsh命令补全",slug:"zsh命令补全",normalizedTitle:"zsh命令补全",charIndex:1838},{level:3,title:"已安装 OH-MY-ZSH",slug:"已安装-oh-my-zsh",normalizedTitle:"已安装 oh-my-zsh",charIndex:1874},{level:3,title:"未安装 OH-MY-ZSH",slug:"未安装-oh-my-zsh",normalizedTitle:"未安装 oh-my-zsh",charIndex:2287},{level:2,title:"容器中执行",slug:"容器中执行",normalizedTitle:"容器中执行",charIndex:2761},{level:2,title:"卸载",slug:"卸载",normalizedTitle:"卸载",charIndex:20}],headersStr:"二进制包 PIP 安装 Bash命令补全 Zsh命令补全 已安装 OH-MY-ZSH 未安装 OH-MY-ZSH 容器中执行 卸载",content:'# Docker Compose 安装与卸载\n\nCompose 支持 Linux、macOS、Windows 10 三大平台。\n\nCompose 可以通过 Python 的包管理工具 pip 进行安装，也可以直接下载编译好的二进制文件使用，甚至能够直接在 Docker 容器中运行。\n\n前两种方式是传统方式，适合本地环境下安装使用；最后一种方式则不破坏系统环境，更适合云计算场景。\n\nDocker for Mac 、Docker for Windows 自带 docker-compose 二进制文件，安装 Docker 之后可以直接使用。\n\ndocker-compose --version\ndocker-compose version 1.29.2, build 8d51620a\n\n\nLinux 系统请使用以下介绍的方法安装。\n\n\n# 二进制包\n\n在 Linux 上的也安装十分简单，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。\n\n例如，在 Linux 64 位系统上直接下载对应的二进制包。\n\n1.运行以下命令以下载Docker Compose的当前稳定版本：\n\nsudo curl -L https://github.com/docker/compose/releases/download/1.29.2/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n\n\n提示\n\n要安装其他版本的Compose，请替换1.29.2 为要使用的Compose版本。\n\n2.将可执行权限应用于二进制文件：\n\nchmod +x /usr/local/bin/docker-compose\n\n\n\n# PIP 安装\n\n注： x86_64 架构的 Linux 建议按照上边的方法下载二进制包进行安装，如果您计算机的架构是 ARM(例如，树莓派)，再使用 pip 安装。\n\n这种方式是将 Compose 当作一个 Python 应用来从 pip 源中安装。\n\n执行安装命令：\n\nsudo pip install -U docker-compose\n\n\n可以看到类似如下输出，说明安装成功。\n\nCollecting docker-compose\n  Downloading docker-compose-1.29.2.tar.gz (149kB): 149kB downloaded\n...\nSuccessfully installed docker-compose cached-property requests texttable websocket-client docker-py dockerpty six enum34 backports.ssl-match-hostname ipaddress\n\n\n\n# Bash命令补全\n\n1.安装bash-completion ubuntu请跳过此步骤\n\nyum install bash-completion -y\n\n\n2.设定docker-compose的补全文件\n\ncurl -L https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose\n\n\n提示\n\n如果网络状态不好，无法直接下载，可以通过浏览器获得，然后使用vi生成此文件也可以。\n\n3.更新配置\n\nsource /usr/share/bash-completion/bash_completion\n\n\n3.结果确认\n\nroot@localhost:~# docker-compose \nbuild    events   kill     ps       rm       stop     version  \nconfig   exec     logs     pull     run      top      \ncreate   help     pause    push     scale    unpause  \ndown     images   port     restart  start    up   \n\n\n\n# Zsh命令补全\n\n确保已安装 oh-my-zsh 在计算机上。\n\n\n# 已安装 OH-MY-ZSH\n\n编辑~/.zshrc文件，在插件列表中添加docker和docker-compose。在以下示例中，...代表您可能已安装的其他Zsh插件。\n\n\n\n\n\n\n\n\n \n\n\n\n...\n# Which plugins would you like to load?\n# Standard plugins can be found in $ZSH/plugins/\n# Custom plugins may be added to $ZSH_CUSTOM/plugins/\n# Example format: plugins=(rails git textmate ruby lighthouse)\n# Add wisely, as too many plugins slow down shell startup.\nplugins=(... docker docker-compose)\n...\n\n\n\n# 未安装 OH-MY-ZSH\n\n 1. 将完成脚本放入您的/path/to/zsh/completion（通常为~/.zsh/completion/）中：\n    \n    mkdir -p ~/.zsh/completion\n    curl -L https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/zsh/_docker-compose > ~/.zsh/completion/_docker-compose\n    \n\n 2. 编辑~/.zshrc文件，添加以下内容:\n    \n    fpath=(~/.zsh/completion $fpath)\n    \n\n 3. 确保compinit已加载或通过添加来完成~/.zshrc：\n    \n    autoload -Uz compinit && compinit -i\n    \n\n 4. 然后重新加载您的shell：\n    \n    exec $SHELL -l\n    \n\n\n# 容器中执行\n\nCompose 既然是一个 Python 应用，自然也可以直接用容器来执行它。\n\ncurl -L https://github.com/docker/compose/releases/download/1.29.2/run.sh > /usr/local/bin/docker-compose\nchmod +x /usr/local/bin/docker-compose\n\n\n实际上，查看下载的 run.sh 脚本内容，如下\n\n#!/bin/sh\n#\n# Run docker-compose in a container\n#\n# This script will attempt to mirror the host paths by using volumes for the\n# following paths:\n#   * $(pwd)\n#   * $(dirname $COMPOSE_FILE) if it\'s set\n#   * $HOME if it\'s set\n#\n# You can add additional volumes (or any docker run options) using\n# the $COMPOSE_OPTIONS environment variable.\n#\n\n\nset -e\n\nVERSION="1.29.2"\nIMAGE="docker/compose:$VERSION"\n\n\n# Setup options for connecting to docker host\nif [ -z "$DOCKER_HOST" ]; then\n    DOCKER_HOST="/var/run/docker.sock"\nfi\nif [ -S "$DOCKER_HOST" ]; then\n    DOCKER_ADDR="-v $DOCKER_HOST:$DOCKER_HOST -e DOCKER_HOST"\nelse\n    DOCKER_ADDR="-e DOCKER_HOST -e DOCKER_TLS_VERIFY -e DOCKER_CERT_PATH"\nfi\n\n\n# Setup volume mounts for compose config and context\nif [ "$(pwd)" != \'/\' ]; then\n    VOLUMES="-v $(pwd):$(pwd)"\nfi\nif [ -n "$COMPOSE_FILE" ]; then\n    COMPOSE_OPTIONS="$COMPOSE_OPTIONS -e COMPOSE_FILE=$COMPOSE_FILE"\n    compose_dir=$(realpath "$(dirname "$COMPOSE_FILE")")\nfi\n# TODO: also check --file argument\nif [ -n "$compose_dir" ]; then\n    VOLUMES="$VOLUMES -v $compose_dir:$compose_dir"\nfi\nif [ -n "$HOME" ]; then\n    VOLUMES="$VOLUMES -v $HOME:$HOME -e HOME" # Pass in HOME to share docker.config and allow ~/-relative paths to work.\nfi\n\n# Only allocate tty if we detect one\nif [ -t 0 ] && [ -t 1 ]; then\n    DOCKER_RUN_OPTIONS="$DOCKER_RUN_OPTIONS -t"\nfi\n\n# Always set -i to support piped and terminal input in run/exec\nDOCKER_RUN_OPTIONS="$DOCKER_RUN_OPTIONS -i"\n\n\n# Handle userns security\nif docker info --format \'{{json .SecurityOptions}}\' 2>/dev/null | grep -q \'name=userns\'; then\n    DOCKER_RUN_OPTIONS="$DOCKER_RUN_OPTIONS --userns=host"\nfi\n\n# shellcheck disable=SC2086\nexec docker run --rm $DOCKER_RUN_OPTIONS $DOCKER_ADDR $COMPOSE_OPTIONS $VOLUMES -w "$(pwd)" $IMAGE "$@"\n\n\n可以看到，它其实是下载了 docker/compose 镜像并运行。\n\n\n# 卸载\n\n如果是二进制包方式安装的，删除二进制文件即可。\n\nsudo rm /usr/local/bin/docker-compose\n\n\n如果是通过 pip 安装的，则执行如下命令即可删除。\n\nsudo pip uninstall docker-compose\n',normalizedContent:'# docker compose 安装与卸载\n\ncompose 支持 linux、macos、windows 10 三大平台。\n\ncompose 可以通过 python 的包管理工具 pip 进行安装，也可以直接下载编译好的二进制文件使用，甚至能够直接在 docker 容器中运行。\n\n前两种方式是传统方式，适合本地环境下安装使用；最后一种方式则不破坏系统环境，更适合云计算场景。\n\ndocker for mac 、docker for windows 自带 docker-compose 二进制文件，安装 docker 之后可以直接使用。\n\ndocker-compose --version\ndocker-compose version 1.29.2, build 8d51620a\n\n\nlinux 系统请使用以下介绍的方法安装。\n\n\n# 二进制包\n\n在 linux 上的也安装十分简单，从 官方 github release 处直接下载编译好的二进制文件即可。\n\n例如，在 linux 64 位系统上直接下载对应的二进制包。\n\n1.运行以下命令以下载docker compose的当前稳定版本：\n\nsudo curl -l https://github.com/docker/compose/releases/download/1.29.2/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n\n\n提示\n\n要安装其他版本的compose，请替换1.29.2 为要使用的compose版本。\n\n2.将可执行权限应用于二进制文件：\n\nchmod +x /usr/local/bin/docker-compose\n\n\n\n# pip 安装\n\n注： x86_64 架构的 linux 建议按照上边的方法下载二进制包进行安装，如果您计算机的架构是 arm(例如，树莓派)，再使用 pip 安装。\n\n这种方式是将 compose 当作一个 python 应用来从 pip 源中安装。\n\n执行安装命令：\n\nsudo pip install -u docker-compose\n\n\n可以看到类似如下输出，说明安装成功。\n\ncollecting docker-compose\n  downloading docker-compose-1.29.2.tar.gz (149kb): 149kb downloaded\n...\nsuccessfully installed docker-compose cached-property requests texttable websocket-client docker-py dockerpty six enum34 backports.ssl-match-hostname ipaddress\n\n\n\n# bash命令补全\n\n1.安装bash-completion ubuntu请跳过此步骤\n\nyum install bash-completion -y\n\n\n2.设定docker-compose的补全文件\n\ncurl -l https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose\n\n\n提示\n\n如果网络状态不好，无法直接下载，可以通过浏览器获得，然后使用vi生成此文件也可以。\n\n3.更新配置\n\nsource /usr/share/bash-completion/bash_completion\n\n\n3.结果确认\n\nroot@localhost:~# docker-compose \nbuild    events   kill     ps       rm       stop     version  \nconfig   exec     logs     pull     run      top      \ncreate   help     pause    push     scale    unpause  \ndown     images   port     restart  start    up   \n\n\n\n# zsh命令补全\n\n确保已安装 oh-my-zsh 在计算机上。\n\n\n# 已安装 oh-my-zsh\n\n编辑~/.zshrc文件，在插件列表中添加docker和docker-compose。在以下示例中，...代表您可能已安装的其他zsh插件。\n\n\n\n\n\n\n\n\n \n\n\n\n...\n# which plugins would you like to load?\n# standard plugins can be found in $zsh/plugins/\n# custom plugins may be added to $zsh_custom/plugins/\n# example format: plugins=(rails git textmate ruby lighthouse)\n# add wisely, as too many plugins slow down shell startup.\nplugins=(... docker docker-compose)\n...\n\n\n\n# 未安装 oh-my-zsh\n\n 1. 将完成脚本放入您的/path/to/zsh/completion（通常为~/.zsh/completion/）中：\n    \n    mkdir -p ~/.zsh/completion\n    curl -l https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/zsh/_docker-compose > ~/.zsh/completion/_docker-compose\n    \n\n 2. 编辑~/.zshrc文件，添加以下内容:\n    \n    fpath=(~/.zsh/completion $fpath)\n    \n\n 3. 确保compinit已加载或通过添加来完成~/.zshrc：\n    \n    autoload -uz compinit && compinit -i\n    \n\n 4. 然后重新加载您的shell：\n    \n    exec $shell -l\n    \n\n\n# 容器中执行\n\ncompose 既然是一个 python 应用，自然也可以直接用容器来执行它。\n\ncurl -l https://github.com/docker/compose/releases/download/1.29.2/run.sh > /usr/local/bin/docker-compose\nchmod +x /usr/local/bin/docker-compose\n\n\n实际上，查看下载的 run.sh 脚本内容，如下\n\n#!/bin/sh\n#\n# run docker-compose in a container\n#\n# this script will attempt to mirror the host paths by using volumes for the\n# following paths:\n#   * $(pwd)\n#   * $(dirname $compose_file) if it\'s set\n#   * $home if it\'s set\n#\n# you can add additional volumes (or any docker run options) using\n# the $compose_options environment variable.\n#\n\n\nset -e\n\nversion="1.29.2"\nimage="docker/compose:$version"\n\n\n# setup options for connecting to docker host\nif [ -z "$docker_host" ]; then\n    docker_host="/var/run/docker.sock"\nfi\nif [ -s "$docker_host" ]; then\n    docker_addr="-v $docker_host:$docker_host -e docker_host"\nelse\n    docker_addr="-e docker_host -e docker_tls_verify -e docker_cert_path"\nfi\n\n\n# setup volume mounts for compose config and context\nif [ "$(pwd)" != \'/\' ]; then\n    volumes="-v $(pwd):$(pwd)"\nfi\nif [ -n "$compose_file" ]; then\n    compose_options="$compose_options -e compose_file=$compose_file"\n    compose_dir=$(realpath "$(dirname "$compose_file")")\nfi\n# todo: also check --file argument\nif [ -n "$compose_dir" ]; then\n    volumes="$volumes -v $compose_dir:$compose_dir"\nfi\nif [ -n "$home" ]; then\n    volumes="$volumes -v $home:$home -e home" # pass in home to share docker.config and allow ~/-relative paths to work.\nfi\n\n# only allocate tty if we detect one\nif [ -t 0 ] && [ -t 1 ]; then\n    docker_run_options="$docker_run_options -t"\nfi\n\n# always set -i to support piped and terminal input in run/exec\ndocker_run_options="$docker_run_options -i"\n\n\n# handle userns security\nif docker info --format \'{{json .securityoptions}}\' 2>/dev/null | grep -q \'name=userns\'; then\n    docker_run_options="$docker_run_options --userns=host"\nfi\n\n# shellcheck disable=sc2086\nexec docker run --rm $docker_run_options $docker_addr $compose_options $volumes -w "$(pwd)" $image "$@"\n\n\n可以看到，它其实是下载了 docker/compose 镜像并运行。\n\n\n# 卸载\n\n如果是二进制包方式安装的，删除二进制文件即可。\n\nsudo rm /usr/local/bin/docker-compose\n\n\n如果是通过 pip 安装的，则执行如下命令即可删除。\n\nsudo pip uninstall docker-compose\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker Compose 入门",frontmatter:{title:"Docker Compose 入门",date:"2022-02-09T09:23:30.000Z",permalink:"/pages/a48959/"},regularPath:"/17.Docker/08.Docker%20%E4%B8%89%E5%89%91%E5%AE%A2%20Compose/03.Docker%20Compose%20%E5%85%A5%E9%97%A8.html",relativePath:"17.Docker/08.Docker 三剑客 Compose/03.Docker Compose 入门.md",key:"v-8eabad82",path:"/pages/a48959/",headers:[{level:2,title:"术语",slug:"术语",normalizedTitle:"术语",charIndex:24},{level:2,title:"场景",slug:"场景",normalizedTitle:"场景",charIndex:165},{level:2,title:"Web 应用",slug:"web-应用",normalizedTitle:"web 应用",charIndex:306},{level:3,title:"第一步: 设置",slug:"第一步-设置",normalizedTitle:"第一步: 设置",charIndex:317},{level:3,title:"第二步: 创建一个 Dockerfile",slug:"第二步-创建一个-dockerfile",normalizedTitle:"第二步: 创建一个 dockerfile",charIndex:995},{level:3,title:"第三步: 在 Compose file中定义服务",slug:"第三步-在-compose-file中定义服务",normalizedTitle:"第三步: 在 compose file中定义服务",charIndex:1383},{level:3,title:"第四步: 用 Compose 构建和运行你的应用程序",slug:"第四步-用-compose-构建和运行你的应用程序",normalizedTitle:"第四步: 用 compose 构建和运行你的应用程序",charIndex:1772},{level:3,title:"第五步: 尝试一些其他命令",slug:"第五步-尝试一些其他命令",normalizedTitle:"第五步: 尝试一些其他命令",charIndex:4486}],headersStr:"术语 场景 Web 应用 第一步: 设置 第二步: 创建一个 Dockerfile 第三步: 在 Compose file中定义服务 第四步: 用 Compose 构建和运行你的应用程序 第五步: 尝试一些其他命令",content:"# Docker Compose 入门\n\n\n# 术语\n\n首先介绍几个术语。\n\n * 服务 (service)：一个应用容器，实际上可以运行多个相同镜像的实例。\n * 项目 (project)：由一组关联的应用容器组成的一个完整业务单元。\n\n可见，一个项目可以由多个服务（容器）关联而成，Compose 面向项目进行管理。\n\n\n# 场景\n\n在这个页面上，您构建了一个运行在 Docker Compose 上的简单 Python web 应用程序。 该应用程序使用了 Flask 框架，并在 Redis 维护了一个点击计数器。 虽然示例使用 Python，但这里演示的概念应该是可以理解的，即使您不熟悉它。\n\n\n# Web 应用\n\n\n# 第一步: 设置\n\n定义应用程序的依赖关系。\n\n1.为项目创建一个目录:\n\nmkdir composetest\ncd composetest\n\n\n2.在项目目录中创建一个名为 app.py 的文件\n\nimport time\n\nimport redis\nfrom flask import Flask\n\napp = Flask(__name__)\ncache = redis.Redis(host='redis', port=6379)\n\n\ndef get_hit_count():\n    retries = 5\n    while True:\n        try:\n            return cache.incr('hits')\n        except redis.exceptions.ConnectionError as exc:\n            if retries == 0:\n                raise exc\n            retries -= 1\n            time.sleep(0.5)\n\n\n@app.route('/')\ndef hello():\n    count = get_hit_count()\n    return 'Hello World! I have been seen {} times.\\n'.format(count)\n\n\n在本例中，redis 是应用程序网络上 redis 容器的主机名。 我们使用 Redis 的默认端口，6379。\n\n\n# 第二步: 创建一个 Dockerfile\n\n在这个步骤中，您将编写一个 Dockerfile 来构建一个 Docker 映像。 映像包含 Python 应用程序所需的所有依赖项，包括 Python 本身。\n\n在项目目录中，创建一个名为 Dockerfile 的文件，并粘贴以下内容:\n\nFROM python:3.7-alpine\nWORKDIR /code\nENV FLASK_APP app.py\nENV FLASK_RUN_HOST 0.0.0.0\nRUN apk add --no-cache gcc musl-dev linux-headers\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"flask\", \"run\"]\n\n\n\n# 第三步: 在 Compose file中定义服务\n\n在项目目录中创建一个名为 docker-compose.yml 的文件，并粘贴以下内容:\n\nversion: '3'\nservices:\n  web:\n    build: .\n    ports:\n      - \"5000:5000\"\n  redis:\n    image: \"redis:alpine\"\n\n\n这个Compose file定义了两个服务: web 和 redis。\n\n# web service\n\n网络服务这个网络服务使用了一个来自 Dockerfile 的工作目录图片。 然后将容器和主机绑定到公开端口5000。 此示例服务使用 Flask web 服务器的默认端口5000。\n\n# Redis service\n\nRedis 服务使用从 Docker Hub 仓库提取的公共 Redis 镜像。\n\n\n# 第四步: 用 Compose 构建和运行你的应用程序\n\n1.在项目目录中，通过运行 docker-compose up 启动应用程序。\n\ndocker-compose up\nCreating composetest_redis_1 ... done\nCreating composetest_web_1   ... done\nAttaching to composetest_redis_1, composetest_web_1\nredis_1  | 1:C 29 Mar 2020 11:20:55.313 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\nredis_1  | 1:C 29 Mar 2020 11:20:55.313 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=1, just started\nredis_1  | 1:C 29 Mar 2020 11:20:55.313 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\nredis_1  | 1:M 29 Mar 2020 11:20:55.314 * Running mode=standalone, port=6379.\nredis_1  | 1:M 29 Mar 2020 11:20:55.314 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\nredis_1  | 1:M 29 Mar 2020 11:20:55.314 # Server initialized\nredis_1  | 1:M 29 Mar 2020 11:20:55.314 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\nredis_1  | 1:M 29 Mar 2020 11:20:55.314 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\nredis_1  | 1:M 29 Mar 2020 11:20:55.314 * Ready to accept connections\nweb_1    |  * Serving Flask app \"app.py\"\nweb_1    |  * Environment: production\nweb_1    |    WARNING: This is a development server. Do not use it in a production deployment.\nweb_1    |    Use a production WSGI server instead.\nweb_1    |  * Debug mode: off\nweb_1    |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n\n\n\nCompose 会拉取 Redis 镜像，为您的代码构建一个镜像，并启动您定义的服务。 在这种情况下，代码在构建时静态地复制到镜像中。\n\n2.在浏览器中输入 http://localhost:5000/ ，查看应用程序的运行情况。\n\n你应该在浏览器中看到这样一条消息:\n\nHello World! I have been seen 1 times.\n\n\n每次刷新页面，计数就会加 1。\n\n3.切换到另一个终端窗口，并键入 docker image ls 列出本地映像。 此时列出的图像应该返回 redis 和 web。\n\ndocker image ls\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\ncomposetest_web     latest              1253fe08c58d        2 hours ago         220MB\npython              3.7-alpine          7fbc871584eb        5 days ago          95.8MB\nredis               alpine              5c5637d8a823        5 days ago          29.8MB\n\n\n4.停止应用程序，要么在第二个终端中从项目目录中运行 docker-compose down，要么在启动应用程序的原始终端中按 ctrl + c。\n\n\n# 第五步: 尝试一些其他命令\n\n如果你想在后台运行你的服务，你可以通过-d 标志(对于“分离”模式)来 docker-compose up 和使用 docker-compose ps 来查看当前运行的是什么:\n\ndocker-compose up -d\nStarting composetest_redis_1 ... done\nStarting composetest_web_1   ... done\n\ndocker-compose ps\n       Name                      Command               State           Ports         \n-------------------------------------------------------------------------------------\ncomposetest_redis_1   docker-entrypoint.sh redis ...   Up      6379/tcp              \ncomposetest_web_1     flask run                        Up      0.0.0.0:5000->5000/tcp\n\n\n如果使用docker-compose up -d命令启动服务时，当需要停止服务，需要执行：\n\ndocker-compose stop\n",normalizedContent:"# docker compose 入门\n\n\n# 术语\n\n首先介绍几个术语。\n\n * 服务 (service)：一个应用容器，实际上可以运行多个相同镜像的实例。\n * 项目 (project)：由一组关联的应用容器组成的一个完整业务单元。\n\n可见，一个项目可以由多个服务（容器）关联而成，compose 面向项目进行管理。\n\n\n# 场景\n\n在这个页面上，您构建了一个运行在 docker compose 上的简单 python web 应用程序。 该应用程序使用了 flask 框架，并在 redis 维护了一个点击计数器。 虽然示例使用 python，但这里演示的概念应该是可以理解的，即使您不熟悉它。\n\n\n# web 应用\n\n\n# 第一步: 设置\n\n定义应用程序的依赖关系。\n\n1.为项目创建一个目录:\n\nmkdir composetest\ncd composetest\n\n\n2.在项目目录中创建一个名为 app.py 的文件\n\nimport time\n\nimport redis\nfrom flask import flask\n\napp = flask(__name__)\ncache = redis.redis(host='redis', port=6379)\n\n\ndef get_hit_count():\n    retries = 5\n    while true:\n        try:\n            return cache.incr('hits')\n        except redis.exceptions.connectionerror as exc:\n            if retries == 0:\n                raise exc\n            retries -= 1\n            time.sleep(0.5)\n\n\n@app.route('/')\ndef hello():\n    count = get_hit_count()\n    return 'hello world! i have been seen {} times.\\n'.format(count)\n\n\n在本例中，redis 是应用程序网络上 redis 容器的主机名。 我们使用 redis 的默认端口，6379。\n\n\n# 第二步: 创建一个 dockerfile\n\n在这个步骤中，您将编写一个 dockerfile 来构建一个 docker 映像。 映像包含 python 应用程序所需的所有依赖项，包括 python 本身。\n\n在项目目录中，创建一个名为 dockerfile 的文件，并粘贴以下内容:\n\nfrom python:3.7-alpine\nworkdir /code\nenv flask_app app.py\nenv flask_run_host 0.0.0.0\nrun apk add --no-cache gcc musl-dev linux-headers\ncopy requirements.txt requirements.txt\nrun pip install -r requirements.txt\ncopy . .\ncmd [\"flask\", \"run\"]\n\n\n\n# 第三步: 在 compose file中定义服务\n\n在项目目录中创建一个名为 docker-compose.yml 的文件，并粘贴以下内容:\n\nversion: '3'\nservices:\n  web:\n    build: .\n    ports:\n      - \"5000:5000\"\n  redis:\n    image: \"redis:alpine\"\n\n\n这个compose file定义了两个服务: web 和 redis。\n\n# web service\n\n网络服务这个网络服务使用了一个来自 dockerfile 的工作目录图片。 然后将容器和主机绑定到公开端口5000。 此示例服务使用 flask web 服务器的默认端口5000。\n\n# redis service\n\nredis 服务使用从 docker hub 仓库提取的公共 redis 镜像。\n\n\n# 第四步: 用 compose 构建和运行你的应用程序\n\n1.在项目目录中，通过运行 docker-compose up 启动应用程序。\n\ndocker-compose up\ncreating composetest_redis_1 ... done\ncreating composetest_web_1   ... done\nattaching to composetest_redis_1, composetest_web_1\nredis_1  | 1:c 29 mar 2020 11:20:55.313 # oo0ooo0ooo0oo redis is starting oo0ooo0ooo0oo\nredis_1  | 1:c 29 mar 2020 11:20:55.313 # redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=1, just started\nredis_1  | 1:c 29 mar 2020 11:20:55.313 # warning: no config file specified, using the default config. in order to specify a config file use redis-server /path/to/redis.conf\nredis_1  | 1:m 29 mar 2020 11:20:55.314 * running mode=standalone, port=6379.\nredis_1  | 1:m 29 mar 2020 11:20:55.314 # warning: the tcp backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\nredis_1  | 1:m 29 mar 2020 11:20:55.314 # server initialized\nredis_1  | 1:m 29 mar 2020 11:20:55.314 # warning overcommit_memory is set to 0! background save may fail under low memory condition. to fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\nredis_1  | 1:m 29 mar 2020 11:20:55.314 # warning you have transparent huge pages (thp) support enabled in your kernel. this will create latency and memory usage issues with redis. to fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. redis must be restarted after thp is disabled.\nredis_1  | 1:m 29 mar 2020 11:20:55.314 * ready to accept connections\nweb_1    |  * serving flask app \"app.py\"\nweb_1    |  * environment: production\nweb_1    |    warning: this is a development server. do not use it in a production deployment.\nweb_1    |    use a production wsgi server instead.\nweb_1    |  * debug mode: off\nweb_1    |  * running on http://0.0.0.0:5000/ (press ctrl+c to quit)\n\n\n\ncompose 会拉取 redis 镜像，为您的代码构建一个镜像，并启动您定义的服务。 在这种情况下，代码在构建时静态地复制到镜像中。\n\n2.在浏览器中输入 http://localhost:5000/ ，查看应用程序的运行情况。\n\n你应该在浏览器中看到这样一条消息:\n\nhello world! i have been seen 1 times.\n\n\n每次刷新页面，计数就会加 1。\n\n3.切换到另一个终端窗口，并键入 docker image ls 列出本地映像。 此时列出的图像应该返回 redis 和 web。\n\ndocker image ls\nrepository          tag                 image id            created             size\ncomposetest_web     latest              1253fe08c58d        2 hours ago         220mb\npython              3.7-alpine          7fbc871584eb        5 days ago          95.8mb\nredis               alpine              5c5637d8a823        5 days ago          29.8mb\n\n\n4.停止应用程序，要么在第二个终端中从项目目录中运行 docker-compose down，要么在启动应用程序的原始终端中按 ctrl + c。\n\n\n# 第五步: 尝试一些其他命令\n\n如果你想在后台运行你的服务，你可以通过-d 标志(对于“分离”模式)来 docker-compose up 和使用 docker-compose ps 来查看当前运行的是什么:\n\ndocker-compose up -d\nstarting composetest_redis_1 ... done\nstarting composetest_web_1   ... done\n\ndocker-compose ps\n       name                      command               state           ports         \n-------------------------------------------------------------------------------------\ncomposetest_redis_1   docker-entrypoint.sh redis ...   up      6379/tcp              \ncomposetest_web_1     flask run                        up      0.0.0.0:5000->5000/tcp\n\n\n如果使用docker-compose up -d命令启动服务时，当需要停止服务，需要执行：\n\ndocker-compose stop\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker Compose 命令说明",frontmatter:{title:"Docker Compose 命令说明",date:"2022-02-09T09:23:30.000Z",permalink:"/pages/cddd90/"},regularPath:"/17.Docker/08.Docker%20%E4%B8%89%E5%89%91%E5%AE%A2%20Compose/04.Docker%20Compose%20%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E.html",relativePath:"17.Docker/08.Docker 三剑客 Compose/04.Docker Compose 命令说明.md",key:"v-262a1fde",path:"/pages/cddd90/",headers:[{level:2,title:"命令对象与格式",slug:"命令对象与格式",normalizedTitle:"命令对象与格式",charIndex:26},{level:2,title:"命令选项",slug:"命令选项",normalizedTitle:"命令选项",charIndex:303},{level:2,title:"命令使用说明",slug:"命令使用说明",normalizedTitle:"命令使用说明",charIndex:572},{level:3,title:"build",slug:"build",normalizedTitle:"build",charIndex:583},{level:3,title:"config",slug:"config",normalizedTitle:"config",charIndex:873},{level:3,title:"down",slug:"down",normalizedTitle:"down",charIndex:927},{level:3,title:"exec",slug:"exec",normalizedTitle:"exec",charIndex:963},{level:3,title:"help",slug:"help",normalizedTitle:"help",charIndex:155},{level:3,title:"images",slug:"images",normalizedTitle:"images",charIndex:1003},{level:3,title:"kill",slug:"kill",normalizedTitle:"kill",charIndex:1036},{level:3,title:"logs",slug:"logs",normalizedTitle:"logs",charIndex:1195},{level:3,title:"pause",slug:"pause",normalizedTitle:"pause",charIndex:1346},{level:3,title:"port",slug:"port",normalizedTitle:"port",charIndex:1407},{level:3,title:"ps",slug:"ps",normalizedTitle:"ps",charIndex:1591},{level:3,title:"pull",slug:"pull",normalizedTitle:"pull",charIndex:840},{level:3,title:"push",slug:"push",normalizedTitle:"push",charIndex:1802},{level:3,title:"restart",slug:"restart",normalizedTitle:"restart",charIndex:1836},{level:3,title:"rm",slug:"rm",normalizedTitle:"rm",charIndex:776},{level:3,title:"run",slug:"run",normalizedTitle:"run",charIndex:2143},{level:3,title:"scale",slug:"scale",normalizedTitle:"scale",charIndex:2917},{level:3,title:"start",slug:"start",normalizedTitle:"start",charIndex:1838},{level:3,title:"stop",slug:"stop",normalizedTitle:"stop",charIndex:2054},{level:3,title:"top",slug:"top",normalizedTitle:"top",charIndex:2055},{level:3,title:"unpause",slug:"unpause",normalizedTitle:"unpause",charIndex:3447},{level:3,title:"up",slug:"up",normalizedTitle:"up",charIndex:941},{level:3,title:"version",slug:"version",normalizedTitle:"version",charIndex:551}],headersStr:"命令对象与格式 命令选项 命令使用说明 build config down exec help images kill logs pause port ps pull push restart rm run scale start stop top unpause up version",content:'# Docker Compose 命令说明\n\n\n# 命令对象与格式\n\n对于 Compose 来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到命令影响。\n\n执行 docker-compose [COMMAND] --help 或者 docker-compose help [COMMAND] 可以查看具体某个命令的使用格式。\n\ndocker-compose 命令的基本的使用格式是\n\ndocker-compose [-f=<arg>...] [options] [COMMAND] [ARGS...]\n\n\n\n# 命令选项\n\n * -f, --file FILE 指定使用的 Compose 模板文件，默认为 docker-compose.yml，可以多次指定。\n * -p, --project-name NAME 指定项目名称，默认将使用所在目录名称作为项目名。\n * --x-networking 使用 Docker 的可拔插网络后端特性\n * --x-network-driver DRIVER 指定网络后端的驱动，默认为 bridge\n * --verbose 输出更多调试信息。\n * -v, --version 打印版本并退出。\n\n\n# 命令使用说明\n\n\n# build\n\n格式为 docker-compose build [options] [SERVICE...]。\n\n构建（重新构建）项目中的服务容器。\n\n服务容器一旦构建后，将会带上一个标记名，例如对于 web 项目中的一个 db 容器，可能是 web_db。\n\n可以随时在项目目录下运行 docker-compose build 来重新构建服务。\n\n选项包括：\n\n * --force-rm 删除构建过程中的临时容器。\n * --no-cache 构建镜像过程中不使用 cache（这将加长构建过程）。\n * --pull 始终尝试通过 pull 来获取更新版本的镜像。\n\n\n# config\n\n验证 Compose 文件格式是否正确，若正确则显示配置，若格式错误显示错误原因。\n\n\n# down\n\n此命令将会停止 up 命令所启动的容器，并移除网络\n\n\n# exec\n\n进入指定的容器。\n\n\n# help\n\n获得一个命令的帮助。\n\n\n# images\n\n列出 Compose 文件中包含的镜像。\n\n\n# kill\n\n格式为 docker-compose kill [options] [SERVICE...]。\n\n通过发送 SIGKILL 信号来强制停止服务容器。\n\n支持通过 -s 参数来指定发送的信号，例如通过如下指令发送 SIGINT 信号。\n\ndocker-compose kill -s SIGINT\n\n\n\n# logs\n\n格式为 docker-compose logs [options] [SERVICE...]。\n\n查看服务容器的输出。默认情况下，docker-compose 将对不同的服务输出使用不同的颜色来区分。可以通过 --no-color 来关闭颜色。\n\n该命令在调试问题的时候十分有用。\n\n\n# pause\n\n格式为 docker-compose pause [SERVICE...]。\n\n暂停一个服务容器。\n\n\n# port\n\n格式为 docker-compose port [options] SERVICE PRIVATE_PORT。\n\n打印某个容器端口所映射的公共端口。\n\n选项：\n\n * --protocol=proto 指定端口协议，tcp（默认值）或者 udp。\n * --index=index 如果同一服务存在多个容器，指定命令对象容器的序号（默认为 1）。\n\n\n# ps\n\n格式为 docker-compose ps [options] [SERVICE...]。\n\n列出项目中目前的所有容器。\n\n选项：\n\n * -q 只打印容器的 ID 信息。\n\n\n# pull\n\n格式为 docker-compose pull [options] [SERVICE...]。\n\n拉取服务依赖的镜像。\n\n选项：\n\n * --ignore-pull-failures 忽略拉取镜像过程中的错误。\n\n\n# push\n\n推送服务依赖的镜像到 Docker 镜像仓库。\n\n\n# restart\n\n格式为 docker-compose restart [options] [SERVICE...]。\n\n重启项目中的服务。\n\n选项：\n\n * -t, --timeout TIMEOUT 指定重启前停止容器的超时（默认为 10 秒）。\n\n\n# rm\n\n格式为 docker-compose rm [options] [SERVICE...]。\n\n删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。\n\n选项：\n\n * -f, --force 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。\n * -v 删除容器所挂载的数据卷。\n\n\n# run\n\n格式为 docker-compose run [options] [-p PORT...] [-e KEY=VAL...] SERVICE [COMMAND] [ARGS...]。\n\n在指定服务上执行一个命令。\n\n例如：\n\ndocker-compose run ubuntu ping docker.com\n\n\n将会启动一个 ubuntu 服务容器，并执行 ping docker.com 命令。\n\n默认情况下，如果存在关联，则所有关联的服务将会自动被启动，除非这些服务已经在运行中。\n\n该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照配置自动创建。\n\n两个不同点：\n\n * 给定命令将会覆盖原有的自动运行命令；\n * 不会自动创建端口，以避免冲突。\n\n如果不希望自动启动关联的容器，可以使用 --no-deps 选项，例如\n\ndocker-compose run --no-deps web python manage.py shell\n\n\n将不会启动 web 容器所关联的其它容器。\n\n选项：\n\n * -d 后台运行容器。\n * --name NAME 为容器指定一个名字。\n * --entrypoint CMD 覆盖默认的容器启动指令。\n * -e KEY=VAL 设置环境变量值，可多次使用选项来设置多个环境变量。\n * -u, --user="" 指定运行容器的用户名或者 uid。\n * --no-deps 不自动启动关联的服务容器。\n * --rm 运行命令后自动删除容器，d 模式下将忽略。\n * -p, --publish=[] 映射容器端口到本地主机。\n * --service-ports 配置服务端口并映射到本地主机。\n * -T 不分配伪 tty，意味着依赖 tty 的指令将无法运行。\n\n\n# scale\n\n格式为 docker-compose scale [options] [SERVICE=NUM...]。\n\n设置指定服务运行的容器个数。\n\n通过 service=num 的参数来设置数量。例如：\n\ndocker-compose scale web=3 db=2\n\n\n将启动 3 个容器运行 web 服务，2 个容器运行 db 服务。\n\n一般的，当指定数目多于该服务当前实际运行容器，将新创建并启动容器；反之，将停止容器。\n\n选项：\n\n * -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。\n\n\n# start\n\n格式为 docker-compose start [SERVICE...]。\n\n启动已经存在的服务容器。\n\n\n# stop\n\n格式为 docker-compose stop [options] [SERVICE...]。\n\n停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器。\n\n选项：\n\n * -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。\n\n\n# top\n\n查看各个服务容器内运行的进程。\n\n\n# unpause\n\n格式为 docker-compose unpause [SERVICE...]。\n\n恢复处于暂停状态中的服务。\n\n\n# up\n\n格式为 docker-compose up [options] [SERVICE...]。\n\n该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。\n\n链接的服务都将会被自动启动，除非已经处于运行状态。\n\n可以说，大部分时候都可以直接通过该命令来启动一个项目。\n\n默认情况，docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。\n\n当通过 Ctrl-C 停止命令时，所有容器将会停止。\n\n如果使用 docker-compose up -d，将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。\n\n默认情况，如果服务容器已经存在，docker-compose up 将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容。如果用户不希望容器被停止并重新创建，可以使用 docker-compose up --no-recreate。这样将只会启动处于停止状态的容器，而忽略已经运行的服务。如果用户只想重新部署某个服务，可以使用 docker-compose up --no-deps -d 来重新创建服务并后台停止旧服务，启动新服务，并不会影响到其所依赖的服务。\n\n选项：\n\n * -d 在后台运行服务容器。\n * --no-color 不使用颜色来区分不同的服务的控制台输出。\n * --no-deps 不启动服务所链接的容器。\n * --force-recreate 强制重新创建容器，不能与 --no-recreate 同时使用。\n * --no-recreate 如果容器已经存在了，则不重新创建，不能与 --force-recreate 同时使用。\n * --no-build 不自动构建缺失的服务镜像。\n * -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。\n\n\n# version\n\n格式为 docker-compose version。\n\n打印版本信息。',normalizedContent:'# docker compose 命令说明\n\n\n# 命令对象与格式\n\n对于 compose 来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到命令影响。\n\n执行 docker-compose [command] --help 或者 docker-compose help [command] 可以查看具体某个命令的使用格式。\n\ndocker-compose 命令的基本的使用格式是\n\ndocker-compose [-f=<arg>...] [options] [command] [args...]\n\n\n\n# 命令选项\n\n * -f, --file file 指定使用的 compose 模板文件，默认为 docker-compose.yml，可以多次指定。\n * -p, --project-name name 指定项目名称，默认将使用所在目录名称作为项目名。\n * --x-networking 使用 docker 的可拔插网络后端特性\n * --x-network-driver driver 指定网络后端的驱动，默认为 bridge\n * --verbose 输出更多调试信息。\n * -v, --version 打印版本并退出。\n\n\n# 命令使用说明\n\n\n# build\n\n格式为 docker-compose build [options] [service...]。\n\n构建（重新构建）项目中的服务容器。\n\n服务容器一旦构建后，将会带上一个标记名，例如对于 web 项目中的一个 db 容器，可能是 web_db。\n\n可以随时在项目目录下运行 docker-compose build 来重新构建服务。\n\n选项包括：\n\n * --force-rm 删除构建过程中的临时容器。\n * --no-cache 构建镜像过程中不使用 cache（这将加长构建过程）。\n * --pull 始终尝试通过 pull 来获取更新版本的镜像。\n\n\n# config\n\n验证 compose 文件格式是否正确，若正确则显示配置，若格式错误显示错误原因。\n\n\n# down\n\n此命令将会停止 up 命令所启动的容器，并移除网络\n\n\n# exec\n\n进入指定的容器。\n\n\n# help\n\n获得一个命令的帮助。\n\n\n# images\n\n列出 compose 文件中包含的镜像。\n\n\n# kill\n\n格式为 docker-compose kill [options] [service...]。\n\n通过发送 sigkill 信号来强制停止服务容器。\n\n支持通过 -s 参数来指定发送的信号，例如通过如下指令发送 sigint 信号。\n\ndocker-compose kill -s sigint\n\n\n\n# logs\n\n格式为 docker-compose logs [options] [service...]。\n\n查看服务容器的输出。默认情况下，docker-compose 将对不同的服务输出使用不同的颜色来区分。可以通过 --no-color 来关闭颜色。\n\n该命令在调试问题的时候十分有用。\n\n\n# pause\n\n格式为 docker-compose pause [service...]。\n\n暂停一个服务容器。\n\n\n# port\n\n格式为 docker-compose port [options] service private_port。\n\n打印某个容器端口所映射的公共端口。\n\n选项：\n\n * --protocol=proto 指定端口协议，tcp（默认值）或者 udp。\n * --index=index 如果同一服务存在多个容器，指定命令对象容器的序号（默认为 1）。\n\n\n# ps\n\n格式为 docker-compose ps [options] [service...]。\n\n列出项目中目前的所有容器。\n\n选项：\n\n * -q 只打印容器的 id 信息。\n\n\n# pull\n\n格式为 docker-compose pull [options] [service...]。\n\n拉取服务依赖的镜像。\n\n选项：\n\n * --ignore-pull-failures 忽略拉取镜像过程中的错误。\n\n\n# push\n\n推送服务依赖的镜像到 docker 镜像仓库。\n\n\n# restart\n\n格式为 docker-compose restart [options] [service...]。\n\n重启项目中的服务。\n\n选项：\n\n * -t, --timeout timeout 指定重启前停止容器的超时（默认为 10 秒）。\n\n\n# rm\n\n格式为 docker-compose rm [options] [service...]。\n\n删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。\n\n选项：\n\n * -f, --force 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。\n * -v 删除容器所挂载的数据卷。\n\n\n# run\n\n格式为 docker-compose run [options] [-p port...] [-e key=val...] service [command] [args...]。\n\n在指定服务上执行一个命令。\n\n例如：\n\ndocker-compose run ubuntu ping docker.com\n\n\n将会启动一个 ubuntu 服务容器，并执行 ping docker.com 命令。\n\n默认情况下，如果存在关联，则所有关联的服务将会自动被启动，除非这些服务已经在运行中。\n\n该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照配置自动创建。\n\n两个不同点：\n\n * 给定命令将会覆盖原有的自动运行命令；\n * 不会自动创建端口，以避免冲突。\n\n如果不希望自动启动关联的容器，可以使用 --no-deps 选项，例如\n\ndocker-compose run --no-deps web python manage.py shell\n\n\n将不会启动 web 容器所关联的其它容器。\n\n选项：\n\n * -d 后台运行容器。\n * --name name 为容器指定一个名字。\n * --entrypoint cmd 覆盖默认的容器启动指令。\n * -e key=val 设置环境变量值，可多次使用选项来设置多个环境变量。\n * -u, --user="" 指定运行容器的用户名或者 uid。\n * --no-deps 不自动启动关联的服务容器。\n * --rm 运行命令后自动删除容器，d 模式下将忽略。\n * -p, --publish=[] 映射容器端口到本地主机。\n * --service-ports 配置服务端口并映射到本地主机。\n * -t 不分配伪 tty，意味着依赖 tty 的指令将无法运行。\n\n\n# scale\n\n格式为 docker-compose scale [options] [service=num...]。\n\n设置指定服务运行的容器个数。\n\n通过 service=num 的参数来设置数量。例如：\n\ndocker-compose scale web=3 db=2\n\n\n将启动 3 个容器运行 web 服务，2 个容器运行 db 服务。\n\n一般的，当指定数目多于该服务当前实际运行容器，将新创建并启动容器；反之，将停止容器。\n\n选项：\n\n * -t, --timeout timeout 停止容器时候的超时（默认为 10 秒）。\n\n\n# start\n\n格式为 docker-compose start [service...]。\n\n启动已经存在的服务容器。\n\n\n# stop\n\n格式为 docker-compose stop [options] [service...]。\n\n停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器。\n\n选项：\n\n * -t, --timeout timeout 停止容器时候的超时（默认为 10 秒）。\n\n\n# top\n\n查看各个服务容器内运行的进程。\n\n\n# unpause\n\n格式为 docker-compose unpause [service...]。\n\n恢复处于暂停状态中的服务。\n\n\n# up\n\n格式为 docker-compose up [options] [service...]。\n\n该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。\n\n链接的服务都将会被自动启动，除非已经处于运行状态。\n\n可以说，大部分时候都可以直接通过该命令来启动一个项目。\n\n默认情况，docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。\n\n当通过 ctrl-c 停止命令时，所有容器将会停止。\n\n如果使用 docker-compose up -d，将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。\n\n默认情况，如果服务容器已经存在，docker-compose up 将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容。如果用户不希望容器被停止并重新创建，可以使用 docker-compose up --no-recreate。这样将只会启动处于停止状态的容器，而忽略已经运行的服务。如果用户只想重新部署某个服务，可以使用 docker-compose up --no-deps -d 来重新创建服务并后台停止旧服务，启动新服务，并不会影响到其所依赖的服务。\n\n选项：\n\n * -d 在后台运行服务容器。\n * --no-color 不使用颜色来区分不同的服务的控制台输出。\n * --no-deps 不启动服务所链接的容器。\n * --force-recreate 强制重新创建容器，不能与 --no-recreate 同时使用。\n * --no-recreate 如果容器已经存在了，则不重新创建，不能与 --force-recreate 同时使用。\n * --no-build 不自动构建缺失的服务镜像。\n * -t, --timeout timeout 停止容器时候的超时（默认为 10 秒）。\n\n\n# version\n\n格式为 docker-compose version。\n\n打印版本信息。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker Compose 模板文件",frontmatter:{title:"Docker Compose 模板文件",date:"2022-02-09T09:23:30.000Z",permalink:"/pages/5e9e65/"},regularPath:"/17.Docker/08.Docker%20%E4%B8%89%E5%89%91%E5%AE%A2%20Compose/05.Docker%20Compose%20%E6%A8%A1%E6%9D%BF%E6%96%87%E4%BB%B6.html",relativePath:"17.Docker/08.Docker 三剑客 Compose/05.Docker Compose 模板文件.md",key:"v-0c749932",path:"/pages/5e9e65/",headers:[{level:2,title:"build",slug:"build",normalizedTitle:"build",charIndex:287},{level:2,title:"cap_add, cap_drop",slug:"cap-add-cap-drop",normalizedTitle:"cap_add, cap_drop",charIndex:948},{level:2,title:"command",slug:"command",normalizedTitle:"command",charIndex:1082},{level:2,title:"configs",slug:"configs",normalizedTitle:"configs",charIndex:1141},{level:2,title:"cgroup_parent",slug:"cgroup-parent",normalizedTitle:"cgroup_parent",charIndex:1169},{level:2,title:"container_name",slug:"container-name",normalizedTitle:"container_name",charIndex:1276},{level:2,title:"deploy",slug:"deploy",normalizedTitle:"deploy",charIndex:1426},{level:2,title:"devices",slug:"devices",normalizedTitle:"devices",charIndex:1453},{level:2,title:"depends_on",slug:"depends-on",normalizedTitle:"depends_on",charIndex:1519},{level:2,title:"dns",slug:"dns",normalizedTitle:"dns",charIndex:1757},{level:2,title:"dns_search",slug:"dns-search",normalizedTitle:"dns_search",charIndex:1848},{level:2,title:"tmpfs",slug:"tmpfs",normalizedTitle:"tmpfs",charIndex:1979},{level:2,title:"env_file",slug:"env-file",normalizedTitle:"env_file",charIndex:2049},{level:2,title:"environment",slug:"environment",normalizedTitle:"environment",charIndex:2171},{level:2,title:"expose",slug:"expose",normalizedTitle:"expose",charIndex:2765},{level:2,title:"external_links",slug:"external-links",normalizedTitle:"external_links",charIndex:2846},{level:2,title:"[#](https://wowox.com/zh/docs-docker/Docker-Compose-模板文件.html#extra-hosts)extra_hosts",slug:"extra-hosts",normalizedTitle:'<a href="https://wowox.com/zh/docs-docker/docker-compose-%e6%a8%a1%e6%9d%bf%e6%96%87%e4%bb%b6.html#extra-hosts" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>extra_hosts',charIndex:null},{level:2,title:"healthcheck",slug:"healthcheck",normalizedTitle:"healthcheck",charIndex:3218},{level:2,title:"image",slug:"image",normalizedTitle:"image",charIndex:183},{level:2,title:"labels",slug:"labels",normalizedTitle:"labels",charIndex:3478},{level:2,title:"links",slug:"links",normalizedTitle:"links",charIndex:2855},{level:2,title:"logging",slug:"logging",normalizedTitle:"logging",charIndex:3722},{level:2,title:"network_mode",slug:"network-mode",normalizedTitle:"network_mode",charIndex:3965},{level:2,title:"networks",slug:"networks",normalizedTitle:"networks",charIndex:4177},{level:2,title:"pid",slug:"pid",normalizedTitle:"pid",charIndex:4343},{level:2,title:"ports",slug:"ports",normalizedTitle:"ports",charIndex:207},{level:2,title:"secrets",slug:"secrets",normalizedTitle:"secrets",charIndex:2273},{level:2,title:"security_opt",slug:"security-opt",normalizedTitle:"security_opt",charIndex:5013},{level:2,title:"stop_signal",slug:"stop-signal",normalizedTitle:"stop_signal",charIndex:5144},{level:2,title:"sysctls",slug:"sysctls",normalizedTitle:"sysctls",charIndex:5222},{level:2,title:"ulimits",slug:"ulimits",normalizedTitle:"ulimits",charIndex:5380},{level:2,title:"volumes",slug:"volumes",normalizedTitle:"volumes",charIndex:234},{level:2,title:"其它指令",slug:"其它指令",normalizedTitle:"其它指令",charIndex:5750},{level:2,title:"读取变量",slug:"读取变量",normalizedTitle:"读取变量",charIndex:6387}],headersStr:"build cap_add, cap_drop command configs cgroup_parent container_name deploy devices depends_on dns dns_search tmpfs env_file environment expose external_links [#](https://wowox.com/zh/docs-docker/Docker-Compose-模板文件.html#extra-hosts)extra_hosts healthcheck image labels links logging network_mode networks pid ports secrets security_opt stop_signal sysctls ulimits volumes 其它指令 读取变量",content:'# Docker Compose 模板文件\n\n模板文件是使用 Compose 的核心，涉及到的指令关键字也比较多。但大家不用担心，这里面大部分指令跟 docker run 相关参数的含义都是类似的。\n\n默认的模板文件名称为 docker-compose.yml，格式为 YAML 格式。\n\nversion: "3"\n\nservices:\n  webapp:\n    image: examples/web\n    ports:\n      - "80:80"\n    volumes:\n      - "/data"\n\n\n注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 Dockerfile）等来自动构建生成镜像。\n\n如果使用 build 指令，在 Dockerfile 中设置的选项(例如：CMD, EXPOSE, VOLUME, ENV 等) 将会自动被获取，无需在 docker-compose.yml 中再次设置。\n\n下面分别介绍各个指令的用法。\n\n\n# build\n\n指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。 Compose 将会利用它自动构建这个镜像，然后使用这个镜像。\n\nversion: \'3\'\nservices:\n\n  webapp:\n    build: ./dir\n\n\n你也可以使用 context 指令指定 Dockerfile 所在文件夹的路径。\n\n使用 dockerfile 指令指定 Dockerfile 文件名。\n\n使用 arg 指令指定构建镜像时的变量。\n\nversion: \'3\'\nservices:\n\n  webapp:\n    build:\n      context: ./dir\n      dockerfile: Dockerfile-alternate\n      args:\n        buildno: 1\n\n\n使用 cache_from 指定构建镜像的缓存\n\nbuild:\n  context: .\n  cache_from:\n    - alpine:latest\n    - corp/web_app:3.14\n\n\n\n# cap_add, cap_drop\n\n指定容器的内核能力（capacity）分配。\n\n例如，让容器拥有所有能力可以指定为：\n\ncap_add:\n  - ALL\n\n\n去掉 NET_ADMIN 能力可以指定为：\n\ncap_drop:\n  - NET_ADMIN\n\n\n\n# command\n\n覆盖容器启动后默认执行的命令。\n\ncommand: echo "hello world"\n\n\n\n# configs\n\n仅用于 Swarm mode\n\n\n# cgroup_parent\n\n指定父 cgroup 组，意味着将继承该组的资源限制。\n\n例如，创建了一个 cgroup 组名称为 cgroups_1。\n\ncgroup_parent: cgroups_1\n\n\n\n# container_name\n\n指定容器名称。默认将会使用 项目名称_服务名称_序号 这样的格式。\n\ncontainer_name: docker-web-container\n\n\n注意\n\n指定容器名称后，该服务将无法进行扩展（scale），因为 Docker 不允许多个容器具有相同的名称。\n\n\n# deploy\n\n仅用于 Swarm mode\n\n\n# devices\n\n指定设备映射关系。\n\ndevices:\n  - "/dev/ttyUSB1:/dev/ttyUSB0"\n\n\n\n# depends_on\n\n解决容器的依赖、启动先后的问题。以下例子中会先启动 redis db 再启动 web\n\nversion: \'3\'\n\nservices:\n  web:\n    build: .\n    depends_on:\n      - db\n      - redis\n\n  redis:\n    image: redis\n\n  db:\n    image: postgres\n\n\n注意\n\nweb 服务不会等待 redis db 「完全启动」之后才启动。\n\n\n# dns\n\n自定义 DNS 服务器。可以是一个值，也可以是一个列表。\n\ndns: 8.8.8.8\n\ndns:\n  - 8.8.8.8\n  - 114.114.114.114\n\n\n\n# dns_search\n\n配置 DNS 搜索域。可以是一个值，也可以是一个列表。\n\ndns_search: example.com\n\ndns_search:\n  - domain1.example.com\n  - domain2.example.com\n\n\n\n# tmpfs\n\n挂载一个 tmpfs 文件系统到容器。\n\ntmpfs: /run\ntmpfs:\n  - /run\n  - /tmp\n\n\n\n# env_file\n\n从文件中获取环境变量，可以为单独的文件路径或列表。\n\n如果通过 docker-compose -f FILE 方式来指定 Compose 模板文件，则 env_file 中变量的路径会基于模板文件路径。\n\n如果有变量名称与 environment 指令冲突，则按照惯例，以后者为准。\n\nenv_file: .env\n\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n\n\n环境变量文件中每一行必须符合格式，支持 # 开头的注释行。\n\n# common.env: Set development environment\nPROG_ENV=development\n\n\n\n# environment\n\n设置环境变量。你可以使用数组或字典两种格式。\n\n只给定名称的变量会自动获取运行 Compose 主机上对应变量的值，可以用来防止泄露不必要的数据。\n\nenvironment:\n  RACK_ENV: development\n  SESSION_SECRET:\n\nenvironment:\n  - RACK_ENV=development\n  - SESSION_SECRET\n\n\n如果变量名称或者值中用到 true|false，yes|no 等表达 布尔 含义的词汇，最好放到引号里，避免 YAML 自动解析某些内容为对应的布尔语义。这些特定词汇，包括\n\ny|Y|yes|Yes|YES|n|N|no|No|NO|true|True|TRUE|false|False|FALSE|on|On|ON|off|Off|OFF\n\n\n\n# expose\n\n暴露端口，但不映射到宿主机，只被连接的服务访问。\n\n仅可以指定内部端口为参数\n\nexpose:\n - "3000"\n - "8000"\n\n\n\n# external_links\n\n注意\n\n不建议使用该指令。\n\n链接到 docker-compose.yml 外部的容器，甚至并非 Compose 管理的外部容器。\n\nexternal_links:\n - redis_1\n - project_db_1:mysql\n - project_db_1:postgresql\n\n\n\n# #extra_hosts\n\n类似 Docker 中的 --add-host 参数，指定额外的 host 名称映射信息。\n\nextra_hosts:\n - "googledns:8.8.8.8"\n - "dockerhub:52.1.157.61"\n\n\n会在启动后的服务容器中 /etc/hosts 文件中添加如下两条条目。\n\n8.8.8.8 googledns\n52.1.157.61 dockerhub\n\n\n\n# healthcheck\n\n通过命令检查容器是否健康运行。\n\nhealthcheck:\n  test: ["CMD", "curl", "-f", "http://localhost"]\n  interval: 1m30s\n  timeout: 10s\n  retries: 3\n\n\n\n# image\n\n指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉取这个镜像。\n\nimage: ubuntu\nimage: orchardup/postgresql\nimage: a4bc65fd\n\n\n\n# labels\n\n为容器添加 Docker 元数据（metadata）信息。例如可以为容器添加辅助说明信息。\n\nlabels:\n  com.startupteam.description: "webapp for a startup team"\n  com.startupteam.department: "devops department"\n  com.startupteam.release: "rc3 for v1.0"\n\n\n\n# links\n\n注意\n\n不推荐使用该指令。\n\n\n# logging\n\n配置日志选项。\n\nlogging:\n  driver: syslog\n  options:\n    syslog-address: "tcp://192.168.0.42:123"\n\n\n目前支持三种日志驱动类型。\n\ndriver: "json-file"\ndriver: "syslog"\ndriver: "none"\n\n\noptions 配置日志驱动的相关参数。\n\noptions:\n  max-size: "200k"\n  max-file: "10"\n\n\n\n# network_mode\n\n设置网络模式。使用和 docker run 的 --network 参数一样的值。\n\nnetwork_mode: "bridge"\nnetwork_mode: "host"\nnetwork_mode: "none"\nnetwork_mode: "service:[service name]"\nnetwork_mode: "container:[container name/id]"\n\n\n\n# networks\n\n配置容器连接的网络。\n\nversion: "3"\nservices:\n\n  some-service:\n    networks:\n     - some-network\n     - other-network\n\nnetworks:\n  some-network:\n  other-network:\n\n\n\n# pid\n\n跟主机系统共享进程命名空间。打开该选项的容器之间，以及容器和宿主机系统之间可以通过进程 ID 来相互访问和操作。\n\npid: "host"\n\n\n\n# ports\n\n暴露端口信息。\n\n使用宿主端口：容器端口 (HOST:CONTAINER) 格式，或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。\n\nports:\n - "3000"\n - "8000:8000"\n - "49100:22"\n - "127.0.0.1:8001:8001"\n\n\n注意\n\n当使用 HOST:CONTAINER 格式来映射端口时，如果你使用的容器端口小于 60 并且没放到引号里，可能会得到错误结果，因为 YAML 会自动解析 xx:yy 这种数字格式为 60 进制。为避免出现这种问题，建议数字串都采用引号包括起来的字符串格式。\n\n\n# secrets\n\n存储敏感数据，例如 mysql 服务密码。\n\nversion: "3.1"\nservices:\n\nmysql:\n  image: mysql\n  environment:\n    MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password\n  secrets:\n    - db_root_password\n    - my_other_secret\n\nsecrets:\n  my_secret:\n    file: ./my_secret.txt\n  my_other_secret:\n    external: true\n\n\n\n# security_opt\n\n指定容器模板标签（label）机制的默认属性（用户、角色、类型、级别等）。例如配置标签的用户名和角色名。\n\nsecurity_opt:\n    - label:user:USER\n    - label:role:ROLE\n\n\n\n# stop_signal\n\n设置另一个信号来停止容器。在默认情况下使用的是 SIGTERM 停止容器。\n\nstop_signal: SIGUSR1\n\n\n\n# sysctls\n\n配置容器内核参数。\n\nsysctls:\n  net.core.somaxconn: 1024\n  net.ipv4.tcp_syncookies: 0\n\nsysctls:\n  - net.core.somaxconn=1024\n  - net.ipv4.tcp_syncookies=0\n\n\n\n# ulimits\n\n指定容器的 ulimits 限制值。\n\n例如，指定最大进程数为 65535，指定文件句柄数为 20000（软限制，应用可以随时修改，不能超过硬限制） 和 40000（系统硬限制，只能 root 用户提高）。\n\n  ulimits:\n    nproc: 65535\n    nofile:\n      soft: 20000\n      hard: 40000\n\n\n\n# volumes\n\n数据卷所挂载路径设置。可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）。\n\n该指令中路径支持相对路径。\n\nvolumes:\n - /var/lib/mysql\n - cache/:/tmp/cache\n - ~/configs:/etc/configs/:ro\n\n\n\n# 其它指令\n\n此外，还有包括 domainname, entrypoint, hostname, ipc, mac_address, privileged, read_only, shm_size, restart, stdin_open, tty, user, working_dir 等指令，基本跟 docker run 中对应参数的功能一致。\n\n指定服务容器启动后执行的入口文件。\n\nentrypoint: /code/entrypoint.sh\n\n\n指定容器中运行应用的用户名。\n\nuser: nginx\n\n\n指定容器中工作目录。\n\nworking_dir: /code\n\n\n指定容器中搜索域名、主机名、mac 地址等。\n\ndomainname: your_website.com\nhostname: test\nmac_address: 08-00-27-00-0C-0A\n\n\n允许容器中运行一些特权命令。\n\nprivileged: true\n\n\n指定容器退出后的重启策略为始终重启。该命令对保持服务始终运行十分有效，在生产环境中推荐配置为 always 或者 unless-stopped。\n\nrestart: always\n\n\n以只读模式挂载容器的 root 文件系统，意味着不能对容器内容进行修改。\n\nread_only: true\n\n\n打开标准输入，可以接受外部输入。\n\nstdin_open: true\n\n\n模拟一个伪终端。\n\ntty: true\n\n\n\n# 读取变量\n\nCompose 模板文件支持动态读取主机的系统环境变量和当前目录下的 .env 文件中的变量。\n\n例如，下面的 Compose 文件将从运行它的环境中读取变量 ${MONGO_VERSION} 的值，并写入执行的指令中。\n\nversion: "3"\nservices:\n\ndb:\n  image: "mongo:${MONGO_VERSION}"\n\n\n如果执行 MONGO_VERSION=3.2 docker-compose up 则会启动一个 mongo:3.2 镜像的容器；如果执行 MONGO_VERSION=2.8 docker-compose up 则会启动一个 mongo:2.8 镜像的容器。\n\n若当前目录存在 .env 文件，执行 docker-compose 命令时将从该文件中读取变量。\n\n在当前目录新建 .env 文件并写入以下内容。\n\n# 支持 # 号注释\nMONGO_VERSION=3.6\n\n\n执行 docker-compose up 则会启动一个 mongo:3.6 镜像的容器。',normalizedContent:'# docker compose 模板文件\n\n模板文件是使用 compose 的核心，涉及到的指令关键字也比较多。但大家不用担心，这里面大部分指令跟 docker run 相关参数的含义都是类似的。\n\n默认的模板文件名称为 docker-compose.yml，格式为 yaml 格式。\n\nversion: "3"\n\nservices:\n  webapp:\n    image: examples/web\n    ports:\n      - "80:80"\n    volumes:\n      - "/data"\n\n\n注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 dockerfile）等来自动构建生成镜像。\n\n如果使用 build 指令，在 dockerfile 中设置的选项(例如：cmd, expose, volume, env 等) 将会自动被获取，无需在 docker-compose.yml 中再次设置。\n\n下面分别介绍各个指令的用法。\n\n\n# build\n\n指定 dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。 compose 将会利用它自动构建这个镜像，然后使用这个镜像。\n\nversion: \'3\'\nservices:\n\n  webapp:\n    build: ./dir\n\n\n你也可以使用 context 指令指定 dockerfile 所在文件夹的路径。\n\n使用 dockerfile 指令指定 dockerfile 文件名。\n\n使用 arg 指令指定构建镜像时的变量。\n\nversion: \'3\'\nservices:\n\n  webapp:\n    build:\n      context: ./dir\n      dockerfile: dockerfile-alternate\n      args:\n        buildno: 1\n\n\n使用 cache_from 指定构建镜像的缓存\n\nbuild:\n  context: .\n  cache_from:\n    - alpine:latest\n    - corp/web_app:3.14\n\n\n\n# cap_add, cap_drop\n\n指定容器的内核能力（capacity）分配。\n\n例如，让容器拥有所有能力可以指定为：\n\ncap_add:\n  - all\n\n\n去掉 net_admin 能力可以指定为：\n\ncap_drop:\n  - net_admin\n\n\n\n# command\n\n覆盖容器启动后默认执行的命令。\n\ncommand: echo "hello world"\n\n\n\n# configs\n\n仅用于 swarm mode\n\n\n# cgroup_parent\n\n指定父 cgroup 组，意味着将继承该组的资源限制。\n\n例如，创建了一个 cgroup 组名称为 cgroups_1。\n\ncgroup_parent: cgroups_1\n\n\n\n# container_name\n\n指定容器名称。默认将会使用 项目名称_服务名称_序号 这样的格式。\n\ncontainer_name: docker-web-container\n\n\n注意\n\n指定容器名称后，该服务将无法进行扩展（scale），因为 docker 不允许多个容器具有相同的名称。\n\n\n# deploy\n\n仅用于 swarm mode\n\n\n# devices\n\n指定设备映射关系。\n\ndevices:\n  - "/dev/ttyusb1:/dev/ttyusb0"\n\n\n\n# depends_on\n\n解决容器的依赖、启动先后的问题。以下例子中会先启动 redis db 再启动 web\n\nversion: \'3\'\n\nservices:\n  web:\n    build: .\n    depends_on:\n      - db\n      - redis\n\n  redis:\n    image: redis\n\n  db:\n    image: postgres\n\n\n注意\n\nweb 服务不会等待 redis db 「完全启动」之后才启动。\n\n\n# dns\n\n自定义 dns 服务器。可以是一个值，也可以是一个列表。\n\ndns: 8.8.8.8\n\ndns:\n  - 8.8.8.8\n  - 114.114.114.114\n\n\n\n# dns_search\n\n配置 dns 搜索域。可以是一个值，也可以是一个列表。\n\ndns_search: example.com\n\ndns_search:\n  - domain1.example.com\n  - domain2.example.com\n\n\n\n# tmpfs\n\n挂载一个 tmpfs 文件系统到容器。\n\ntmpfs: /run\ntmpfs:\n  - /run\n  - /tmp\n\n\n\n# env_file\n\n从文件中获取环境变量，可以为单独的文件路径或列表。\n\n如果通过 docker-compose -f file 方式来指定 compose 模板文件，则 env_file 中变量的路径会基于模板文件路径。\n\n如果有变量名称与 environment 指令冲突，则按照惯例，以后者为准。\n\nenv_file: .env\n\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n\n\n环境变量文件中每一行必须符合格式，支持 # 开头的注释行。\n\n# common.env: set development environment\nprog_env=development\n\n\n\n# environment\n\n设置环境变量。你可以使用数组或字典两种格式。\n\n只给定名称的变量会自动获取运行 compose 主机上对应变量的值，可以用来防止泄露不必要的数据。\n\nenvironment:\n  rack_env: development\n  session_secret:\n\nenvironment:\n  - rack_env=development\n  - session_secret\n\n\n如果变量名称或者值中用到 true|false，yes|no 等表达 布尔 含义的词汇，最好放到引号里，避免 yaml 自动解析某些内容为对应的布尔语义。这些特定词汇，包括\n\ny|y|yes|yes|yes|n|n|no|no|no|true|true|true|false|false|false|on|on|on|off|off|off\n\n\n\n# expose\n\n暴露端口，但不映射到宿主机，只被连接的服务访问。\n\n仅可以指定内部端口为参数\n\nexpose:\n - "3000"\n - "8000"\n\n\n\n# external_links\n\n注意\n\n不建议使用该指令。\n\n链接到 docker-compose.yml 外部的容器，甚至并非 compose 管理的外部容器。\n\nexternal_links:\n - redis_1\n - project_db_1:mysql\n - project_db_1:postgresql\n\n\n\n# #extra_hosts\n\n类似 docker 中的 --add-host 参数，指定额外的 host 名称映射信息。\n\nextra_hosts:\n - "googledns:8.8.8.8"\n - "dockerhub:52.1.157.61"\n\n\n会在启动后的服务容器中 /etc/hosts 文件中添加如下两条条目。\n\n8.8.8.8 googledns\n52.1.157.61 dockerhub\n\n\n\n# healthcheck\n\n通过命令检查容器是否健康运行。\n\nhealthcheck:\n  test: ["cmd", "curl", "-f", "http://localhost"]\n  interval: 1m30s\n  timeout: 10s\n  retries: 3\n\n\n\n# image\n\n指定为镜像名称或镜像 id。如果镜像在本地不存在，compose 将会尝试拉取这个镜像。\n\nimage: ubuntu\nimage: orchardup/postgresql\nimage: a4bc65fd\n\n\n\n# labels\n\n为容器添加 docker 元数据（metadata）信息。例如可以为容器添加辅助说明信息。\n\nlabels:\n  com.startupteam.description: "webapp for a startup team"\n  com.startupteam.department: "devops department"\n  com.startupteam.release: "rc3 for v1.0"\n\n\n\n# links\n\n注意\n\n不推荐使用该指令。\n\n\n# logging\n\n配置日志选项。\n\nlogging:\n  driver: syslog\n  options:\n    syslog-address: "tcp://192.168.0.42:123"\n\n\n目前支持三种日志驱动类型。\n\ndriver: "json-file"\ndriver: "syslog"\ndriver: "none"\n\n\noptions 配置日志驱动的相关参数。\n\noptions:\n  max-size: "200k"\n  max-file: "10"\n\n\n\n# network_mode\n\n设置网络模式。使用和 docker run 的 --network 参数一样的值。\n\nnetwork_mode: "bridge"\nnetwork_mode: "host"\nnetwork_mode: "none"\nnetwork_mode: "service:[service name]"\nnetwork_mode: "container:[container name/id]"\n\n\n\n# networks\n\n配置容器连接的网络。\n\nversion: "3"\nservices:\n\n  some-service:\n    networks:\n     - some-network\n     - other-network\n\nnetworks:\n  some-network:\n  other-network:\n\n\n\n# pid\n\n跟主机系统共享进程命名空间。打开该选项的容器之间，以及容器和宿主机系统之间可以通过进程 id 来相互访问和操作。\n\npid: "host"\n\n\n\n# ports\n\n暴露端口信息。\n\n使用宿主端口：容器端口 (host:container) 格式，或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。\n\nports:\n - "3000"\n - "8000:8000"\n - "49100:22"\n - "127.0.0.1:8001:8001"\n\n\n注意\n\n当使用 host:container 格式来映射端口时，如果你使用的容器端口小于 60 并且没放到引号里，可能会得到错误结果，因为 yaml 会自动解析 xx:yy 这种数字格式为 60 进制。为避免出现这种问题，建议数字串都采用引号包括起来的字符串格式。\n\n\n# secrets\n\n存储敏感数据，例如 mysql 服务密码。\n\nversion: "3.1"\nservices:\n\nmysql:\n  image: mysql\n  environment:\n    mysql_root_password_file: /run/secrets/db_root_password\n  secrets:\n    - db_root_password\n    - my_other_secret\n\nsecrets:\n  my_secret:\n    file: ./my_secret.txt\n  my_other_secret:\n    external: true\n\n\n\n# security_opt\n\n指定容器模板标签（label）机制的默认属性（用户、角色、类型、级别等）。例如配置标签的用户名和角色名。\n\nsecurity_opt:\n    - label:user:user\n    - label:role:role\n\n\n\n# stop_signal\n\n设置另一个信号来停止容器。在默认情况下使用的是 sigterm 停止容器。\n\nstop_signal: sigusr1\n\n\n\n# sysctls\n\n配置容器内核参数。\n\nsysctls:\n  net.core.somaxconn: 1024\n  net.ipv4.tcp_syncookies: 0\n\nsysctls:\n  - net.core.somaxconn=1024\n  - net.ipv4.tcp_syncookies=0\n\n\n\n# ulimits\n\n指定容器的 ulimits 限制值。\n\n例如，指定最大进程数为 65535，指定文件句柄数为 20000（软限制，应用可以随时修改，不能超过硬限制） 和 40000（系统硬限制，只能 root 用户提高）。\n\n  ulimits:\n    nproc: 65535\n    nofile:\n      soft: 20000\n      hard: 40000\n\n\n\n# volumes\n\n数据卷所挂载路径设置。可以设置宿主机路径 （host:container） 或加上访问模式 （host:container:ro）。\n\n该指令中路径支持相对路径。\n\nvolumes:\n - /var/lib/mysql\n - cache/:/tmp/cache\n - ~/configs:/etc/configs/:ro\n\n\n\n# 其它指令\n\n此外，还有包括 domainname, entrypoint, hostname, ipc, mac_address, privileged, read_only, shm_size, restart, stdin_open, tty, user, working_dir 等指令，基本跟 docker run 中对应参数的功能一致。\n\n指定服务容器启动后执行的入口文件。\n\nentrypoint: /code/entrypoint.sh\n\n\n指定容器中运行应用的用户名。\n\nuser: nginx\n\n\n指定容器中工作目录。\n\nworking_dir: /code\n\n\n指定容器中搜索域名、主机名、mac 地址等。\n\ndomainname: your_website.com\nhostname: test\nmac_address: 08-00-27-00-0c-0a\n\n\n允许容器中运行一些特权命令。\n\nprivileged: true\n\n\n指定容器退出后的重启策略为始终重启。该命令对保持服务始终运行十分有效，在生产环境中推荐配置为 always 或者 unless-stopped。\n\nrestart: always\n\n\n以只读模式挂载容器的 root 文件系统，意味着不能对容器内容进行修改。\n\nread_only: true\n\n\n打开标准输入，可以接受外部输入。\n\nstdin_open: true\n\n\n模拟一个伪终端。\n\ntty: true\n\n\n\n# 读取变量\n\ncompose 模板文件支持动态读取主机的系统环境变量和当前目录下的 .env 文件中的变量。\n\n例如，下面的 compose 文件将从运行它的环境中读取变量 ${mongo_version} 的值，并写入执行的指令中。\n\nversion: "3"\nservices:\n\ndb:\n  image: "mongo:${mongo_version}"\n\n\n如果执行 mongo_version=3.2 docker-compose up 则会启动一个 mongo:3.2 镜像的容器；如果执行 mongo_version=2.8 docker-compose up 则会启动一个 mongo:2.8 镜像的容器。\n\n若当前目录存在 .env 文件，执行 docker-compose 命令时将从该文件中读取变量。\n\n在当前目录新建 .env 文件并写入以下内容。\n\n# 支持 # 号注释\nmongo_version=3.6\n\n\n执行 docker-compose up 则会启动一个 mongo:3.6 镜像的容器。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker Compose 实战 WordPress",frontmatter:{title:"Docker Compose 实战 WordPress",date:"2022-02-09T09:23:30.000Z",permalink:"/pages/7285c6/"},regularPath:"/17.Docker/08.Docker%20%E4%B8%89%E5%89%91%E5%AE%A2%20Compose/06.Docker%20Compose%20%E5%AE%9E%E6%88%98%20WordPress.html",relativePath:"17.Docker/08.Docker 三剑客 Compose/06.Docker Compose 实战 WordPress.md",key:"v-3c3f9ba1",path:"/pages/7285c6/",headers:[{level:2,title:"创建空文件夹",slug:"创建空文件夹",normalizedTitle:"创建空文件夹",charIndex:95},{level:2,title:"创建 docker-compose.yml 文件",slug:"创建-docker-compose-yml-文件",normalizedTitle:"创建 docker-compose.yml 文件",charIndex:142},{level:2,title:"构建并运行项目",slug:"构建并运行项目",normalizedTitle:"构建并运行项目",charIndex:780}],headersStr:"创建空文件夹 创建 docker-compose.yml 文件 构建并运行项目",content:'# Docker Compose 实战 WordPress\n\n本小节内容适合 PHP 开发人员阅读。\n\nCompose 可以很便捷的让 Wordpress 运行在一个独立的环境中。\n\n\n# 创建空文件夹\n\n假设新建一个名为 wordpress 的文件夹，然后进入这个文件夹。\n\n\n# 创建 docker-compose.yml 文件\n\ndocker-compose.yml 文件将开启一个 wordpress 服务和一个独立的 MySQL 实例：\n\nversion: "3"\nservices:\n\n   db:\n     image: mysql:5.7\n     volumes:\n       - db_data:/var/lib/mysql\n     restart: always\n     environment:\n       MYSQL_ROOT_PASSWORD: somewordpress\n       MYSQL_DATABASE: wordpress\n       MYSQL_USER: wordpress\n       MYSQL_PASSWORD: wordpress\n\n   wordpress:\n     depends_on:\n       - db\n     image: wordpress:latest\n     ports:\n       - "8000:80"\n     restart: always\n     environment:\n       WORDPRESS_DB_HOST: db:3306\n       WORDPRESS_DB_USER: wordpress\n       WORDPRESS_DB_PASSWORD: wordpress\nvolumes:\n    db_data:\n\n\n\n# 构建并运行项目\n\n运行 docker-compose up -d Compose 就会拉取镜像再创建我们所需要的镜像，然后启动 wordpress 和数据库容器。 接着浏览器访问 127.0.0.1:8000 端口就能看到 WordPress 安装界面了。',normalizedContent:'# docker compose 实战 wordpress\n\n本小节内容适合 php 开发人员阅读。\n\ncompose 可以很便捷的让 wordpress 运行在一个独立的环境中。\n\n\n# 创建空文件夹\n\n假设新建一个名为 wordpress 的文件夹，然后进入这个文件夹。\n\n\n# 创建 docker-compose.yml 文件\n\ndocker-compose.yml 文件将开启一个 wordpress 服务和一个独立的 mysql 实例：\n\nversion: "3"\nservices:\n\n   db:\n     image: mysql:5.7\n     volumes:\n       - db_data:/var/lib/mysql\n     restart: always\n     environment:\n       mysql_root_password: somewordpress\n       mysql_database: wordpress\n       mysql_user: wordpress\n       mysql_password: wordpress\n\n   wordpress:\n     depends_on:\n       - db\n     image: wordpress:latest\n     ports:\n       - "8000:80"\n     restart: always\n     environment:\n       wordpress_db_host: db:3306\n       wordpress_db_user: wordpress\n       wordpress_db_password: wordpress\nvolumes:\n    db_data:\n\n\n\n# 构建并运行项目\n\n运行 docker-compose up -d compose 就会拉取镜像再创建我们所需要的镜像，然后启动 wordpress 和数据库容器。 接着浏览器访问 127.0.0.1:8000 端口就能看到 wordpress 安装界面了。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker Compose 实战 Tomcat",frontmatter:{title:"Docker Compose 实战 Tomcat",date:"2022-02-09T09:23:30.000Z",permalink:"/pages/8a9ceb/"},regularPath:"/17.Docker/08.Docker%20%E4%B8%89%E5%89%91%E5%AE%A2%20Compose/07.Docker%20Compose%20%E5%AE%9E%E6%88%98%20Tomcat.html",relativePath:"17.Docker/08.Docker 三剑客 Compose/07.Docker Compose 实战 Tomcat.md",key:"v-ae86ef32",path:"/pages/8a9ceb/",headersStr:null,content:"# Docker Compose 实战 Tomcat\n\nversion: '3.1'\nservices:\n  tomcat:\n    restart: always\n    image: tomcat\n    container_name: tomcat\n    ports:\n      - 8080:8080\n    volumes:\n      - /usr/local/docker/tomcat/webapps/test:/usr/local/tomcat/webapps/test\n    environment:\n      TZ: Asia/Shanghai\n",normalizedContent:"# docker compose 实战 tomcat\n\nversion: '3.1'\nservices:\n  tomcat:\n    restart: always\n    image: tomcat\n    container_name: tomcat\n    ports:\n      - 8080:8080\n    volumes:\n      - /usr/local/docker/tomcat/webapps/test:/usr/local/tomcat/webapps/test\n    environment:\n      tz: asia/shanghai\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"微服务的缺点",frontmatter:{title:"微服务的缺点",date:"2022-02-09T11:40:48.000Z",permalink:"/pages/fb92c3/"},regularPath:"/14.%E5%BE%AE%E6%9C%8D%E5%8A%A1/01.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%AE%80%E4%BB%8B/05.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E7%BC%BA%E7%82%B9.html",relativePath:"14.微服务/01.微服务简介/05.微服务的缺点.md",key:"v-28665664",path:"/pages/fb92c3/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:13},{level:2,title:"小结",slug:"小结",normalizedTitle:"小结",charIndex:1532}],headersStr:"概述 小结",content:"# 微服务的缺点\n\n\n# 概述\n\n就像 Fred Brooks 大约在 30 年前写的《人月神话》中说的，没有银弹。与其他技术一样，微服务架构模式也存在着缺点。其中一个缺点就是名称本身。微服务这个术语的重点过多偏向于服务的规模。事实上，有些开发者主张构建极细粒度的 10 至 100 LOC（代码行） 服务，虽然这对于小型服务可能比较好，但重要的是要记住，小型服务只是一种手段，而不是主要目标。微服务的目标在于充分分解应用程序以方便应用敏捷开发和部署。\n\n微服务另一个主要缺点是由于微服务是一个分布式系统，其使得整体变得复杂。开发者需要选择和实现基于消息或者 RPC 的进程间通信机制。此外，由于目标请求可能很慢或者不可用，他们必须要编写代码来处理局部故障。虽然这些并不是很复杂、高深，但模块间通过语言级方法/过程调用相互调用，这比单体应用要复杂得多。\n\n微服务的另一个挑战是分区数据库架构。更新多个业务实体的业务事务是相当普遍的。这些事务在单体应用中的实现显得微不足道，因为单体只存在一个单独的数据库。在基于微服务的应用程序中，您需要更新不同服务所用的数据库。通常不会选择分布式事务，不仅仅是因为 CAP 定理。他们根本不支持如今高度可扩展的 NoSQL 数据库和消息代理。您最后不得不使用基于最终一致性的方法，这对于开发人员来说更具挑战性。\n\n测试微服务应用程序也很复杂。例如，使用现代框架如 Spring Boot，只需要编写一个测试类来启动一个单体 web 应用程序并测试其 REST API。相比之下，一个类似的测试类对于微服务来说需要启动该服务及其所依赖的所有服务，或者至少为这些服务配置存根。再次声明，虽然这不是一件高深的事情，但不要低估了这样做的复杂性。\n\n微服务架构模式的另一个主要挑战是实现了跨越多服务变更。例如，我们假设您正在实现一个变更服务 A、服务 B 和 服务 C 的需求，其中 A 依赖于 B，且 B 依赖于 C。在单体应用程序中，您可以简单地修改相应的模块、整合变更并一次性部署他们。相反，在微服务中您需要仔细规划和协调出现的变更至每个服务。例如，您需要更新服务 C，然后更新服务 B，最后更新服务 A。幸运的是，大多数变更只会影响一个服务，需要协调的多服务变更相对较少。\n\n部署基于微服务的应用程序也是相当复杂的。一个单体应用可以很容易地部署到基于传统负载均衡器的一组相同服务器上。每个应用程序实例都配置有基础设施服务的位置（主机和端口），比如数据库和消息代理。相比之下，微服务应用程序通常由大量的服务组成。例如，据 Adrian Cockcroft 了解到，Hailo 拥有 160 个不同的服务，Netflix 拥有的服务超过 600 个。\n\n每个服务都有多个运行时实例。还有更多的移动部件需要配置、部署、扩展和监控。此外，您还需要实现服务发现机制，使得服务能够发现需要与之通信的任何其他服务的位置（主机和端口）。比较传统麻烦的基于票据（ticket-based）和手动的操作方式无法扩展到如此复杂程度。因此，要成功部署微服务应用程序，需要求开发人员能高度控制部署方式和高度自动化。\n\n一种自动化方式是使用现成的平台即服务（PaaS），如 Cloud Foundry。PaaS 为开发人员提供了一种简单的方式来部署和管理他们的微服务。它让开发人员避开了诸如采购和配置 IT 资源等烦恼。同时，配置 PaaS 的系统人员和网络专业人员可以确保达到最佳实践以落实公司策略。\n\n自动化微服务部署的另一个方式是开发自己的 PaaS。一个普遍的起点是使用集群方案，如 Kubernetes，与 Docker 等容器技术相结合。\n\n\n# 小结\n\n构建复杂的微服务应用程序本质上是困难的。单体架构模式只适用于简单、轻量级的应用程序，如果您使用它来构建复杂应用，您最终会陷入痛苦的境地。微服务架构模式是复杂、持续发展应用的一个更好的选择。尽管它存在着缺点和实现挑战。",normalizedContent:"# 微服务的缺点\n\n\n# 概述\n\n就像 fred brooks 大约在 30 年前写的《人月神话》中说的，没有银弹。与其他技术一样，微服务架构模式也存在着缺点。其中一个缺点就是名称本身。微服务这个术语的重点过多偏向于服务的规模。事实上，有些开发者主张构建极细粒度的 10 至 100 loc（代码行） 服务，虽然这对于小型服务可能比较好，但重要的是要记住，小型服务只是一种手段，而不是主要目标。微服务的目标在于充分分解应用程序以方便应用敏捷开发和部署。\n\n微服务另一个主要缺点是由于微服务是一个分布式系统，其使得整体变得复杂。开发者需要选择和实现基于消息或者 rpc 的进程间通信机制。此外，由于目标请求可能很慢或者不可用，他们必须要编写代码来处理局部故障。虽然这些并不是很复杂、高深，但模块间通过语言级方法/过程调用相互调用，这比单体应用要复杂得多。\n\n微服务的另一个挑战是分区数据库架构。更新多个业务实体的业务事务是相当普遍的。这些事务在单体应用中的实现显得微不足道，因为单体只存在一个单独的数据库。在基于微服务的应用程序中，您需要更新不同服务所用的数据库。通常不会选择分布式事务，不仅仅是因为 cap 定理。他们根本不支持如今高度可扩展的 nosql 数据库和消息代理。您最后不得不使用基于最终一致性的方法，这对于开发人员来说更具挑战性。\n\n测试微服务应用程序也很复杂。例如，使用现代框架如 spring boot，只需要编写一个测试类来启动一个单体 web 应用程序并测试其 rest api。相比之下，一个类似的测试类对于微服务来说需要启动该服务及其所依赖的所有服务，或者至少为这些服务配置存根。再次声明，虽然这不是一件高深的事情，但不要低估了这样做的复杂性。\n\n微服务架构模式的另一个主要挑战是实现了跨越多服务变更。例如，我们假设您正在实现一个变更服务 a、服务 b 和 服务 c 的需求，其中 a 依赖于 b，且 b 依赖于 c。在单体应用程序中，您可以简单地修改相应的模块、整合变更并一次性部署他们。相反，在微服务中您需要仔细规划和协调出现的变更至每个服务。例如，您需要更新服务 c，然后更新服务 b，最后更新服务 a。幸运的是，大多数变更只会影响一个服务，需要协调的多服务变更相对较少。\n\n部署基于微服务的应用程序也是相当复杂的。一个单体应用可以很容易地部署到基于传统负载均衡器的一组相同服务器上。每个应用程序实例都配置有基础设施服务的位置（主机和端口），比如数据库和消息代理。相比之下，微服务应用程序通常由大量的服务组成。例如，据 adrian cockcroft 了解到，hailo 拥有 160 个不同的服务，netflix 拥有的服务超过 600 个。\n\n每个服务都有多个运行时实例。还有更多的移动部件需要配置、部署、扩展和监控。此外，您还需要实现服务发现机制，使得服务能够发现需要与之通信的任何其他服务的位置（主机和端口）。比较传统麻烦的基于票据（ticket-based）和手动的操作方式无法扩展到如此复杂程度。因此，要成功部署微服务应用程序，需要求开发人员能高度控制部署方式和高度自动化。\n\n一种自动化方式是使用现成的平台即服务（paas），如 cloud foundry。paas 为开发人员提供了一种简单的方式来部署和管理他们的微服务。它让开发人员避开了诸如采购和配置 it 资源等烦恼。同时，配置 paas 的系统人员和网络专业人员可以确保达到最佳实践以落实公司策略。\n\n自动化微服务部署的另一个方式是开发自己的 paas。一个普遍的起点是使用集群方案，如 kubernetes，与 docker 等容器技术相结合。\n\n\n# 小结\n\n构建复杂的微服务应用程序本质上是困难的。单体架构模式只适用于简单、轻量级的应用程序，如果您使用它来构建复杂应用，您最终会陷入痛苦的境地。微服务架构模式是复杂、持续发展应用的一个更好的选择。尽管它存在着缺点和实现挑战。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:41:52",lastUpdatedTimestamp:1644378112e3},{title:"Docker Compose 实战 MySQL",frontmatter:{title:"Docker Compose 实战 MySQL",date:"2022-02-09T09:23:30.000Z",permalink:"/pages/853cbd/"},regularPath:"/17.Docker/08.Docker%20%E4%B8%89%E5%89%91%E5%AE%A2%20Compose/08.Docker%20Compose%20%E5%AE%9E%E6%88%98%20MySQL.html",relativePath:"17.Docker/08.Docker 三剑客 Compose/08.Docker Compose 实战 MySQL.md",key:"v-4c82f50b",path:"/pages/853cbd/",headers:[{level:2,title:"MySQL5",slug:"mysql5",normalizedTitle:"mysql5",charIndex:30},{level:2,title:"MySQL8",slug:"mysql8",normalizedTitle:"mysql8",charIndex:664}],headersStr:"MySQL5 MySQL8",content:"# Docker Compose 实战 MySQL\n\n\n# MySQL5\n\nversion: '3.1'\nservices:\n  mysql:\n    restart: always\n    image: mysql:5.7.22\n    container_name: mysql\n    ports:\n      - 3306:3306\n    environment:\n      TZ: Asia/Shanghai\n      MYSQL_ROOT_PASSWORD: 123456\n    command:\n      --character-set-server=utf8mb4\n      --collation-server=utf8mb4_general_ci\n      --explicit_defaults_for_timestamp=true\n      --lower_case_table_names=1\n      --max_allowed_packet=128M\n      --sql-mode=\"STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO\"\n    volumes:\n      - mysql-data:/var/lib/mysql\n\nvolumes:\n  mysql-data:\n\n\n\n# MySQL8\n\nversion: '3.1'\nservices:\n  db:\n    image: mysql\n    restart: always\n    environment:\n      TZ: Asia/Shanghai\n      MYSQL_ROOT_PASSWORD: 123456\n    command:\n      --default-authentication-plugin=mysql_native_password\n      --character-set-server=utf8mb4\n      --collation-server=utf8mb4_general_ci\n      --explicit_defaults_for_timestamp=true\n      --lower_case_table_names=1\n    ports:\n      - 3306:3306\n    volumes:\n      - mysql-data:/var/lib/mysql\n\n  adminer:\n    image: adminer\n    restart: always\n    ports:\n      - 8080:8080\n\nvolumes:\n  mysql-data:\n",normalizedContent:"# docker compose 实战 mysql\n\n\n# mysql5\n\nversion: '3.1'\nservices:\n  mysql:\n    restart: always\n    image: mysql:5.7.22\n    container_name: mysql\n    ports:\n      - 3306:3306\n    environment:\n      tz: asia/shanghai\n      mysql_root_password: 123456\n    command:\n      --character-set-server=utf8mb4\n      --collation-server=utf8mb4_general_ci\n      --explicit_defaults_for_timestamp=true\n      --lower_case_table_names=1\n      --max_allowed_packet=128m\n      --sql-mode=\"strict_trans_tables,no_auto_create_user,no_engine_substitution,no_zero_date,no_zero_in_date,error_for_division_by_zero\"\n    volumes:\n      - mysql-data:/var/lib/mysql\n\nvolumes:\n  mysql-data:\n\n\n\n# mysql8\n\nversion: '3.1'\nservices:\n  db:\n    image: mysql\n    restart: always\n    environment:\n      tz: asia/shanghai\n      mysql_root_password: 123456\n    command:\n      --default-authentication-plugin=mysql_native_password\n      --character-set-server=utf8mb4\n      --collation-server=utf8mb4_general_ci\n      --explicit_defaults_for_timestamp=true\n      --lower_case_table_names=1\n    ports:\n      - 3306:3306\n    volumes:\n      - mysql-data:/var/lib/mysql\n\n  adminer:\n    image: adminer\n    restart: always\n    ports:\n      - 8080:8080\n\nvolumes:\n  mysql-data:\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"YAML 配置文件语言",frontmatter:{title:"YAML 配置文件语言",date:"2022-02-09T09:23:30.000Z",permalink:"/pages/bd41de/"},regularPath:"/17.Docker/08.Docker%20%E4%B8%89%E5%89%91%E5%AE%A2%20Compose/10.YAML%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%AD%E8%A8%80.html",relativePath:"17.Docker/08.Docker 三剑客 Compose/10.YAML 配置文件语言.md",key:"v-6272dfb6",path:"/pages/bd41de/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:18},{level:2,title:"对象",slug:"对象",normalizedTitle:"对象",charIndex:246},{level:2,title:"数组",slug:"数组",normalizedTitle:"数组",charIndex:303},{level:2,title:"复合结构",slug:"复合结构",normalizedTitle:"复合结构",charIndex:531},{level:2,title:"纯量",slug:"纯量",normalizedTitle:"纯量",charIndex:346}],headersStr:"简介 对象 数组 复合结构 纯量",content:"# YAML 配置文件语言\n\n\n# 简介\n\nYAML 是专门用来写配置文件的语言，非常简洁和强大，远比 JSON 格式方便。\n\nYAML 语言的设计目标，就是方便人类读写。它实质上是一种通用的数据串行化格式。它的基本语法规则如下：\n\n * 大小写敏感\n * 使用缩进表示层级关系\n * 缩进时不允许使用Tab键，只允许使用空格。\n * 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可\n\n# 表示注释，从这个字符一直到行尾，都会被解析器忽略。\n\nYAML 支持的数据结构有三种：\n\n * 对象：键值对的集合，又称为映射（mapping）/ 哈希（hashes） / 字典（dictionary）\n * 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list）\n * 纯量（scalars）：单个的、不可再分的值\n\n\n# 对象\n\n对象的一组键值对，使用冒号结构表示\n\nanimal: pets1\n\n\n\n# 数组\n\n一组连词线开头的行，构成一个数组\n\n- Cat\n- Dog\n- Goldfish\n\n\n数据结构的子成员是一个数组，则可以在该项下面缩进一个空格\n\n- Array\n - Cat\n - Dog\n - Goldfish\n\n\n\n# 复合结构\n\n对象和数组可以结合使用，形成复合结构\n\nlanguages:\n - Ruby\n - Perl\n - Python \nwebsites:\n YAML: yaml.org \n Ruby: ruby-lang.org \n Python: python.org \n Perl: use.perl.org \n\n\n\n# 纯量\n\n纯量是最基本的、不可再分的值。以下数据类型都属于 JavaScript 的纯量\n\n * 字符串\n * 布尔值\n * 整数\n * 浮点数\n * Null\n * 时间\n * 日期",normalizedContent:"# yaml 配置文件语言\n\n\n# 简介\n\nyaml 是专门用来写配置文件的语言，非常简洁和强大，远比 json 格式方便。\n\nyaml 语言的设计目标，就是方便人类读写。它实质上是一种通用的数据串行化格式。它的基本语法规则如下：\n\n * 大小写敏感\n * 使用缩进表示层级关系\n * 缩进时不允许使用tab键，只允许使用空格。\n * 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可\n\n# 表示注释，从这个字符一直到行尾，都会被解析器忽略。\n\nyaml 支持的数据结构有三种：\n\n * 对象：键值对的集合，又称为映射（mapping）/ 哈希（hashes） / 字典（dictionary）\n * 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list）\n * 纯量（scalars）：单个的、不可再分的值\n\n\n# 对象\n\n对象的一组键值对，使用冒号结构表示\n\nanimal: pets1\n\n\n\n# 数组\n\n一组连词线开头的行，构成一个数组\n\n- cat\n- dog\n- goldfish\n\n\n数据结构的子成员是一个数组，则可以在该项下面缩进一个空格\n\n- array\n - cat\n - dog\n - goldfish\n\n\n\n# 复合结构\n\n对象和数组可以结合使用，形成复合结构\n\nlanguages:\n - ruby\n - perl\n - python \nwebsites:\n yaml: yaml.org \n ruby: ruby-lang.org \n python: python.org \n perl: use.perl.org \n\n\n\n# 纯量\n\n纯量是最基本的、不可再分的值。以下数据类型都属于 javascript 的纯量\n\n * 字符串\n * 布尔值\n * 整数\n * 浮点数\n * null\n * 时间\n * 日期",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"Docker Compose 常用命令",frontmatter:{title:"Docker Compose 常用命令",date:"2022-02-09T09:23:30.000Z",permalink:"/pages/f508e3/"},regularPath:"/17.Docker/08.Docker%20%E4%B8%89%E5%89%91%E5%AE%A2%20Compose/09.Docker%20Compose%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html",relativePath:"17.Docker/08.Docker 三剑客 Compose/09.Docker Compose 常用命令.md",key:"v-1dfe3ec2",path:"/pages/f508e3/",headers:[{level:2,title:"前台运行",slug:"前台运行",normalizedTitle:"前台运行",charIndex:26},{level:2,title:"后台运行",slug:"后台运行",normalizedTitle:"后台运行",charIndex:55},{level:2,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:87},{level:2,title:"停止",slug:"停止",normalizedTitle:"停止",charIndex:117},{level:2,title:"停止并移除容器",slug:"停止并移除容器",normalizedTitle:"停止并移除容器",charIndex:146}],headersStr:"前台运行 后台运行 启动 停止 停止并移除容器",content:"# Docker Compose 常用命令\n\n\n# 前台运行\n\ndocker-compose up\n\n\n\n# 后台运行\n\ndocker-compose up -d\n\n\n\n# 启动\n\ndocker-compose start\n\n\n\n# 停止\n\ndocker-compose stop\n\n\n\n# 停止并移除容器\n\ndocker-compose down\n",normalizedContent:"# docker compose 常用命令\n\n\n# 前台运行\n\ndocker-compose up\n\n\n\n# 后台运行\n\ndocker-compose up -d\n\n\n\n# 启动\n\ndocker-compose start\n\n\n\n# 停止\n\ndocker-compose stop\n\n\n\n# 停止并移除容器\n\ndocker-compose down\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"附：为什么说 JSON 不适合做配置文件？",frontmatter:{title:"附：为什么说 JSON 不适合做配置文件？",date:"2022-02-09T09:23:30.000Z",permalink:"/pages/0c8168/"},regularPath:"/17.Docker/08.Docker%20%E4%B8%89%E5%89%91%E5%AE%A2%20Compose/11.%E9%99%84%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4%20JSON%20%E4%B8%8D%E9%80%82%E5%90%88%E5%81%9A%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%9F.html",relativePath:"17.Docker/08.Docker 三剑客 Compose/11.附：为什么说 JSON 不适合做配置文件？.md",key:"v-a0974e96",path:"/pages/0c8168/",headers:[{level:2,title:"为什么流行使用 JSON 作为配置语言？",slug:"为什么流行使用-json-作为配置语言",normalizedTitle:"为什么流行使用 json 作为配置语言？",charIndex:275},{level:2,title:"JSON 的问题",slug:"json-的问题",normalizedTitle:"json 的问题",charIndex:477},{level:3,title:"缺乏注释",slug:"缺乏注释",normalizedTitle:"缺乏注释",charIndex:490},{level:3,title:"过于严格",slug:"过于严格",normalizedTitle:"过于严格",charIndex:989},{level:3,title:"低信噪比",slug:"低信噪比",normalizedTitle:"低信噪比",charIndex:1065},{level:3,title:"长字符串",slug:"长字符串",normalizedTitle:"长字符串",charIndex:1407},{level:3,title:"数字",slug:"数字",normalizedTitle:"数字",charIndex:1599},{level:2,title:"JSON 的替代方案",slug:"json-的替代方案",normalizedTitle:"json 的替代方案",charIndex:2564},{level:3,title:"HJSON",slug:"hjson",normalizedTitle:"hjson",charIndex:3376},{level:3,title:"HOCON",slug:"hocon",normalizedTitle:"hocon",charIndex:4369},{level:3,title:"YAML",slug:"yaml",normalizedTitle:"yaml",charIndex:135},{level:3,title:"脚本语言",slug:"脚本语言",normalizedTitle:"脚本语言",charIndex:5557},{level:3,title:"自定义配置格式",slug:"自定义配置格式",normalizedTitle:"自定义配置格式",charIndex:5763},{level:2,title:"结论",slug:"结论",normalizedTitle:"结论",charIndex:5892}],headersStr:"为什么流行使用 JSON 作为配置语言？ JSON 的问题 缺乏注释 过于严格 低信噪比 长字符串 数字 JSON 的替代方案 HJSON HOCON YAML 脚本语言 自定义配置格式 结论",content:'# 附：为什么说 JSON 不适合做配置文件？\n\n提示\n\n很多项目使用 JSON 作为配置文件，最明显的例子就是 npm 和 yarn 使用的 package.json 文件。当然，还有很多其他文件，例如 CloudFormation（最初只有 JSON，但现在也支持 YAML）和 composer（PHP）。\n\n但是，JSON 实际上是一种非常糟糕的配置语言。别误会我的意思，我其实是喜欢 JSON 的。它是一种相对灵活的文本格式，对于机器和人类来说都很容易阅读，而且是一种非常好的数据交换和存储格式。但作为一种配置语言，它有它的不足。\n\n\n# 为什么流行使用 JSON 作为配置语言？\n\n将 JSON 用作配置文件有几个方面的原因，其中最大的原因可能是它很容易实现。很多编程语言的标准库都支持 JSON，开发人员或用户可能已经很熟悉 JSON，所以不需要学习新的配置格式就可以使用那些产品。现在几乎所有的工具都提供 JSON 支持，包括语法突出显示、自动格式化、验证工具等。\n\n这些都是很好的理由，但这种无处不在的格式其实不适合用作配置。\n\n\n# JSON 的问题\n\n\n# 缺乏注释\n\n注释对于配置语言而言绝对是一个重要的功能。注释可用于标注不同的配置选项、解释为什么要配置成特定的值，更重要的是，在使用不同的配置进行测试和调试时需要临时注释掉部分配置。当然，如果只是把 JSON 当作是一种数据交换格式，那么就不需要用到注释。\n\n我们可以通过一些方法给 JSON 添加注释。一种常见的方法是在对象中使用特殊的键作为注释，例如“//”或“__comment”。但是，这种语法的可读性不高，并且为了在单个对象中包含多个注释，需要为每个注释使用唯一的键。David Crockford（JSON 的发明者）建议使用预处理器来删除注释。如果你的应用程序需要使用 JSON 作为配置，那么完全没问题，不过这确实带来了一些额外的工作量。\n\n一些 JSON 库允许将注释作为输入。例如，Ruby 的 JSON 模块和启用了 JsonParser.Feature.ALLOW_COMMENTS 功能的 Java Jackson 库可以处理 JavaScript 风格的注释。但是，这不是标准的方式，而且很多编辑器无法正确处理 JSON 文件中的注释，这让编辑它们变得更加困难。\n\n\n# 过于严格\n\nJSON 规范非常严格，这也是为什么实现 JSON 解析器会这么简单，但在我看来，它还会影响可读性，并且在较小程度上会影响可写性。\n\n\n# 低信噪比\n\n与其他配置语言相比，JSON 显得非常嘈杂。JSON 的很多标点符号对可读性毫无帮助，况且，对象中的键几乎都是标识符，所以键的引号其实是多余的。\n\n此外，JSON 需要使用花括号将整个文档包围起来，所以 JSON 是 JavaScript 的子集，并在流中发送多个对象时用于界定不同的对象。但是，对于配置文件来说，最外面的大括号其实没有任何用处。在配置文件中，键值对之间的逗号也是没有必要的。通常情况下，每行只有一个键值对，所以使用换行作为分隔符更有意义。\n\n说到逗号，JSON 居然不允许在结尾出现逗号。如果你需要在每个键值对之后使用逗号，那么至少应该接受结尾的逗号，因为有了结尾的逗号，在添加新条目时会更容易，而且在进行 commit diff 时也更清晰。\n\n\n# 长字符串\n\nJSON 作为配置格式的另一个问题是，它不支持多行字符串。如果你想在字符串中换行，必须使用 “\\n” 进行转义，更糟糕的是，如果你想要一个字符串在文件中另起一行显示，那就彻底没办法了。如果你的配置项里没有很长的字符串，那就不是问题。但是，如果你的配置项里包括了长字符串，例如项目描述或 GPG 密钥，你可能不希望只是使用 “\\n” 来转义而不是使用真实的换行符。\n\n\n# 数字\n\n此外，在某些情况下，JSON 对数字的定义可能会有问题。JSON 规范中将数字定义成使用十进制表示的任意精度有限浮点数。对于大多数应用程序来说，这没有问题。但是，如果你需要使用十六进制表示法或表示无穷大或 NaN 等值时，那么 TOML 或 YAML 将能够更好地处理它们。\n\n{\n\n  "name": "example",\n\n  "description": "A really long description that needs multiple lines.\\nThis is a sample project to illustrate why JSON is not a good configuration format. This description is pretty long, but it doesn\'t have any way to go onto multiple lines.",\n\n  "version": "0.0.1",\n\n  "main": "index.js",\n\n  "//": "This is as close to a comment as you are going to get",\n\n  "keywords": ["example", "config"],\n\n  "scripts": {\n\n    "test": "./test.sh",\n\n    "do_stuff": "./do_stuff.sh"\n\n  },\n\n  "bugs": {\n\n    "url": "https://example.com/bugs"\n\n  },\n\n  "contributors": [{\n\n    "name": "John Doe",\n\n    "email": "johndoe@example.com"\n\n  }, {\n\n    "name": "Ivy Lane",\n\n    "url": "https://example.com/ivylane"\n\n  }],\n\n  "dependencies": {\n\n    "dep1": "^1.0.0",\n\n    "dep2": "3.40",\n\n    "dep3": "6.7"\n\n  }\n\n}\n\n\n\n# JSON 的替代方案\n\n选择哪一种配置语言取决于你的应用程序。每种语言都有各自的优缺点，下面列出了一些可以考虑的选项。它们都是为配置而设计的语言，每一种都比 JSON 这样的数据语言更好。\n\nname = "example"\n\ndescription = """\n\nA really long description that needs multiple lines.\n\nThis is a sample project to illustrate why JSON is not a \\\n\ngood configuration format. This description is pretty long, \\\n\nbut it doesn\'t have any way to go onto multiple lines."""\n\n\n\nversion = "0.0.1"\n\nmain = "index.js"\n\n# This is a comment\n\nkeywords = ["example", "config"]\n\n\n\n[bugs]\n\nurl = "https://example.com/bugs"\n\n\n\n[scripts]\n\n\n\ntest = "./test.sh"\n\ndo_stuff = "./do_stuff.sh"\n\n\n\n[[contributors]]\n\nname = "John Doe"\n\nemail = "johndow@example.com"\n\n\n\n[[contributors]]\n\nname = "Ivy Lane"\n\nurl = "https://example.com/ivylane"\n\n\n\n[dependencies]\n\n\n\ndep1 = "^1.0.0"\n\n# Why we depend on dep2\n\ndep2 = "3.40"\n\ndep3 = "6.7"\n\n\n\n# HJSON\n\nHJSON 是一种基于 JSON 的格式，但具有更大的灵活性，可读性也更强。它支持注释、多行字符串、不带引号的键和字符串，以及可选的逗号。如果你想要 JSON 结构的简单性，同时对配置文件更友好，那么可以考虑 HJSON。有一些可以将 HJSON 转换为 JSON 的命令行工具，如果你使用的工具是基于 JSON 的，可以先用 HJSON 编写配置，然后再转换成 JSON。JSON5 是另一个与 HJSON 非常相似的配置语言。\n\n{\n\n  name: example\n\n  description: \'\'\'\n\n  A really long description that needs multiple lines.\n\n  This is a sample project to illustrate why JSON is \n\n  not a good configuration format.  This description \n\n  is pretty long, but it doesn\'t have any way to go \n\n  onto multiple lines.\n\n  \'\'\'\n\n  version: 0.0.1\n\n  main: index.js\n\n  # This is a a comment\n\n  keywords: ["example", "config"]\n\n  scripts: {\n\n    test: ./test.sh\n\n    do_stuff: ./do_stuff.sh\n\n  }\n\n  bugs: {\n\n    url: https://example.com/bugs\n\n  }\n\n  contributors: [{\n\n    name: John Doe\n\n    email: johndoe@example.com\n\n  } {\n\n    name: Ivy Lane\n\n    url: https://example.com/ivylane\n\n  }]\n\n  dependencies: {\n\n    dep1: ^1.0.0\n\n    # Why we have this dependency\n\n    dep2: "3.40"\n\n    dep3: "6.7"\n\n  }\n\n}\n\n\n\n# HOCON\n\nHOCON 是为 Play 框架设计的配置格式，在 Scala 项目中非常流行。它是 JSON 的超集，因此可以使用现有的 JSON 文件。除了注释、可选逗号和多行字符串这些标准特性外，HOCON 还支持从其他文件导入和引用其他值的键，避免重复代码，并使用以点作为分隔符的键来指定值的路径，因此用户可以不必将所有值直接放在花括号对象中。\n\nname = example\n\ndescription = """\n\nA really long description that needs multiple lines.\n\n\n\nThis is a sample project to illustrate why JSON is \n\nnot a good configuration format.  This description \n\nis pretty long, but it doesn\'t have any way to go \n\nonto multiple lines.\n\n"""\n\nversion = 0.0.1\n\nmain = index.js\n\n# This is a a comment\n\nkeywords = ["example", "config"]\n\nscripts {\n\n  test = ./test.sh\n\n  do_stuff = ./do_stuff.sh\n\n}\n\nbugs.url = "https://example.com/bugs"\n\ncontributors = [\n\n  {\n\n    name = John Doe\n\n    email = johndoe@example.com\n\n  }\n\n  {\n\n    name = Ivy Lane\n\n    url = "https://example.com/ivylane"\n\n  }\n\n]\n\ndependencies {\n\n  dep1 = ^1.0.0\n\n  # Why we have this dependency\n\n  dep2 = "3.40"\n\n  dep3 = "6.7"\n\n}\n\n\n\n# YAML\n\nYAML（YAML 不是标记语言）是一种非常灵活的格式，几乎是 JSON 的超集，已经被用在一些著名的项目中，如 Travis CI、Circle CI 和 AWS CloudFormation。YAML 的库几乎和 JSON 一样无处不在。除了支持注释、换行符分隔、多行字符串、裸字符串和更灵活的类型系统之外，YAML 也支持引用文件，以避免重复代码。\n\nYAML 的主要缺点是规范非常复杂，不同的实现之间可能存在不一致的情况。它将缩进视为严格语法的一部分（类似于 Python），有些人喜欢，有些人不喜欢。这会让复制和粘贴变得很麻烦。\n\n\n# 脚本语言\n\n如果你的应用程序是使用 Python 或 Ruby 等脚本语言开发的，并且你知道配置的来源是可靠的，那么最好的选择可能就是使用这些语言进行配置。如果你需要一个真正灵活的配置选项，也可以在编译语言中嵌入诸如 Lua 之类的脚本语言。这样可以获得脚本语言的灵活性，而且比使用不同的配置语言更容易实现。使用脚本语言的缺点是它可能过于强大，当然，如果配置来源是不受信任的，可能会引入严重的安全问题。\n\n\n# 自定义配置格式\n\n如果由于某种原因，键值配置格式不能满足你的要求，并且由于性能或大小限制而无法使用脚本语言，那么可以考虑自定义配置格式。如果是这种情况，那么在做出选择之前要想清楚，因为你不仅要编写和维护一个解析器，还要让你的用户熟悉另一种配置格式。\n\n\n# 结论\n\n有了这么多更好的配置语言，没有理由还要使用 JSON。如果要创建需要用到配置的新应用程序、框架或库，请选择 JSON 以外的其他选项。\n\n英文原文：https://www.lucidchart.com/techblog/2018/07/16/why-json-isnt-a-good-configuration-language/',normalizedContent:'# 附：为什么说 json 不适合做配置文件？\n\n提示\n\n很多项目使用 json 作为配置文件，最明显的例子就是 npm 和 yarn 使用的 package.json 文件。当然，还有很多其他文件，例如 cloudformation（最初只有 json，但现在也支持 yaml）和 composer（php）。\n\n但是，json 实际上是一种非常糟糕的配置语言。别误会我的意思，我其实是喜欢 json 的。它是一种相对灵活的文本格式，对于机器和人类来说都很容易阅读，而且是一种非常好的数据交换和存储格式。但作为一种配置语言，它有它的不足。\n\n\n# 为什么流行使用 json 作为配置语言？\n\n将 json 用作配置文件有几个方面的原因，其中最大的原因可能是它很容易实现。很多编程语言的标准库都支持 json，开发人员或用户可能已经很熟悉 json，所以不需要学习新的配置格式就可以使用那些产品。现在几乎所有的工具都提供 json 支持，包括语法突出显示、自动格式化、验证工具等。\n\n这些都是很好的理由，但这种无处不在的格式其实不适合用作配置。\n\n\n# json 的问题\n\n\n# 缺乏注释\n\n注释对于配置语言而言绝对是一个重要的功能。注释可用于标注不同的配置选项、解释为什么要配置成特定的值，更重要的是，在使用不同的配置进行测试和调试时需要临时注释掉部分配置。当然，如果只是把 json 当作是一种数据交换格式，那么就不需要用到注释。\n\n我们可以通过一些方法给 json 添加注释。一种常见的方法是在对象中使用特殊的键作为注释，例如“//”或“__comment”。但是，这种语法的可读性不高，并且为了在单个对象中包含多个注释，需要为每个注释使用唯一的键。david crockford（json 的发明者）建议使用预处理器来删除注释。如果你的应用程序需要使用 json 作为配置，那么完全没问题，不过这确实带来了一些额外的工作量。\n\n一些 json 库允许将注释作为输入。例如，ruby 的 json 模块和启用了 jsonparser.feature.allow_comments 功能的 java jackson 库可以处理 javascript 风格的注释。但是，这不是标准的方式，而且很多编辑器无法正确处理 json 文件中的注释，这让编辑它们变得更加困难。\n\n\n# 过于严格\n\njson 规范非常严格，这也是为什么实现 json 解析器会这么简单，但在我看来，它还会影响可读性，并且在较小程度上会影响可写性。\n\n\n# 低信噪比\n\n与其他配置语言相比，json 显得非常嘈杂。json 的很多标点符号对可读性毫无帮助，况且，对象中的键几乎都是标识符，所以键的引号其实是多余的。\n\n此外，json 需要使用花括号将整个文档包围起来，所以 json 是 javascript 的子集，并在流中发送多个对象时用于界定不同的对象。但是，对于配置文件来说，最外面的大括号其实没有任何用处。在配置文件中，键值对之间的逗号也是没有必要的。通常情况下，每行只有一个键值对，所以使用换行作为分隔符更有意义。\n\n说到逗号，json 居然不允许在结尾出现逗号。如果你需要在每个键值对之后使用逗号，那么至少应该接受结尾的逗号，因为有了结尾的逗号，在添加新条目时会更容易，而且在进行 commit diff 时也更清晰。\n\n\n# 长字符串\n\njson 作为配置格式的另一个问题是，它不支持多行字符串。如果你想在字符串中换行，必须使用 “\\n” 进行转义，更糟糕的是，如果你想要一个字符串在文件中另起一行显示，那就彻底没办法了。如果你的配置项里没有很长的字符串，那就不是问题。但是，如果你的配置项里包括了长字符串，例如项目描述或 gpg 密钥，你可能不希望只是使用 “\\n” 来转义而不是使用真实的换行符。\n\n\n# 数字\n\n此外，在某些情况下，json 对数字的定义可能会有问题。json 规范中将数字定义成使用十进制表示的任意精度有限浮点数。对于大多数应用程序来说，这没有问题。但是，如果你需要使用十六进制表示法或表示无穷大或 nan 等值时，那么 toml 或 yaml 将能够更好地处理它们。\n\n{\n\n  "name": "example",\n\n  "description": "a really long description that needs multiple lines.\\nthis is a sample project to illustrate why json is not a good configuration format. this description is pretty long, but it doesn\'t have any way to go onto multiple lines.",\n\n  "version": "0.0.1",\n\n  "main": "index.js",\n\n  "//": "this is as close to a comment as you are going to get",\n\n  "keywords": ["example", "config"],\n\n  "scripts": {\n\n    "test": "./test.sh",\n\n    "do_stuff": "./do_stuff.sh"\n\n  },\n\n  "bugs": {\n\n    "url": "https://example.com/bugs"\n\n  },\n\n  "contributors": [{\n\n    "name": "john doe",\n\n    "email": "johndoe@example.com"\n\n  }, {\n\n    "name": "ivy lane",\n\n    "url": "https://example.com/ivylane"\n\n  }],\n\n  "dependencies": {\n\n    "dep1": "^1.0.0",\n\n    "dep2": "3.40",\n\n    "dep3": "6.7"\n\n  }\n\n}\n\n\n\n# json 的替代方案\n\n选择哪一种配置语言取决于你的应用程序。每种语言都有各自的优缺点，下面列出了一些可以考虑的选项。它们都是为配置而设计的语言，每一种都比 json 这样的数据语言更好。\n\nname = "example"\n\ndescription = """\n\na really long description that needs multiple lines.\n\nthis is a sample project to illustrate why json is not a \\\n\ngood configuration format. this description is pretty long, \\\n\nbut it doesn\'t have any way to go onto multiple lines."""\n\n\n\nversion = "0.0.1"\n\nmain = "index.js"\n\n# this is a comment\n\nkeywords = ["example", "config"]\n\n\n\n[bugs]\n\nurl = "https://example.com/bugs"\n\n\n\n[scripts]\n\n\n\ntest = "./test.sh"\n\ndo_stuff = "./do_stuff.sh"\n\n\n\n[[contributors]]\n\nname = "john doe"\n\nemail = "johndow@example.com"\n\n\n\n[[contributors]]\n\nname = "ivy lane"\n\nurl = "https://example.com/ivylane"\n\n\n\n[dependencies]\n\n\n\ndep1 = "^1.0.0"\n\n# why we depend on dep2\n\ndep2 = "3.40"\n\ndep3 = "6.7"\n\n\n\n# hjson\n\nhjson 是一种基于 json 的格式，但具有更大的灵活性，可读性也更强。它支持注释、多行字符串、不带引号的键和字符串，以及可选的逗号。如果你想要 json 结构的简单性，同时对配置文件更友好，那么可以考虑 hjson。有一些可以将 hjson 转换为 json 的命令行工具，如果你使用的工具是基于 json 的，可以先用 hjson 编写配置，然后再转换成 json。json5 是另一个与 hjson 非常相似的配置语言。\n\n{\n\n  name: example\n\n  description: \'\'\'\n\n  a really long description that needs multiple lines.\n\n  this is a sample project to illustrate why json is \n\n  not a good configuration format.  this description \n\n  is pretty long, but it doesn\'t have any way to go \n\n  onto multiple lines.\n\n  \'\'\'\n\n  version: 0.0.1\n\n  main: index.js\n\n  # this is a a comment\n\n  keywords: ["example", "config"]\n\n  scripts: {\n\n    test: ./test.sh\n\n    do_stuff: ./do_stuff.sh\n\n  }\n\n  bugs: {\n\n    url: https://example.com/bugs\n\n  }\n\n  contributors: [{\n\n    name: john doe\n\n    email: johndoe@example.com\n\n  } {\n\n    name: ivy lane\n\n    url: https://example.com/ivylane\n\n  }]\n\n  dependencies: {\n\n    dep1: ^1.0.0\n\n    # why we have this dependency\n\n    dep2: "3.40"\n\n    dep3: "6.7"\n\n  }\n\n}\n\n\n\n# hocon\n\nhocon 是为 play 框架设计的配置格式，在 scala 项目中非常流行。它是 json 的超集，因此可以使用现有的 json 文件。除了注释、可选逗号和多行字符串这些标准特性外，hocon 还支持从其他文件导入和引用其他值的键，避免重复代码，并使用以点作为分隔符的键来指定值的路径，因此用户可以不必将所有值直接放在花括号对象中。\n\nname = example\n\ndescription = """\n\na really long description that needs multiple lines.\n\n\n\nthis is a sample project to illustrate why json is \n\nnot a good configuration format.  this description \n\nis pretty long, but it doesn\'t have any way to go \n\nonto multiple lines.\n\n"""\n\nversion = 0.0.1\n\nmain = index.js\n\n# this is a a comment\n\nkeywords = ["example", "config"]\n\nscripts {\n\n  test = ./test.sh\n\n  do_stuff = ./do_stuff.sh\n\n}\n\nbugs.url = "https://example.com/bugs"\n\ncontributors = [\n\n  {\n\n    name = john doe\n\n    email = johndoe@example.com\n\n  }\n\n  {\n\n    name = ivy lane\n\n    url = "https://example.com/ivylane"\n\n  }\n\n]\n\ndependencies {\n\n  dep1 = ^1.0.0\n\n  # why we have this dependency\n\n  dep2 = "3.40"\n\n  dep3 = "6.7"\n\n}\n\n\n\n# yaml\n\nyaml（yaml 不是标记语言）是一种非常灵活的格式，几乎是 json 的超集，已经被用在一些著名的项目中，如 travis ci、circle ci 和 aws cloudformation。yaml 的库几乎和 json 一样无处不在。除了支持注释、换行符分隔、多行字符串、裸字符串和更灵活的类型系统之外，yaml 也支持引用文件，以避免重复代码。\n\nyaml 的主要缺点是规范非常复杂，不同的实现之间可能存在不一致的情况。它将缩进视为严格语法的一部分（类似于 python），有些人喜欢，有些人不喜欢。这会让复制和粘贴变得很麻烦。\n\n\n# 脚本语言\n\n如果你的应用程序是使用 python 或 ruby 等脚本语言开发的，并且你知道配置的来源是可靠的，那么最好的选择可能就是使用这些语言进行配置。如果你需要一个真正灵活的配置选项，也可以在编译语言中嵌入诸如 lua 之类的脚本语言。这样可以获得脚本语言的灵活性，而且比使用不同的配置语言更容易实现。使用脚本语言的缺点是它可能过于强大，当然，如果配置来源是不受信任的，可能会引入严重的安全问题。\n\n\n# 自定义配置格式\n\n如果由于某种原因，键值配置格式不能满足你的要求，并且由于性能或大小限制而无法使用脚本语言，那么可以考虑自定义配置格式。如果是这种情况，那么在做出选择之前要想清楚，因为你不仅要编写和维护一个解析器，还要让你的用户熟悉另一种配置格式。\n\n\n# 结论\n\n有了这么多更好的配置语言，没有理由还要使用 json。如果要创建需要用到配置的新应用程序、框架或库，请选择 json 以外的其他选项。\n\n英文原文：https://www.lucidchart.com/techblog/2018/07/16/why-json-isnt-a-good-configuration-language/',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"docker磁盘空间不足解决办法",frontmatter:{title:"docker磁盘空间不足解决办法",date:"2022-02-11T14:32:31.000Z",permalink:"/pages/be0094/"},regularPath:"/17.Docker/09.%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/01.docker%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E4%B8%8D%E8%B6%B3%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95.html",relativePath:"17.Docker/09.常见问题/01.docker磁盘空间不足解决办法.md",key:"v-321c5958",path:"/pages/be0094/",headers:[{level:2,title:"导入docker镜像时，错误提示：磁盘空间不足。",slug:"导入docker镜像时-错误提示-磁盘空间不足。",normalizedTitle:"导入docker镜像时，错误提示：磁盘空间不足。",charIndex:23},{level:2,title:"常见docker清理方法",slug:"常见docker清理方法",normalizedTitle:"常见docker清理方法",charIndex:717}],headersStr:"导入docker镜像时，错误提示：磁盘空间不足。 常见docker清理方法",content:'# docker磁盘空间不足解决办法\n\n\n# 导入docker镜像时，错误提示：磁盘空间不足。\n\n1.查看docker镜像存放目录空间大小\n\ndu -hs /var/lib/docker/\n\n\n2.停止docker服务。\n\nsystemctl stop docker\n\n\n3.查看磁盘容量大的空间，且在上面创建新的docker目录。\n\ndf -h\nmkdir -p /data/docker/lib\n\n\n4.迁移/var/lib/docker目录下的文件到新创建的目录/data/docker/lib\n\nrsync -avz /var/lib/docker /data/docker/lib/\n\n\n5.编辑 /etc/docker/daemon.json 添加如下参数\n\n{\n  "graph": "/data/docker/lib/docker"\n}\n\n\n6.重新加载docker，并重启docker服务。\n\nsystemctl daemon-reload && systemctl restart docker\n\n\n7.检查docker是否变更为新目录/data/docker/lib/docker\n\n[root@localhost ~]# docker info\n\nDocker Root Dir: /data/docker/lib/docker\n\nDebug Mode (client): false\n\nDebug Mode (server): false\n\nRegistry: https://index.docker.io/v1/\n\n\n8.删掉docker旧目录\n\nrm -rf /var/lib/docker\n\n\n\n# 常见docker清理方法\n\n * docker system df 类似于Linux上的df命令，用于查看Docker的磁盘使用情况:\n\n * docker system prune 可以用于清理磁盘，删除关闭的容器、无用的数据卷和网络，以及dangling镜像(即无tag的镜像)。\n\n * docker system prune -a 清理得更加彻底，可以将没有容器使用Docker镜像都删掉。注意，这两个命令会把你暂时关闭的容器，以及暂时没有用到的Docker镜像都删掉了…所以使用之前一定要想清楚.。我没用过，因为会清理 没有开启的 Docker',normalizedContent:'# docker磁盘空间不足解决办法\n\n\n# 导入docker镜像时，错误提示：磁盘空间不足。\n\n1.查看docker镜像存放目录空间大小\n\ndu -hs /var/lib/docker/\n\n\n2.停止docker服务。\n\nsystemctl stop docker\n\n\n3.查看磁盘容量大的空间，且在上面创建新的docker目录。\n\ndf -h\nmkdir -p /data/docker/lib\n\n\n4.迁移/var/lib/docker目录下的文件到新创建的目录/data/docker/lib\n\nrsync -avz /var/lib/docker /data/docker/lib/\n\n\n5.编辑 /etc/docker/daemon.json 添加如下参数\n\n{\n  "graph": "/data/docker/lib/docker"\n}\n\n\n6.重新加载docker，并重启docker服务。\n\nsystemctl daemon-reload && systemctl restart docker\n\n\n7.检查docker是否变更为新目录/data/docker/lib/docker\n\n[root@localhost ~]# docker info\n\ndocker root dir: /data/docker/lib/docker\n\ndebug mode (client): false\n\ndebug mode (server): false\n\nregistry: https://index.docker.io/v1/\n\n\n8.删掉docker旧目录\n\nrm -rf /var/lib/docker\n\n\n\n# 常见docker清理方法\n\n * docker system df 类似于linux上的df命令，用于查看docker的磁盘使用情况:\n\n * docker system prune 可以用于清理磁盘，删除关闭的容器、无用的数据卷和网络，以及dangling镜像(即无tag的镜像)。\n\n * docker system prune -a 清理得更加彻底，可以将没有容器使用docker镜像都删掉。注意，这两个命令会把你暂时关闭的容器，以及暂时没有用到的docker镜像都删掉了…所以使用之前一定要想清楚.。我没用过，因为会清理 没有开启的 docker',charsets:{cjk:!0},lastUpdated:"2022/02/11, 14:49:04",lastUpdatedTimestamp:1644562144e3},{title:"什么是 Kubernetes",frontmatter:{title:"什么是 Kubernetes",date:"2022-02-09T11:48:30.000Z",permalink:"/pages/419ea5/"},regularPath:"/18.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/01.%E4%BB%80%E4%B9%88%E6%98%AF%20Kubernetes.html",relativePath:"18.Kubernetes/01.Kubernetes 入门/01.什么是 Kubernetes.md",key:"v-efa3c938",path:"/pages/419ea5/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:21},{level:2,title:"特点",slug:"特点",normalizedTitle:"特点",charIndex:284},{level:2,title:"从传统到容器化部署",slug:"从传统到容器化部署",normalizedTitle:"从传统到容器化部署",charIndex:380},{level:3,title:"传统的部署方式",slug:"传统的部署方式",normalizedTitle:"传统的部署方式",charIndex:396},{level:3,title:"容器化部署的优势",slug:"容器化部署的优势",normalizedTitle:"容器化部署的优势",charIndex:530},{level:2,title:"为什么需要 Kubernetes",slug:"为什么需要-kubernetes",normalizedTitle:"为什么需要 kubernetes",charIndex:870}],headersStr:"概述 特点 从传统到容器化部署 传统的部署方式 容器化部署的优势 为什么需要 Kubernetes",content:"# 什么是 Kubernetes\n\n\n# 概述\n\n\n\nKubernetes 是 Google 2014 年创建管理的，是 Google 10 多年大规模容器管理技术 Borg 的开源版本。\n\nKubernetes 是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。使用 Kubernetes 我们可以：\n\n * 快速部署应用\n * 快速扩展应用\n * 无缝对接新的应用功能\n * 节省资源，优化硬件资源的使用\n\nKubernetes 的目标是促进完善组件和工具的生态系统，以减轻应用程序在公有云或私有云中运行的负担。\n\n\n# 特点\n\n * 可移植： 支持公有云，私有云，混合云，多重云（多个公共云）\n * 可扩展： 模块化，插件化，可挂载，可组合\n * 自动化： 自动部署，自动重启，自动复制，自动伸缩/扩展\n\n\n# 从传统到容器化部署\n\n\n\n\n# 传统的部署方式\n\n传统的应用部署方式是通过插件或脚本来安装应用。这样做的缺点是应用的运行、配置、管理、所有生存周期将与当前操作系统绑定，这样做并不利于应用的升级更新/回滚等操作，当然也可以通过创建虚机的方式来实现某些功能，但是虚拟机非常重，并不利于可移植性。\n\n\n# 容器化部署的优势\n\n * 快速创建/部署应用： 与虚拟机相比，容器镜像的创建更加容易。\n * 持续开发、集成和部署： 提供可靠且频繁的容器镜像构建/部署，并使用快速和简单的回滚(由于镜像不可变性)。\n * 开发和运行相分离： 在 build 或者 release 阶段创建容器镜像，使得应用和基础设施解耦。\n * 开发，测试和生产环境一致性： 在本地或外网（生产环境）运行的一致性。\n * 云平台或其他操作系统： 可以在 Ubuntu、RHEL、CoreOS、on-prem、Google Container Engine 或其它任何环境中运行。\n * 分布式，弹性，微服务化： 应用程序分为更小的、独立的部件，可以动态部署和管理。\n * 资源隔离\n * 资源利用更高效\n\n\n# 为什么需要 Kubernetes\n\n可以在物理或虚拟机的 Kubernetes 集群上运行容器化应用，Kubernetes 能提供一个以 “容器为中心的基础架构”，满足在生产环境中运行应用的一些常见需求，如：\n\n * 多个进程协同工作\n * 存储系统挂载\n * 应用健康检查\n * 应用实例的复制\n * 自动伸缩/扩展\n * 注册与发现\n * 负载均衡\n * 滚动更新\n * 资源监控\n * 日志访问\n * 调试应用程序\n * 提供认证和授权",normalizedContent:"# 什么是 kubernetes\n\n\n# 概述\n\n\n\nkubernetes 是 google 2014 年创建管理的，是 google 10 多年大规模容器管理技术 borg 的开源版本。\n\nkubernetes 是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。使用 kubernetes 我们可以：\n\n * 快速部署应用\n * 快速扩展应用\n * 无缝对接新的应用功能\n * 节省资源，优化硬件资源的使用\n\nkubernetes 的目标是促进完善组件和工具的生态系统，以减轻应用程序在公有云或私有云中运行的负担。\n\n\n# 特点\n\n * 可移植： 支持公有云，私有云，混合云，多重云（多个公共云）\n * 可扩展： 模块化，插件化，可挂载，可组合\n * 自动化： 自动部署，自动重启，自动复制，自动伸缩/扩展\n\n\n# 从传统到容器化部署\n\n\n\n\n# 传统的部署方式\n\n传统的应用部署方式是通过插件或脚本来安装应用。这样做的缺点是应用的运行、配置、管理、所有生存周期将与当前操作系统绑定，这样做并不利于应用的升级更新/回滚等操作，当然也可以通过创建虚机的方式来实现某些功能，但是虚拟机非常重，并不利于可移植性。\n\n\n# 容器化部署的优势\n\n * 快速创建/部署应用： 与虚拟机相比，容器镜像的创建更加容易。\n * 持续开发、集成和部署： 提供可靠且频繁的容器镜像构建/部署，并使用快速和简单的回滚(由于镜像不可变性)。\n * 开发和运行相分离： 在 build 或者 release 阶段创建容器镜像，使得应用和基础设施解耦。\n * 开发，测试和生产环境一致性： 在本地或外网（生产环境）运行的一致性。\n * 云平台或其他操作系统： 可以在 ubuntu、rhel、coreos、on-prem、google container engine 或其它任何环境中运行。\n * 分布式，弹性，微服务化： 应用程序分为更小的、独立的部件，可以动态部署和管理。\n * 资源隔离\n * 资源利用更高效\n\n\n# 为什么需要 kubernetes\n\n可以在物理或虚拟机的 kubernetes 集群上运行容器化应用，kubernetes 能提供一个以 “容器为中心的基础架构”，满足在生产环境中运行应用的一些常见需求，如：\n\n * 多个进程协同工作\n * 存储系统挂载\n * 应用健康检查\n * 应用实例的复制\n * 自动伸缩/扩展\n * 注册与发现\n * 负载均衡\n * 滚动更新\n * 资源监控\n * 日志访问\n * 调试应用程序\n * 提供认证和授权",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Kubernetes 安装前的准备",frontmatter:{title:"Kubernetes 安装前的准备",date:"2022-02-09T11:48:30.000Z",permalink:"/pages/b2e7fa/"},regularPath:"/18.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/02.Kubernetes%20%E5%AE%89%E8%A3%85%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87.html",relativePath:"18.Kubernetes/01.Kubernetes 入门/02.Kubernetes 安装前的准备.md",key:"v-255f6b29",path:"/pages/b2e7fa/",headers:[{level:2,title:"前提条件",slug:"前提条件",normalizedTitle:"前提条件",charIndex:58},{level:2,title:"修改主机名",slug:"修改主机名",normalizedTitle:"修改主机名",charIndex:1201},{level:2,title:"修改/etc/hosts文件",slug:"修改-etc-hosts文件",normalizedTitle:"修改/etc/hosts文件",charIndex:1378},{level:2,title:"关闭防火墙和selinux",slug:"关闭防火墙和selinux",normalizedTitle:"关闭防火墙和selinux",charIndex:1507},{level:2,title:"关闭swap",slug:"关闭swap",normalizedTitle:"关闭swap",charIndex:1668},{level:2,title:"配置时间同步",slug:"配置时间同步",normalizedTitle:"配置时间同步",charIndex:2042},{level:2,title:"配置所有node节点",slug:"配置所有node节点",normalizedTitle:"配置所有node节点",charIndex:2535},{level:2,title:"修改iptables相关参数",slug:"修改iptables相关参数",normalizedTitle:"修改iptables相关参数",charIndex:3166},{level:2,title:"加载ipvs相关模块",slug:"加载ipvs相关模块",normalizedTitle:"加载ipvs相关模块",charIndex:3497},{level:2,title:"安装 Docker",slug:"安装-docker",normalizedTitle:"安装 docker",charIndex:4135},{level:3,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:13},{level:3,title:"验证",slug:"验证",normalizedTitle:"验证",charIndex:4576},{level:3,title:"配置加速器",slug:"配置加速器",normalizedTitle:"配置加速器",charIndex:5421}],headersStr:"前提条件 修改主机名 修改/etc/hosts文件 关闭防火墙和selinux 关闭swap 配置时间同步 配置所有node节点 修改iptables相关参数 加载ipvs相关模块 安装 Docker 安装 验证 配置加速器",content:"# Kubernetes 安装前的准备\n\n官方文档\n\n安装 kubeadm 使用 kubeadm 创建集群\n\n\n# 前提条件\n\n本次安装采用 CentOS 7 版本安装 kubernetes 集群环境，集群节点为 1 主 2 从模式，此次对虚拟机会有些基本要求，如下：\n\n * OS：CentOS 7.9\n * CPU：最低要求，1 CPU 2 核\n * 内存：最低要求，2GB\n * 磁盘：最低要求，20GB\n * 网络：可以访问外网，且集群中的所有机器的网络可以相互通信\n\n基于Ubuntu 部署K8S教程\n\n请点击这里\n\n创建三台虚拟机，分别命名如下：\n\n * k8s-master\n * k8s-node1\n * k8s-node2\n\n主机名          IP地址              角色       OS          组件                         配置\nk8s-master   192.168.199.101   master   CentOS7.7   kube-apiserver             2核2G\n                                                    kube-controller-manasger\n                                                    kube-scheduler\n                                                    kube-proxy\n                                                    etcd\n                                                    coredns\n                                                    kube-flannel\nk8s-node1    192.168.199.102   node     CentOS7.7   kube-proxy                 2核2G\n                                                    kube-flannel\nk8s-node2    192.168.199.103   node     CentOS7.7   kube-proxy                 2核2G\n                                                    kube-flannel\n\n提示\n\n无特殊说明以下操作在所有节点执行\n\n\n# 修改主机名\n\n在同一局域网中主机名不应该相同，所以我们需要做修改。\n\n#master节点:\nhostnamectl set-hostname k8s-master\n#node1节点：\nhostnamectl set-hostname k8s-node1\n#node2节点:\nhostnamectl set-hostname k8s-node2\n\n\n\n# 修改/etc/hosts文件\n\ncat >> /etc/hosts << EOF\n192.168.199.101 k8s-master\n192.168.199.102 k8s-node1\n192.168.199.103 k8s-node2\nEOF\n\n\n\n# 关闭防火墙和selinux\n\nsystemctl stop firewalld && systemctl disable firewalld\nsed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config && setenforce 0\n\n\n\n# 关闭swap\n\nswapoff -a\nyes | cp /etc/fstab /etc/fstab_bak\ncat /etc/fstab_bak |grep -v swap > /etc/fstab\n\n\n使用free -h命令检查配置是否生效\n\n\n\n\n\n \n\n\n[root@k8s-master etc]# free -h   \n              total        used        free      shared  buff/cache   available\nMem:           1.8G        142M        1.5G        8.5M        122M        1.5G\nSwap:            0B          0B          0B\n\n\n\n# 配置时间同步\n\n使用chrony同步时间，配置master节点与网络NTP服务器同步时间，所有node节点与master节点同步时间。\n\n配置master节点：\n\n#安装chrony：\nyum install -y chrony\n#注释默认ntp服务器\nsed -i 's/^server/#&/' /etc/chrony.conf\n#指定上游公共 ntp 服务器，并允许其他节点同步时间\ncat >> /etc/chrony.conf << EOF\nserver 0.asia.pool.ntp.org iburst\nserver 1.asia.pool.ntp.org iburst\nserver 2.asia.pool.ntp.org iburst\nserver 3.asia.pool.ntp.org iburst\nallow all\nEOF\n#重启chronyd服务并设为开机启动：\nsystemctl enable chronyd && systemctl restart chronyd\n#开启网络时间同步功能\ntimedatectl set-ntp true\n\n\n\n# 配置所有node节点\n\n(注意修改master IP地址)\n\n\n\n\n\n\n\n \n\n\n\n\n#安装chrony：\nyum install -y chrony\n#注释默认服务器\nsed -i 's/^server/#&/' /etc/chrony.conf\n#指定内网 master节点为上游NTP服务器\necho server 192.168.199.101 iburst >> /etc/chrony.conf\n#重启服务并设为开机启动：\nsystemctl enable chronyd && systemctl restart chronyd\n\n\n所有节点执行chronyc sources命令，查看存在以^*开头的行，说明已经与服务器时间同步\n\n[root@localhost ~]# chronyc sources\n210 Number of sources = 1\nMS Name/IP address         Stratum Poll Reach LastRx Last sample               \n===============================================================================\n^* k8s-master                    3   6     7     0   +358ns[  -18ms] +/-   36ms\n\n\n\n# 修改iptables相关参数\n\nRHEL/CentOS 7上的一些用户报告了由于iptables被绕过而导致流量路由不正确的问题。创建/etc/sysctl.d/k8s.conf文件，添加如下内容：\n\ncat <<EOF >  /etc/sysctl.d/k8s.conf\nvm.swappiness = 0\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n\n# 使配置生效\nmodprobe br_netfilter\nsysctl -p /etc/sysctl.d/k8s.conf\n\n\n\n# 加载ipvs相关模块\n\n由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块： 在所有的Kubernetes节点执行以下脚本:\n\ncat > /etc/sysconfig/modules/ipvs.modules <<EOF\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nEOF\n\n#执行脚本\nchmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4\n\n\n上面脚本创建了/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。 使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块。 接下来还需要确保各个节点上已经安装了ipset软件包。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm。\n\nyum install ipset ipvsadm -y\n\n\n\n# 安装 Docker\n\nKubernetes默认的容器运行时仍然是Docker，使用的是kubelet中内置dockershim CRI实现。\n\n\n# 安装\n\n# 安装所需依赖\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n# 配置docker yum源\nsudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n# 安装 Docker Engine-Community\nsudo yum install docker-ce docker-ce-cli containerd.io\n# 启动 Docker CE 并设置开机自启动\nsudo systemctl start docker && sudo systemctl enable docker\n\n\n\n# 验证\n\n[root@k8s-master ~]# docker version  \nClient: Docker Engine - Community\n Version:           19.03.8\n API version:       1.40\n Go version:        go1.12.17\n Git commit:        afacb8b\n Built:             Wed Mar 11 01:27:04 2020\n OS/Arch:           linux/amd64\n Experimental:      false\n\nServer: Docker Engine - Community\n Engine:\n  Version:          19.03.8\n  API version:      1.40 (minimum version 1.12)\n  Go version:       go1.12.17\n  Git commit:       afacb8b\n  Built:            Wed Mar 11 01:25:42 2020\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.2.13\n  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429\n runc:\n  Version:          1.0.0-rc10\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\n docker-init:\n  Version:          0.18.0\n  GitCommit:        fec3683\n\n\n\n# 配置加速器\n\n对于使用 systemd 的系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件）\n\ncat > /etc/docker/daemon.json <<EOF\n{\n  \"registry-mirrors\": [\n    \"https://registry.docker-cn.com\"\n  ]\n}\nEOF\n\n\n验证加速器是否配置成功：\n\n\n\n\n\n\n\n \n\n\n\nsudo systemctl restart docker\ndocker info\n...\n# 出现如下语句即表示配置成功\nRegistry Mirrors:\n https://registry.docker-cn.com/\n...\n",normalizedContent:"# kubernetes 安装前的准备\n\n官方文档\n\n安装 kubeadm 使用 kubeadm 创建集群\n\n\n# 前提条件\n\n本次安装采用 centos 7 版本安装 kubernetes 集群环境，集群节点为 1 主 2 从模式，此次对虚拟机会有些基本要求，如下：\n\n * os：centos 7.9\n * cpu：最低要求，1 cpu 2 核\n * 内存：最低要求，2gb\n * 磁盘：最低要求，20gb\n * 网络：可以访问外网，且集群中的所有机器的网络可以相互通信\n\n基于ubuntu 部署k8s教程\n\n请点击这里\n\n创建三台虚拟机，分别命名如下：\n\n * k8s-master\n * k8s-node1\n * k8s-node2\n\n主机名          ip地址              角色       os          组件                         配置\nk8s-master   192.168.199.101   master   centos7.7   kube-apiserver             2核2g\n                                                    kube-controller-manasger\n                                                    kube-scheduler\n                                                    kube-proxy\n                                                    etcd\n                                                    coredns\n                                                    kube-flannel\nk8s-node1    192.168.199.102   node     centos7.7   kube-proxy                 2核2g\n                                                    kube-flannel\nk8s-node2    192.168.199.103   node     centos7.7   kube-proxy                 2核2g\n                                                    kube-flannel\n\n提示\n\n无特殊说明以下操作在所有节点执行\n\n\n# 修改主机名\n\n在同一局域网中主机名不应该相同，所以我们需要做修改。\n\n#master节点:\nhostnamectl set-hostname k8s-master\n#node1节点：\nhostnamectl set-hostname k8s-node1\n#node2节点:\nhostnamectl set-hostname k8s-node2\n\n\n\n# 修改/etc/hosts文件\n\ncat >> /etc/hosts << eof\n192.168.199.101 k8s-master\n192.168.199.102 k8s-node1\n192.168.199.103 k8s-node2\neof\n\n\n\n# 关闭防火墙和selinux\n\nsystemctl stop firewalld && systemctl disable firewalld\nsed -i 's/^selinux=enforcing$/selinux=disabled/' /etc/selinux/config && setenforce 0\n\n\n\n# 关闭swap\n\nswapoff -a\nyes | cp /etc/fstab /etc/fstab_bak\ncat /etc/fstab_bak |grep -v swap > /etc/fstab\n\n\n使用free -h命令检查配置是否生效\n\n\n\n\n\n \n\n\n[root@k8s-master etc]# free -h   \n              total        used        free      shared  buff/cache   available\nmem:           1.8g        142m        1.5g        8.5m        122m        1.5g\nswap:            0b          0b          0b\n\n\n\n# 配置时间同步\n\n使用chrony同步时间，配置master节点与网络ntp服务器同步时间，所有node节点与master节点同步时间。\n\n配置master节点：\n\n#安装chrony：\nyum install -y chrony\n#注释默认ntp服务器\nsed -i 's/^server/#&/' /etc/chrony.conf\n#指定上游公共 ntp 服务器，并允许其他节点同步时间\ncat >> /etc/chrony.conf << eof\nserver 0.asia.pool.ntp.org iburst\nserver 1.asia.pool.ntp.org iburst\nserver 2.asia.pool.ntp.org iburst\nserver 3.asia.pool.ntp.org iburst\nallow all\neof\n#重启chronyd服务并设为开机启动：\nsystemctl enable chronyd && systemctl restart chronyd\n#开启网络时间同步功能\ntimedatectl set-ntp true\n\n\n\n# 配置所有node节点\n\n(注意修改master ip地址)\n\n\n\n\n\n\n\n \n\n\n\n\n#安装chrony：\nyum install -y chrony\n#注释默认服务器\nsed -i 's/^server/#&/' /etc/chrony.conf\n#指定内网 master节点为上游ntp服务器\necho server 192.168.199.101 iburst >> /etc/chrony.conf\n#重启服务并设为开机启动：\nsystemctl enable chronyd && systemctl restart chronyd\n\n\n所有节点执行chronyc sources命令，查看存在以^*开头的行，说明已经与服务器时间同步\n\n[root@localhost ~]# chronyc sources\n210 number of sources = 1\nms name/ip address         stratum poll reach lastrx last sample               \n===============================================================================\n^* k8s-master                    3   6     7     0   +358ns[  -18ms] +/-   36ms\n\n\n\n# 修改iptables相关参数\n\nrhel/centos 7上的一些用户报告了由于iptables被绕过而导致流量路由不正确的问题。创建/etc/sysctl.d/k8s.conf文件，添加如下内容：\n\ncat <<eof >  /etc/sysctl.d/k8s.conf\nvm.swappiness = 0\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\neof\n\n# 使配置生效\nmodprobe br_netfilter\nsysctl -p /etc/sysctl.d/k8s.conf\n\n\n\n# 加载ipvs相关模块\n\n由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块： 在所有的kubernetes节点执行以下脚本:\n\ncat > /etc/sysconfig/modules/ipvs.modules <<eof\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\neof\n\n#执行脚本\nchmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4\n\n\n上面脚本创建了/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。 使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块。 接下来还需要确保各个节点上已经安装了ipset软件包。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm。\n\nyum install ipset ipvsadm -y\n\n\n\n# 安装 docker\n\nkubernetes默认的容器运行时仍然是docker，使用的是kubelet中内置dockershim cri实现。\n\n\n# 安装\n\n# 安装所需依赖\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n# 配置docker yum源\nsudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n# 安装 docker engine-community\nsudo yum install docker-ce docker-ce-cli containerd.io\n# 启动 docker ce 并设置开机自启动\nsudo systemctl start docker && sudo systemctl enable docker\n\n\n\n# 验证\n\n[root@k8s-master ~]# docker version  \nclient: docker engine - community\n version:           19.03.8\n api version:       1.40\n go version:        go1.12.17\n git commit:        afacb8b\n built:             wed mar 11 01:27:04 2020\n os/arch:           linux/amd64\n experimental:      false\n\nserver: docker engine - community\n engine:\n  version:          19.03.8\n  api version:      1.40 (minimum version 1.12)\n  go version:       go1.12.17\n  git commit:       afacb8b\n  built:            wed mar 11 01:25:42 2020\n  os/arch:          linux/amd64\n  experimental:     false\n containerd:\n  version:          1.2.13\n  gitcommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429\n runc:\n  version:          1.0.0-rc10\n  gitcommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\n docker-init:\n  version:          0.18.0\n  gitcommit:        fec3683\n\n\n\n# 配置加速器\n\n对于使用 systemd 的系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件）\n\ncat > /etc/docker/daemon.json <<eof\n{\n  \"registry-mirrors\": [\n    \"https://registry.docker-cn.com\"\n  ]\n}\neof\n\n\n验证加速器是否配置成功：\n\n\n\n\n\n\n\n \n\n\n\nsudo systemctl restart docker\ndocker info\n...\n# 出现如下语句即表示配置成功\nregistry mirrors:\n https://registry.docker-cn.com/\n...\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"安装 kubeadm",frontmatter:{title:"安装 kubeadm",date:"2022-02-09T11:48:30.000Z",permalink:"/pages/babf51/"},regularPath:"/18.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/03.%E5%AE%89%E8%A3%85%20kubeadm.html",relativePath:"18.Kubernetes/01.Kubernetes 入门/03.安装 kubeadm.md",key:"v-28726c01",path:"/pages/babf51/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:17},{level:2,title:"配置软件源",slug:"配置软件源",normalizedTitle:"配置软件源",charIndex:76},{level:2,title:"安装 kubeadm，kubelet，kubectl",slug:"安装-kubeadm-kubelet-kubectl",normalizedTitle:"安装 kubeadm，kubelet，kubectl",charIndex:458},{level:2,title:"设置 kubelet 自启动，并启动 kubelet",slug:"设置-kubelet-自启动-并启动-kubelet",normalizedTitle:"设置 kubelet 自启动，并启动 kubelet",charIndex:11490}],headersStr:"概述 配置软件源 安装 kubeadm，kubelet，kubectl 设置 kubelet 自启动，并启动 kubelet",content:'# 安装 kubeadm\n\n\n# 概述\n\nkubeadm 是 kubernetes 的集群安装工具，能够快速安装 kubernetes 集群。\n\n\n# 配置软件源\n\n配置kubernetes.repo的源，由于官方源国内无法访问，这里使用阿里云yum源\n\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n\n\n\n# 安装 kubeadm，kubelet，kubectl\n\n在所有节点上安装指定版本 kubelet、kubeadm 和 kubectl\n\nyum install -y kubelet kubeadm kubectl\n\n# 安装过程如下，注意 kubeadm 的版本号\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirrors.huaweicloud.com\n * extras: mirrors.huaweicloud.com\n * updates: mirrors.huaweicloud.com\nkubernetes/signature                                                                                    |  454 B  00:00:00     \nRetrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nImporting GPG key 0xA7317B0F:\n Userid     : "Google Cloud Packages Automatic Signing Key <gc-team@google.com>"\n Fingerprint: d0bc 747f d8ca f711 7500 d6fa 3746 c208 a731 7b0f\n From       : https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nRetrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nkubernetes/signature                                                                                    | 1.4 kB  00:00:00 !!! \nkubernetes/primary                                                                                      |  67 kB  00:00:00     \nkubernetes                                                                                                             493/493\nResolving Dependencies\n--\x3e Running transaction check\n---\x3e Package kubeadm.x86_64 0:1.18.2-0 will be installed\n--\x3e Processing Dependency: kubernetes-cni >= 0.7.5 for package: kubeadm-1.18.2-0.x86_64\n--\x3e Processing Dependency: cri-tools >= 1.13.0 for package: kubeadm-1.18.2-0.x86_64\n---\x3e Package kubectl.x86_64 0:1.18.2-0 will be installed\n---\x3e Package kubelet.x86_64 0:1.18.2-0 will be installed\n--\x3e Processing Dependency: socat for package: kubelet-1.18.2-0.x86_64\n--\x3e Processing Dependency: conntrack for package: kubelet-1.18.2-0.x86_64\n--\x3e Running transaction check\n---\x3e Package conntrack-tools.x86_64 0:1.4.4-5.el7_7.2 will be installed\n--\x3e Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.1)(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n--\x3e Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.0)(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n--\x3e Processing Dependency: libnetfilter_cthelper.so.0(LIBNETFILTER_CTHELPER_1.0)(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n--\x3e Processing Dependency: libnetfilter_queue.so.1()(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n--\x3e Processing Dependency: libnetfilter_cttimeout.so.1()(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n--\x3e Processing Dependency: libnetfilter_cthelper.so.0()(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n---\x3e Package cri-tools.x86_64 0:1.13.0-0 will be installed\n---\x3e Package kubernetes-cni.x86_64 0:0.7.5-0 will be installed\n---\x3e Package socat.x86_64 0:1.7.3.2-2.el7 will be installed\n--\x3e Running transaction check\n---\x3e Package libnetfilter_cthelper.x86_64 0:1.0.0-10.el7_7.1 will be installed\n---\x3e Package libnetfilter_cttimeout.x86_64 0:1.0.0-6.el7_7.1 will be installed\n---\x3e Package libnetfilter_queue.x86_64 0:1.0.2-2.el7_2 will be installed\n--\x3e Finished Dependency Resolution\n\nDependencies Resolved\n\n===============================================================================================================================\n Package                               Arch                  Version                           Repository                 Size\n===============================================================================================================================\nInstalling:\n kubeadm                               x86_64                1.18.2-0                          kubernetes                8.8 M\n kubectl                               x86_64                1.18.2-0                          kubernetes                9.5 M\n kubelet                               x86_64                1.18.2-0                          kubernetes                 21 M\nInstalling for dependencies:\n conntrack-tools                       x86_64                1.4.4-5.el7_7.2                   updates                   187 k\n cri-tools                             x86_64                1.13.0-0                          kubernetes                5.1 M\n kubernetes-cni                        x86_64                0.7.5-0                           kubernetes                 10 M\n libnetfilter_cthelper                 x86_64                1.0.0-10.el7_7.1                  updates                    18 k\n libnetfilter_cttimeout                x86_64                1.0.0-6.el7_7.1                   updates                    18 k\n libnetfilter_queue                    x86_64                1.0.2-2.el7_2                     base                       23 k\n socat                                 x86_64                1.7.3.2-2.el7                     base                      290 k\n\nTransaction Summary\n===============================================================================================================================\nInstall  3 Packages (+7 Dependent packages)\n\nTotal download size: 55 M\nInstalled size: 246 M\nDownloading packages:\n(1/10): conntrack-tools-1.4.4-5.el7_7.2.x86_64.rpm                                                      | 187 kB  00:00:00     \nwarning: /var/cache/yum/x86_64/7/kubernetes/packages/14bfe6e75a9efc8eca3f638eb22c7e2ce759c67f95b43b16fae4ebabde1549f3-cri-tools-1.13.0-0.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 3e1ba8d5: NOKEY\nPublic key for 14bfe6e75a9efc8eca3f638eb22c7e2ce759c67f95b43b16fae4ebabde1549f3-cri-tools-1.13.0-0.x86_64.rpm is not installed\n(2/10): 14bfe6e75a9efc8eca3f638eb22c7e2ce759c67f95b43b16fae4ebabde1549f3-cri-tools-1.13.0-0.x86_64.rpm  | 5.1 MB  00:00:01     \n(3/10): 515b36bce35de42218470b171ae2ba5cd82132e63d98c7bb87e4298d61fde1dc-kubeadm-1.18.2-0.x86_64.rpm    | 8.8 MB  00:00:01     \n(4/10): 29337f2b1dfd3e11eb9465dbf87b35a1cc48ff0a5e8f992c9f1da70f7641c6bf-kubectl-1.18.2-0.x86_64.rpm    | 9.5 MB  00:00:01     \n(5/10): libnetfilter_cthelper-1.0.0-10.el7_7.1.x86_64.rpm                                               |  18 kB  00:00:00     \n(6/10): libnetfilter_cttimeout-1.0.0-6.el7_7.1.x86_64.rpm                                               |  18 kB  00:00:00     \n(7/10): libnetfilter_queue-1.0.2-2.el7_2.x86_64.rpm                                                     |  23 kB  00:00:00     \n(8/10): socat-1.7.3.2-2.el7.x86_64.rpm                                                                  | 290 kB  00:00:00     \n(9/10): 548a0dcd865c16a50980420ddfa5fbccb8b59621179798e6dc905c9bf8af3b34-kubernetes-cni-0.7.5-0.x86_64. |  10 MB  00:00:02     \n(10/10): d9c0a14c480bd39ac0890746493f9a01325e9ef8fc6fc923520bfc7d0f11744e-kubelet-1.18.2-0.x86_64.rpm   |  21 MB  00:00:04     \n-------------------------------------------------------------------------------------------------------------------------------\nTotal                                                                                          8.5 MB/s |  55 MB  00:00:06     \nRetrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nImporting GPG key 0xA7317B0F:\n Userid     : "Google Cloud Packages Automatic Signing Key <gc-team@google.com>"\n Fingerprint: d0bc 747f d8ca f711 7500 d6fa 3746 c208 a731 7b0f\n From       : https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nRetrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nImporting GPG key 0x3E1BA8D5:\n Userid     : "Google Cloud Packages RPM Signing Key <gc-team@google.com>"\n Fingerprint: 3749 e1ba 95a8 6ce0 5454 6ed2 f09c 394c 3e1b a8d5\n From       : https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nRunning transaction check\nRunning transaction test\nTransaction test succeeded\nRunning transaction\n  Installing : libnetfilter_cthelper-1.0.0-10.el7_7.1.x86_64                                                              1/10 \n  Installing : libnetfilter_cttimeout-1.0.0-6.el7_7.1.x86_64                                                              2/10 \n  Installing : kubectl-1.18.2-0.x86_64                                                                                    3/10 \n  Installing : socat-1.7.3.2-2.el7.x86_64                                                                                 4/10 \n  Installing : cri-tools-1.13.0-0.x86_64                                                                                  5/10 \n  Installing : libnetfilter_queue-1.0.2-2.el7_2.x86_64                                                                    6/10 \n  Installing : conntrack-tools-1.4.4-5.el7_7.2.x86_64                                                                     7/10 \n  Installing : kubernetes-cni-0.7.5-0.x86_64                                                                              8/10 \n  Installing : kubelet-1.18.2-0.x86_64                                                                                    9/10 \n  Installing : kubeadm-1.18.2-0.x86_64                                                                                   10/10 \n  Verifying  : kubeadm-1.18.2-0.x86_64                                                                                    1/10 \n  Verifying  : conntrack-tools-1.4.4-5.el7_7.2.x86_64                                                                     2/10 \n  Verifying  : libnetfilter_queue-1.0.2-2.el7_2.x86_64                                                                    3/10 \n  Verifying  : kubelet-1.18.2-0.x86_64                                                                                    4/10 \n  Verifying  : cri-tools-1.13.0-0.x86_64                                                                                  5/10 \n  Verifying  : kubernetes-cni-0.7.5-0.x86_64                                                                              6/10 \n  Verifying  : socat-1.7.3.2-2.el7.x86_64                                                                                 7/10 \n  Verifying  : kubectl-1.18.2-0.x86_64                                                                                    8/10 \n  Verifying  : libnetfilter_cttimeout-1.0.0-6.el7_7.1.x86_64                                                              9/10 \n  Verifying  : libnetfilter_cthelper-1.0.0-10.el7_7.1.x86_64                                                             10/10 \n\nInstalled:\n  kubeadm.x86_64 0:1.18.2-0                 kubectl.x86_64 0:1.18.2-0                 kubelet.x86_64 0:1.18.2-0                \n\nDependency Installed:\n  conntrack-tools.x86_64 0:1.4.4-5.el7_7.2                       cri-tools.x86_64 0:1.13.0-0                                   \n  kubernetes-cni.x86_64 0:0.7.5-0                                libnetfilter_cthelper.x86_64 0:1.0.0-10.el7_7.1               \n  libnetfilter_cttimeout.x86_64 0:1.0.0-6.el7_7.1                libnetfilter_queue.x86_64 0:1.0.2-2.el7_2                     \n  socat.x86_64 0:1.7.3.2-2.el7                                  \n\nComplete!\n\n\n * kubeadm：用于初始化 Kubernetes 集群\n * kubectl：Kubernetes 的命令行工具，主要作用是部署和管理应用，查看各种资源，创建，删除和更新组件\n * kubelet：主要负责启动 Pod 和容器\n\n\n# 设置 kubelet 自启动，并启动 kubelet\n\nsystemctl enable kubelet && systemctl start kubelet\n',normalizedContent:'# 安装 kubeadm\n\n\n# 概述\n\nkubeadm 是 kubernetes 的集群安装工具，能够快速安装 kubernetes 集群。\n\n\n# 配置软件源\n\n配置kubernetes.repo的源，由于官方源国内无法访问，这里使用阿里云yum源\n\ncat <<eof > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\neof\n\n\n\n# 安装 kubeadm，kubelet，kubectl\n\n在所有节点上安装指定版本 kubelet、kubeadm 和 kubectl\n\nyum install -y kubelet kubeadm kubectl\n\n# 安装过程如下，注意 kubeadm 的版本号\nloaded plugins: fastestmirror\nloading mirror speeds from cached hostfile\n * base: mirrors.huaweicloud.com\n * extras: mirrors.huaweicloud.com\n * updates: mirrors.huaweicloud.com\nkubernetes/signature                                                                                    |  454 b  00:00:00     \nretrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nimporting gpg key 0xa7317b0f:\n userid     : "google cloud packages automatic signing key <gc-team@google.com>"\n fingerprint: d0bc 747f d8ca f711 7500 d6fa 3746 c208 a731 7b0f\n from       : https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nretrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nkubernetes/signature                                                                                    | 1.4 kb  00:00:00 !!! \nkubernetes/primary                                                                                      |  67 kb  00:00:00     \nkubernetes                                                                                                             493/493\nresolving dependencies\n--\x3e running transaction check\n---\x3e package kubeadm.x86_64 0:1.18.2-0 will be installed\n--\x3e processing dependency: kubernetes-cni >= 0.7.5 for package: kubeadm-1.18.2-0.x86_64\n--\x3e processing dependency: cri-tools >= 1.13.0 for package: kubeadm-1.18.2-0.x86_64\n---\x3e package kubectl.x86_64 0:1.18.2-0 will be installed\n---\x3e package kubelet.x86_64 0:1.18.2-0 will be installed\n--\x3e processing dependency: socat for package: kubelet-1.18.2-0.x86_64\n--\x3e processing dependency: conntrack for package: kubelet-1.18.2-0.x86_64\n--\x3e running transaction check\n---\x3e package conntrack-tools.x86_64 0:1.4.4-5.el7_7.2 will be installed\n--\x3e processing dependency: libnetfilter_cttimeout.so.1(libnetfilter_cttimeout_1.1)(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n--\x3e processing dependency: libnetfilter_cttimeout.so.1(libnetfilter_cttimeout_1.0)(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n--\x3e processing dependency: libnetfilter_cthelper.so.0(libnetfilter_cthelper_1.0)(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n--\x3e processing dependency: libnetfilter_queue.so.1()(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n--\x3e processing dependency: libnetfilter_cttimeout.so.1()(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n--\x3e processing dependency: libnetfilter_cthelper.so.0()(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64\n---\x3e package cri-tools.x86_64 0:1.13.0-0 will be installed\n---\x3e package kubernetes-cni.x86_64 0:0.7.5-0 will be installed\n---\x3e package socat.x86_64 0:1.7.3.2-2.el7 will be installed\n--\x3e running transaction check\n---\x3e package libnetfilter_cthelper.x86_64 0:1.0.0-10.el7_7.1 will be installed\n---\x3e package libnetfilter_cttimeout.x86_64 0:1.0.0-6.el7_7.1 will be installed\n---\x3e package libnetfilter_queue.x86_64 0:1.0.2-2.el7_2 will be installed\n--\x3e finished dependency resolution\n\ndependencies resolved\n\n===============================================================================================================================\n package                               arch                  version                           repository                 size\n===============================================================================================================================\ninstalling:\n kubeadm                               x86_64                1.18.2-0                          kubernetes                8.8 m\n kubectl                               x86_64                1.18.2-0                          kubernetes                9.5 m\n kubelet                               x86_64                1.18.2-0                          kubernetes                 21 m\ninstalling for dependencies:\n conntrack-tools                       x86_64                1.4.4-5.el7_7.2                   updates                   187 k\n cri-tools                             x86_64                1.13.0-0                          kubernetes                5.1 m\n kubernetes-cni                        x86_64                0.7.5-0                           kubernetes                 10 m\n libnetfilter_cthelper                 x86_64                1.0.0-10.el7_7.1                  updates                    18 k\n libnetfilter_cttimeout                x86_64                1.0.0-6.el7_7.1                   updates                    18 k\n libnetfilter_queue                    x86_64                1.0.2-2.el7_2                     base                       23 k\n socat                                 x86_64                1.7.3.2-2.el7                     base                      290 k\n\ntransaction summary\n===============================================================================================================================\ninstall  3 packages (+7 dependent packages)\n\ntotal download size: 55 m\ninstalled size: 246 m\ndownloading packages:\n(1/10): conntrack-tools-1.4.4-5.el7_7.2.x86_64.rpm                                                      | 187 kb  00:00:00     \nwarning: /var/cache/yum/x86_64/7/kubernetes/packages/14bfe6e75a9efc8eca3f638eb22c7e2ce759c67f95b43b16fae4ebabde1549f3-cri-tools-1.13.0-0.x86_64.rpm: header v4 rsa/sha512 signature, key id 3e1ba8d5: nokey\npublic key for 14bfe6e75a9efc8eca3f638eb22c7e2ce759c67f95b43b16fae4ebabde1549f3-cri-tools-1.13.0-0.x86_64.rpm is not installed\n(2/10): 14bfe6e75a9efc8eca3f638eb22c7e2ce759c67f95b43b16fae4ebabde1549f3-cri-tools-1.13.0-0.x86_64.rpm  | 5.1 mb  00:00:01     \n(3/10): 515b36bce35de42218470b171ae2ba5cd82132e63d98c7bb87e4298d61fde1dc-kubeadm-1.18.2-0.x86_64.rpm    | 8.8 mb  00:00:01     \n(4/10): 29337f2b1dfd3e11eb9465dbf87b35a1cc48ff0a5e8f992c9f1da70f7641c6bf-kubectl-1.18.2-0.x86_64.rpm    | 9.5 mb  00:00:01     \n(5/10): libnetfilter_cthelper-1.0.0-10.el7_7.1.x86_64.rpm                                               |  18 kb  00:00:00     \n(6/10): libnetfilter_cttimeout-1.0.0-6.el7_7.1.x86_64.rpm                                               |  18 kb  00:00:00     \n(7/10): libnetfilter_queue-1.0.2-2.el7_2.x86_64.rpm                                                     |  23 kb  00:00:00     \n(8/10): socat-1.7.3.2-2.el7.x86_64.rpm                                                                  | 290 kb  00:00:00     \n(9/10): 548a0dcd865c16a50980420ddfa5fbccb8b59621179798e6dc905c9bf8af3b34-kubernetes-cni-0.7.5-0.x86_64. |  10 mb  00:00:02     \n(10/10): d9c0a14c480bd39ac0890746493f9a01325e9ef8fc6fc923520bfc7d0f11744e-kubelet-1.18.2-0.x86_64.rpm   |  21 mb  00:00:04     \n-------------------------------------------------------------------------------------------------------------------------------\ntotal                                                                                          8.5 mb/s |  55 mb  00:00:06     \nretrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nimporting gpg key 0xa7317b0f:\n userid     : "google cloud packages automatic signing key <gc-team@google.com>"\n fingerprint: d0bc 747f d8ca f711 7500 d6fa 3746 c208 a731 7b0f\n from       : https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nretrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nimporting gpg key 0x3e1ba8d5:\n userid     : "google cloud packages rpm signing key <gc-team@google.com>"\n fingerprint: 3749 e1ba 95a8 6ce0 5454 6ed2 f09c 394c 3e1b a8d5\n from       : https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nrunning transaction check\nrunning transaction test\ntransaction test succeeded\nrunning transaction\n  installing : libnetfilter_cthelper-1.0.0-10.el7_7.1.x86_64                                                              1/10 \n  installing : libnetfilter_cttimeout-1.0.0-6.el7_7.1.x86_64                                                              2/10 \n  installing : kubectl-1.18.2-0.x86_64                                                                                    3/10 \n  installing : socat-1.7.3.2-2.el7.x86_64                                                                                 4/10 \n  installing : cri-tools-1.13.0-0.x86_64                                                                                  5/10 \n  installing : libnetfilter_queue-1.0.2-2.el7_2.x86_64                                                                    6/10 \n  installing : conntrack-tools-1.4.4-5.el7_7.2.x86_64                                                                     7/10 \n  installing : kubernetes-cni-0.7.5-0.x86_64                                                                              8/10 \n  installing : kubelet-1.18.2-0.x86_64                                                                                    9/10 \n  installing : kubeadm-1.18.2-0.x86_64                                                                                   10/10 \n  verifying  : kubeadm-1.18.2-0.x86_64                                                                                    1/10 \n  verifying  : conntrack-tools-1.4.4-5.el7_7.2.x86_64                                                                     2/10 \n  verifying  : libnetfilter_queue-1.0.2-2.el7_2.x86_64                                                                    3/10 \n  verifying  : kubelet-1.18.2-0.x86_64                                                                                    4/10 \n  verifying  : cri-tools-1.13.0-0.x86_64                                                                                  5/10 \n  verifying  : kubernetes-cni-0.7.5-0.x86_64                                                                              6/10 \n  verifying  : socat-1.7.3.2-2.el7.x86_64                                                                                 7/10 \n  verifying  : kubectl-1.18.2-0.x86_64                                                                                    8/10 \n  verifying  : libnetfilter_cttimeout-1.0.0-6.el7_7.1.x86_64                                                              9/10 \n  verifying  : libnetfilter_cthelper-1.0.0-10.el7_7.1.x86_64                                                             10/10 \n\ninstalled:\n  kubeadm.x86_64 0:1.18.2-0                 kubectl.x86_64 0:1.18.2-0                 kubelet.x86_64 0:1.18.2-0                \n\ndependency installed:\n  conntrack-tools.x86_64 0:1.4.4-5.el7_7.2                       cri-tools.x86_64 0:1.13.0-0                                   \n  kubernetes-cni.x86_64 0:0.7.5-0                                libnetfilter_cthelper.x86_64 0:1.0.0-10.el7_7.1               \n  libnetfilter_cttimeout.x86_64 0:1.0.0-6.el7_7.1                libnetfilter_queue.x86_64 0:1.0.2-2.el7_2                     \n  socat.x86_64 0:1.7.3.2-2.el7                                  \n\ncomplete!\n\n\n * kubeadm：用于初始化 kubernetes 集群\n * kubectl：kubernetes 的命令行工具，主要作用是部署和管理应用，查看各种资源，创建，删除和更新组件\n * kubelet：主要负责启动 pod 和容器\n\n\n# 设置 kubelet 自启动，并启动 kubelet\n\nsystemctl enable kubelet && systemctl start kubelet\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"配置 kubeadm",frontmatter:{title:"配置 kubeadm",date:"2022-02-09T11:48:30.000Z",permalink:"/pages/14ea2d/"},regularPath:"/18.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/05.%E9%85%8D%E7%BD%AE%20kubeadm.html",relativePath:"18.Kubernetes/01.Kubernetes 入门/05.配置 kubeadm.md",key:"v-4a1b2e40",path:"/pages/14ea2d/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:17},{level:2,title:"创建并修改配置",slug:"创建并修改配置",normalizedTitle:"创建并修改配置",charIndex:568},{level:2,title:"查看和拉取镜像",slug:"查看和拉取镜像",normalizedTitle:"查看和拉取镜像",charIndex:1689}],headersStr:"概述 创建并修改配置 查看和拉取镜像",content:'# 配置 kubeadm\n\n\n# 概述\n\nkubeadm已经进入GA阶段，其控制面初始化和加入节点步骤都支持大量可定制内容，因此kubeadm还提供了配置文件功能用于复杂定制。同时，kubeadm将配置文件以ConfigMap的形式保存到集群中，便于后续的查询和升级工作。kubeadm config子命令提供了对这一组功能的支持：\n\n * kubeadm config upload from-file：由配置文件上传到集群中生成ConfigMap。\n * kubeadm config upload from-flags：由配置参数生成ConfigMap。\n * kubeadm config view：查看当前集群中的配置值。\n * kubeadm config print init-defaults：输出kubeadm init默认参数文件的内容。\n * kubeadm config print join-defaults：输出kubeadm join默认参数文件的内容。\n * kubeadm config migrate：在新旧版本之间进行配置转换。\n * kubeadm config images list：列出所需的镜像列表。\n * kubeadm config images pull：拉取镜像到本地。\n\n\n# 创建并修改配置\n\n导出配置文件\n\nkubeadm config print init-defaults > init-config.yaml\n\n\n修改配置为如下内容\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\napiVersion: kubeadm.k8s.io/v1beta2\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  # 修改为主节点 IP\n  advertiseAddress: 192.169.199.101\n  bindPort: 6443\nnodeRegistration:\n  criSocket: /var/run/dockershim.sock\n  name: k8s-master\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/master\n---\napiServer:\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta2\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrollerManager: {}\ndns:\n  type: CoreDNS\netcd:\n  local:\n    dataDir: /var/lib/etcd\n# 国内不能访问 Google，修改为阿里云\nimageRepository: registry.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.18.0\nnetworking:\n  dnsDomain: cluster.local\n  # 配置成 Calico 的默认网段\n  podSubnet: "10.244.0.0/16"\n  serviceSubnet: 10.96.0.0/12\nscheduler: {}\n\n\n将上面的内容保存为init-config.yaml备用。\n\n\n# 查看和拉取镜像\n\n查看所需镜像列表\n\nkubeadm config images list --config init-config.yaml\n\n\n拉取镜像\n\nkubeadm config images pull --config init-config.yaml\n\n\n查看镜像\n\ndocker images\n\nREPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZE\nregistry.aliyuncs.com/google_containers/kube-proxy                v1.18.0             43940c34f24f        10 days ago         117MB\nregistry.aliyuncs.com/google_containers/kube-apiserver            v1.18.0             74060cea7f70        10 days ago         173MB\nregistry.aliyuncs.com/google_containers/kube-controller-manager   v1.18.0             d3e55153f52f        10 days ago         162MB\nregistry.aliyuncs.com/google_containers/kube-scheduler            v1.18.0             a31f78c7c8ce        10 days ago         95.3MB\nregistry.aliyuncs.com/google_containers/pause                     3.2                 80d28bedfe5d        7 weeks ago         683kB\nregistry.aliyuncs.com/google_containers/coredns                   1.6.7               67da37a9a360        2 months ago        43.8MB\nregistry.aliyuncs.com/google_containers/etcd                      3.4.3-0             303ce5db0e90        5 months ago        288MB\n\n\n在镜像下载完成之后，就可以进行安装了。',normalizedContent:'# 配置 kubeadm\n\n\n# 概述\n\nkubeadm已经进入ga阶段，其控制面初始化和加入节点步骤都支持大量可定制内容，因此kubeadm还提供了配置文件功能用于复杂定制。同时，kubeadm将配置文件以configmap的形式保存到集群中，便于后续的查询和升级工作。kubeadm config子命令提供了对这一组功能的支持：\n\n * kubeadm config upload from-file：由配置文件上传到集群中生成configmap。\n * kubeadm config upload from-flags：由配置参数生成configmap。\n * kubeadm config view：查看当前集群中的配置值。\n * kubeadm config print init-defaults：输出kubeadm init默认参数文件的内容。\n * kubeadm config print join-defaults：输出kubeadm join默认参数文件的内容。\n * kubeadm config migrate：在新旧版本之间进行配置转换。\n * kubeadm config images list：列出所需的镜像列表。\n * kubeadm config images pull：拉取镜像到本地。\n\n\n# 创建并修改配置\n\n导出配置文件\n\nkubeadm config print init-defaults > init-config.yaml\n\n\n修改配置为如下内容\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\napiversion: kubeadm.k8s.io/v1beta2\nbootstraptokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: initconfiguration\nlocalapiendpoint:\n  # 修改为主节点 ip\n  advertiseaddress: 192.169.199.101\n  bindport: 6443\nnoderegistration:\n  crisocket: /var/run/dockershim.sock\n  name: k8s-master\n  taints:\n  - effect: noschedule\n    key: node-role.kubernetes.io/master\n---\napiserver:\n  timeoutforcontrolplane: 4m0s\napiversion: kubeadm.k8s.io/v1beta2\ncertificatesdir: /etc/kubernetes/pki\nclustername: kubernetes\ncontrollermanager: {}\ndns:\n  type: coredns\netcd:\n  local:\n    datadir: /var/lib/etcd\n# 国内不能访问 google，修改为阿里云\nimagerepository: registry.aliyuncs.com/google_containers\nkind: clusterconfiguration\nkubernetesversion: v1.18.0\nnetworking:\n  dnsdomain: cluster.local\n  # 配置成 calico 的默认网段\n  podsubnet: "10.244.0.0/16"\n  servicesubnet: 10.96.0.0/12\nscheduler: {}\n\n\n将上面的内容保存为init-config.yaml备用。\n\n\n# 查看和拉取镜像\n\n查看所需镜像列表\n\nkubeadm config images list --config init-config.yaml\n\n\n拉取镜像\n\nkubeadm config images pull --config init-config.yaml\n\n\n查看镜像\n\ndocker images\n\nrepository                                                        tag                 image id            created             size\nregistry.aliyuncs.com/google_containers/kube-proxy                v1.18.0             43940c34f24f        10 days ago         117mb\nregistry.aliyuncs.com/google_containers/kube-apiserver            v1.18.0             74060cea7f70        10 days ago         173mb\nregistry.aliyuncs.com/google_containers/kube-controller-manager   v1.18.0             d3e55153f52f        10 days ago         162mb\nregistry.aliyuncs.com/google_containers/kube-scheduler            v1.18.0             a31f78c7c8ce        10 days ago         95.3mb\nregistry.aliyuncs.com/google_containers/pause                     3.2                 80d28bedfe5d        7 weeks ago         683kb\nregistry.aliyuncs.com/google_containers/coredns                   1.6.7               67da37a9a360        2 months ago        43.8mb\nregistry.aliyuncs.com/google_containers/etcd                      3.4.3-0             303ce5db0e90        5 months ago        288mb\n\n\n在镜像下载完成之后，就可以进行安装了。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"使用 kubeadm 部署 Master 节点",frontmatter:{title:"使用 kubeadm 部署 Master 节点",date:"2022-02-09T11:48:30.000Z",permalink:"/pages/9b17d0/"},regularPath:"/18.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/06.%E4%BD%BF%E7%94%A8%20kubeadm%20%E9%83%A8%E7%BD%B2%20Master%20%E8%8A%82%E7%82%B9.html",relativePath:"18.Kubernetes/01.Kubernetes 入门/06.使用 kubeadm 部署 Master 节点.md",key:"v-f8a50a56",path:"/pages/9b17d0/",headers:[{level:2,title:"运行 kubeadm init 命令安装 Master",slug:"运行-kubeadm-init-命令安装-master",normalizedTitle:"运行 kubeadm init 命令安装 master",charIndex:30},{level:2,title:"配置 kubectl",slug:"配置-kubectl",normalizedTitle:"配置 kubectl",charIndex:5967},{level:2,title:"验证是否成功",slug:"验证是否成功",normalizedTitle:"验证是否成功",charIndex:6808},{level:2,title:"kubeadm init 的执行过程",slug:"kubeadm-init-的执行过程",normalizedTitle:"kubeadm init 的执行过程",charIndex:10337}],headersStr:"运行 kubeadm init 命令安装 Master 配置 kubectl 验证是否成功 kubeadm init 的执行过程",content:'# 使用 kubeadm 部署 Master 节点\n\n\n# 运行 kubeadm init 命令安装 Master\n\n至此，准备工作已经就绪。执行 kubeadm init 命令即可一键安装 Kubernetes 的 Master。\n\n在开始之前需要注意：kubeadm 的安装过程不涉及网络插件（CNI）的初始化，因此 kubeadm 初步安装完成的集群不具备网络功能，任何 Pod 包括自带的 CoreDNS 都无法正常工作。而网络插件的安装往往对 kubeadm init 命令的参数有一定的要求。例如，安装 Calico 插件时需要指定 --pod-network-cidr=192.168.0.0/16。\n\n接下来使用 kubeadm init 命令，使用前面创建的配置文件进行集群的初始化：\n\nkubeadm init --config=init-config.yaml | tee kubeadm-init.log\n\n\n追加的 tee kubeadm-init.log 用以输出日志。运行后，控制台将输出如下内容：\n\nW0419 17:11:25.925070   12136 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\n[init] Using Kubernetes version: v1.18.0\n[preflight] Running pre-flight checks\n        [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using \'kubeadm config images pull\'\n[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"\n[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"\n[kubelet-start] Starting the kubelet\n[certs] Using certificateDir folder "/etc/kubernetes/pki"\n[certs] Generating "ca" certificate and key\n[certs] Generating "apiserver" certificate and key\n[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.199.101]\n[certs] Generating "apiserver-kubelet-client" certificate and key\n[certs] Generating "front-proxy-ca" certificate and key\n[certs] Generating "front-proxy-client" certificate and key\n[certs] Generating "etcd/ca" certificate and key\n[certs] Generating "etcd/server" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.199.101 127.0.0.1 ::1]\n[certs] Generating "etcd/peer" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.199.101 127.0.0.1 ::1]\n[certs] Generating "etcd/healthcheck-client" certificate and key\n[certs] Generating "apiserver-etcd-client" certificate and key\n[certs] Generating "sa" key and public key\n[kubeconfig] Using kubeconfig folder "/etc/kubernetes"\n[kubeconfig] Writing "admin.conf" kubeconfig file\n[kubeconfig] Writing "kubelet.conf" kubeconfig file\n[kubeconfig] Writing "controller-manager.conf" kubeconfig file\n[kubeconfig] Writing "scheduler.conf" kubeconfig file\n[control-plane] Using manifest folder "/etc/kubernetes/manifests"\n[control-plane] Creating static Pod manifest for "kube-apiserver"\nW0419 17:11:31.212283   12136 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"\nW0419 17:11:31.213179   12136 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"\n[control-plane] Creating static Pod manifest for "kube-controller-manager"\n[control-plane] Creating static Pod manifest for "kube-scheduler"\n[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s\n[apiclient] All control plane components are healthy after 21.005920 seconds\n[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace\n[kubelet] Creating a ConfigMap "kubelet-config-1.18" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node k8s-master as control-plane by adding the label "node-role.kubernetes.io/master=\'\'"\n[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[bootstrap-token] Using token: abcdef.0123456789abcdef\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace\n[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun "kubectl apply -f [podnetwork].yaml" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.199.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:b694f8ad9e298f3ce7ec83f4fa60c784e09bd34c995a01cc862c269a4ac51cef\n\n\n注意\n\n记录下初始化结果中的kubeadm join命令，部署worker节点时会用到\n\n提示\n\n如果安装 kubernetes 版本和下载的镜像版本不统一则会出现 timed out waiting for the condition 错误。中途失败或是想修改配置可以使用 kubeadm reset 命令重置配置，再做初始化操作即可。\n\n\n# 配置 kubectl\n\nkubectl 是管理 Kubernetes Cluster 的命令行工具，前面我们已经在所有的节点安装了 kubectl。Master 初始化完成后需要做一些配置工作，然后 kubectl 就能使用了。 依照 kubeadm init 输出的最后提示，推荐用 Linux 普通用户执行 kubectl。\n\n创建普通用户centos\n\n#创建普通用户并设置密码123456\nuseradd centos && echo "centos:123456" | chpasswd centos\n\n#追加sudo权限,并配置sudo免密\nsed -i \'/^root/a\\centos  ALL=(ALL)       NOPASSWD:ALL\' /etc/sudoers\n\n#保存集群安全配置文件到当前用户.kube目录\nsu - centos\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n#启用 kubectl 命令自动补全功能（注销重新登录生效）\necho "source <(kubectl completion bash)" >> ~/.bashrc\n\n\n需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的 .kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。 如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。 配置完成后 centos 用户就可以使用 kubectl 命令管理集群了。\n\n\n# 验证是否成功\n\n查看集群状态：\n\n[centos@k8s-master ~]$ kubectl get cs\nNAME                 STATUS    MESSAGE             ERROR\ncontroller-manager   Healthy   ok                  \nscheduler            Healthy   ok                  \netcd-0               Healthy   {"health":"true"}  \n\n\n确认各个组件都处于healthy状态。 查看节点状态\n\n[centos@k8s-master ~]$ kubectl get nodes\nNAME         STATUS     ROLES    AGE   VERSION\nk8s-master   NotReady   master   23m   v1.18.2\n\n\n可以看到，当前只存在1个master节点，并且这个节点的状态是 NotReady。 使用 kubectl describe 命令来查看这个节点（Node）对象的详细信息、状态和事件（Event）：\n\n[centos@k8s-master ~]$ kubectl describe node k8s-master \n......\nEvents:\n  Type    Reason                   Age                From                    Message\n  ----    ------                   ----               ----                    -------\n  Normal  Starting                 27m                kubelet, k8s-master     Starting kubelet.\n  Normal  NodeHasSufficientMemory  27m (x3 over 27m)  kubelet, k8s-master     Node k8s-master status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    27m (x3 over 27m)  kubelet, k8s-master     Node k8s-master status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     27m (x2 over 27m)  kubelet, k8s-master     Node k8s-master status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  27m                kubelet, k8s-master     Updated Node Allocatable limit across pods\n  Normal  Starting                 27m                kubelet, k8s-master     Starting kubelet.\n  Normal  NodeHasSufficientMemory  27m                kubelet, k8s-master     Node k8s-master status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    27m                kubelet, k8s-master     Node k8s-master status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     27m                kubelet, k8s-master     Node k8s-master status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  27m                kubelet, k8s-master     Updated Node Allocatable limit across pods\n  Normal  Starting                 27m                kube-proxy, k8s-master  Starting kube-proxy.\n\n\n通过 kubectl describe 指令的输出，我们可以看到 NodeNotReady 的原因在于，我们尚未部署任何网络插件，kube-proxy 等组件还处于 starting 状态。\n\n另外，我们还可以通过 kubectl 检查这个节点上各个系统 Pod 的状态，其中，kube-system 是 Kubernetes 项目预留的系统 Pod 的工作空间（Namespace，注意它并不是 Linux Namespace，它只是 Kubernetes 划分不同工作空间的单位）：\n\n[centos@k8s-master ~]$ kubectl get pod -n kube-system -o wide\nNAME                                 READY   STATUS    RESTARTS   AGE   IP                NODE         NOMINATED NODE   READINESS GATES\ncoredns-78d4cf999f-7jdx7             0/1     Pending   0          32m   <none>            <none>       <none>           <none>\ncoredns-78d4cf999f-s6mhk             0/1     Pending   0          32m   <none>            <none>       <none>           <none>\netcd-k8s-master                      1/1     Running   0          32m   192.168.199.101   k8s-master   <none>           <none>\nkube-apiserver-k8s-master            1/1     Running   0          32m   192.168.199.101   k8s-master   <none>           <none>\nkube-controller-manager-k8s-master   1/1     Running   0          32m   192.168.199.101   k8s-master   <none>           <none>\nkube-proxy-cg29f                     1/1     Running   0          32m   192.168.199.101   k8s-master   <none>           <none>\nkube-scheduler-k8s-master            1/1     Running   0          32m   192.168.199.101   k8s-master   <none>           <none>\n\n\n可以看到，CoreDNS 依赖于网络的 Pod 都处于 Pending 状态，即调度失败。这当然是符合预期的：因为这个 Master 节点的网络尚未就绪。\n\n至此主节点配置完成。\n\n\n# kubeadm init 的执行过程\n\n * init：指定版本进行初始化操作\n * preflight：初始化前的检查和下载所需要的 Docker 镜像文件\n * kubelet-start：生成 kubelet 的配置文件 var/lib/kubelet/config.yaml，没有这个文件 kubelet 无法启动，所以初始化之前的 kubelet 实际上启动不会成功\n * certificates：生成 Kubernetes 使用的证书，存放在 /etc/kubernetes/pki 目录中\n * kubeconfig：生成 KubeConfig 文件，存放在 /etc/kubernetes 目录中，组件之间通信需要使用对应文件\n * control-plane：使用 /etc/kubernetes/manifest 目录下的 YAML 文件，安装 Master 组件\n * etcd：使用 /etc/kubernetes/manifest/etcd.yaml 安装 Etcd 服务\n * wait-control-plane：等待 control-plan 部署的 Master 组件启动\n * apiclient：检查 Master 组件服务状态。\n * uploadconfig：更新配置\n * kubelet：使用 configMap 配置 kubelet\n * patchnode：更新 CNI 信息到 Node 上，通过注释的方式记录\n * mark-control-plane：为当前节点打标签，打了角色 Master，和不可调度标签，这样默认就不会使用 Master 节点来运行 Pod\n * bootstrap-token：生成 token 记录下来，后边使用 kubeadm join 往集群中添加节点时会用到\n * addons：安装附加组件 CoreDNS 和 kube-proxy',normalizedContent:'# 使用 kubeadm 部署 master 节点\n\n\n# 运行 kubeadm init 命令安装 master\n\n至此，准备工作已经就绪。执行 kubeadm init 命令即可一键安装 kubernetes 的 master。\n\n在开始之前需要注意：kubeadm 的安装过程不涉及网络插件（cni）的初始化，因此 kubeadm 初步安装完成的集群不具备网络功能，任何 pod 包括自带的 coredns 都无法正常工作。而网络插件的安装往往对 kubeadm init 命令的参数有一定的要求。例如，安装 calico 插件时需要指定 --pod-network-cidr=192.168.0.0/16。\n\n接下来使用 kubeadm init 命令，使用前面创建的配置文件进行集群的初始化：\n\nkubeadm init --config=init-config.yaml | tee kubeadm-init.log\n\n\n追加的 tee kubeadm-init.log 用以输出日志。运行后，控制台将输出如下内容：\n\nw0419 17:11:25.925070   12136 configset.go:202] warning: kubeadm cannot validate component configs for api groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\n[init] using kubernetes version: v1.18.0\n[preflight] running pre-flight checks\n        [warning isdockersystemdcheck]: detected "cgroupfs" as the docker cgroup driver. the recommended driver is "systemd". please follow the guide at https://kubernetes.io/docs/setup/cri/\n[preflight] pulling images required for setting up a kubernetes cluster\n[preflight] this might take a minute or two, depending on the speed of your internet connection\n[preflight] you can also perform this action in beforehand using \'kubeadm config images pull\'\n[kubelet-start] writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"\n[kubelet-start] writing kubelet configuration to file "/var/lib/kubelet/config.yaml"\n[kubelet-start] starting the kubelet\n[certs] using certificatedir folder "/etc/kubernetes/pki"\n[certs] generating "ca" certificate and key\n[certs] generating "apiserver" certificate and key\n[certs] apiserver serving cert is signed for dns names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and ips [10.96.0.1 192.168.199.101]\n[certs] generating "apiserver-kubelet-client" certificate and key\n[certs] generating "front-proxy-ca" certificate and key\n[certs] generating "front-proxy-client" certificate and key\n[certs] generating "etcd/ca" certificate and key\n[certs] generating "etcd/server" certificate and key\n[certs] etcd/server serving cert is signed for dns names [k8s-master localhost] and ips [192.168.199.101 127.0.0.1 ::1]\n[certs] generating "etcd/peer" certificate and key\n[certs] etcd/peer serving cert is signed for dns names [k8s-master localhost] and ips [192.168.199.101 127.0.0.1 ::1]\n[certs] generating "etcd/healthcheck-client" certificate and key\n[certs] generating "apiserver-etcd-client" certificate and key\n[certs] generating "sa" key and public key\n[kubeconfig] using kubeconfig folder "/etc/kubernetes"\n[kubeconfig] writing "admin.conf" kubeconfig file\n[kubeconfig] writing "kubelet.conf" kubeconfig file\n[kubeconfig] writing "controller-manager.conf" kubeconfig file\n[kubeconfig] writing "scheduler.conf" kubeconfig file\n[control-plane] using manifest folder "/etc/kubernetes/manifests"\n[control-plane] creating static pod manifest for "kube-apiserver"\nw0419 17:11:31.212283   12136 manifests.go:225] the default kube-apiserver authorization-mode is "node,rbac"; using "node,rbac"\nw0419 17:11:31.213179   12136 manifests.go:225] the default kube-apiserver authorization-mode is "node,rbac"; using "node,rbac"\n[control-plane] creating static pod manifest for "kube-controller-manager"\n[control-plane] creating static pod manifest for "kube-scheduler"\n[etcd] creating static pod manifest for local etcd in "/etc/kubernetes/manifests"\n[wait-control-plane] waiting for the kubelet to boot up the control plane as static pods from directory "/etc/kubernetes/manifests". this can take up to 4m0s\n[apiclient] all control plane components are healthy after 21.005920 seconds\n[upload-config] storing the configuration used in configmap "kubeadm-config" in the "kube-system" namespace\n[kubelet] creating a configmap "kubelet-config-1.18" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] skipping phase. please see --upload-certs\n[mark-control-plane] marking the node k8s-master as control-plane by adding the label "node-role.kubernetes.io/master=\'\'"\n[mark-control-plane] marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:noschedule]\n[bootstrap-token] using token: abcdef.0123456789abcdef\n[bootstrap-token] configuring bootstrap tokens, cluster-info configmap, rbac roles\n[bootstrap-token] configured rbac rules to allow node bootstrap tokens to get nodes\n[bootstrap-token] configured rbac rules to allow node bootstrap tokens to post csrs in order for nodes to get long term certificate credentials\n[bootstrap-token] configured rbac rules to allow the csrapprover controller automatically approve csrs from a node bootstrap token\n[bootstrap-token] configured rbac rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] creating the "cluster-info" configmap in the "kube-public" namespace\n[kubelet-finalize] updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key\n[addons] applied essential addon: coredns\n[addons] applied essential addon: kube-proxy\n\nyour kubernetes control-plane has initialized successfully!\n\nto start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $home/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $home/.kube/config\n  sudo chown $(id -u):$(id -g) $home/.kube/config\n\nyou should now deploy a pod network to the cluster.\nrun "kubectl apply -f [podnetwork].yaml" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nthen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.199.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:b694f8ad9e298f3ce7ec83f4fa60c784e09bd34c995a01cc862c269a4ac51cef\n\n\n注意\n\n记录下初始化结果中的kubeadm join命令，部署worker节点时会用到\n\n提示\n\n如果安装 kubernetes 版本和下载的镜像版本不统一则会出现 timed out waiting for the condition 错误。中途失败或是想修改配置可以使用 kubeadm reset 命令重置配置，再做初始化操作即可。\n\n\n# 配置 kubectl\n\nkubectl 是管理 kubernetes cluster 的命令行工具，前面我们已经在所有的节点安装了 kubectl。master 初始化完成后需要做一些配置工作，然后 kubectl 就能使用了。 依照 kubeadm init 输出的最后提示，推荐用 linux 普通用户执行 kubectl。\n\n创建普通用户centos\n\n#创建普通用户并设置密码123456\nuseradd centos && echo "centos:123456" | chpasswd centos\n\n#追加sudo权限,并配置sudo免密\nsed -i \'/^root/a\\centos  all=(all)       nopasswd:all\' /etc/sudoers\n\n#保存集群安全配置文件到当前用户.kube目录\nsu - centos\nmkdir -p $home/.kube\nsudo cp -i /etc/kubernetes/admin.conf $home/.kube/config\nsudo chown $(id -u):$(id -g) $home/.kube/config\n\n#启用 kubectl 命令自动补全功能（注销重新登录生效）\necho "source <(kubectl completion bash)" >> ~/.bashrc\n\n\n需要这些配置命令的原因是：kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 kubernetes 集群的安全配置文件，保存到当前用户的 .kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 kubernetes 集群。 如果不这么做的话，我们每次都需要通过 export kubeconfig 环境变量告诉 kubectl 这个安全配置文件的位置。 配置完成后 centos 用户就可以使用 kubectl 命令管理集群了。\n\n\n# 验证是否成功\n\n查看集群状态：\n\n[centos@k8s-master ~]$ kubectl get cs\nname                 status    message             error\ncontroller-manager   healthy   ok                  \nscheduler            healthy   ok                  \netcd-0               healthy   {"health":"true"}  \n\n\n确认各个组件都处于healthy状态。 查看节点状态\n\n[centos@k8s-master ~]$ kubectl get nodes\nname         status     roles    age   version\nk8s-master   notready   master   23m   v1.18.2\n\n\n可以看到，当前只存在1个master节点，并且这个节点的状态是 notready。 使用 kubectl describe 命令来查看这个节点（node）对象的详细信息、状态和事件（event）：\n\n[centos@k8s-master ~]$ kubectl describe node k8s-master \n......\nevents:\n  type    reason                   age                from                    message\n  ----    ------                   ----               ----                    -------\n  normal  starting                 27m                kubelet, k8s-master     starting kubelet.\n  normal  nodehassufficientmemory  27m (x3 over 27m)  kubelet, k8s-master     node k8s-master status is now: nodehassufficientmemory\n  normal  nodehasnodiskpressure    27m (x3 over 27m)  kubelet, k8s-master     node k8s-master status is now: nodehasnodiskpressure\n  normal  nodehassufficientpid     27m (x2 over 27m)  kubelet, k8s-master     node k8s-master status is now: nodehassufficientpid\n  normal  nodeallocatableenforced  27m                kubelet, k8s-master     updated node allocatable limit across pods\n  normal  starting                 27m                kubelet, k8s-master     starting kubelet.\n  normal  nodehassufficientmemory  27m                kubelet, k8s-master     node k8s-master status is now: nodehassufficientmemory\n  normal  nodehasnodiskpressure    27m                kubelet, k8s-master     node k8s-master status is now: nodehasnodiskpressure\n  normal  nodehassufficientpid     27m                kubelet, k8s-master     node k8s-master status is now: nodehassufficientpid\n  normal  nodeallocatableenforced  27m                kubelet, k8s-master     updated node allocatable limit across pods\n  normal  starting                 27m                kube-proxy, k8s-master  starting kube-proxy.\n\n\n通过 kubectl describe 指令的输出，我们可以看到 nodenotready 的原因在于，我们尚未部署任何网络插件，kube-proxy 等组件还处于 starting 状态。\n\n另外，我们还可以通过 kubectl 检查这个节点上各个系统 pod 的状态，其中，kube-system 是 kubernetes 项目预留的系统 pod 的工作空间（namespace，注意它并不是 linux namespace，它只是 kubernetes 划分不同工作空间的单位）：\n\n[centos@k8s-master ~]$ kubectl get pod -n kube-system -o wide\nname                                 ready   status    restarts   age   ip                node         nominated node   readiness gates\ncoredns-78d4cf999f-7jdx7             0/1     pending   0          32m   <none>            <none>       <none>           <none>\ncoredns-78d4cf999f-s6mhk             0/1     pending   0          32m   <none>            <none>       <none>           <none>\netcd-k8s-master                      1/1     running   0          32m   192.168.199.101   k8s-master   <none>           <none>\nkube-apiserver-k8s-master            1/1     running   0          32m   192.168.199.101   k8s-master   <none>           <none>\nkube-controller-manager-k8s-master   1/1     running   0          32m   192.168.199.101   k8s-master   <none>           <none>\nkube-proxy-cg29f                     1/1     running   0          32m   192.168.199.101   k8s-master   <none>           <none>\nkube-scheduler-k8s-master            1/1     running   0          32m   192.168.199.101   k8s-master   <none>           <none>\n\n\n可以看到，coredns 依赖于网络的 pod 都处于 pending 状态，即调度失败。这当然是符合预期的：因为这个 master 节点的网络尚未就绪。\n\n至此主节点配置完成。\n\n\n# kubeadm init 的执行过程\n\n * init：指定版本进行初始化操作\n * preflight：初始化前的检查和下载所需要的 docker 镜像文件\n * kubelet-start：生成 kubelet 的配置文件 var/lib/kubelet/config.yaml，没有这个文件 kubelet 无法启动，所以初始化之前的 kubelet 实际上启动不会成功\n * certificates：生成 kubernetes 使用的证书，存放在 /etc/kubernetes/pki 目录中\n * kubeconfig：生成 kubeconfig 文件，存放在 /etc/kubernetes 目录中，组件之间通信需要使用对应文件\n * control-plane：使用 /etc/kubernetes/manifest 目录下的 yaml 文件，安装 master 组件\n * etcd：使用 /etc/kubernetes/manifest/etcd.yaml 安装 etcd 服务\n * wait-control-plane：等待 control-plan 部署的 master 组件启动\n * apiclient：检查 master 组件服务状态。\n * uploadconfig：更新配置\n * kubelet：使用 configmap 配置 kubelet\n * patchnode：更新 cni 信息到 node 上，通过注释的方式记录\n * mark-control-plane：为当前节点打标签，打了角色 master，和不可调度标签，这样默认就不会使用 master 节点来运行 pod\n * bootstrap-token：生成 token 记录下来，后边使用 kubeadm join 往集群中添加节点时会用到\n * addons：安装附加组件 coredns 和 kube-proxy',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"安装网络插件",frontmatter:{title:"安装网络插件",date:"2022-02-09T11:48:30.000Z",permalink:"/pages/9477b8/"},regularPath:"/18.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/07.%E5%AE%89%E8%A3%85%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6.html",relativePath:"18.Kubernetes/01.Kubernetes 入门/07.安装网络插件.md",key:"v-4b869476",path:"/pages/9477b8/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:13},{level:2,title:"什么是 CNI",slug:"什么是-cni",normalizedTitle:"什么是 cni",charIndex:265},{level:2,title:"Kubernetes 中的 CNI 插件",slug:"kubernetes-中的-cni-插件",normalizedTitle:"kubernetes 中的 cni 插件",charIndex:441},{level:2,title:"什么是 Calico",slug:"什么是-calico",normalizedTitle:"什么是 calico",charIndex:908},{level:2,title:"安装网络插件 Calico",slug:"安装网络插件-calico",normalizedTitle:"安装网络插件 calico",charIndex:1118},{level:2,title:"解决 ImagePullBackOff",slug:"解决-imagepullbackoff",normalizedTitle:"解决 imagepullbackoff",charIndex:4570}],headersStr:"概述 什么是 CNI Kubernetes 中的 CNI 插件 什么是 Calico 安装网络插件 Calico 解决 ImagePullBackOff",content:"# 安装网络插件\n\n\n# 概述\n\n容器网络是容器选择连接到其他容器、主机和外部网络的机制。容器的 runtime 提供了各种网络模式，每种模式都会产生不同的体验。例如，Docker 默认情况下可以为容器配置以下网络：\n\n * none： 将容器添加到一个容器专门的网络堆栈中，没有对外连接。\n * host： 将容器添加到主机的网络堆栈中，没有隔离。\n * default bridge： 默认网络模式。每个容器可以通过 IP 地址相互连接。\n * 自定义网桥： 用户定义的网桥，具有更多的灵活性、隔离性和其他便利功能。\n\n\n# 什么是 CNI\n\nCNI(Container Network Interface) 是一个标准的，通用的接口。在容器平台，Docker，Kubernetes，Mesos 容器网络解决方案 flannel，calico，weave。只要提供一个标准的接口，就能为同样满足该协议的所有容器平台提供网络功能，而 CNI 正是这样的一个标准接口协议。\n\n\n# Kubernetes 中的 CNI 插件\n\nCNI 的初衷是创建一个框架，用于在配置或销毁容器时动态配置适当的网络配置和资源。插件负责为接口配置和管理 IP 地址，并且通常提供与 IP 管理、每个容器的 IP 分配、以及多主机连接相关的功能。容器运行时会调用网络插件，从而在容器启动时分配 IP 地址并配置网络，并在删除容器时再次调用它以清理这些资源。\n\n运行时或协调器决定了容器应该加入哪个网络以及它需要调用哪个插件。然后，插件会将接口添加到容器网络命名空间中，作为一个 veth 对的一侧。接着，它会在主机上进行更改，包括将 veth 的其他部分连接到网桥。再之后，它会通过调用单独的 IPAM（IP地址管理）插件来分配 IP 地址并设置路由。\n\n在 Kubernetes 中，kubelet 可以在适当的时间调用它找到的插件，为通过 kubelet 启动的 pod进行自动的网络配置。\n\nKubernetes 中可选的 CNI 插件如下：\n\n * Flannel\n * Calico\n * Canal\n * Weave\n\n\n# 什么是 Calico\n\nCalico 为容器和虚拟机提供了安全的网络连接解决方案，并经过了大规模生产验证（在公有云和跨数千个集群节点中），可与 Kubernetes，OpenShift，Docker，Mesos，DC / OS 和 OpenStack 集成。\n\nCalico 还提供网络安全规则的动态实施。使用 Calico 的简单策略语言，您可以实现对容器，虚拟机工作负载和裸机主机端点之间通信的细粒度控制。\n\n\n# 安装网络插件 Calico\n\n提示\n\n截止到文章发表日期 2020 年 04 月 5 日，Calico 官方版本为 v3.13.2\n\n参考官方文档安装：https://docs.projectcalico.org/getting-started/kubernetes/\n\n# 在 Master 节点操作即可\nkubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n\n# 安装时显示如下输出\nconfigmap/calico-config created\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\nclusterrole.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrole.rbac.authorization.k8s.io/calico-node created\nclusterrolebinding.rbac.authorization.k8s.io/calico-node created\ndaemonset.extensions/calico-node created\nserviceaccount/calico-node created\ndeployment.extensions/calico-kube-controllers created\nserviceaccount/calico-kube-controllers created\n\n\n确认安装是否成功\n\nwatch kubectl get pods --all-namespaces\n\n# 需要等待所有状态为 Running，注意时间可能较久，3 - 5 分钟的样子\nEvery 2.0s: kubectl get pods --all-namespaces                                                                                                    kubernetes-master: Fri May 10 18:16:51 2019\n\nNAMESPACE     NAME                                        READY   STATUS    RESTARTS   AGE\nkube-system   calico-kube-controllers-555fc8cc5c-mcmst   1/1     Running   0          12m\nkube-system   calico-node-s76tm                          1/1     Running   0          12m\nkube-system   coredns-7ff77c879f-h95kn                   1/1     Running   0          13m\nkube-system   coredns-7ff77c879f-lczbq                   1/1     Running   0          13m\nkube-system   etcd-k8s-master                            1/1     Running   0          13m\nkube-system   kube-apiserver-k8s-master                  1/1     Running   0          13m\nkube-system   kube-controller-manager-k8s-master         1/1     Running   0          13m\nkube-system   kube-proxy-2kg9t                           1/1     Running   0          13m\nkube-system   kube-scheduler-k8s-master                  1/1     Running   0          13m\n\n\n再次查看master节点状态已经为ready状态：\n\n[centos@k8s-master ~]$ kubectl get nodes\nNAME         STATUS   ROLES    AGE   VERSION\nk8s-master   Ready    master   19m   v1.18.2\n\n\n至此，Kubernetes 的 Master 节点就部署完成了。如果你只需要一个单节点的 Kubernetes，现在你就可以使用了。不过，在默认情况下，Kubernetes 的 Master 节点是不能运行用户 Pod 的。\n\n\n# 解决 ImagePullBackOff\n\n在使用 watch kubectl get pods --all-namespaces 命令观察 Pods 状态时如果出现 ImagePullBackOff 无法 Running 的情况，请尝试使用如下步骤处理：\n\n * Master 中删除 Nodes：kubectl delete nodes <NAME>\n * Slave 中重置配置：kubeadm reset\n * Slave 重启计算机：reboot\n * Slave 重新加入集群：kubeadm join",normalizedContent:"# 安装网络插件\n\n\n# 概述\n\n容器网络是容器选择连接到其他容器、主机和外部网络的机制。容器的 runtime 提供了各种网络模式，每种模式都会产生不同的体验。例如，docker 默认情况下可以为容器配置以下网络：\n\n * none： 将容器添加到一个容器专门的网络堆栈中，没有对外连接。\n * host： 将容器添加到主机的网络堆栈中，没有隔离。\n * default bridge： 默认网络模式。每个容器可以通过 ip 地址相互连接。\n * 自定义网桥： 用户定义的网桥，具有更多的灵活性、隔离性和其他便利功能。\n\n\n# 什么是 cni\n\ncni(container network interface) 是一个标准的，通用的接口。在容器平台，docker，kubernetes，mesos 容器网络解决方案 flannel，calico，weave。只要提供一个标准的接口，就能为同样满足该协议的所有容器平台提供网络功能，而 cni 正是这样的一个标准接口协议。\n\n\n# kubernetes 中的 cni 插件\n\ncni 的初衷是创建一个框架，用于在配置或销毁容器时动态配置适当的网络配置和资源。插件负责为接口配置和管理 ip 地址，并且通常提供与 ip 管理、每个容器的 ip 分配、以及多主机连接相关的功能。容器运行时会调用网络插件，从而在容器启动时分配 ip 地址并配置网络，并在删除容器时再次调用它以清理这些资源。\n\n运行时或协调器决定了容器应该加入哪个网络以及它需要调用哪个插件。然后，插件会将接口添加到容器网络命名空间中，作为一个 veth 对的一侧。接着，它会在主机上进行更改，包括将 veth 的其他部分连接到网桥。再之后，它会通过调用单独的 ipam（ip地址管理）插件来分配 ip 地址并设置路由。\n\n在 kubernetes 中，kubelet 可以在适当的时间调用它找到的插件，为通过 kubelet 启动的 pod进行自动的网络配置。\n\nkubernetes 中可选的 cni 插件如下：\n\n * flannel\n * calico\n * canal\n * weave\n\n\n# 什么是 calico\n\ncalico 为容器和虚拟机提供了安全的网络连接解决方案，并经过了大规模生产验证（在公有云和跨数千个集群节点中），可与 kubernetes，openshift，docker，mesos，dc / os 和 openstack 集成。\n\ncalico 还提供网络安全规则的动态实施。使用 calico 的简单策略语言，您可以实现对容器，虚拟机工作负载和裸机主机端点之间通信的细粒度控制。\n\n\n# 安装网络插件 calico\n\n提示\n\n截止到文章发表日期 2020 年 04 月 5 日，calico 官方版本为 v3.13.2\n\n参考官方文档安装：https://docs.projectcalico.org/getting-started/kubernetes/\n\n# 在 master 节点操作即可\nkubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n\n# 安装时显示如下输出\nconfigmap/calico-config created\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\nclusterrole.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrole.rbac.authorization.k8s.io/calico-node created\nclusterrolebinding.rbac.authorization.k8s.io/calico-node created\ndaemonset.extensions/calico-node created\nserviceaccount/calico-node created\ndeployment.extensions/calico-kube-controllers created\nserviceaccount/calico-kube-controllers created\n\n\n确认安装是否成功\n\nwatch kubectl get pods --all-namespaces\n\n# 需要等待所有状态为 running，注意时间可能较久，3 - 5 分钟的样子\nevery 2.0s: kubectl get pods --all-namespaces                                                                                                    kubernetes-master: fri may 10 18:16:51 2019\n\nnamespace     name                                        ready   status    restarts   age\nkube-system   calico-kube-controllers-555fc8cc5c-mcmst   1/1     running   0          12m\nkube-system   calico-node-s76tm                          1/1     running   0          12m\nkube-system   coredns-7ff77c879f-h95kn                   1/1     running   0          13m\nkube-system   coredns-7ff77c879f-lczbq                   1/1     running   0          13m\nkube-system   etcd-k8s-master                            1/1     running   0          13m\nkube-system   kube-apiserver-k8s-master                  1/1     running   0          13m\nkube-system   kube-controller-manager-k8s-master         1/1     running   0          13m\nkube-system   kube-proxy-2kg9t                           1/1     running   0          13m\nkube-system   kube-scheduler-k8s-master                  1/1     running   0          13m\n\n\n再次查看master节点状态已经为ready状态：\n\n[centos@k8s-master ~]$ kubectl get nodes\nname         status   roles    age   version\nk8s-master   ready    master   19m   v1.18.2\n\n\n至此，kubernetes 的 master 节点就部署完成了。如果你只需要一个单节点的 kubernetes，现在你就可以使用了。不过，在默认情况下，kubernetes 的 master 节点是不能运行用户 pod 的。\n\n\n# 解决 imagepullbackoff\n\n在使用 watch kubectl get pods --all-namespaces 命令观察 pods 状态时如果出现 imagepullbackoff 无法 running 的情况，请尝试使用如下步骤处理：\n\n * master 中删除 nodes：kubectl delete nodes <name>\n * slave 中重置配置：kubeadm reset\n * slave 重启计算机：reboot\n * slave 重新加入集群：kubeadm join",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"使用 kubeadm 配置 Worker 节点",frontmatter:{title:"使用 kubeadm 配置 Worker 节点",date:"2022-02-09T11:48:30.000Z",permalink:"/pages/f6617c/"},regularPath:"/18.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/08.%E4%BD%BF%E7%94%A8%20kubeadm%20%E9%85%8D%E7%BD%AE%20Worker%20%E8%8A%82%E7%82%B9.html",relativePath:"18.Kubernetes/01.Kubernetes 入门/08.使用 kubeadm 配置 Worker 节点.md",key:"v-20e14c4b",path:"/pages/f6617c/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:30},{level:2,title:"加入集群",slug:"加入集群",normalizedTitle:"加入集群",charIndex:283},{level:2,title:"验证是否成功",slug:"验证是否成功",normalizedTitle:"验证是否成功",charIndex:2533},{level:2,title:"测试集群各个组件",slug:"测试集群各个组件",normalizedTitle:"测试集群各个组件",charIndex:10480},{level:2,title:"验证kube-proxy",slug:"验证kube-proxy",normalizedTitle:"验证kube-proxy",charIndex:11342},{level:2,title:"停止服务",slug:"停止服务",normalizedTitle:"停止服务",charIndex:12784},{level:2,title:"Pod调度到Master节点",slug:"pod调度到master节点",normalizedTitle:"pod调度到master节点",charIndex:12958},{level:2,title:"kube-proxy开启ipvs",slug:"kube-proxy开启ipvs",normalizedTitle:"kube-proxy开启ipvs",charIndex:13530},{level:2,title:"移除节点和集群",slug:"移除节点和集群",normalizedTitle:"移除节点和集群",charIndex:15406}],headersStr:"概述 加入集群 验证是否成功 测试集群各个组件 验证kube-proxy 停止服务 Pod调度到Master节点 kube-proxy开启ipvs 移除节点和集群",content:'# 使用 kubeadm 配置 Worker 节点\n\n\n# 概述\n\n部署worker节点 Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod。 在 k8s-node1 和 k8s-node2 上分别执行如下命令，将其注册到 Cluster 中：\n\n\n# 加入集群\n\n执行以下命令将节点接入集群\n\nkubeadm join 192.168.199.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:d7df0d887d0407995eb46df24d3481c5e9b6ffc13da97be7f5f685f3aca1d8e7 \n\n\n如果执行kubeadm init时没有记录下加入集群的命令，可以通过以下命令重新创建 kubeadm token create --print-join-command 在k8s-node1上执行kubeadm join ：\n\n[root@k8s-node1 ~]# kubeadm join 192.168.199.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:d7df0d887d0407995eb46df24d3481c5e9b6ffc13da97be7f5f685f3aca1d8e7 \nW0419 19:00:43.118672   32562 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.\n[preflight] Running pre-flight checks\n        [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/\n[preflight] Reading configuration from the cluster...\n[preflight] FYI: You can look at this config file with \'kubectl -n kube-system get cm kubeadm-config -oyaml\'\n[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.18" ConfigMap in the kube-system namespace\n[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"\n[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"\n[kubelet-start] Starting the kubelet\n[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...\n\nThis node has joined the cluster:\n* Certificate signing request was sent to apiserver and a response was received.\n* The Kubelet was informed of the new secure connection details.\n\nRun \'kubectl get nodes\' on the control-plane to see this node join the cluster.\n\n\n重复执行以上操作将k8s-node2也加进去（注意重新执行kubeadm token create --print-join-command）。 然后根据提示，我们可以通过 kubectl get nodes 查看节点的状态：\n\n说明：\n\n * token\n   * 可以通过安装 master 时的日志查看 token 信息\n   * 可以通过 kubeadm token list 命令打印出 token 信息\n   * 如果 token 过期，可以使用 kubeadm token create 命令创建新的 token\n * discovery-token-ca-cert-hash\n   * 可以通过安装 master 时的日志查看 sha256 信息\n   * 可以通过 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed \'s/^.* //\' 命令查看 sha256 信息\n\n\n# 验证是否成功\n\n回到 master 服务器,可以看到 node 成功加入 master\n\n[centos@k8s-master ~]$ kubectl get nodes\nNAME         STATUS   ROLES    AGE     VERSION\nk8s-master   Ready    master   38m     v1.18.2\nk8s-node1    Ready    <none>   7m57s   v1.18.2\nk8s-node2    Ready    <none>   7m37s   v1.18.2\n\n\n提示\n\n如果 node 节点加入 master 时配置有问题可以在 node 节点上使用 kubeadm reset 重置配置再使用 kubeadm join 命令重新加入即可。希望在 master 节点删除 node ，可以使用 kubeadm delete nodes 删除。\n\nnodes状态全部为ready，由于每个节点都需要启动若干组件，如果node节点的状态是 NotReady，可以查看所有节点pod状态，确保所有pod成功拉取到镜像并处于running状态：\n\n[centos@k8s-master ~]$ kubectl get pod --all-namespaces -o wide\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE   IP                NODE         NOMINATED NODE   READINESS GATES\nkube-system   calico-kube-controllers-555fc8cc5c-mcmst   1/1     Running   0          66m   10.244.235.193    k8s-master   <none>           <none>\nkube-system   calico-node-lh5j2                          1/1     Running   0          37m   192.168.199.102   k8s-node1    <none>           <none>\nkube-system   calico-node-s76tm                          1/1     Running   0          66m   192.168.199.101   k8s-master   <none>           <none>\nkube-system   calico-node-wd4rj                          1/1     Running   0          36m   192.168.199.103   k8s-node2    <none>           <none>\nkube-system   coredns-7ff77c879f-h95kn                   1/1     Running   0          67m   10.244.235.195    k8s-master   <none>           <none>\nkube-system   coredns-7ff77c879f-lczbq                   1/1     Running   0          67m   10.244.235.194    k8s-master   <none>           <none>\nkube-system   etcd-k8s-master                            1/1     Running   0          67m   192.168.199.101   k8s-master   <none>           <none>\nkube-system   kube-apiserver-k8s-master                  1/1     Running   0          67m   192.168.199.101   k8s-master   <none>           <none>\nkube-system   kube-controller-manager-k8s-master         1/1     Running   0          67m   192.168.199.101   k8s-master   <none>           <none>\nkube-system   kube-proxy-2kg9t                           1/1     Running   0          67m   192.168.199.101   k8s-master   <none>           <none>\nkube-system   kube-proxy-7fkx9                           1/1     Running   0          36m   192.168.199.103   k8s-node2    <none>           <none>\nkube-system   kube-proxy-mcjc5                           1/1     Running   0          37m   192.168.199.102   k8s-node1    <none>           <none>\nkube-system   kube-scheduler-k8s-master                  1/1     Running   0          67m   192.168.199.101   k8s-master   <none>           <none>\n\n\n这时，所有的节点都已经 Ready，Kubernetes Cluster 创建成功，一切准备就绪。 如果pod状态为Pending、ContainerCreating、ImagePullBackOff 都表明 Pod 没有就绪，Running 才是就绪状态。 如果有pod提示Init:ImagePullBackOff，说明这个pod的镜像在对应节点上拉取失败，我们可以通过 kubectl describe pod 查看 Pod 具体情况，以确认拉取失败的镜像：\n\n[centos@k8s-master ~]$ kubectl describe pod calico-node-lh5j2 --namespace=kube-system\n......\nEvents:\n  Type     Reason     Age   From                Message\n  ----     ------     ----  ----                -------\n  Normal   Scheduled  42m   default-scheduler   Successfully assigned kube-system/calico-node-lh5j2 to k8s-node1\n  Normal   Pulling    42m   kubelet, k8s-node1  Pulling image "calico/cni:v3.13.2"\n  Normal   Pulled     37m   kubelet, k8s-node1  Successfully pulled image "calico/cni:v3.13.2"\n  Normal   Created    37m   kubelet, k8s-node1  Created container upgrade-ipam\n  Normal   Started    37m   kubelet, k8s-node1  Started container upgrade-ipam\n  Normal   Pulled     37m   kubelet, k8s-node1  Container image "calico/cni:v3.13.2" already present on machine\n  Normal   Created    37m   kubelet, k8s-node1  Created container install-cni\n  Normal   Started    37m   kubelet, k8s-node1  Started container install-cni\n  Normal   Pulling    37m   kubelet, k8s-node1  Pulling image "calico/pod2daemon-flexvol:v3.13.2"\n  Normal   Pulled     37m   kubelet, k8s-node1  Successfully pulled image "calico/pod2daemon-flexvol:v3.13.2"\n  Normal   Created    37m   kubelet, k8s-node1  Created container flexvol-driver\n  Normal   Started    37m   kubelet, k8s-node1  Started container flexvol-driver\n  Normal   Pulling    37m   kubelet, k8s-node1  Pulling image "calico/node:v3.13.2"\n  Normal   Pulled     33m   kubelet, k8s-node1  Successfully pulled image "calico/node:v3.13.2"\n  Normal   Created    33m   kubelet, k8s-node1  Created container calico-node\n  Normal   Started    33m   kubelet, k8s-node1  Started container calico-node\n  Warning  Unhealthy  33m   kubelet, k8s-node1  Readiness probe failed: calico/node is not ready: BIRD is not ready: BGP not established with 192.168.199.1032020-04-19 11:10:12.409 [INFO][168] health.go 156: Number of node(s) with BGP peering established = 1\n\n\n查看master节点下载了哪些镜像\n\n[centos@k8s-master ~]$  sudo docker images\nREPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZE\ncalico/pod2daemon-flexvol                                         v3.13.2             77a3fabf99a5        2 weeks ago         111MB\ncalico/cni                                                        v3.13.2             a89faaa1676a        2 weeks ago         224MB\ncalico/kube-controllers                                           v3.13.2             0c3bf0adad2b        2 weeks ago         56.6MB\ncalico/node                                                       v3.13.2             6f674c890b23        2 weeks ago         260MB\nregistry.aliyuncs.com/google_containers/kube-proxy                v1.18.0             43940c34f24f        3 weeks ago         117MB\nregistry.aliyuncs.com/google_containers/kube-apiserver            v1.18.0             74060cea7f70        3 weeks ago         173MB\nregistry.aliyuncs.com/google_containers/kube-scheduler            v1.18.0             a31f78c7c8ce        3 weeks ago         95.3MB\nregistry.aliyuncs.com/google_containers/kube-controller-manager   v1.18.0             d3e55153f52f        3 weeks ago         162MB\nregistry.aliyuncs.com/google_containers/pause                     3.2                 80d28bedfe5d        2 months ago        683kB\nregistry.aliyuncs.com/google_containers/coredns                   1.6.7               67da37a9a360        2 months ago        43.8MB\nregistry.aliyuncs.com/google_containers/etcd                      3.4.3-0             303ce5db0e90        5 months ago        288MB\n\n\n查看worker节点下载了哪些镜像：\n\n[root@k8s-node1 ~]#  sudo docker images\nREPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZE\ncalico/pod2daemon-flexvol                                         v3.13.2             77a3fabf99a5        2 weeks ago         111MB\ncalico/cni                                                        v3.13.2             a89faaa1676a        2 weeks ago         224MB\ncalico/node                                                       v3.13.2             6f674c890b23        2 weeks ago         260MB\nregistry.aliyuncs.com/google_containers/kube-proxy                v1.18.0             43940c34f24f        3 weeks ago         117MB\nregistry.aliyuncs.com/google_containers/kube-controller-manager   v1.18.0             d3e55153f52f        3 weeks ago         162MB\nregistry.aliyuncs.com/google_containers/kube-scheduler            v1.18.0             a31f78c7c8ce        3 weeks ago         95.3MB\nregistry.aliyuncs.com/google_containers/kube-apiserver            v1.18.0             74060cea7f70        3 weeks ago         173MB\nregistry.aliyuncs.com/google_containers/pause                     3.2                 80d28bedfe5d        2 months ago        683kB\nregistry.aliyuncs.com/google_containers/coredns                   1.6.7               67da37a9a360        2 months ago        43.8MB\nregistry.aliyuncs.com/google_containers/etcd                      3.4.3-0             303ce5db0e90        5 months ago        288MB\n\n\n\n# 测试集群各个组件\n\n首先验证kube-apiserver, kube-controller-manager, kube-scheduler, pod network 是否正常： 部署一个 Nginx Deployment，包含2个Pod 参考：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\n\n[centos@k8s-master ~]$ kubectl create deployment nginx --image=nginx:alpine\ndeployment.apps/nginx created\n[centos@k8s-master ~]$ kubectl scale deployment nginx --replicas=2\ndeployment.apps/nginx scaled\n\n\n验证Nginx Pod是否正确运行，并且会分配10.244.开头的集群IP\n\n[centos@k8s-master ~]$ kubectl get pods -l app=nginx -o wide      \nNAME                     READY   STATUS    RESTARTS   AGE     IP               NODE        NOMINATED NODE   READINESS GATES\nnginx-745b4df97d-jb5f4   1/1     Running   0          3m10s   10.244.169.129   k8s-node2   <none>           <none>\nnginx-745b4df97d-zgdbr   1/1     Running   0          2m29s   10.244.36.65     k8s-node1   <none>           <none>\n\n\n\n# 验证kube-proxy\n\n以 NodePort 方式对外提供服务 参考：https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/\n\n[centos@k8s-master ~]$ kubectl expose deployment nginx --port=80 --type=NodePort\nservice/nginx exposed\n[centos@k8s-master ~]$ kubectl get services nginx\nNAME    TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE\nnginx   NodePort   10.97.5.8    <none>        80:32350/TCP   19s\n\n\n可以通过任意 NodeIP:Port 在集群外部访问这个服务：\n\n[centos@k8s-master ~]$ curl 192.168.199.101:32350\n[centos@k8s-master ~]$ curl 192.168.199.102:32350\n[centos@k8s-master ~]$ curl 192.168.199.103:32350\n\n\n最后验证一下dns, pod network是否正常： 运行Busybox并进入交互模式\n\n[centos@k8s-master ~]$ kubectl run -it curl --image=radial/busyboxplus:curl\nIf you don\'t see a command prompt, try pressing enter.\n\n\n输入nslookup nginx查看是否可以正确解析出集群内的IP，以验证DNS是否正常\n\n[ root@curl:/ ]$ nslookup nginx\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      nginx\nAddress 1: 10.97.5.8 nginx.default.svc.cluster.local\n\n\n通过服务名进行访问，验证kube-proxy是否正常\n\n[ root@curl:/ ]$ curl http://nginx/\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n......\n</body>\n</html>\n# 分别访问一下2个Pod的内网IP，验证跨Node的网络通信是否正常\n[ root@curl:/ ]$ curl 10.244.169.129\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n......\n</body>\n</html>\n[ root@curl:/ ]$ curl 10.244.36.65\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n......\n</body>\n</html>\n\n\n\n# 停止服务\n\n[centos@k8s-master ~]$ kubectl delete deployment nginx\ndeployment.apps "nginx" deleted\n[centos@k8s-master ~]$ kubectl delete service nginx\nservice "nginx" deleted\n\n\n\n# Pod调度到Master节点\n\n出于安全考虑，默认配置下Kubernetes不会将Pod调度到Master节点。查看Taints字段默认配置：\n\n[centos@k8s-master ~]$ kubectl describe node k8s-master \n......\nTaints:             node-role.kubernetes.io/master:NoSchedule\n\n\n如果希望将k8s-master也当作Node节点使用，可以执行如下命令,其中k8s-master是主机节点hostname：\n\nkubectl taint node k8s-master node-role.kubernetes.io/master-\n\n\n修改后Taints字段状态：\n\n[centos@k8s-master ~]$ kubectl describe node k8s-master                             \n......\nTaints:             <none>\n\n\n如果要恢复Master Only状态，执行如下命令：\n\nkubectl taint node k8s-master node-role.kubernetes.io/master=:NoSchedule\n\n\n\n# kube-proxy开启ipvs\n\n修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”：\n\n[centos@k8s-master ~]$ kubectl edit cm kube-proxy -n kube-system\nconfigmap/kube-proxy edited\n之后重启各个节点上的kube-proxy pod：\n\n[centos@k8s-master ~]$ kubectl get pod -n kube-system | grep kube-proxy | awk \'{system("kubectl delete pod "$1" -n kube-system")}\'\npod "kube-proxy-2w9sh" deleted\npod "kube-proxy-gw4lx" deleted\npod "kube-proxy-thv4c" deleted\n[centos@k8s-master ~]$ kubectl get pod -n kube-system | grep kube-proxy\nkube-proxy-6qlgv                        1/1     Running   0          65s\nkube-proxy-fdtjd                        1/1     Running   0          47s\nkube-proxy-m8zkx                        1/1     Running   0          52s\n[centos@k8s-master ~]$\n\n\n查看日志：\n\n[centos@k8s-master ~]$ kubectl logs kube-proxy-6qlgv -n kube-system\nI1213 09:50:15.414493       1 server_others.go:189] Using ipvs Proxier.\nW1213 09:50:15.414908       1 proxier.go:365] IPVS scheduler not specified, use rr by default\nI1213 09:50:15.415021       1 server_others.go:216] Tearing down inactive rules.\nI1213 09:50:15.461658       1 server.go:464] Version: v1.13.0\nI1213 09:50:15.467827       1 conntrack.go:52] Setting nf_conntrack_max to 131072\nI1213 09:50:15.467997       1 config.go:202] Starting service config controller\nI1213 09:50:15.468010       1 controller_utils.go:1027] Waiting for caches to sync for service config controller\nI1213 09:50:15.468092       1 config.go:102] Starting endpoints config controller\nI1213 09:50:15.468100       1 controller_utils.go:1027] Waiting for caches to sync for endpoints config controller\nI1213 09:50:15.568766       1 controller_utils.go:1034] Caches are synced for endpoints config controller\nI1213 09:50:15.568950       1 controller_utils.go:1034] Caches are synced for service config controller\n[centos@k8s-master ~]$\n\n\n日志中打印出了Using ipvs Proxier，说明ipvs模式已经开启。\n\n\n# 移除节点和集群\n\nkubernetes集群移除节点，以移除k8s-node2节点为例，在Master节点上运行：\n\nkubectl drain k8s-node2 --delete-local-data --force --ignore-daemonsets\nkubectl delete node k8s-node2\n\n\n上面两条命令执行完成后，在k8s-node2节点执行清理命令，重置kubeadm的安装状态：\n\nkubeadm reset\n\n\n在master上删除node并不会清理k8s-node2运行的容器，需要在删除节点上面手动运行清理命令。 如果你想重新配置集群，使用新的参数重新运行kubeadm init或者kubeadm join即可。\n\n至此3个节点的集群搭建完成，后续可以继续添加node节点，或者部署dashboard、helm包管理工具、EFK日志系统、Prometheus Operator监控系统、rook+ceph存储系统等组件。',normalizedContent:'# 使用 kubeadm 配置 worker 节点\n\n\n# 概述\n\n部署worker节点 kubernetes 的 worker 节点跟 master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 pod。 在 k8s-node1 和 k8s-node2 上分别执行如下命令，将其注册到 cluster 中：\n\n\n# 加入集群\n\n执行以下命令将节点接入集群\n\nkubeadm join 192.168.199.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:d7df0d887d0407995eb46df24d3481c5e9b6ffc13da97be7f5f685f3aca1d8e7 \n\n\n如果执行kubeadm init时没有记录下加入集群的命令，可以通过以下命令重新创建 kubeadm token create --print-join-command 在k8s-node1上执行kubeadm join ：\n\n[root@k8s-node1 ~]# kubeadm join 192.168.199.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:d7df0d887d0407995eb46df24d3481c5e9b6ffc13da97be7f5f685f3aca1d8e7 \nw0419 19:00:43.118672   32562 join.go:346] [preflight] warning: joincontrolpane.controlplane settings will be ignored when control-plane flag is not set.\n[preflight] running pre-flight checks\n        [warning isdockersystemdcheck]: detected "cgroupfs" as the docker cgroup driver. the recommended driver is "systemd". please follow the guide at https://kubernetes.io/docs/setup/cri/\n[preflight] reading configuration from the cluster...\n[preflight] fyi: you can look at this config file with \'kubectl -n kube-system get cm kubeadm-config -oyaml\'\n[kubelet-start] downloading configuration for the kubelet from the "kubelet-config-1.18" configmap in the kube-system namespace\n[kubelet-start] writing kubelet configuration to file "/var/lib/kubelet/config.yaml"\n[kubelet-start] writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"\n[kubelet-start] starting the kubelet\n[kubelet-start] waiting for the kubelet to perform the tls bootstrap...\n\nthis node has joined the cluster:\n* certificate signing request was sent to apiserver and a response was received.\n* the kubelet was informed of the new secure connection details.\n\nrun \'kubectl get nodes\' on the control-plane to see this node join the cluster.\n\n\n重复执行以上操作将k8s-node2也加进去（注意重新执行kubeadm token create --print-join-command）。 然后根据提示，我们可以通过 kubectl get nodes 查看节点的状态：\n\n说明：\n\n * token\n   * 可以通过安装 master 时的日志查看 token 信息\n   * 可以通过 kubeadm token list 命令打印出 token 信息\n   * 如果 token 过期，可以使用 kubeadm token create 命令创建新的 token\n * discovery-token-ca-cert-hash\n   * 可以通过安装 master 时的日志查看 sha256 信息\n   * 可以通过 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed \'s/^.* //\' 命令查看 sha256 信息\n\n\n# 验证是否成功\n\n回到 master 服务器,可以看到 node 成功加入 master\n\n[centos@k8s-master ~]$ kubectl get nodes\nname         status   roles    age     version\nk8s-master   ready    master   38m     v1.18.2\nk8s-node1    ready    <none>   7m57s   v1.18.2\nk8s-node2    ready    <none>   7m37s   v1.18.2\n\n\n提示\n\n如果 node 节点加入 master 时配置有问题可以在 node 节点上使用 kubeadm reset 重置配置再使用 kubeadm join 命令重新加入即可。希望在 master 节点删除 node ，可以使用 kubeadm delete nodes 删除。\n\nnodes状态全部为ready，由于每个节点都需要启动若干组件，如果node节点的状态是 notready，可以查看所有节点pod状态，确保所有pod成功拉取到镜像并处于running状态：\n\n[centos@k8s-master ~]$ kubectl get pod --all-namespaces -o wide\nnamespace     name                                       ready   status    restarts   age   ip                node         nominated node   readiness gates\nkube-system   calico-kube-controllers-555fc8cc5c-mcmst   1/1     running   0          66m   10.244.235.193    k8s-master   <none>           <none>\nkube-system   calico-node-lh5j2                          1/1     running   0          37m   192.168.199.102   k8s-node1    <none>           <none>\nkube-system   calico-node-s76tm                          1/1     running   0          66m   192.168.199.101   k8s-master   <none>           <none>\nkube-system   calico-node-wd4rj                          1/1     running   0          36m   192.168.199.103   k8s-node2    <none>           <none>\nkube-system   coredns-7ff77c879f-h95kn                   1/1     running   0          67m   10.244.235.195    k8s-master   <none>           <none>\nkube-system   coredns-7ff77c879f-lczbq                   1/1     running   0          67m   10.244.235.194    k8s-master   <none>           <none>\nkube-system   etcd-k8s-master                            1/1     running   0          67m   192.168.199.101   k8s-master   <none>           <none>\nkube-system   kube-apiserver-k8s-master                  1/1     running   0          67m   192.168.199.101   k8s-master   <none>           <none>\nkube-system   kube-controller-manager-k8s-master         1/1     running   0          67m   192.168.199.101   k8s-master   <none>           <none>\nkube-system   kube-proxy-2kg9t                           1/1     running   0          67m   192.168.199.101   k8s-master   <none>           <none>\nkube-system   kube-proxy-7fkx9                           1/1     running   0          36m   192.168.199.103   k8s-node2    <none>           <none>\nkube-system   kube-proxy-mcjc5                           1/1     running   0          37m   192.168.199.102   k8s-node1    <none>           <none>\nkube-system   kube-scheduler-k8s-master                  1/1     running   0          67m   192.168.199.101   k8s-master   <none>           <none>\n\n\n这时，所有的节点都已经 ready，kubernetes cluster 创建成功，一切准备就绪。 如果pod状态为pending、containercreating、imagepullbackoff 都表明 pod 没有就绪，running 才是就绪状态。 如果有pod提示init:imagepullbackoff，说明这个pod的镜像在对应节点上拉取失败，我们可以通过 kubectl describe pod 查看 pod 具体情况，以确认拉取失败的镜像：\n\n[centos@k8s-master ~]$ kubectl describe pod calico-node-lh5j2 --namespace=kube-system\n......\nevents:\n  type     reason     age   from                message\n  ----     ------     ----  ----                -------\n  normal   scheduled  42m   default-scheduler   successfully assigned kube-system/calico-node-lh5j2 to k8s-node1\n  normal   pulling    42m   kubelet, k8s-node1  pulling image "calico/cni:v3.13.2"\n  normal   pulled     37m   kubelet, k8s-node1  successfully pulled image "calico/cni:v3.13.2"\n  normal   created    37m   kubelet, k8s-node1  created container upgrade-ipam\n  normal   started    37m   kubelet, k8s-node1  started container upgrade-ipam\n  normal   pulled     37m   kubelet, k8s-node1  container image "calico/cni:v3.13.2" already present on machine\n  normal   created    37m   kubelet, k8s-node1  created container install-cni\n  normal   started    37m   kubelet, k8s-node1  started container install-cni\n  normal   pulling    37m   kubelet, k8s-node1  pulling image "calico/pod2daemon-flexvol:v3.13.2"\n  normal   pulled     37m   kubelet, k8s-node1  successfully pulled image "calico/pod2daemon-flexvol:v3.13.2"\n  normal   created    37m   kubelet, k8s-node1  created container flexvol-driver\n  normal   started    37m   kubelet, k8s-node1  started container flexvol-driver\n  normal   pulling    37m   kubelet, k8s-node1  pulling image "calico/node:v3.13.2"\n  normal   pulled     33m   kubelet, k8s-node1  successfully pulled image "calico/node:v3.13.2"\n  normal   created    33m   kubelet, k8s-node1  created container calico-node\n  normal   started    33m   kubelet, k8s-node1  started container calico-node\n  warning  unhealthy  33m   kubelet, k8s-node1  readiness probe failed: calico/node is not ready: bird is not ready: bgp not established with 192.168.199.1032020-04-19 11:10:12.409 [info][168] health.go 156: number of node(s) with bgp peering established = 1\n\n\n查看master节点下载了哪些镜像\n\n[centos@k8s-master ~]$  sudo docker images\nrepository                                                        tag                 image id            created             size\ncalico/pod2daemon-flexvol                                         v3.13.2             77a3fabf99a5        2 weeks ago         111mb\ncalico/cni                                                        v3.13.2             a89faaa1676a        2 weeks ago         224mb\ncalico/kube-controllers                                           v3.13.2             0c3bf0adad2b        2 weeks ago         56.6mb\ncalico/node                                                       v3.13.2             6f674c890b23        2 weeks ago         260mb\nregistry.aliyuncs.com/google_containers/kube-proxy                v1.18.0             43940c34f24f        3 weeks ago         117mb\nregistry.aliyuncs.com/google_containers/kube-apiserver            v1.18.0             74060cea7f70        3 weeks ago         173mb\nregistry.aliyuncs.com/google_containers/kube-scheduler            v1.18.0             a31f78c7c8ce        3 weeks ago         95.3mb\nregistry.aliyuncs.com/google_containers/kube-controller-manager   v1.18.0             d3e55153f52f        3 weeks ago         162mb\nregistry.aliyuncs.com/google_containers/pause                     3.2                 80d28bedfe5d        2 months ago        683kb\nregistry.aliyuncs.com/google_containers/coredns                   1.6.7               67da37a9a360        2 months ago        43.8mb\nregistry.aliyuncs.com/google_containers/etcd                      3.4.3-0             303ce5db0e90        5 months ago        288mb\n\n\n查看worker节点下载了哪些镜像：\n\n[root@k8s-node1 ~]#  sudo docker images\nrepository                                                        tag                 image id            created             size\ncalico/pod2daemon-flexvol                                         v3.13.2             77a3fabf99a5        2 weeks ago         111mb\ncalico/cni                                                        v3.13.2             a89faaa1676a        2 weeks ago         224mb\ncalico/node                                                       v3.13.2             6f674c890b23        2 weeks ago         260mb\nregistry.aliyuncs.com/google_containers/kube-proxy                v1.18.0             43940c34f24f        3 weeks ago         117mb\nregistry.aliyuncs.com/google_containers/kube-controller-manager   v1.18.0             d3e55153f52f        3 weeks ago         162mb\nregistry.aliyuncs.com/google_containers/kube-scheduler            v1.18.0             a31f78c7c8ce        3 weeks ago         95.3mb\nregistry.aliyuncs.com/google_containers/kube-apiserver            v1.18.0             74060cea7f70        3 weeks ago         173mb\nregistry.aliyuncs.com/google_containers/pause                     3.2                 80d28bedfe5d        2 months ago        683kb\nregistry.aliyuncs.com/google_containers/coredns                   1.6.7               67da37a9a360        2 months ago        43.8mb\nregistry.aliyuncs.com/google_containers/etcd                      3.4.3-0             303ce5db0e90        5 months ago        288mb\n\n\n\n# 测试集群各个组件\n\n首先验证kube-apiserver, kube-controller-manager, kube-scheduler, pod network 是否正常： 部署一个 nginx deployment，包含2个pod 参考：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\n\n[centos@k8s-master ~]$ kubectl create deployment nginx --image=nginx:alpine\ndeployment.apps/nginx created\n[centos@k8s-master ~]$ kubectl scale deployment nginx --replicas=2\ndeployment.apps/nginx scaled\n\n\n验证nginx pod是否正确运行，并且会分配10.244.开头的集群ip\n\n[centos@k8s-master ~]$ kubectl get pods -l app=nginx -o wide      \nname                     ready   status    restarts   age     ip               node        nominated node   readiness gates\nnginx-745b4df97d-jb5f4   1/1     running   0          3m10s   10.244.169.129   k8s-node2   <none>           <none>\nnginx-745b4df97d-zgdbr   1/1     running   0          2m29s   10.244.36.65     k8s-node1   <none>           <none>\n\n\n\n# 验证kube-proxy\n\n以 nodeport 方式对外提供服务 参考：https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/\n\n[centos@k8s-master ~]$ kubectl expose deployment nginx --port=80 --type=nodeport\nservice/nginx exposed\n[centos@k8s-master ~]$ kubectl get services nginx\nname    type       cluster-ip   external-ip   port(s)        age\nnginx   nodeport   10.97.5.8    <none>        80:32350/tcp   19s\n\n\n可以通过任意 nodeip:port 在集群外部访问这个服务：\n\n[centos@k8s-master ~]$ curl 192.168.199.101:32350\n[centos@k8s-master ~]$ curl 192.168.199.102:32350\n[centos@k8s-master ~]$ curl 192.168.199.103:32350\n\n\n最后验证一下dns, pod network是否正常： 运行busybox并进入交互模式\n\n[centos@k8s-master ~]$ kubectl run -it curl --image=radial/busyboxplus:curl\nif you don\'t see a command prompt, try pressing enter.\n\n\n输入nslookup nginx查看是否可以正确解析出集群内的ip，以验证dns是否正常\n\n[ root@curl:/ ]$ nslookup nginx\nserver:    10.96.0.10\naddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n\nname:      nginx\naddress 1: 10.97.5.8 nginx.default.svc.cluster.local\n\n\n通过服务名进行访问，验证kube-proxy是否正常\n\n[ root@curl:/ ]$ curl http://nginx/\n<!doctype html>\n<html>\n<head>\n<title>welcome to nginx!</title>\n......\n</body>\n</html>\n# 分别访问一下2个pod的内网ip，验证跨node的网络通信是否正常\n[ root@curl:/ ]$ curl 10.244.169.129\n<!doctype html>\n<html>\n<head>\n<title>welcome to nginx!</title>\n......\n</body>\n</html>\n[ root@curl:/ ]$ curl 10.244.36.65\n<!doctype html>\n<html>\n<head>\n<title>welcome to nginx!</title>\n......\n</body>\n</html>\n\n\n\n# 停止服务\n\n[centos@k8s-master ~]$ kubectl delete deployment nginx\ndeployment.apps "nginx" deleted\n[centos@k8s-master ~]$ kubectl delete service nginx\nservice "nginx" deleted\n\n\n\n# pod调度到master节点\n\n出于安全考虑，默认配置下kubernetes不会将pod调度到master节点。查看taints字段默认配置：\n\n[centos@k8s-master ~]$ kubectl describe node k8s-master \n......\ntaints:             node-role.kubernetes.io/master:noschedule\n\n\n如果希望将k8s-master也当作node节点使用，可以执行如下命令,其中k8s-master是主机节点hostname：\n\nkubectl taint node k8s-master node-role.kubernetes.io/master-\n\n\n修改后taints字段状态：\n\n[centos@k8s-master ~]$ kubectl describe node k8s-master                             \n......\ntaints:             <none>\n\n\n如果要恢复master only状态，执行如下命令：\n\nkubectl taint node k8s-master node-role.kubernetes.io/master=:noschedule\n\n\n\n# kube-proxy开启ipvs\n\n修改configmap的kube-system/kube-proxy中的config.conf，mode: “ipvs”：\n\n[centos@k8s-master ~]$ kubectl edit cm kube-proxy -n kube-system\nconfigmap/kube-proxy edited\n之后重启各个节点上的kube-proxy pod：\n\n[centos@k8s-master ~]$ kubectl get pod -n kube-system | grep kube-proxy | awk \'{system("kubectl delete pod "$1" -n kube-system")}\'\npod "kube-proxy-2w9sh" deleted\npod "kube-proxy-gw4lx" deleted\npod "kube-proxy-thv4c" deleted\n[centos@k8s-master ~]$ kubectl get pod -n kube-system | grep kube-proxy\nkube-proxy-6qlgv                        1/1     running   0          65s\nkube-proxy-fdtjd                        1/1     running   0          47s\nkube-proxy-m8zkx                        1/1     running   0          52s\n[centos@k8s-master ~]$\n\n\n查看日志：\n\n[centos@k8s-master ~]$ kubectl logs kube-proxy-6qlgv -n kube-system\ni1213 09:50:15.414493       1 server_others.go:189] using ipvs proxier.\nw1213 09:50:15.414908       1 proxier.go:365] ipvs scheduler not specified, use rr by default\ni1213 09:50:15.415021       1 server_others.go:216] tearing down inactive rules.\ni1213 09:50:15.461658       1 server.go:464] version: v1.13.0\ni1213 09:50:15.467827       1 conntrack.go:52] setting nf_conntrack_max to 131072\ni1213 09:50:15.467997       1 config.go:202] starting service config controller\ni1213 09:50:15.468010       1 controller_utils.go:1027] waiting for caches to sync for service config controller\ni1213 09:50:15.468092       1 config.go:102] starting endpoints config controller\ni1213 09:50:15.468100       1 controller_utils.go:1027] waiting for caches to sync for endpoints config controller\ni1213 09:50:15.568766       1 controller_utils.go:1034] caches are synced for endpoints config controller\ni1213 09:50:15.568950       1 controller_utils.go:1034] caches are synced for service config controller\n[centos@k8s-master ~]$\n\n\n日志中打印出了using ipvs proxier，说明ipvs模式已经开启。\n\n\n# 移除节点和集群\n\nkubernetes集群移除节点，以移除k8s-node2节点为例，在master节点上运行：\n\nkubectl drain k8s-node2 --delete-local-data --force --ignore-daemonsets\nkubectl delete node k8s-node2\n\n\n上面两条命令执行完成后，在k8s-node2节点执行清理命令，重置kubeadm的安装状态：\n\nkubeadm reset\n\n\n在master上删除node并不会清理k8s-node2运行的容器，需要在删除节点上面手动运行清理命令。 如果你想重新配置集群，使用新的参数重新运行kubeadm init或者kubeadm join即可。\n\n至此3个节点的集群搭建完成，后续可以继续添加node节点，或者部署dashboard、helm包管理工具、efk日志系统、prometheus operator监控系统、rook+ceph存储系统等组件。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:50:39",lastUpdatedTimestamp:1644378639e3},{title:"概念总结",frontmatter:{title:"概念总结",date:"2022-02-09T11:48:30.000Z",permalink:"/pages/f4b696/"},regularPath:"/18.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/09.%E6%A6%82%E5%BF%B5%E6%80%BB%E7%BB%93.html",relativePath:"18.Kubernetes/01.Kubernetes 入门/09.概念总结.md",key:"v-7e4ade2c",path:"/pages/f4b696/",headers:[{level:2,title:"什么是 Kubernetes",slug:"什么是-kubernetes",normalizedTitle:"什么是 kubernetes",charIndex:11},{level:2,title:"Kubernetes Master",slug:"kubernetes-master",normalizedTitle:"kubernetes master",charIndex:756},{level:2,title:"Kubernetes Node",slug:"kubernetes-node",normalizedTitle:"kubernetes node",charIndex:865},{level:2,title:"Kubernetes 架构",slug:"kubernetes-架构",normalizedTitle:"kubernetes 架构",charIndex:1538}],headersStr:"什么是 Kubernetes Kubernetes Master Kubernetes Node Kubernetes 架构",content:"# 概念总结\n\n\n# 什么是 Kubernetes\n\nKubernetes 是一个开源的 Docker 容器编排系统，它可以调度计算集群的节点，动态管理上面的作业，保证它们按用户期望的状态运行。通过使用「labels」和「pods」的概念，Kubernetes 将应用按逻辑单元进行分组，方便管理和服务发现。\n\n\n\n * pods： 是一组紧密关联的容器集合，它们共享 IPC(进程间通信)、Network(网络) 和 UTS namespace(UTS 命名空间是 Linux 命名空间的一个子系统，主要作用是完成对容器 Hostname 和 Domain 的隔离，同时保存内核名称、版本、以及底层体系结构类型等信息)，是 Kubernetes 调度的基本单位。\n * labels： 键值对(key/value)标签，可以被关联到如 Pod 这样的对象上，主要作用是给用户一个直观的感受，比如这个 Pod 是用来放置数据库的\n * GUI： 用户图形界面，可以是 Web 用户界面，比如使用 kubernetes-dashboard 组件，用户可以通过 Dashboard 在 Kubernetes 集群中部署容器化的应用，可以查看集群中应用的运行情况，同时也能够基于 Dashboard 创建或修改部署、任务、服务等 Kubernetes 的资源。通过部署向导，用户能够对部署进行扩缩容，进行滚动更新、重启 Pod 和部署新应用。当然，通过 Dashboard 也能够查看 Kubernetes 资源的状态\n * kubectl： 用于管理 Kubernetes 集群的命令行工具\n * kube-apiserver： 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制\n * Kubernetes Master： Kubernetes 集群主节点，主要由 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 四个模块组成\n * Kubernetes Node： Kubernetes 集群子节点，主要由 kubelet、kube-proxy、runtime 三个模块组成\n * Image Registry： 镜像仓库，比如：Ducker HUB 或 Docker 私服\n\n\n# Kubernetes Master\n\n\n\n * kube-apiserver： 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制\n * kube-scheduler： 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上\n * kube-controller-manager： 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等\n * etcd： CoreOS 基于 Raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）\n\n\n# Kubernetes Node\n\n\n\n * runtime： 负责镜像管理以及 Pod 和容器的真正运行（CRI，Container Runtime Interface），默认的容器运行时为 Docker，还支持 RKT 容器\n * kubelet： 负责维持容器的生命周期，同时也负责 Volume（CVI，Container Volume Interface）和网络（CNI，Container Network Interface）的管理\n * kube-proxy： 负责为 Service 提供 cluster 内部的服务发现和负载均衡\n\n\n# Kubernetes 架构\n\n\n\n",normalizedContent:"# 概念总结\n\n\n# 什么是 kubernetes\n\nkubernetes 是一个开源的 docker 容器编排系统，它可以调度计算集群的节点，动态管理上面的作业，保证它们按用户期望的状态运行。通过使用「labels」和「pods」的概念，kubernetes 将应用按逻辑单元进行分组，方便管理和服务发现。\n\n\n\n * pods： 是一组紧密关联的容器集合，它们共享 ipc(进程间通信)、network(网络) 和 uts namespace(uts 命名空间是 linux 命名空间的一个子系统，主要作用是完成对容器 hostname 和 domain 的隔离，同时保存内核名称、版本、以及底层体系结构类型等信息)，是 kubernetes 调度的基本单位。\n * labels： 键值对(key/value)标签，可以被关联到如 pod 这样的对象上，主要作用是给用户一个直观的感受，比如这个 pod 是用来放置数据库的\n * gui： 用户图形界面，可以是 web 用户界面，比如使用 kubernetes-dashboard 组件，用户可以通过 dashboard 在 kubernetes 集群中部署容器化的应用，可以查看集群中应用的运行情况，同时也能够基于 dashboard 创建或修改部署、任务、服务等 kubernetes 的资源。通过部署向导，用户能够对部署进行扩缩容，进行滚动更新、重启 pod 和部署新应用。当然，通过 dashboard 也能够查看 kubernetes 资源的状态\n * kubectl： 用于管理 kubernetes 集群的命令行工具\n * kube-apiserver： 提供了资源操作的唯一入口，并提供认证、授权、访问控制、api 注册和发现等机制\n * kubernetes master： kubernetes 集群主节点，主要由 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 四个模块组成\n * kubernetes node： kubernetes 集群子节点，主要由 kubelet、kube-proxy、runtime 三个模块组成\n * image registry： 镜像仓库，比如：ducker hub 或 docker 私服\n\n\n# kubernetes master\n\n\n\n * kube-apiserver： 提供了资源操作的唯一入口，并提供认证、授权、访问控制、api 注册和发现等机制\n * kube-scheduler： 负责资源的调度，按照预定的调度策略将 pod 调度到相应的机器上\n * kube-controller-manager： 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等\n * etcd： coreos 基于 raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）\n\n\n# kubernetes node\n\n\n\n * runtime： 负责镜像管理以及 pod 和容器的真正运行（cri，container runtime interface），默认的容器运行时为 docker，还支持 rkt 容器\n * kubelet： 负责维持容器的生命周期，同时也负责 volume（cvi，container volume interface）和网络（cni，container network interface）的管理\n * kube-proxy： 负责为 service 提供 cluster 内部的服务发现和负载均衡\n\n\n# kubernetes 架构\n\n\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Kubernetes Dashboard",frontmatter:{title:"Kubernetes Dashboard",date:"2022-02-09T11:48:30.000Z",permalink:"/pages/e68987/"},regularPath:"/18.Kubernetes/02.Kubernetes%20%E5%AE%9E%E8%B7%B5/01.Kubernetes%20Dashboard.html",relativePath:"18.Kubernetes/02.Kubernetes 实践/01.Kubernetes Dashboard.md",key:"v-61b52e07",path:"/pages/e68987/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:27},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:88},{level:2,title:"访问",slug:"访问",normalizedTitle:"访问",charIndex:1785},{level:2,title:"登录",slug:"登录",normalizedTitle:"登录",charIndex:2312}],headersStr:"概述 安装 访问 登录",content:'# Kubernetes Dashboard\n\n\n# 概述\n\nKubernetes Dashboard 是 Kubernetes 集群的 Web UI，用于管理集群。\n\n\n# 安装\n\nGitHub 地址：Kubernetes Dashboard\n\n下载配置文件\n\nwget https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml\n\n\n修改配置如下\n\n# 省略部分代码...\n\n# ------------------- Dashboard Deployment ------------------- #\n\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard\n    spec:\n      containers:\n      - name: kubernetes-dashboard\n        # 修改镜像地址为阿里云\n        image: registry.aliyuncs.com/google_containers/dashboard:v2.5.0\n        ports:\n        - containerPort: 8443\n          protocol: TCP\n        args:\n          - --auto-generate-certificates\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /\n            port: 8443\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n\n---\n# ------------------- Dashboard Service ------------------- #\n\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  # 修改类型为 NodePort 访问\n  type: NodePort\n  ports:\n    - port: 443\n      targetPort: 8443\n      # 设置端口号为 30001\n      nodePort: 30001\n  selector:\n    k8s-app: kubernetes-dashboard\n\n\n部署到集群\n\n# 部署\nkubectl apply -f kubernetes-dashboard.yaml\n\n# 查看\nkubectl -n kubernetes-dashboard get pods\nkubectl -n kubernetes-dashboard get service kubernetes-dashboard\nkubectl -n kubernetes-dashboard describe service kubernetes-dashboard\n\n\n\n# 访问\n\n需要使用 NodeIP:30001 访问 Dashboard，因为证书原因除火狐浏览器外其它浏览器无法直接打开页面\n\nChrome 浏览器显示如下\n\n\n\nFirefox 浏览器显示如下\n\n\n\n点击 接受风险并继续 即可显示欢迎界面\n\n\n\n\n# 登录\n\n我们采用 Token 方式登录\n\n * 创建登录账号，创建一个名为 dashboard-adminuser.yaml 的配置文件\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n\n\nkubectl apply -f dashboard-adminuser.yaml\n\n\n * 现在我们需要找到可以用来登录的令牌。执行以下命令：\n\nkubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"\n\n\n它应该打印如下内容：\n\neyJhbGciOiJSUzI1NiIsImtpZCI6IkQ5NmxnbUlWcENuMFJDYzM1ZDYxSFI2ZF9BRXYwaVVFUENfYTlBU2swVEkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTUyODJoIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3NGJkNjk4MC1kMzcwLTQzN2ItYjE3Zi03NmQ3MmViZTk3NTMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.XjU01lmdwtRfQbcTcK2BUgRpZOgYlz44qJkoRKAqkQFj2lEFyGwRFCz2oaHD_1fTWH6Q9NetAHrkaRPrt9D0JMrEKnGtiH2wqcuXkleIMx51tmpHsjDxJa_B7C3AD2rDfxyr1DpKmp8vIAx276loAO5ke0ggbGYOHlopLZzcahQhAjvIJtb6U15eZ76VAoxsTYq0CU3J_cgydH56_KYZQ76wI3qnjLf2fuQHvryMLV9UhHBqLLYn3jH24Kqye5TOuIEG9gL5vA3cLbv5fLGFAkZp7T4qEQhlULs1P9W0JEV7u7N9nqXqSvteoGP-HE5l3DwizaCig8hkhKLeB3dx4g\n\n\n * 将 Token 输入浏览器，成功登陆后效果如下\n\n',normalizedContent:'# kubernetes dashboard\n\n\n# 概述\n\nkubernetes dashboard 是 kubernetes 集群的 web ui，用于管理集群。\n\n\n# 安装\n\ngithub 地址：kubernetes dashboard\n\n下载配置文件\n\nwget https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml\n\n\n修改配置如下\n\n# 省略部分代码...\n\n# ------------------- dashboard deployment ------------------- #\n\nkind: deployment\napiversion: apps/v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  replicas: 1\n  revisionhistorylimit: 10\n  selector:\n    matchlabels:\n      k8s-app: kubernetes-dashboard\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard\n    spec:\n      containers:\n      - name: kubernetes-dashboard\n        # 修改镜像地址为阿里云\n        image: registry.aliyuncs.com/google_containers/dashboard:v2.5.0\n        ports:\n        - containerport: 8443\n          protocol: tcp\n        args:\n          - --auto-generate-certificates\n        volumemounts:\n        - name: kubernetes-dashboard-certs\n          mountpath: /certs\n        - mountpath: /tmp\n          name: tmp-volume\n        livenessprobe:\n          httpget:\n            scheme: https\n            path: /\n            port: 8443\n          initialdelayseconds: 30\n          timeoutseconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretname: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptydir: {}\n      serviceaccountname: kubernetes-dashboard\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: noschedule\n\n---\n# ------------------- dashboard service ------------------- #\n\nkind: service\napiversion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  # 修改类型为 nodeport 访问\n  type: nodeport\n  ports:\n    - port: 443\n      targetport: 8443\n      # 设置端口号为 30001\n      nodeport: 30001\n  selector:\n    k8s-app: kubernetes-dashboard\n\n\n部署到集群\n\n# 部署\nkubectl apply -f kubernetes-dashboard.yaml\n\n# 查看\nkubectl -n kubernetes-dashboard get pods\nkubectl -n kubernetes-dashboard get service kubernetes-dashboard\nkubectl -n kubernetes-dashboard describe service kubernetes-dashboard\n\n\n\n# 访问\n\n需要使用 nodeip:30001 访问 dashboard，因为证书原因除火狐浏览器外其它浏览器无法直接打开页面\n\nchrome 浏览器显示如下\n\n\n\nfirefox 浏览器显示如下\n\n\n\n点击 接受风险并继续 即可显示欢迎界面\n\n\n\n\n# 登录\n\n我们采用 token 方式登录\n\n * 创建登录账号，创建一个名为 dashboard-adminuser.yaml 的配置文件\n\napiversion: v1\nkind: serviceaccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiversion: rbac.authorization.k8s.io/v1\nkind: clusterrolebinding\nmetadata:\n  name: admin-user\nroleref:\n  apigroup: rbac.authorization.k8s.io\n  kind: clusterrole\n  name: cluster-admin\nsubjects:\n- kind: serviceaccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n\n\nkubectl apply -f dashboard-adminuser.yaml\n\n\n * 现在我们需要找到可以用来登录的令牌。执行以下命令：\n\nkubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"\n\n\n它应该打印如下内容：\n\neyjhbgcioijsuzi1niisimtpzci6ikq5nmxnbulwcenumfjdyzm1zdyxsfi2zf9brxywavvfuenfytlbu2swvekifq.eyjpc3mioijrdwjlcm5ldgvzl3nlcnzpy2vhy2nvdw50iiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9uyw1lc3bhy2uioijrdwjlcm5ldgvzlwrhc2hib2fyzcisimt1ymvybmv0zxmuaw8vc2vydmljzwfjy291bnqvc2vjcmv0lm5hbwuioijhzg1pbi11c2vylxrva2vultuyodjoiiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9zzxj2awnllwfjy291bnqubmftzsi6imfkbwlulxvzzxiilcjrdwjlcm5ldgvzlmlvl3nlcnzpy2vhy2nvdw50l3nlcnzpy2utywnjb3vudc51awqioii3ngjknjk4mc1kmzcwltqzn2ityje3zi03nmq3mmviztk3ntmilcjzdwiioijzexn0zw06c2vydmljzwfjy291bnq6a3vizxjuzxrlcy1kyxnoym9hcmq6ywrtaw4tdxnlcij9.xju01lmdwtrfqbctck2bugrpzogylz44qjkorkaqkqfj2lefygwrfcz2oahd_1ftwh6q9netahrkarprt9d0jmrekngtih2wqcuxkleimx51tmphsjdxja_b7c3ad2rdfxyr1dpkmp8viax276loao5ke0ggbgyohloplzzcahqhajvijtb6u15ez76vaoxstyq0cu3j_cgydh56_kyzq76wi3qnjlf2fuqhvrymlv9uhhbqllyn3jh24kqye5touieg9gl5va3clbv5flgfakzp7t4qeqhluls1p9w0jev7u7n9nqxqsvteogp-he5l3dwizacig8hkhkleb3dx4g\n\n\n * 将 token 输入浏览器，成功登陆后效果如下\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/01, 13:42:47",lastUpdatedTimestamp:1646113367e3},{title:"NFS 高可用方案",frontmatter:{title:"NFS 高可用方案",date:"2022-03-01T13:55:00.000Z",permalink:"/pages/1b012a/"},regularPath:"/18.Kubernetes/02.Kubernetes%20%E5%AE%9E%E8%B7%B5/02.NFS%20%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88.html",relativePath:"18.Kubernetes/02.Kubernetes 实践/02.NFS 高可用方案.md",key:"v-88106b62",path:"/pages/1b012a/",headers:[{level:2,title:"安装环境",slug:"安装环境",normalizedTitle:"安装环境",charIndex:107},{level:2,title:"准备工作",slug:"准备工作",normalizedTitle:"准备工作",charIndex:115},{level:2,title:"安装 NFS",slug:"安装-nfs",normalizedTitle:"安装 nfs",charIndex:123},{level:2,title:"测试 NFS",slug:"测试-nfs",normalizedTitle:"测试 nfs",charIndex:133},{level:2,title:"在 Slave 进行同步 Master 数据",slug:"在-slave-进行同步-master-数据",normalizedTitle:"在 slave 进行同步 master 数据",charIndex:143},{level:2,title:"在 Master 进行同步 Slave 数据",slug:"在-master-进行同步-slave-数据",normalizedTitle:"在 master 进行同步 slave 数据",charIndex:169},{level:2,title:"Master安装 Keepalived",slug:"master安装-keepalived",normalizedTitle:"master安装 keepalived",charIndex:195},{level:2,title:"Slave安装 Keepalived",slug:"slave安装-keepalived",normalizedTitle:"slave安装 keepalived",charIndex:218},{level:2,title:"测试 NFS",slug:"测试-nfs-2",normalizedTitle:"测试 nfs",charIndex:133},{level:2,title:"k8s集群node节点挂在",slug:"k8s集群node节点挂在",normalizedTitle:"k8s集群node节点挂在",charIndex:250},{level:2,title:"设置 Keepalived 脚本",slug:"设置-keepalived-脚本",normalizedTitle:"设置 keepalived 脚本",charIndex:267}],headersStr:"安装环境 准备工作 安装 NFS 测试 NFS 在 Slave 进行同步 Master 数据 在 Master 进行同步 Slave 数据 Master安装 Keepalived Slave安装 Keepalived 测试 NFS k8s集群node节点挂在 设置 Keepalived 脚本",content:'# NFS (Kubernetes) 高可用方案（NFS+keepalived+Sersync)\n\n提示\n\nNFS (Kubernetes) 高可用方案（NFS+keepalived+Sersync)\n\n\n\n * 安装环境\n * 准备工作\n * 安装 NFS\n * 测试 NFS\n * 在 Slave 进行同步 Master 数据\n * 在 Master 进行同步 Slave 数据\n * Master安装 Keepalived\n * Slave安装 Keepalived\n * 测试 NFS\n * k8s集群node节点挂在\n * 设置 Keepalived 脚本\n\n\n\n\n# 安装环境\n\n本方案是 NFS 的高可用方案，客户端为 k8s集群三台，文件服务器分别为 Master 和 Slave，使用 keepalived 生成一个虚拟 IP，使用 Sersync 进行 Master 与 Slave 之间文件相互同步，确保高可用。\n\n角色       系统           IP\n虚拟IP                  192.168.100.110\nClient   k8s集群        192.168.100.116,192.168.100.117,192.168.100.118\nMaster   centos 7.9   192.168.100.111\nSlave    centos 7.9   192.168.100.112\n\n\n# 准备工作\n\n安装 vim，并且关闭防火墙\n\nyum install vim -y\nsystemctl stop firewalld && systemctl disable firewalld\n\n\n查看内核版本\n\n[root@localhost ~]# uname -smr\nLinux 3.10.0-1160.59.1.el7.x86_64 x86_64\n\n\n> 建议升级到最新的长期支持版本 升级内核\n\n[root@localhost ~]# cat /etc/redhat-release #查看系统版本\nCentOS Linux release 7.9.2009 (Core)\n[root@localhost ~]# uname -r #查看系统内核版本\n3.10.0-1160.59.1.el7.x86_64\n[root@localhost ~]# uname -m #查看系统是否64位\nx86_64\n\n\n\n# 安装 NFS\n\n在 master 和 slave 上面创建共享目录，数据盘挂在到/data。\n\nmkdir -p /data/nfs-share\n## 赋予写权限\nchmod o+w /data/nfs-share\n\n\n在master 和 slave上安装nfs服务\n\nyum -y install nfs-utils rpcbind\n\n\n设定分享的目录（配置要改动需要reload nfs 服务）\n\n## 指定具体k8s node节点IP地址\necho \'/data/nfs-share 192.168.100.116,192.168.100.117,192.168.100.118(rw,sync,all_squash)\' >> /etc/exports\n\n\n分享目录的范围详解（给哪些客户端分享）\n\n点击查看\n\n(1).指定ip地址的主机：192.168.0.100\n\n只允许IP地址为192.168.0.100的主机可以登录到此共享目录\n\n(2).指定子网中的所有主机：192.168.0.0/24或者192.168.0.0/255.255.255.0\n\n只允许192.168.0.1~192.168.0.254这个网段内的IP登录到此共享目录\n\n(3).指定域名的主机：centosNameNode.pigxcloud.com\n\n只允许主机名为centosNameNode.pigxcloud.com的可以登录到此共享目录\n\n(4).指定域内的所有主机：*.pigxcloud.com\n\n只允许域名为.pigxcloud.com的可以登录到此共享目录\n\n(5).指定所有的主机：*\n\n分享目录的权限详解（客户端能对分享的目录进行哪些操作）\n\n点击查看\n\n(1).rw：可读可写的权限\n\n(2).ro：只读的权限\n\n(3).no_root_squash：登入分享目录的使用者如果是root的话，那么对于这个分享的目录来说，他就具有root的权限！这一项不安全，不建议使用！\n\n(4).root_squash：登入分享目录的使用者如果是root的话，那么这个使用者的权限将被压缩成匿名使用者，通常他的UID与GID都会变成nobody那个系统账号的身份。\n\n(5).all_squash：不论登入分享目录的使用者是什么身份，都会被压缩成匿名使用者。\n\n(6).anonuid：可以自行设定使用者UID的值！也就是使用者登录到分享目录中，身份成为UID这个使用者，必须在/etc/passwd中存在这个UID。\n\n(7).anongid：可以自行设定使用者GID的值！也就是使用者登录到分享目录中，身份成为GID这个使用者，必须在/etc/group中存在这个GID。\n\n(8).sync：数据同步写入到内存与硬盘当中\n\n(9).async：数据会先暂存于内存当中，而非直接写入硬盘\n\n开启服务\n\nsystemctl start rpcbind && systemctl start nfs\n\n\n设置开机自启动\n\nsystemctl enable rpcbind && systemctl enable nfs\n\n\n\n# 测试 NFS\n\n在k8s集群里面找个node 测试是否能链接上nfs服务\n\n下载nfs\n\nyum -y install nfs-utils rpcbind\n\n\n挂载目录\n\nmkdir -p /data/nfs-data\n\n\n挂载nfs\n\nmount -t nfs 192.168.100.111:/data/nfs-share /data/nfs-data\n\n\n查看\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n[root@localhost ~]# df -TH\nFilesystem                      Type      Size  Used Avail Use% Mounted on\ndevtmpfs                        devtmpfs  508M     0  508M   0% /dev\ntmpfs                           tmpfs     520M     0  520M   0% /dev/shm\ntmpfs                           tmpfs     520M  7.1M  513M   2% /run\ntmpfs                           tmpfs     520M     0  520M   0% /sys/fs/cgroup\n/dev/mapper/centos-root         xfs        54G  1.7G   52G   4% /\n/dev/mapper/centos-home         xfs       1.1T   34M  1.1T   1% /home\n/dev/sda1                       xfs       1.1G  208M  856M  20% /boot\ntmpfs                           tmpfs     104M     0  104M   0% /run/user/0\n192.168.100.111:/data/nfs-share nfs4       54G  1.8G   52G   4% /data1/nfs-data\n\n\n卸载挂载\n\numount /data1/nfs-data\n\n\n检查 nfs 服务器端是否有设置共享目录 showmount -e $(nfs服务器的IP)\n\n[root@localhost ~]# showmount -e 192.168.100.111\nExport list for 192.168.100.111:\n/data/nfs-share 192.168.100.0/23\n\n\n\n# 在 Slave 进行同步 Master 数据\n\n提到数据同步就必然会谈到rsync，一般简单的服务器数据传输会使用ftp/sftp等方式，但是这样的方式效率不高，不支持差异化增量同步也不支持实时传输。针对数据实时同步需求大多数人会选择rsync+inotify-tools的解决方案，但是这样的方案也存在一些缺陷，sersync是国人基于前两者开发的工具，不仅保留了优点同时还强化了实时监控，文件过滤，简化配置等功能，帮助用户提高运行效率，节省时间和网络资源。\n\n在 Slave 上编辑配置文件\n\nvim /etc/rsyncd.conf\n\n\n点击查看\n\n# 设置rsync运行权限为root\nuid = root\n# 设置rsync运行权限为root\ngid = root\n# 默认端口\nport = 873\n# pid文件的存放位置\npid file = /var/rsyncd.pid\n# 日志文件位置，启动rsync后自动产生这个文件，无需提前创建\nlog file = /var/log/rsyncd.log\n# 默认为true，修改为no，增加对目录文件软连接的备份\nuse chroot = no\n# 最大连接数\nmax connections = 200\n# 设置rsync服务端文件为读写权限\nread only = no\n# 不显示rsync服务端资源列表 \nlist = false\nfake super = yes\nignore errors\n[data]\n# rsync服务端数据目录路径\npath = /data/nfs-share\n# 执行数据同步的用户名，可以设置多个，用英文状态下逗号隔开\nauth users = root\n# 用户认证配置文件，里面保存用户名称和密码，后面会创建这个文件\nsecrets file = /etc/rsync_slave.pass\n# 允许进行数据同步的客户端IP地址，可以设置多个，用英文状态下逗号隔开\nhosts allow = 192.168.100.111\n# 禁止数据同步的客户端IP地址，可以设置多个，用英文状态下逗号隔开\n#hosts deny = 192.168.0.1\n\n\n在 Slave 上安装启动\n\nyum install rsync\necho \'root:123456\' > /etc/rsync_slave.pass\nchmod 600 /etc/rsync_slave.pass\nchown -R root:root /data/nfs-share\nrsync --daemon --config=/etc/rsyncd.conf\n\n\n在 Master 上测试下\n\nyum -y install rsync\nchown -R root:root /data/nfs-share\necho "123456" > /etc/rsync_slave.pass\nchmod 600 /etc/rsync_slave.pass\n// 创建测试文件,测试推送\ncd /data/nfs-share\necho "test" > slave.txt\nrsync -arv /data/nfs-share/ root@192.168.100.112::data --password-file=/etc/rsync_slave.pass\n#在 slave 上测试\nls /fs/nsf-data\n# 出现 slave.txt 即可\n\n\n在 Master 上配置自动同步安装\n\n# 下载\nwget https://raw.githubusercontent.com/orangle/sersync/master/release/sersync2.5.4_64bit_binary_stable_final.tar.gz\n# 解压\ntar zxvf sersync2.5.4_64bit_binary_stable_final.tar.gz  \n# 移动目录到 /usr/local/sersync\nmv GNU-Linux-x86  /usr/local/sersync   \n\n\n在 Master 上配置自动同步配置\n\ncd /usr/local/sersync/ && vim /usr/local/sersync/confxml.xml\n\n\n修改配置文件里面节点数据\n\n点击查看\n\n<delete start="flase"/>\n\n#源服务器同步目录\n<localpath watch="/data/nfs-share/">  \n\n#remote ip="172.12.17.167"  #目标服务器ip，每行一个 \n#name="data"   目标服务器rsync同步目录模块名称\n<remote ip="172.12.17.167" name="data"/> \n\n#rsync同步认证\n#start="true"   \n#users="root" 目标服务器rsync同步用户名\n#passwordfile="/etc/rsync_slave.pass" 目标服务器rsync同步用户的密码在源服务器的存放路径\n<auth start="true" users="root" passwordfile="/etc/rsync_slave.pass"/>\n\n# 设置rsync远程服务端口，远程非默认端口则需打开自定义\n<userDefinedPort start="false" port="873"/>\x3c!-- port=874 --\x3e\n\n#脚本运行失败日志记录\nfailLog path="/tmp/rsync_fail_log.sh"\n\n\n启动\n\n./sersync2 -r -d -o confxml.xml\n\n\n测试\n\ncd /data/nfs-share && touch slave.txt\n\n\n在 Slave 里面 /data/nfs-share 里面去看看数据是否存在\n\n\n\n\n\n\n \n\n\n[root@localhost nfs-share]# ls -al \ntotal 12512\ndrwxr-xrwx. 2 root root      146 Mar  1 12:28 .\ndrwxr-xr-x. 3 root root       23 Mar  1 10:43 ..\n-rw-r--r--. 1 root root        5 Mar  1 10:56 slave.txt\n\n\n\n# 在 Master 进行同步 Slave 数据\n\n注意\n\n如果Master挂掉后恢复不能同步数据到Slave，这里也同样配置Master到Slave的自动同步\n\n在Master上编辑配置文件\n\nvim /etc/rsyncd.conf\n\n\n点击查看\n\n# 设置rsync运行权限为root\nuid = root\n# 设置rsync运行权限为root\ngid = root\n# 默认端口\nport = 873\n# pid文件的存放位置\npid file = /var/rsyncd.pid\n# 日志文件位置，启动rsync后自动产生这个文件，无需提前创建\nlog file = /var/log/rsyncd.log\n# 默认为true，修改为no，增加对目录文件软连接的备份\nuse chroot = no\n# 最大连接数\nmax connections = 200\n# 设置rsync服务端文件为读写权限\nread only = no\n# 不显示rsync服务端资源列表 \nlist = false\nfake super = yes\nignore errors\n[data]\n# rsync服务端数据目录路径\npath = /data/nfs-share\n# 执行数据同步的用户名，可以设置多个，用英文状态下逗号隔开\nauth users = root\n# 用户认证配置文件，里面保存用户名称和密码，后面会创建这个文件\nsecrets file = /etc/rsync_master.pass\n# 允许进行数据同步的客户端IP地址，可以设置多个，用英文状态下逗号隔开\nhosts allow = 192.168.100.112\n# 禁止数据同步的客户端IP地址，可以设置多个，用英文状态下逗号隔开\n#hosts deny = 192.168.0.1\n\n\n在Master上安装启动\n\nyum install rsync\necho \'root:123456\' > /etc/rsync_master.pass\nchmod 600 /etc/rsync_master.pass\nchown -R root:root /data/nfs-share\nrsync --daemon --config=/etc/rsyncd.conf\n\n\n在 Slave 上测试下\n\nyum -y install rsync\nchown -R root:root /data/nfs-share\necho "123456" > /etc/rsync_master.pass\nchmod 600 /etc/rsync_master.pass\n# 创建测试文件,测试推送\ncd /data/nfs-share\necho "test" > master.txt\nrsync -arv /data/nfs-share/ root@192.168.100.111::data --password-file=/etc/rsync_master.pass\n# 在 Master 上测试\nls /data/nfs-share\n# 出现 master.txt 即可\n\n\n在 Slave 上配置自动同步安装\n\n# 下载\nwget https://raw.githubusercontent.com/orangle/sersync/master/release/sersync2.5.4_64bit_binary_stable_final.tar.gz\n# 解压\ntar zxvf sersync2.5.4_64bit_binary_stable_final.tar.gz  \n# 移动目录到 /usr/local/sersync\nmv GNU-Linux-x86  /usr/local/sersync   \n\n\n在 Slave 上配置自动同步配置\n\ncd /usr/local/sersync/ && vim /usr/local/sersync/confxml.xml\n\n\n修改配置文件里面节点数据\n\n点击查看\n\n<delete start="flase"/>\n\n#源服务器同步目录\n<localpath watch="/data/nfs-share/">  \n\n#remote ip="192.168.100.111"  #目标服务器ip，每行一个 \n#name="data"   目标服务器rsync同步目录模块名称\n<remote ip="192.168.100.111" name="data"/> \n\n#rsync同步认证\n#start="true"   \n#users="root" 目标服务器rsync同步用户名\n#passwordfile="/etc/rsync_master.pass" 目标服务器rsync同步用户的密码在源服务器的存放路径\n<auth start="true" users="root" passwordfile="/etc/rsync_master.pass"/>\n\n# 设置rsync远程服务端口，远程非默认端口则需打开自定义\n<userDefinedPort start="false" port="873"/>\x3c!-- port=874 --\x3e\n\n#脚本运行失败日志记录\nfailLog path="/tmp/rsync_fail_log.sh"\n\n\n启动\n\n./sersync2 -r -d -o confxml.xml\n# 测试\ncd /data/nfs-share\ntouch Master1.txt\n# 在 Slave 里面 /data/nfs-share 里面去看看数据是否存在\n\n\n\n# Master安装 Keepalived\n\n安装 keepalived\n\nyum -y install keepalived\n\n\n修改配置文件\n\nvim /etc/keepalived/keepalived.conf\n\n\n点击查看\n\n# 其中 enp0s3 为绑定网卡名称，可以使用 ip addr 命令查看\n# 其中 192.168.100.123  为虚拟 ip ，注意不要和其它 ip 冲突\n! Configuration File for keepalived\nglobal_defs {\n   router_id lb01\n}\n\nvrrp_instance VI_1 {\n    state MASTER\n    interface enp0s3\n    # 虚拟路由标识，这个标识是一个数字，同一个vrrp实例使用唯一的标识。\n    # 即同一vrrp_instance下，MASTER和BACKUP必须是一致的\n    virtual_router_id 51\n    # 定义优先级，数字越大，优先级越高（0-255）。\n    # 在同一个vrrp_instance下，MASTER 的优先级必须大于 BACKUP 的优先级\n    priority 150\n    # 设定 MASTER 与 BACKUP 负载均衡器之间同步检查的时间间隔，单位是秒\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 123456\n    }\n    virtual_ipaddress {\n        192.168.100.123/23 brd 192.168.101.255\n    }\n}\n\n\n启动服务\n\nsystemctl start keepalived.service && systemctl enable keepalived.service\n\n\n\n# Slave安装 Keepalived\n\n安装 keepalived\n\nyum -y install keepalived\n\n\n修改配置文件\n\nvim /etc/keepalived/keepalived.conf\n\n\n点击查看\n\n# 其中 enp0s3 为绑定网卡名称，可以使用 ip addr 命令查看\n# 其中 192.168.100.123  为虚拟 ip ，注意不要和其它 ip 冲突\n! Configuration File for keepalived\nglobal_defs {\n   router_id lb02\n}\n\nvrrp_instance VI_1 {\n    state MASTER\n    interface enp0s3\n    # 虚拟路由标识，这个标识是一个数字，同一个vrrp实例使用唯一的标识。\n    # 即同一vrrp_instance下，MASTER和BACKUP必须是一致的\n    virtual_router_id 51\n    # 定义优先级，数字越大，优先级越高（0-255）。\n    # 在同一个vrrp_instance下，MASTER 的优先级必须大于 BACKUP 的优先级\n    priority 100\n    # 设定 MASTER 与 BACKUP 负载均衡器之间同步检查的时间间隔，单位是秒\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 123456\n    }\n    virtual_ipaddress {\n        192.168.100.123/23 brd 192.168.101.255\n    }\n}\n\n\n启动服务\n\nsystemctl start keepalived.service && systemctl enable keepalived.service\n\n\n\n# 测试 NFS\n\n在 Master上执行，有输出\n\n[root@localhost ~]# ip a | grep  192.168.100.123\n    inet 192.168.100.123/23 brd 192.168.101.255 scope global secondary enp0s3\n\n\n在 Slave上执行，无输出\n\n[root@localhost ~]# ip a | grep  192.168.100.123\n\n\n关闭 Master 上keepalived测试IP是否更换\n\n在 Master上\n\nsystemctl stop keepalived\n\n\n在 Slave 上,有输出表示自动切换成功\n\n[root@localhost ~]# ip a | grep  192.168.100.123\n    inet 192.168.100.123/23 brd 192.168.101.255 scope global secondary enp0s3\n\n\n\n# k8s集群node节点挂在\n\n# 挂载目录\nmkdir -p /data/nfs-share-123\n# 挂载nfs\nmount -t nfs 192.168.100.123:/data/nfs-share /data/nfs-share-123\n# 查看\ndf -TH\n输出里面含有  192.168.100.111:/data/nfs-share nfs4       54G  1.8G   52G   4% /data1/nfs-data\n# 卸载挂载\numount /data/nfs-share-123\n\n\n\n# 设置 Keepalived 脚本\n\n在安装Keepalived的机器上设置 Keepalived 脚本\n\n> ip 的漂移是根据 Keepalived 的存活来判断的，所以在 nfs 挂之后需要停止 keepalived 服务\n\necho \'systemctl status nfs &>/dev/null\nif [ $? -ne 0 ]; then\n  systemctl start nfs\n  systemctl status nfs &>/dev/null\n  if [ $? -ne 0 ]; then\n    systemctl stop keepalived.service\n  fi\nfi\' >> /usr/local/sbin/task_nfs.sh\n\n\n加入定时任务\n\nchmod 777 /usr/local/sbin/task_nfs.sh\ncrontab -e\n# 添加定时任务 5秒执行一次\n* * * * * sleep 5;/usr/local/sbin/task_nfs.sh &> /dev/null\n\n\n完结。',normalizedContent:'# nfs (kubernetes) 高可用方案（nfs+keepalived+sersync)\n\n提示\n\nnfs (kubernetes) 高可用方案（nfs+keepalived+sersync)\n\n\n\n * 安装环境\n * 准备工作\n * 安装 nfs\n * 测试 nfs\n * 在 slave 进行同步 master 数据\n * 在 master 进行同步 slave 数据\n * master安装 keepalived\n * slave安装 keepalived\n * 测试 nfs\n * k8s集群node节点挂在\n * 设置 keepalived 脚本\n\n\n\n\n# 安装环境\n\n本方案是 nfs 的高可用方案，客户端为 k8s集群三台，文件服务器分别为 master 和 slave，使用 keepalived 生成一个虚拟 ip，使用 sersync 进行 master 与 slave 之间文件相互同步，确保高可用。\n\n角色       系统           ip\n虚拟ip                  192.168.100.110\nclient   k8s集群        192.168.100.116,192.168.100.117,192.168.100.118\nmaster   centos 7.9   192.168.100.111\nslave    centos 7.9   192.168.100.112\n\n\n# 准备工作\n\n安装 vim，并且关闭防火墙\n\nyum install vim -y\nsystemctl stop firewalld && systemctl disable firewalld\n\n\n查看内核版本\n\n[root@localhost ~]# uname -smr\nlinux 3.10.0-1160.59.1.el7.x86_64 x86_64\n\n\n> 建议升级到最新的长期支持版本 升级内核\n\n[root@localhost ~]# cat /etc/redhat-release #查看系统版本\ncentos linux release 7.9.2009 (core)\n[root@localhost ~]# uname -r #查看系统内核版本\n3.10.0-1160.59.1.el7.x86_64\n[root@localhost ~]# uname -m #查看系统是否64位\nx86_64\n\n\n\n# 安装 nfs\n\n在 master 和 slave 上面创建共享目录，数据盘挂在到/data。\n\nmkdir -p /data/nfs-share\n## 赋予写权限\nchmod o+w /data/nfs-share\n\n\n在master 和 slave上安装nfs服务\n\nyum -y install nfs-utils rpcbind\n\n\n设定分享的目录（配置要改动需要reload nfs 服务）\n\n## 指定具体k8s node节点ip地址\necho \'/data/nfs-share 192.168.100.116,192.168.100.117,192.168.100.118(rw,sync,all_squash)\' >> /etc/exports\n\n\n分享目录的范围详解（给哪些客户端分享）\n\n点击查看\n\n(1).指定ip地址的主机：192.168.0.100\n\n只允许ip地址为192.168.0.100的主机可以登录到此共享目录\n\n(2).指定子网中的所有主机：192.168.0.0/24或者192.168.0.0/255.255.255.0\n\n只允许192.168.0.1~192.168.0.254这个网段内的ip登录到此共享目录\n\n(3).指定域名的主机：centosnamenode.pigxcloud.com\n\n只允许主机名为centosnamenode.pigxcloud.com的可以登录到此共享目录\n\n(4).指定域内的所有主机：*.pigxcloud.com\n\n只允许域名为.pigxcloud.com的可以登录到此共享目录\n\n(5).指定所有的主机：*\n\n分享目录的权限详解（客户端能对分享的目录进行哪些操作）\n\n点击查看\n\n(1).rw：可读可写的权限\n\n(2).ro：只读的权限\n\n(3).no_root_squash：登入分享目录的使用者如果是root的话，那么对于这个分享的目录来说，他就具有root的权限！这一项不安全，不建议使用！\n\n(4).root_squash：登入分享目录的使用者如果是root的话，那么这个使用者的权限将被压缩成匿名使用者，通常他的uid与gid都会变成nobody那个系统账号的身份。\n\n(5).all_squash：不论登入分享目录的使用者是什么身份，都会被压缩成匿名使用者。\n\n(6).anonuid：可以自行设定使用者uid的值！也就是使用者登录到分享目录中，身份成为uid这个使用者，必须在/etc/passwd中存在这个uid。\n\n(7).anongid：可以自行设定使用者gid的值！也就是使用者登录到分享目录中，身份成为gid这个使用者，必须在/etc/group中存在这个gid。\n\n(8).sync：数据同步写入到内存与硬盘当中\n\n(9).async：数据会先暂存于内存当中，而非直接写入硬盘\n\n开启服务\n\nsystemctl start rpcbind && systemctl start nfs\n\n\n设置开机自启动\n\nsystemctl enable rpcbind && systemctl enable nfs\n\n\n\n# 测试 nfs\n\n在k8s集群里面找个node 测试是否能链接上nfs服务\n\n下载nfs\n\nyum -y install nfs-utils rpcbind\n\n\n挂载目录\n\nmkdir -p /data/nfs-data\n\n\n挂载nfs\n\nmount -t nfs 192.168.100.111:/data/nfs-share /data/nfs-data\n\n\n查看\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n[root@localhost ~]# df -th\nfilesystem                      type      size  used avail use% mounted on\ndevtmpfs                        devtmpfs  508m     0  508m   0% /dev\ntmpfs                           tmpfs     520m     0  520m   0% /dev/shm\ntmpfs                           tmpfs     520m  7.1m  513m   2% /run\ntmpfs                           tmpfs     520m     0  520m   0% /sys/fs/cgroup\n/dev/mapper/centos-root         xfs        54g  1.7g   52g   4% /\n/dev/mapper/centos-home         xfs       1.1t   34m  1.1t   1% /home\n/dev/sda1                       xfs       1.1g  208m  856m  20% /boot\ntmpfs                           tmpfs     104m     0  104m   0% /run/user/0\n192.168.100.111:/data/nfs-share nfs4       54g  1.8g   52g   4% /data1/nfs-data\n\n\n卸载挂载\n\numount /data1/nfs-data\n\n\n检查 nfs 服务器端是否有设置共享目录 showmount -e $(nfs服务器的ip)\n\n[root@localhost ~]# showmount -e 192.168.100.111\nexport list for 192.168.100.111:\n/data/nfs-share 192.168.100.0/23\n\n\n\n# 在 slave 进行同步 master 数据\n\n提到数据同步就必然会谈到rsync，一般简单的服务器数据传输会使用ftp/sftp等方式，但是这样的方式效率不高，不支持差异化增量同步也不支持实时传输。针对数据实时同步需求大多数人会选择rsync+inotify-tools的解决方案，但是这样的方案也存在一些缺陷，sersync是国人基于前两者开发的工具，不仅保留了优点同时还强化了实时监控，文件过滤，简化配置等功能，帮助用户提高运行效率，节省时间和网络资源。\n\n在 slave 上编辑配置文件\n\nvim /etc/rsyncd.conf\n\n\n点击查看\n\n# 设置rsync运行权限为root\nuid = root\n# 设置rsync运行权限为root\ngid = root\n# 默认端口\nport = 873\n# pid文件的存放位置\npid file = /var/rsyncd.pid\n# 日志文件位置，启动rsync后自动产生这个文件，无需提前创建\nlog file = /var/log/rsyncd.log\n# 默认为true，修改为no，增加对目录文件软连接的备份\nuse chroot = no\n# 最大连接数\nmax connections = 200\n# 设置rsync服务端文件为读写权限\nread only = no\n# 不显示rsync服务端资源列表 \nlist = false\nfake super = yes\nignore errors\n[data]\n# rsync服务端数据目录路径\npath = /data/nfs-share\n# 执行数据同步的用户名，可以设置多个，用英文状态下逗号隔开\nauth users = root\n# 用户认证配置文件，里面保存用户名称和密码，后面会创建这个文件\nsecrets file = /etc/rsync_slave.pass\n# 允许进行数据同步的客户端ip地址，可以设置多个，用英文状态下逗号隔开\nhosts allow = 192.168.100.111\n# 禁止数据同步的客户端ip地址，可以设置多个，用英文状态下逗号隔开\n#hosts deny = 192.168.0.1\n\n\n在 slave 上安装启动\n\nyum install rsync\necho \'root:123456\' > /etc/rsync_slave.pass\nchmod 600 /etc/rsync_slave.pass\nchown -r root:root /data/nfs-share\nrsync --daemon --config=/etc/rsyncd.conf\n\n\n在 master 上测试下\n\nyum -y install rsync\nchown -r root:root /data/nfs-share\necho "123456" > /etc/rsync_slave.pass\nchmod 600 /etc/rsync_slave.pass\n// 创建测试文件,测试推送\ncd /data/nfs-share\necho "test" > slave.txt\nrsync -arv /data/nfs-share/ root@192.168.100.112::data --password-file=/etc/rsync_slave.pass\n#在 slave 上测试\nls /fs/nsf-data\n# 出现 slave.txt 即可\n\n\n在 master 上配置自动同步安装\n\n# 下载\nwget https://raw.githubusercontent.com/orangle/sersync/master/release/sersync2.5.4_64bit_binary_stable_final.tar.gz\n# 解压\ntar zxvf sersync2.5.4_64bit_binary_stable_final.tar.gz  \n# 移动目录到 /usr/local/sersync\nmv gnu-linux-x86  /usr/local/sersync   \n\n\n在 master 上配置自动同步配置\n\ncd /usr/local/sersync/ && vim /usr/local/sersync/confxml.xml\n\n\n修改配置文件里面节点数据\n\n点击查看\n\n<delete start="flase"/>\n\n#源服务器同步目录\n<localpath watch="/data/nfs-share/">  \n\n#remote ip="172.12.17.167"  #目标服务器ip，每行一个 \n#name="data"   目标服务器rsync同步目录模块名称\n<remote ip="172.12.17.167" name="data"/> \n\n#rsync同步认证\n#start="true"   \n#users="root" 目标服务器rsync同步用户名\n#passwordfile="/etc/rsync_slave.pass" 目标服务器rsync同步用户的密码在源服务器的存放路径\n<auth start="true" users="root" passwordfile="/etc/rsync_slave.pass"/>\n\n# 设置rsync远程服务端口，远程非默认端口则需打开自定义\n<userdefinedport start="false" port="873"/>\x3c!-- port=874 --\x3e\n\n#脚本运行失败日志记录\nfaillog path="/tmp/rsync_fail_log.sh"\n\n\n启动\n\n./sersync2 -r -d -o confxml.xml\n\n\n测试\n\ncd /data/nfs-share && touch slave.txt\n\n\n在 slave 里面 /data/nfs-share 里面去看看数据是否存在\n\n\n\n\n\n\n \n\n\n[root@localhost nfs-share]# ls -al \ntotal 12512\ndrwxr-xrwx. 2 root root      146 mar  1 12:28 .\ndrwxr-xr-x. 3 root root       23 mar  1 10:43 ..\n-rw-r--r--. 1 root root        5 mar  1 10:56 slave.txt\n\n\n\n# 在 master 进行同步 slave 数据\n\n注意\n\n如果master挂掉后恢复不能同步数据到slave，这里也同样配置master到slave的自动同步\n\n在master上编辑配置文件\n\nvim /etc/rsyncd.conf\n\n\n点击查看\n\n# 设置rsync运行权限为root\nuid = root\n# 设置rsync运行权限为root\ngid = root\n# 默认端口\nport = 873\n# pid文件的存放位置\npid file = /var/rsyncd.pid\n# 日志文件位置，启动rsync后自动产生这个文件，无需提前创建\nlog file = /var/log/rsyncd.log\n# 默认为true，修改为no，增加对目录文件软连接的备份\nuse chroot = no\n# 最大连接数\nmax connections = 200\n# 设置rsync服务端文件为读写权限\nread only = no\n# 不显示rsync服务端资源列表 \nlist = false\nfake super = yes\nignore errors\n[data]\n# rsync服务端数据目录路径\npath = /data/nfs-share\n# 执行数据同步的用户名，可以设置多个，用英文状态下逗号隔开\nauth users = root\n# 用户认证配置文件，里面保存用户名称和密码，后面会创建这个文件\nsecrets file = /etc/rsync_master.pass\n# 允许进行数据同步的客户端ip地址，可以设置多个，用英文状态下逗号隔开\nhosts allow = 192.168.100.112\n# 禁止数据同步的客户端ip地址，可以设置多个，用英文状态下逗号隔开\n#hosts deny = 192.168.0.1\n\n\n在master上安装启动\n\nyum install rsync\necho \'root:123456\' > /etc/rsync_master.pass\nchmod 600 /etc/rsync_master.pass\nchown -r root:root /data/nfs-share\nrsync --daemon --config=/etc/rsyncd.conf\n\n\n在 slave 上测试下\n\nyum -y install rsync\nchown -r root:root /data/nfs-share\necho "123456" > /etc/rsync_master.pass\nchmod 600 /etc/rsync_master.pass\n# 创建测试文件,测试推送\ncd /data/nfs-share\necho "test" > master.txt\nrsync -arv /data/nfs-share/ root@192.168.100.111::data --password-file=/etc/rsync_master.pass\n# 在 master 上测试\nls /data/nfs-share\n# 出现 master.txt 即可\n\n\n在 slave 上配置自动同步安装\n\n# 下载\nwget https://raw.githubusercontent.com/orangle/sersync/master/release/sersync2.5.4_64bit_binary_stable_final.tar.gz\n# 解压\ntar zxvf sersync2.5.4_64bit_binary_stable_final.tar.gz  \n# 移动目录到 /usr/local/sersync\nmv gnu-linux-x86  /usr/local/sersync   \n\n\n在 slave 上配置自动同步配置\n\ncd /usr/local/sersync/ && vim /usr/local/sersync/confxml.xml\n\n\n修改配置文件里面节点数据\n\n点击查看\n\n<delete start="flase"/>\n\n#源服务器同步目录\n<localpath watch="/data/nfs-share/">  \n\n#remote ip="192.168.100.111"  #目标服务器ip，每行一个 \n#name="data"   目标服务器rsync同步目录模块名称\n<remote ip="192.168.100.111" name="data"/> \n\n#rsync同步认证\n#start="true"   \n#users="root" 目标服务器rsync同步用户名\n#passwordfile="/etc/rsync_master.pass" 目标服务器rsync同步用户的密码在源服务器的存放路径\n<auth start="true" users="root" passwordfile="/etc/rsync_master.pass"/>\n\n# 设置rsync远程服务端口，远程非默认端口则需打开自定义\n<userdefinedport start="false" port="873"/>\x3c!-- port=874 --\x3e\n\n#脚本运行失败日志记录\nfaillog path="/tmp/rsync_fail_log.sh"\n\n\n启动\n\n./sersync2 -r -d -o confxml.xml\n# 测试\ncd /data/nfs-share\ntouch master1.txt\n# 在 slave 里面 /data/nfs-share 里面去看看数据是否存在\n\n\n\n# master安装 keepalived\n\n安装 keepalived\n\nyum -y install keepalived\n\n\n修改配置文件\n\nvim /etc/keepalived/keepalived.conf\n\n\n点击查看\n\n# 其中 enp0s3 为绑定网卡名称，可以使用 ip addr 命令查看\n# 其中 192.168.100.123  为虚拟 ip ，注意不要和其它 ip 冲突\n! configuration file for keepalived\nglobal_defs {\n   router_id lb01\n}\n\nvrrp_instance vi_1 {\n    state master\n    interface enp0s3\n    # 虚拟路由标识，这个标识是一个数字，同一个vrrp实例使用唯一的标识。\n    # 即同一vrrp_instance下，master和backup必须是一致的\n    virtual_router_id 51\n    # 定义优先级，数字越大，优先级越高（0-255）。\n    # 在同一个vrrp_instance下，master 的优先级必须大于 backup 的优先级\n    priority 150\n    # 设定 master 与 backup 负载均衡器之间同步检查的时间间隔，单位是秒\n    advert_int 1\n    authentication {\n        auth_type pass\n        auth_pass 123456\n    }\n    virtual_ipaddress {\n        192.168.100.123/23 brd 192.168.101.255\n    }\n}\n\n\n启动服务\n\nsystemctl start keepalived.service && systemctl enable keepalived.service\n\n\n\n# slave安装 keepalived\n\n安装 keepalived\n\nyum -y install keepalived\n\n\n修改配置文件\n\nvim /etc/keepalived/keepalived.conf\n\n\n点击查看\n\n# 其中 enp0s3 为绑定网卡名称，可以使用 ip addr 命令查看\n# 其中 192.168.100.123  为虚拟 ip ，注意不要和其它 ip 冲突\n! configuration file for keepalived\nglobal_defs {\n   router_id lb02\n}\n\nvrrp_instance vi_1 {\n    state master\n    interface enp0s3\n    # 虚拟路由标识，这个标识是一个数字，同一个vrrp实例使用唯一的标识。\n    # 即同一vrrp_instance下，master和backup必须是一致的\n    virtual_router_id 51\n    # 定义优先级，数字越大，优先级越高（0-255）。\n    # 在同一个vrrp_instance下，master 的优先级必须大于 backup 的优先级\n    priority 100\n    # 设定 master 与 backup 负载均衡器之间同步检查的时间间隔，单位是秒\n    advert_int 1\n    authentication {\n        auth_type pass\n        auth_pass 123456\n    }\n    virtual_ipaddress {\n        192.168.100.123/23 brd 192.168.101.255\n    }\n}\n\n\n启动服务\n\nsystemctl start keepalived.service && systemctl enable keepalived.service\n\n\n\n# 测试 nfs\n\n在 master上执行，有输出\n\n[root@localhost ~]# ip a | grep  192.168.100.123\n    inet 192.168.100.123/23 brd 192.168.101.255 scope global secondary enp0s3\n\n\n在 slave上执行，无输出\n\n[root@localhost ~]# ip a | grep  192.168.100.123\n\n\n关闭 master 上keepalived测试ip是否更换\n\n在 master上\n\nsystemctl stop keepalived\n\n\n在 slave 上,有输出表示自动切换成功\n\n[root@localhost ~]# ip a | grep  192.168.100.123\n    inet 192.168.100.123/23 brd 192.168.101.255 scope global secondary enp0s3\n\n\n\n# k8s集群node节点挂在\n\n# 挂载目录\nmkdir -p /data/nfs-share-123\n# 挂载nfs\nmount -t nfs 192.168.100.123:/data/nfs-share /data/nfs-share-123\n# 查看\ndf -th\n输出里面含有  192.168.100.111:/data/nfs-share nfs4       54g  1.8g   52g   4% /data1/nfs-data\n# 卸载挂载\numount /data/nfs-share-123\n\n\n\n# 设置 keepalived 脚本\n\n在安装keepalived的机器上设置 keepalived 脚本\n\n> ip 的漂移是根据 keepalived 的存活来判断的，所以在 nfs 挂之后需要停止 keepalived 服务\n\necho \'systemctl status nfs &>/dev/null\nif [ $? -ne 0 ]; then\n  systemctl start nfs\n  systemctl status nfs &>/dev/null\n  if [ $? -ne 0 ]; then\n    systemctl stop keepalived.service\n  fi\nfi\' >> /usr/local/sbin/task_nfs.sh\n\n\n加入定时任务\n\nchmod 777 /usr/local/sbin/task_nfs.sh\ncrontab -e\n# 添加定时任务 5秒执行一次\n* * * * * sleep 5;/usr/local/sbin/task_nfs.sh &> /dev/null\n\n\n完结。',charsets:{cjk:!0},lastUpdated:"2022/03/01, 16:23:59",lastUpdatedTimestamp:1646123039e3},{title:"离线部署 Kubernetess + KubeSphere",frontmatter:{title:"离线部署 Kubernetess + KubeSphere",date:"2022-03-03T13:43:40.000Z",permalink:"/pages/8a1473/"},regularPath:"/18.Kubernetes/02.Kubernetes%20%E5%AE%9E%E8%B7%B5/03.%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2%20Kubernetess%20+%20KubeSphere.html",relativePath:"18.Kubernetes/02.Kubernetes 实践/03.离线部署 Kubernetess + KubeSphere.md",key:"v-18890533",path:"/pages/8a1473/",headers:[{level:2,title:"背景",slug:"背景",normalizedTitle:"背景",charIndex:100},{level:2,title:"环境准备",slug:"环境准备",normalizedTitle:"环境准备",charIndex:201},{level:3,title:"节点",slug:"节点",normalizedTitle:"节点",charIndex:265},{level:3,title:"端口要求",slug:"端口要求",normalizedTitle:"端口要求",charIndex:683},{level:3,title:"依赖软件",slug:"依赖软件",normalizedTitle:"依赖软件",charIndex:1777},{level:3,title:"安装docker",slug:"安装docker",normalizedTitle:"安装docker",charIndex:1863},{level:3,title:"镜像仓库",slug:"镜像仓库",normalizedTitle:"镜像仓库",charIndex:2370},{level:3,title:"安装包下载：",slug:"安装包下载",normalizedTitle:"安装包下载：",charIndex:2435},{level:2,title:"安装步骤：",slug:"安装步骤",normalizedTitle:"安装步骤：",charIndex:2680},{level:3,title:"1.创建集群配置文件",slug:"_1-创建集群配置文件",normalizedTitle:"1.创建集群配置文件",charIndex:2690},{level:3,title:"2.环境初始化",slug:"_2-环境初始化",normalizedTitle:"2.环境初始化",charIndex:4635},{level:3,title:"3.导入镜像",slug:"_3-导入镜像",normalizedTitle:"3.导入镜像",charIndex:5134},{level:3,title:"4.执行安装",slug:"_4-执行安装",normalizedTitle:"4.执行安装",charIndex:6391},{level:3,title:"5.查看结果",slug:"_5-查看结果",normalizedTitle:"5.查看结果",charIndex:7655}],headersStr:"背景 环境准备 节点 端口要求 依赖软件 安装docker 镜像仓库 安装包下载： 安装步骤： 1.创建集群配置文件 2.环境初始化 3.导入镜像 4.执行安装 5.查看结果",content:'# 三步搞定 ARM64 离线部署 Kubernetess + KubeSphere\n\n提示\n\n本文将主要介绍如何在 ARM64 环境下部署 Kubernetes 和 KubeSphere。\n\n\n# 背景\n\n随着我国对信息安全的愈发重视，国产化的趋势也越来越浓，包括国产操作系统、国产 CPU 等。由于 ARM 架构国产 CPU 在维持创新可信和先进性方面的潜在优势，其应用也将会越来越广泛。\n\n\n# 环境准备\n\n安装过程录屏地址：https://www.bilibili.com/video/BV1VL4y137Fn/\n\n\n# 节点\n\nkubeSphere 支持的操作系统包括：\n\n * Ubuntu 16.04, 18.04\n * Debian Buster, Stretch\n * CentOS/RHEL 7\n * SUSE Linux Enterprise Server 15\n * openEuler\n\n准备已下三台 CentOS Linux release 7.9.2009 (AltArch) 为例：\n\n主机名          IP地址            角色       OS          配置\nk8s-master   192.168.7.38    master   CentOS7.9   4核8G\nk8s-node1    192.168.7.20    worker   CentOS7.9   8核16G\nk8s-node2    192.168.7.187   worker   CentOS7.9   8核16G\n\n\n# 端口要求\n\nKubeSphere 需要某些端口用于服务之间的通信。如果您的网络配置有防火墙规则，则需要确保基础设施组件可以通过特定端口相互通信。这些端口用作某些进程或服务的通信端点。\n\n服务               协议               行为      起始端口    结束端口    备注\nssh              TCP              allow   22              \netcd             TCP              allow   2379    2380    \napiserver        TCP              allow   6443            \ncalico           TCP              allow   9099    9100    \nbgp              TCP              allow   179             \nnodeport         TCP              allow   30000   32767   \nmaster           TCP              allow   10250   10258   \ndns              TCP              allow   53              \ndns              UDP              allow   53              \nlocal-registry   TCP              allow   5000            离线环境需要\nlocal-apt        TCP              allow   5080            离线环境需要\nrpcbind          TCP              allow   111             使用 NFS 时需要\nipip             IPENCAP / IPIP   allow                   Calico 需要使用 IPIP 协议\nmetrics-server   TCP              allow   8443            \n\n注意\n\n当您使用 Calico 网络插件并且在云平台上使用经典网络运行集群时，您需要对源地址启用 IPENCAP 和 IPIP 协议。\n\n\n# 依赖软件\n\n确保机器已经安装所需依赖软件\n\nyum install curl openssl ebtables socat ipset conntrack -y\n\n\n\n# 安装docker\n\nKubernetes默认的容器运行时仍然是Docker，使用的是kubelet中内置dockershim CRI实现。\n\n安装所需依赖\n\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n\n\n配置docker yum源\n\nsudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n\n\n安装 Docker Engine-Community\n\nsudo yum install docker-ce docker-ce-cli containerd.io -y\n\n\n启动 Docker CE 并设置开机自启动\n\nsudo systemctl start docker && sudo systemctl enable docker\n\n\n提示\n\n可将安装了所有依赖软件的操作系统制作成系统镜像使用，避免每台机器都安装依赖软件，即可提升交付部署效率，又可避免依赖问题的发生。\n\n\n# 镜像仓库\n\n可使用harbor或其他第三方镜像仓库。在接下来的安装步骤中，初始化环境过程我们使用kk创建自签名镜像仓库。\n\n\n# 安装包下载：\n\n提示\n\n该安装包仅包含 Kubernetes + KubeSphere-core 镜像，如需更多组件 arm64 镜像，可自行编译构建。\n\n# md5: 3ad57823faf2dfe945e2fe3dcfd4ace9\ncurl -Ok https://kubesphere-installer.pek3b.qingstor.com/offline/v3.0.0/kubesphere-core-v3.0.0-offline-linux-arm64.tar.gz\n\n\n\n# 安装步骤：\n\n\n# 1.创建集群配置文件\n\n安装包解压后进入kubesphere-core-v3.0.0-offline-linux-arm64\n\n# 解压\ntar -zxvf kubesphere-core-v3.0.0-offline-linux-arm64.tar.gz\n\n\n[root@node1 kubesphere-core-v3.0.0-offline-linux-arm64]# ls -al \ntotal 55364\ndrwxr-xr-x  6 root root     4096 Mar  3 19:27 .\ndr-xr-x---. 8 root root     4096 Mar  3 18:02 ..\ndrwxr-xr-x  3 root root     4096 Oct 27  2020 charts\n-rw-r--r--  1 root root      727 Mar  3 17:35 config-sample.yaml\ndrwxr-xr-x  2 root root     4096 Mar  3 17:24 dependencies\n-rwxr-xr-x  1 root root 56661021 Dec  3  2020 kk\ndrwxr-xr-x  4 root root     4096 Dec  3  2020 kubekey\ndrwxr-xr-x  3 root root     4096 Dec 10  2020 kubesphere-images-v3.0.0\n\n\n./kk create config --with-kubesphere v3.0.0\n\n\n根据实际环境信息修改生成的配置文件config-sample.yaml，也可使用-f参数自定义配置文件路径。kk 详细用法可参考：https://github.com/kubesphere/kubekey\n\n注意\n\n填写正确的私有仓库地址privateRegistry（如已准备好私有仓库可设置为已有仓库地址，若使用 kk 创建私有仓库，则该参数设置为：dockerhub.kubekey.local）\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n\n \n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\napiVersion: kubekey.kubesphere.io/v1alpha1\nkind: Cluster\nmetadata:\n  name: sample\nspec:\n  hosts:\n  # 注意指定节点 arch 为 arm64\n  - {name: k8s-master, address: 192.168.7.38, internalAddress: 192.168.7.38, password: Qcloud@123, arch: arm64}\n  - {name: k8s-node1, address: 192.168.7.20, internalAddress: 192.168.7.20, password: Qcloud@123, arch: arm64}\n  - {name: k8s-node2, address: 192.168.7.187, internalAddress: 192.168.7.187, password: Qcloud@123, arch: arm64}\n  roleGroups:\n    etcd:\n    - k8s-master\n    master:\n    - k8s-master\n    worker:\n    - k8s-node1\n    - k8s-node2\n  controlPlaneEndpoint:\n    domain: lb.kubesphere.local\n    address: ""\n    port: 6443\n  kubernetes:\n    version: v1.17.9\n    imageRepo: kubesphere\n    clusterName: cluster.local\n  network:\n    plugin: calico\n    kubePodsCIDR: 10.233.64.0/18\n    kubeServiceCIDR: 10.233.0.0/18\n  registry:\n    registryMirrors: []\n    insecureRegistries: []\n    # 设置私有仓库\n    privateRegistry: dockerhub.kubekey.local\n  addons: []\n  \n\n\n\n# 2.环境初始化\n\n提示\n\n若已安装相关依赖，并且已经准备好镜像仓库，可略过该步骤。 (为避免依赖问题的产生，建议提前安装相关依赖或使用已安装相关依赖的系统镜像执行安装)\n\n注意\n\n如需使用kk创建自签名镜像仓库，则会在当前机器启动docker registry服务，请确保当前机器存在registry:2，如没有，可从kubesphere-images-v3.0.0/registry.tar中导入。\n\n# 导入命令：\ndocker load < registry.tar\n\n\n注意\n\n由kk启动的镜像仓库端口为443，请确保所有机器均可访问当前机器443端口。镜像数据存储到本地/mnt/registry (建议单独挂盘)。\n\n# 执行如下命令会对配置文件中所有节点安装依赖：\n./kk init os -f config-sample.yaml -s ./dependencies/\n\n# 如需使用kk创建自签名镜像仓库，可执行如下命令：\n./kk init os -f config-sample.yaml -s ./dependencies/ --add-images-repo\n\n\n\n# 3.导入镜像\n\n进入kubesphere-all-v3.0.0-offline-linux-arm64/kubesphere-images-v3.0.0\n\n[root@node1 kubesphere-images-v3.0.0]# ls -al \ntotal 24540\ndrwxr-xr-x 3 root root     4096 Dec 10  2020 .\ndrwxr-xr-x 6 root root     4096 Mar  3 21:45 ..\n-rw-r--r-- 1 root root     2890 Dec 10  2020 images-list-v3.0.0.txt\ndrwxr-xr-x 2 root root     4096 Dec 10  2020 kubesphere-images\n-rwxr-xr-x 1 root root     6762 Nov 10  2020 offline-installation-tool.sh\n-rw------- 1 root root 25104384 Oct 27  2020 registry.tar\n\n\n使用 offline-installation-tool.sh 将镜像导入之前准备的仓库中：\n\n注意\n\n脚本后镜像仓库地址请填写真实仓库地址：dockerhub.kubekey.local\n\n./offline-installation-tool.sh -l images-list-v3.0.0.txt -d kubesphere-images -r dockerhub.kubekey.local\n\n\n使用kk创建自签名镜像仓库证书问题，如果遇到一下情况，可以通过修改docker.service 配置文件指定证书验证参数\n\n报错信息\n\nGet "https://dockerhub.kubekey.local/v2/": x509: certificate relies on legacy Common Name field, use SANs or temporarily enable Common Name matching with GODEBUG=x509ignoreCN=0\n\n解决方法\n\nvi /usr/lib/systemd/system/docker.service\n\n\n找到[Service]配置节点，下面追加如下内容：\n\n\n\n\n \n\n\n\n...\n[Service]\nEnvironment=GODEBUG=x509ignoreCN=0\n...\n\n\n重启docker服务\n\nsystemctl daemon-reload\nsystemctl restart docker.service\n\n\n详情参考：https://forum.gitlab.com/t/docker-push-to-gilab-registry-self-signed-certs-problem/52831/3\n\n\n# 4.执行安装\n\n提示\n\n以上准备工作完成且再次检查配置文件无误后，执行安装。\n\n./kk create cluster -f config-sample.yaml --with-kubesphere\n\n\n注意\n\n如果中途安装失败执行如下命令删除集群以后再重新执行安装命令\n\n./kk delete cluster -f config-sample.yaml\n\n\n安装完成后控制台输出内容如下：\n\n#####################################################\n###              Welcome to KubeSphere!           ###\n#####################################################\n\nConsole: http://192.168.7.38:30880\nAccount: admin\nPassword: P@88w0rd\n\nNOTES：\n  1. After logging into the console, please check the\n     monitoring status of service components in\n     the "Cluster Management". If any service is not\n     ready, please wait patiently until all components \n     are ready.\n  2. Please modify the default password after login.\n\n#####################################################\nhttps://kubesphere.io             2022-03-04 03:34:18\n#####################################################\nINFO[02:16:07 CST] Installation is complete.\n\nPlease check the result using the command:\n\n       kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=\'{.items[0].metadata.name}\') -f\n\n\n\n安装日志查看\n\nkubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=\'{.items[0].metadata.name}\') -f\n\n\n\n# 5.查看结果\n\n打开浏览器访问KubeSphere管控台：http://117.78.8.124:30880/login\n\n> Account: admin\n> \n> Password: P@88w0rd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n',normalizedContent:'# 三步搞定 arm64 离线部署 kubernetess + kubesphere\n\n提示\n\n本文将主要介绍如何在 arm64 环境下部署 kubernetes 和 kubesphere。\n\n\n# 背景\n\n随着我国对信息安全的愈发重视，国产化的趋势也越来越浓，包括国产操作系统、国产 cpu 等。由于 arm 架构国产 cpu 在维持创新可信和先进性方面的潜在优势，其应用也将会越来越广泛。\n\n\n# 环境准备\n\n安装过程录屏地址：https://www.bilibili.com/video/bv1vl4y137fn/\n\n\n# 节点\n\nkubesphere 支持的操作系统包括：\n\n * ubuntu 16.04, 18.04\n * debian buster, stretch\n * centos/rhel 7\n * suse linux enterprise server 15\n * openeuler\n\n准备已下三台 centos linux release 7.9.2009 (altarch) 为例：\n\n主机名          ip地址            角色       os          配置\nk8s-master   192.168.7.38    master   centos7.9   4核8g\nk8s-node1    192.168.7.20    worker   centos7.9   8核16g\nk8s-node2    192.168.7.187   worker   centos7.9   8核16g\n\n\n# 端口要求\n\nkubesphere 需要某些端口用于服务之间的通信。如果您的网络配置有防火墙规则，则需要确保基础设施组件可以通过特定端口相互通信。这些端口用作某些进程或服务的通信端点。\n\n服务               协议               行为      起始端口    结束端口    备注\nssh              tcp              allow   22              \netcd             tcp              allow   2379    2380    \napiserver        tcp              allow   6443            \ncalico           tcp              allow   9099    9100    \nbgp              tcp              allow   179             \nnodeport         tcp              allow   30000   32767   \nmaster           tcp              allow   10250   10258   \ndns              tcp              allow   53              \ndns              udp              allow   53              \nlocal-registry   tcp              allow   5000            离线环境需要\nlocal-apt        tcp              allow   5080            离线环境需要\nrpcbind          tcp              allow   111             使用 nfs 时需要\nipip             ipencap / ipip   allow                   calico 需要使用 ipip 协议\nmetrics-server   tcp              allow   8443            \n\n注意\n\n当您使用 calico 网络插件并且在云平台上使用经典网络运行集群时，您需要对源地址启用 ipencap 和 ipip 协议。\n\n\n# 依赖软件\n\n确保机器已经安装所需依赖软件\n\nyum install curl openssl ebtables socat ipset conntrack -y\n\n\n\n# 安装docker\n\nkubernetes默认的容器运行时仍然是docker，使用的是kubelet中内置dockershim cri实现。\n\n安装所需依赖\n\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n\n\n配置docker yum源\n\nsudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n\n\n安装 docker engine-community\n\nsudo yum install docker-ce docker-ce-cli containerd.io -y\n\n\n启动 docker ce 并设置开机自启动\n\nsudo systemctl start docker && sudo systemctl enable docker\n\n\n提示\n\n可将安装了所有依赖软件的操作系统制作成系统镜像使用，避免每台机器都安装依赖软件，即可提升交付部署效率，又可避免依赖问题的发生。\n\n\n# 镜像仓库\n\n可使用harbor或其他第三方镜像仓库。在接下来的安装步骤中，初始化环境过程我们使用kk创建自签名镜像仓库。\n\n\n# 安装包下载：\n\n提示\n\n该安装包仅包含 kubernetes + kubesphere-core 镜像，如需更多组件 arm64 镜像，可自行编译构建。\n\n# md5: 3ad57823faf2dfe945e2fe3dcfd4ace9\ncurl -ok https://kubesphere-installer.pek3b.qingstor.com/offline/v3.0.0/kubesphere-core-v3.0.0-offline-linux-arm64.tar.gz\n\n\n\n# 安装步骤：\n\n\n# 1.创建集群配置文件\n\n安装包解压后进入kubesphere-core-v3.0.0-offline-linux-arm64\n\n# 解压\ntar -zxvf kubesphere-core-v3.0.0-offline-linux-arm64.tar.gz\n\n\n[root@node1 kubesphere-core-v3.0.0-offline-linux-arm64]# ls -al \ntotal 55364\ndrwxr-xr-x  6 root root     4096 mar  3 19:27 .\ndr-xr-x---. 8 root root     4096 mar  3 18:02 ..\ndrwxr-xr-x  3 root root     4096 oct 27  2020 charts\n-rw-r--r--  1 root root      727 mar  3 17:35 config-sample.yaml\ndrwxr-xr-x  2 root root     4096 mar  3 17:24 dependencies\n-rwxr-xr-x  1 root root 56661021 dec  3  2020 kk\ndrwxr-xr-x  4 root root     4096 dec  3  2020 kubekey\ndrwxr-xr-x  3 root root     4096 dec 10  2020 kubesphere-images-v3.0.0\n\n\n./kk create config --with-kubesphere v3.0.0\n\n\n根据实际环境信息修改生成的配置文件config-sample.yaml，也可使用-f参数自定义配置文件路径。kk 详细用法可参考：https://github.com/kubesphere/kubekey\n\n注意\n\n填写正确的私有仓库地址privateregistry（如已准备好私有仓库可设置为已有仓库地址，若使用 kk 创建私有仓库，则该参数设置为：dockerhub.kubekey.local）\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n\n \n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\napiversion: kubekey.kubesphere.io/v1alpha1\nkind: cluster\nmetadata:\n  name: sample\nspec:\n  hosts:\n  # 注意指定节点 arch 为 arm64\n  - {name: k8s-master, address: 192.168.7.38, internaladdress: 192.168.7.38, password: qcloud@123, arch: arm64}\n  - {name: k8s-node1, address: 192.168.7.20, internaladdress: 192.168.7.20, password: qcloud@123, arch: arm64}\n  - {name: k8s-node2, address: 192.168.7.187, internaladdress: 192.168.7.187, password: qcloud@123, arch: arm64}\n  rolegroups:\n    etcd:\n    - k8s-master\n    master:\n    - k8s-master\n    worker:\n    - k8s-node1\n    - k8s-node2\n  controlplaneendpoint:\n    domain: lb.kubesphere.local\n    address: ""\n    port: 6443\n  kubernetes:\n    version: v1.17.9\n    imagerepo: kubesphere\n    clustername: cluster.local\n  network:\n    plugin: calico\n    kubepodscidr: 10.233.64.0/18\n    kubeservicecidr: 10.233.0.0/18\n  registry:\n    registrymirrors: []\n    insecureregistries: []\n    # 设置私有仓库\n    privateregistry: dockerhub.kubekey.local\n  addons: []\n  \n\n\n\n# 2.环境初始化\n\n提示\n\n若已安装相关依赖，并且已经准备好镜像仓库，可略过该步骤。 (为避免依赖问题的产生，建议提前安装相关依赖或使用已安装相关依赖的系统镜像执行安装)\n\n注意\n\n如需使用kk创建自签名镜像仓库，则会在当前机器启动docker registry服务，请确保当前机器存在registry:2，如没有，可从kubesphere-images-v3.0.0/registry.tar中导入。\n\n# 导入命令：\ndocker load < registry.tar\n\n\n注意\n\n由kk启动的镜像仓库端口为443，请确保所有机器均可访问当前机器443端口。镜像数据存储到本地/mnt/registry (建议单独挂盘)。\n\n# 执行如下命令会对配置文件中所有节点安装依赖：\n./kk init os -f config-sample.yaml -s ./dependencies/\n\n# 如需使用kk创建自签名镜像仓库，可执行如下命令：\n./kk init os -f config-sample.yaml -s ./dependencies/ --add-images-repo\n\n\n\n# 3.导入镜像\n\n进入kubesphere-all-v3.0.0-offline-linux-arm64/kubesphere-images-v3.0.0\n\n[root@node1 kubesphere-images-v3.0.0]# ls -al \ntotal 24540\ndrwxr-xr-x 3 root root     4096 dec 10  2020 .\ndrwxr-xr-x 6 root root     4096 mar  3 21:45 ..\n-rw-r--r-- 1 root root     2890 dec 10  2020 images-list-v3.0.0.txt\ndrwxr-xr-x 2 root root     4096 dec 10  2020 kubesphere-images\n-rwxr-xr-x 1 root root     6762 nov 10  2020 offline-installation-tool.sh\n-rw------- 1 root root 25104384 oct 27  2020 registry.tar\n\n\n使用 offline-installation-tool.sh 将镜像导入之前准备的仓库中：\n\n注意\n\n脚本后镜像仓库地址请填写真实仓库地址：dockerhub.kubekey.local\n\n./offline-installation-tool.sh -l images-list-v3.0.0.txt -d kubesphere-images -r dockerhub.kubekey.local\n\n\n使用kk创建自签名镜像仓库证书问题，如果遇到一下情况，可以通过修改docker.service 配置文件指定证书验证参数\n\n报错信息\n\nget "https://dockerhub.kubekey.local/v2/": x509: certificate relies on legacy common name field, use sans or temporarily enable common name matching with godebug=x509ignorecn=0\n\n解决方法\n\nvi /usr/lib/systemd/system/docker.service\n\n\n找到[service]配置节点，下面追加如下内容：\n\n\n\n\n \n\n\n\n...\n[service]\nenvironment=godebug=x509ignorecn=0\n...\n\n\n重启docker服务\n\nsystemctl daemon-reload\nsystemctl restart docker.service\n\n\n详情参考：https://forum.gitlab.com/t/docker-push-to-gilab-registry-self-signed-certs-problem/52831/3\n\n\n# 4.执行安装\n\n提示\n\n以上准备工作完成且再次检查配置文件无误后，执行安装。\n\n./kk create cluster -f config-sample.yaml --with-kubesphere\n\n\n注意\n\n如果中途安装失败执行如下命令删除集群以后再重新执行安装命令\n\n./kk delete cluster -f config-sample.yaml\n\n\n安装完成后控制台输出内容如下：\n\n#####################################################\n###              welcome to kubesphere!           ###\n#####################################################\n\nconsole: http://192.168.7.38:30880\naccount: admin\npassword: p@88w0rd\n\nnotes：\n  1. after logging into the console, please check the\n     monitoring status of service components in\n     the "cluster management". if any service is not\n     ready, please wait patiently until all components \n     are ready.\n  2. please modify the default password after login.\n\n#####################################################\nhttps://kubesphere.io             2022-03-04 03:34:18\n#####################################################\ninfo[02:16:07 cst] installation is complete.\n\nplease check the result using the command:\n\n       kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=\'{.items[0].metadata.name}\') -f\n\n\n\n安装日志查看\n\nkubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=\'{.items[0].metadata.name}\') -f\n\n\n\n# 5.查看结果\n\n打开浏览器访问kubesphere管控台：http://117.78.8.124:30880/login\n\n> account: admin\n> \n> password: p@88w0rd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/04, 04:07:03",lastUpdatedTimestamp:1646338023e3},{title:"介绍",frontmatter:{title:"介绍",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/1b4b4e/"},regularPath:"/19.Spring%20Security%20OAuth2/01.Spring%20Security%20OAuth2/01.%E4%BB%8B%E7%BB%8D.html",relativePath:"19.Spring Security OAuth2/01.Spring Security OAuth2/01.介绍.md",key:"v-74c698d0",path:"/pages/1b4b4e/",headers:[{level:2,title:"什么是 OAuth",slug:"什么是-oauth",normalizedTitle:"什么是 oauth",charIndex:9},{level:2,title:"什么是 Spring Security",slug:"什么是-spring-security",normalizedTitle:"什么是 spring security",charIndex:379}],headersStr:"什么是 OAuth 什么是 Spring Security",content:"# 介绍\n\n\n# 什么是 OAuth\n\nOAuth(开放授权)是一个开放标准，允许用户授权第三方应用访问他们存储在另外的服务提供者上的信息，而不 需要将用户名和密码提供给第三方应用或分享他们数据的所有内容。OAuth2.0是OAuth协议的延续版本，但不向 后兼容OAuth 1.0即完全废止了OAuth1.0。很多大公司如Google，Yahoo，Microsoft等都提供了OAUTH认证服 务，这些都足以说明OAUTH标准逐渐成为开放资源授权的标准。\n\nOauth协议目前发展到2.0版本，1.0版本过于复杂，2.0版本已得到广泛应用。\n\n参考:https://baike.baidu.com/item/oAuth/7153134?fr=aladdin\n\nOauth协议:https://tools.ietf.org/html/rfc6749\n\n\n# 什么是 Spring Security\n\nSpring Security 是一个安全框架，前身是 Acegi Security，能够为 Spring 企业应用系统提供声明式的安全访问控制。Spring Security 基于 Servlet 过滤器、IoC 和 AOP，为 Web 请求和方法调用提供身份确认和授权处理，避免了代码耦合，减少了大量重复代码工作。",normalizedContent:"# 介绍\n\n\n# 什么是 oauth\n\noauth(开放授权)是一个开放标准，允许用户授权第三方应用访问他们存储在另外的服务提供者上的信息，而不 需要将用户名和密码提供给第三方应用或分享他们数据的所有内容。oauth2.0是oauth协议的延续版本，但不向 后兼容oauth 1.0即完全废止了oauth1.0。很多大公司如google，yahoo，microsoft等都提供了oauth认证服 务，这些都足以说明oauth标准逐渐成为开放资源授权的标准。\n\noauth协议目前发展到2.0版本，1.0版本过于复杂，2.0版本已得到广泛应用。\n\n参考:https://baike.baidu.com/item/oauth/7153134?fr=aladdin\n\noauth协议:https://tools.ietf.org/html/rfc6749\n\n\n# 什么是 spring security\n\nspring security 是一个安全框架，前身是 acegi security，能够为 spring 企业应用系统提供声明式的安全访问控制。spring security 基于 servlet 过滤器、ioc 和 aop，为 web 请求和方法调用提供身份确认和授权处理，避免了代码耦合，减少了大量重复代码工作。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"为什么需要 OAuth2",frontmatter:{title:"为什么需要 OAuth2",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/17b300/"},regularPath:"/19.Spring%20Security%20OAuth2/01.Spring%20Security%20OAuth2/02.%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%20OAuth2.html",relativePath:"19.Spring Security OAuth2/01.Spring Security OAuth2/02.为什么需要 OAuth2.md",key:"v-78a70910",path:"/pages/17b300/",headers:[{level:2,title:"应用场景",slug:"应用场景",normalizedTitle:"应用场景",charIndex:19},{level:2,title:"名词解释",slug:"名词解释",normalizedTitle:"名词解释",charIndex:459},{level:2,title:"交互过程",slug:"交互过程",normalizedTitle:"交互过程",charIndex:1035}],headersStr:"应用场景 名词解释 交互过程",content:'# 为什么需要 OAuth2\n\n\n# 应用场景\n\n我们假设你有一个“云笔记”产品，并提供了“云笔记服务”和“云相册服务”，此时用户需要在不同的设备（PC、Android、iPhone、TV、Watch）上去访问这些“资源”（笔记，图片）\n\n那么用户如何才能访问属于自己的那部分资源呢？此时传统的做法就是提供自己的账号和密码给我们的“云笔记”，登录成功后就可以获取资源了。但这样的做法会有以下几个问题：\n\n * “云笔记服务”和“云相册服务”会分别部署，难道我们要分别登录吗？\n * 如果有第三方应用程序想要接入我们的“云笔记”，难道需要用户提供账号和密码给第三方应用程序，让他记录后再访问我们的资源吗？\n * 用户如何限制第三方应用程序在我们“云笔记”的授权范围和使用期限？难道把所有资料都永久暴露给它吗？\n * 如果用户修改了密码收回了权限，那么所有第三方应用程序会全部失效。\n * 只要有一个接入的第三方应用程序遭到破解，那么用户的密码就会泄露，后果不堪设想。\n\n为了解决如上问题，OAuth 应用而生。\n\n\n# 名词解释\n\n * 第三方应用程序（Third-party application）： 又称之为客户端（client），比如上节中提到的设备（PC、Android、iPhone、TV、Watch），我们会在这些设备中安装我们自己研发的 APP。又比如我们的产品想要使用 QQ、微信等第三方登录。对我们的产品来说，QQ、微信登录是第三方登录系统。我们又需要第三方登录系统的资源（头像、昵称等）。对于 QQ、微信等系统我们又是第三方应用程序。\n * HTTP 服务提供商（HTTP service）： 我们的云笔记产品以及 QQ、微信等都可以称之为“服务提供商”。\n * 资源所有者（Resource Owner）： 又称之为用户（user）。\n * 用户代理（User Agent）： 比如浏览器，代替用户去访问这些资源。\n * 认证服务器（Authorization server）： 即服务提供商专门用来处理认证的服务器，简单点说就是登录功能（验证用户的账号密码是否正确以及分配相应的权限）\n * 资源服务器（Resource server）： 即服务提供商存放用户生成的资源的服务器。它与认证服务器，可以是同一台服务器，也可以是不同的服务器。简单点说就是资源的访问入口，比如上节中提到的“云笔记服务”和“云相册服务”都可以称之为资源服务器。\n\n\n# 交互过程\n\nOAuth 在 "客户端" 与 "服务提供商" 之间，设置了一个授权层（authorization layer）。"客户端" 不能直接登录 "服务提供商"，只能登录授权层，以此将用户与客户端区分开来。"客户端" 登录授权层所用的令牌（token），与用户的密码不同。用户可以在登录的时候，指定授权层令牌的权限范围和有效期。"客户端" 登录授权层以后，"服务提供商" 根据令牌的权限范围和有效期，向 "客户端" 开放用户储存的资料。\n\n',normalizedContent:'# 为什么需要 oauth2\n\n\n# 应用场景\n\n我们假设你有一个“云笔记”产品，并提供了“云笔记服务”和“云相册服务”，此时用户需要在不同的设备（pc、android、iphone、tv、watch）上去访问这些“资源”（笔记，图片）\n\n那么用户如何才能访问属于自己的那部分资源呢？此时传统的做法就是提供自己的账号和密码给我们的“云笔记”，登录成功后就可以获取资源了。但这样的做法会有以下几个问题：\n\n * “云笔记服务”和“云相册服务”会分别部署，难道我们要分别登录吗？\n * 如果有第三方应用程序想要接入我们的“云笔记”，难道需要用户提供账号和密码给第三方应用程序，让他记录后再访问我们的资源吗？\n * 用户如何限制第三方应用程序在我们“云笔记”的授权范围和使用期限？难道把所有资料都永久暴露给它吗？\n * 如果用户修改了密码收回了权限，那么所有第三方应用程序会全部失效。\n * 只要有一个接入的第三方应用程序遭到破解，那么用户的密码就会泄露，后果不堪设想。\n\n为了解决如上问题，oauth 应用而生。\n\n\n# 名词解释\n\n * 第三方应用程序（third-party application）： 又称之为客户端（client），比如上节中提到的设备（pc、android、iphone、tv、watch），我们会在这些设备中安装我们自己研发的 app。又比如我们的产品想要使用 qq、微信等第三方登录。对我们的产品来说，qq、微信登录是第三方登录系统。我们又需要第三方登录系统的资源（头像、昵称等）。对于 qq、微信等系统我们又是第三方应用程序。\n * http 服务提供商（http service）： 我们的云笔记产品以及 qq、微信等都可以称之为“服务提供商”。\n * 资源所有者（resource owner）： 又称之为用户（user）。\n * 用户代理（user agent）： 比如浏览器，代替用户去访问这些资源。\n * 认证服务器（authorization server）： 即服务提供商专门用来处理认证的服务器，简单点说就是登录功能（验证用户的账号密码是否正确以及分配相应的权限）\n * 资源服务器（resource server）： 即服务提供商存放用户生成的资源的服务器。它与认证服务器，可以是同一台服务器，也可以是不同的服务器。简单点说就是资源的访问入口，比如上节中提到的“云笔记服务”和“云相册服务”都可以称之为资源服务器。\n\n\n# 交互过程\n\noauth 在 "客户端" 与 "服务提供商" 之间，设置了一个授权层（authorization layer）。"客户端" 不能直接登录 "服务提供商"，只能登录授权层，以此将用户与客户端区分开来。"客户端" 登录授权层所用的令牌（token），与用户的密码不同。用户可以在登录的时候，指定授权层令牌的权限范围和有效期。"客户端" 登录授权层以后，"服务提供商" 根据令牌的权限范围和有效期，向 "客户端" 开放用户储存的资料。\n\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"开放平台",frontmatter:{title:"开放平台",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/5cb943/"},regularPath:"/19.Spring%20Security%20OAuth2/01.Spring%20Security%20OAuth2/03.%E5%BC%80%E6%94%BE%E5%B9%B3%E5%8F%B0.html",relativePath:"19.Spring Security OAuth2/01.Spring Security OAuth2/03.开放平台.md",key:"v-3222f444",path:"/pages/5cb943/",headers:[{level:2,title:"交互模型",slug:"交互模型",normalizedTitle:"交互模型",charIndex:11},{level:3,title:"认证服务器",slug:"认证服务器",normalizedTitle:"认证服务器",charIndex:72},{level:2,title:"OAuth2 开放平台",slug:"oauth2-开放平台",normalizedTitle:"oauth2 开放平台",charIndex:580}],headersStr:"交互模型 认证服务器 OAuth2 开放平台",content:'# 开放平台\n\n\n# 交互模型\n\n交互模型涉及三方：\n\n * 资源拥有者：用户\n * 客户端：APP\n * 服务提供方：包含两个角色\n   * 认证服务器\n   * 资源服务器\n\n\n# 认证服务器\n\n认证服务器负责对用户进行认证，并授权给客户端权限。认证很容易实现（验证账号密码即可），问题在于如何授权。比如我们使用第三方登录 "有道云笔记"，你可以看到如使用 QQ 登录的授权页面上有 "有道云笔记将获得以下权限" 的字样以及权限信息\n\n\n\n认证服务器需要知道请求授权的客户端的身份以及该客户端请求的权限。我们可以为每一个客户端预先分配一个 id，并给每个 id 对应一个名称以及权限信息。这些信息可以写在认证服务器上的配置文件里。然后，客户端每次打开授权页面的时候，把属于自己的 id 传过来，如：\n\nhttps://www.ivoov.com/oauth/authorize?client_id=yourClientId&response_type=code&scope=all&redirect_uri=https://www.ivoov.com\n\n\n随着时间的推移和业务的增长，会发现，修改配置的工作消耗了太多的人力。有没有办法把这个过程自动化起来，把人工从这些繁琐的操作中解放出来？当开始考虑这一步，开放平台的成型也就是水到渠成的事情了。\n\n\n# OAuth2 开放平台\n\n开放平台是由 OAuth2.0 协议衍生出来的一个产品。它的作用是让客户端自己去这上面进行注册、申请，通过之后系统自动分配 client_id ，并完成配置的自动更新（通常是写进数据库）。\n\n客户端要完成申请，通常需要填写客户端程序的类型（Web、App 等）、企业介绍、执照、想要获取的权限等等信息。这些信息在得到服务提供方的人工审核通过后，开发平台就会自动分配一个 client_id 给客户端了。\n\n到这里，已经实现了登录认证、授权页的信息展示。那么接下来，当用户成功进行授权之后，认证服务器需要把产生的 access_token 发送给客户端，方案如下：\n\n * 让客户端在开放平台申请的时候，填写一个 URL，例如：https://www.ivoov.com\n * 每次当有用户授权成功之后，认证服务器将页面重定向到这个 URL（回调），并带上 access_token，例如：https://www.ivoov.com?access_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0ZW5hbn ...\n * 客户端接收到了这个 access_token，而且认证服务器的授权动作已经完成，刚好可以把程序的控制权转交回客户端，由客户端决定接下来向用户展示什么内容',normalizedContent:'# 开放平台\n\n\n# 交互模型\n\n交互模型涉及三方：\n\n * 资源拥有者：用户\n * 客户端：app\n * 服务提供方：包含两个角色\n   * 认证服务器\n   * 资源服务器\n\n\n# 认证服务器\n\n认证服务器负责对用户进行认证，并授权给客户端权限。认证很容易实现（验证账号密码即可），问题在于如何授权。比如我们使用第三方登录 "有道云笔记"，你可以看到如使用 qq 登录的授权页面上有 "有道云笔记将获得以下权限" 的字样以及权限信息\n\n\n\n认证服务器需要知道请求授权的客户端的身份以及该客户端请求的权限。我们可以为每一个客户端预先分配一个 id，并给每个 id 对应一个名称以及权限信息。这些信息可以写在认证服务器上的配置文件里。然后，客户端每次打开授权页面的时候，把属于自己的 id 传过来，如：\n\nhttps://www.ivoov.com/oauth/authorize?client_id=yourclientid&response_type=code&scope=all&redirect_uri=https://www.ivoov.com\n\n\n随着时间的推移和业务的增长，会发现，修改配置的工作消耗了太多的人力。有没有办法把这个过程自动化起来，把人工从这些繁琐的操作中解放出来？当开始考虑这一步，开放平台的成型也就是水到渠成的事情了。\n\n\n# oauth2 开放平台\n\n开放平台是由 oauth2.0 协议衍生出来的一个产品。它的作用是让客户端自己去这上面进行注册、申请，通过之后系统自动分配 client_id ，并完成配置的自动更新（通常是写进数据库）。\n\n客户端要完成申请，通常需要填写客户端程序的类型（web、app 等）、企业介绍、执照、想要获取的权限等等信息。这些信息在得到服务提供方的人工审核通过后，开发平台就会自动分配一个 client_id 给客户端了。\n\n到这里，已经实现了登录认证、授权页的信息展示。那么接下来，当用户成功进行授权之后，认证服务器需要把产生的 access_token 发送给客户端，方案如下：\n\n * 让客户端在开放平台申请的时候，填写一个 url，例如：https://www.ivoov.com\n * 每次当有用户授权成功之后，认证服务器将页面重定向到这个 url（回调），并带上 access_token，例如：https://www.ivoov.com?access_token=eyjhbgcioijiuzi1niisinr5cci6ikpxvcj9.eyj0zw5hbn ...\n * 客户端接收到了这个 access_token，而且认证服务器的授权动作已经完成，刚好可以把程序的控制权转交回客户端，由客户端决定接下来向用户展示什么内容',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"令牌的访问与刷新",frontmatter:{title:"令牌的访问与刷新",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/37f648/"},regularPath:"/19.Spring%20Security%20OAuth2/01.Spring%20Security%20OAuth2/04.%E4%BB%A4%E7%89%8C%E7%9A%84%E8%AE%BF%E9%97%AE%E4%B8%8E%E5%88%B7%E6%96%B0.html",relativePath:"19.Spring Security OAuth2/01.Spring Security OAuth2/04.令牌的访问与刷新.md",key:"v-3c995a47",path:"/pages/37f648/",headers:[{level:2,title:"Access Token",slug:"access-token",normalizedTitle:"access token",charIndex:15},{level:2,title:"Refresh Token",slug:"refresh-token",normalizedTitle:"refresh token",charIndex:323}],headersStr:"Access Token Refresh Token",content:"# 令牌的访问与刷新\n\n\n# Access Token\n\nAccess Token 是客户端访问资源服务器的令牌。拥有这个令牌代表着得到用户的授权。然而，这个授权应该是 临时 的，有一定有效期。这是因为，Access Token 在使用的过程中 可能会泄露。给 Access Token 限定一个 较短的有效期 可以降低因 Access Token 泄露而带来的风险。\n\n然而引入了有效期之后，客户端使用起来就不那么方便了。每当 Access Token 过期，客户端就必须重新向用户索要授权。这样用户可能每隔几天，甚至每天都需要进行授权操作。这是一件非常影响用户体验的事情。希望有一种方法，可以避免这种情况。\n\n于是 OAuth2.0 引入了 Refresh Token 机制\n\n\n# Refresh Token\n\nRefresh Token 的作用是用来刷新 Access Token。认证服务器提供一个刷新接口，例如：\n\nhttps://www.ivoov.com/refresh?refresh_token=&client_id=\n\n\n传入 refresh_token 和 client_id，认证服务器验证通过后，返回一个新的 Access Token。为了安全，OAuth2.0 引入了两个措施：\n\n * OAuth2.0 要求，Refresh Token 一定是保存在客户端的服务器上 ，而绝不能存放在狭义的客户端（例如 App、PC 端软件）上。调用 refresh 接口的时候，一定是从服务器到服务器的访问。\n * OAuth2.0 引入了 client_secret 机制。即每一个 client_id 都对应一个 client_secret。这个 client_secret 会在客户端申请 client_id 时，随 client_id 一起分配给客户端。客户端必须把 client_secret 妥善保管在服务器上，决不能泄露。刷新 Access Token 时，需要验证这个 client_secret。\n\n实际上的刷新接口类似于：\n\nhttps://www.ivoov.com/oauth/refresh?refresh_token=&client_id=&client_secret=\n\n\n以上就是 Refresh Token 机制。Refresh Token 的有效期非常长，会在用户授权时，随 Access Token 一起重定向到回调 URL，传递给客户端。",normalizedContent:"# 令牌的访问与刷新\n\n\n# access token\n\naccess token 是客户端访问资源服务器的令牌。拥有这个令牌代表着得到用户的授权。然而，这个授权应该是 临时 的，有一定有效期。这是因为，access token 在使用的过程中 可能会泄露。给 access token 限定一个 较短的有效期 可以降低因 access token 泄露而带来的风险。\n\n然而引入了有效期之后，客户端使用起来就不那么方便了。每当 access token 过期，客户端就必须重新向用户索要授权。这样用户可能每隔几天，甚至每天都需要进行授权操作。这是一件非常影响用户体验的事情。希望有一种方法，可以避免这种情况。\n\n于是 oauth2.0 引入了 refresh token 机制\n\n\n# refresh token\n\nrefresh token 的作用是用来刷新 access token。认证服务器提供一个刷新接口，例如：\n\nhttps://www.ivoov.com/refresh?refresh_token=&client_id=\n\n\n传入 refresh_token 和 client_id，认证服务器验证通过后，返回一个新的 access token。为了安全，oauth2.0 引入了两个措施：\n\n * oauth2.0 要求，refresh token 一定是保存在客户端的服务器上 ，而绝不能存放在狭义的客户端（例如 app、pc 端软件）上。调用 refresh 接口的时候，一定是从服务器到服务器的访问。\n * oauth2.0 引入了 client_secret 机制。即每一个 client_id 都对应一个 client_secret。这个 client_secret 会在客户端申请 client_id 时，随 client_id 一起分配给客户端。客户端必须把 client_secret 妥善保管在服务器上，决不能泄露。刷新 access token 时，需要验证这个 client_secret。\n\n实际上的刷新接口类似于：\n\nhttps://www.ivoov.com/oauth/refresh?refresh_token=&client_id=&client_secret=\n\n\n以上就是 refresh token 机制。refresh token 的有效期非常长，会在用户授权时，随 access token 一起重定向到回调 url，传递给客户端。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"客户端授权模式",frontmatter:{title:"客户端授权模式",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/fbe541/"},regularPath:"/19.Spring%20Security%20OAuth2/01.Spring%20Security%20OAuth2/05.%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%8E%88%E6%9D%83%E6%A8%A1%E5%BC%8F.html",relativePath:"19.Spring Security OAuth2/01.Spring Security OAuth2/05.客户端授权模式.md",key:"v-18111e3e",path:"/pages/fbe541/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:14},{level:2,title:"简化模式",slug:"简化模式",normalizedTitle:"简化模式",charIndex:107},{level:3,title:"测试",slug:"测试",normalizedTitle:"测试",charIndex:959},{level:2,title:"授权码模式",slug:"授权码模式",normalizedTitle:"授权码模式",charIndex:140},{level:3,title:"测试",slug:"测试-2",normalizedTitle:"测试",charIndex:959},{level:2,title:"密码模式",slug:"密码模式",normalizedTitle:"密码模式",charIndex:185},{level:3,title:"测试",slug:"测试-3",normalizedTitle:"测试",charIndex:959},{level:2,title:"客户端模式",slug:"客户端模式",normalizedTitle:"客户端模式",charIndex:212},{level:3,title:"测试",slug:"测试-4",normalizedTitle:"测试",charIndex:959}],headersStr:"概述 简化模式 测试 授权码模式 测试 密码模式 测试 客户端模式 测试",content:'# 客户端授权模式\n\n\n# 概述\n\n客户端必须得到用户的授权（authorization grant），才能获得令牌（access token）。OAuth 2.0 定义了四种授权方式。\n\n * implicit：简化模式，不推荐使用\n * authorization code：授权码模式\n * resource owner password credentials：密码模式\n * client credentials：客户端模式\n\n\n# 简化模式\n\n简化模式适用于纯静态页面应用。所谓纯静态页面应用，也就是应用没有在服务器上执行代码的权限（通常是把代码托管在别人的服务器上），只有前端 JS 代码的控制权。\n\n这种场景下，应用是没有持久化存储的能力的。因此，按照 OAuth2.0 的规定，这种应用是拿不到 Refresh Token 的。其整个授权流程如下：\n\n\n\n该模式下，access_token 容易泄露且不可刷新\n\n（1）资源拥有者打开客户端，客户端要求资源拥有者给予授权，它将浏览器被重定向到授权服务器，重定向时会附加客户端的身份信息。如：\n\n/oauth/authorize?client_id=simplify&response_type=token&scope=server&redirect_uri=https://www.ivoov.com\n\n\n参数描述同授权码模式 ，注意response_type=token，说明是简化模式。\n\n（2）浏览器出现向授权服务器授权页面，之后将用户同意授权。\n\n（3）授权服务器将授权码将令牌（access_token）以Hash的形式存放在重定向uri的fargment中发送给浏览器。\n\n提示\n\nfragment 主要是用来标识 URI 所标识资源里的某个资源，在 URI 的末尾通过 （#）作为 fragment 的开头，其中 # 不属于 fragment 的值。如https://domain/index#L18这个 URI 中 L18 就是 fragment 的值。大家只需要知道js通过响应浏览器地址栏变化的方式能获取到fragment 就行了。一般来说，简化模式用于没有服务器端的第三方单页面应用，因为没有服务器端就无法接收授权码。\n\n\n# 测试\n\n浏览器访问认证页面：\n\nhttp://localhost:3000/oauth/authorize?client_id=simplify&response_type=token&scope=server&redirect_uri=https://www.ivoov.com\n\n\n\n\n然后输入模拟的账号和密码点登陆之后进入授权页面：\n\n\n\n确认授权后，浏览器会重定向到指定路径（oauth_client_details表中的web_server_redirect_uri）并以Hash的形式存放在重定向uri的fargment中,如：\n\nhttps://www.ivoov.com/#access_token=273b7f10-3b4d-495e-9b15-b5bc801ff265&token_type=bearer&expires_in=43199&scope=server\n\n\n\n# 授权码模式\n\n授权码模式适用于有自己的服务器的应用，它是一个一次性的临时凭证，用来换取 access_token 和 refresh_token。认证服务器提供了一个类似这样的接口：\n\n/oauth/authorize?response_type=code&client_id=ci&scope=server&redirect_uri=https://www.ivoov.com\n\n\n需要传入 code、client_id 以及 client_secret。验证通过后，返回 access_token 和 refresh_token。一旦换取成功，code 立即作废，不能再使用第二次。流程图如下：\n\n\n\n（1）资源拥有者打开客户端，客户端要求资源拥有者给予授权，它将浏览器被重定向到授权服务器，重定向时会附加客户端的身份信息。如：\n\n/oauth/authorize?client_id=simplify&response_type=code&scope=server&redirect_uri=https://www.ivoov.com\n\n\n参数列表如下：\n\n * client_id：客户端准入标识。\n * response_type：授权码模式固定为code。\n * scope：客户端权限。\n * redirect_uri：跳转uri，当授权码申请成功后会跳转到此地址，并在后边带上code参数（授权码）。\n\n（2）浏览器出现向授权服务器授权页面，之后将用户同意授权。\n\n（3）授权服务器将授权码（AuthorizationCode）转经浏览器发送给client(通过redirect_uri)。\n\n（4）客户端拿着授权码向授权服务器索要访问access_token，请求如下：\n\n/oauth/token?client_id=simplify&client_secret=secret&grant_type=authorization_code&code=5PgfcD&redirect_uri=https://www.ivoov.com\n\n\n参数列表如下\n\n * client_id：客户端准入标识。\n * client_secret：客户端秘钥。\n * grant_type：授权类型，填写authorization_code，表示授权码模式\n * code：授权码，就是刚刚获取的授权码，注意：授权码只使用一次就无效了，需要重新申请。\n * redirect_uri：申请授权码时的跳转url，一定和申请授权码时用的redirect_uri一致。\n\n（5）授权服务器返回令牌(access_token)\n\n这种模式是四种模式中最安全的一种模式。一般用于client是Web服务器端应用或第三方的原生App调用资源服务的时候。因为在这种模式中access_token不会经过浏览器或移动端的App，而是直接从服务端去交换，这样就最大限度的减小了令牌泄漏的风险。\n\n这个 code 的作用是保护 token 的安全性。上一节说到，简单模式下，token 是不安全的。这是因为在第 4 步当中直接把 token 返回给应用。而这一步容易被拦截、窃听。引入了 code 之后，即使攻击者能够窃取到 code，但是由于他无法获得应用保存在服务器的 client_secret，因此也无法通过 code 换取 token。而第 5 步，为什么不容易被拦截、窃听呢？这是因为，首先，这是一个从服务器到服务器的访问，黑客比较难捕捉到；其次，这个请求通常要求是 https 的实现。即使能窃听到数据包也无法解析出内容。\n\n有了这个 code，token 的安全性大大提高。因此，OAuth2.0 鼓励使用这种方式进行授权，而简单模式则是在不得已情况下才会使用。\n\n\n# 测试\n\n浏览器访问认证页面：\n\n/oauth/authorize?client_id=simplify&response_type=code&scope=server&redirect_uri=https://www.ivoov.com\n\n\n\n\n然后输入模拟的账号和密码点登陆之后进入授权页面：\n\n\n\n确认授权后，浏览器会重定向到指定路径（oauth_client_details表中的web_server_redirect_uri）并附加验证码?code=hzzmj7（每次不一样），最后使用该验证码获取token。\n\n使用Postman请求授权服务器。\n\n\n\n\n# 密码模式\n\n密码模式中，用户向客户端提供自己的用户名和密码。客户端使用这些信息，向 "服务商提供商" 索要授权。在这种模式中，用户必须把自己的密码给客户端，但是客户端不得储存密码。这通常用在用户对客户端高度信任的情况下，比如客户端是操作系统的一部分。\n\n一个典型的例子是同一个企业内部的不同产品要使用本企业的 oAuth2.0 体系。在有些情况下，产品希望能够定制化授权页面。由于是同个企业，不需要向用户展示“xxx将获取以下权限”等字样并询问用户的授权意向，而只需进行用户的身份认证即可。这个时候，由具体的产品团队开发定制化的授权界面，接收用户输入账号密码，并直接传递给鉴权服务器进行授权即可。\n\n\n\n（1）资源拥有者将用户名、密码发送给客户端\n\n（2）客户端拿着资源拥有者的用户名、密码向授权服务器请求令牌（access_token），请求如下：\n\n/oauth/token?client_id=simplify&client_secret=simplify&username=admin&grant_type=password&password=123456\n\n\n参数列表如下：\n\n * client_id：客户端准入标识。\n * client_secret：客户端秘钥。\n * grant_type：授权类型，填写password表示密码模式\n * username：资源拥有者用户名。\n * password：资源拥有者密码。\n\n（3）授权服务器将令牌（access_token）发送给client\n\n这种模式十分简单，但是却意味着直接将用户敏感信息泄漏给了client，因此这就说明这种模式只能用于client是我们自己开发的情况下。因此密码模式一般用于我们自己开发的，第一方原生App或第一方单页面应用。\n\n有一点需要特别注意的是，在第 2 步中，认证服务器需要对客户端的身份进行验证，确保是受信任的客户端。\n\n\n# 测试\n\n使用Postman请求授权服务器。\n\n\n\n\n# 客户端模式\n\n如果信任关系再进一步，或者调用者是一个后端的模块，没有用户界面的时候，可以使用客户端模式。鉴权服务器直接对客户端进行身份验证，验证通过后，返回 token。\n\n\n\n（1）客户端向授权服务器发送自己的身份信息，并请求令牌（access_token） （2）确认客户端身份无误后，将令牌（access_token）发送给client，请求如下：\n\n/oauth/token?client_id=simplify&client_secret=simplify&grant_type=client_credentials\n\n\n参数列表如下：\n\n * client_id：客户端准入标识。\n * client_secret：客户端秘钥。\n * grant_type：授权类型，填写client_credentials表示客户端模式\n\n这种模式是最方便但最不安全的模式。因此这就要求我们对client完全的信任，而client本身也是安全的。因此这种模式一般用来提供给我们完全信任的服务器端服务。比如，合作方系统对接，拉取一组用户信息。\n\n\n# 测试\n\n使用Postman请求授权服务器。\n\n',normalizedContent:'# 客户端授权模式\n\n\n# 概述\n\n客户端必须得到用户的授权（authorization grant），才能获得令牌（access token）。oauth 2.0 定义了四种授权方式。\n\n * implicit：简化模式，不推荐使用\n * authorization code：授权码模式\n * resource owner password credentials：密码模式\n * client credentials：客户端模式\n\n\n# 简化模式\n\n简化模式适用于纯静态页面应用。所谓纯静态页面应用，也就是应用没有在服务器上执行代码的权限（通常是把代码托管在别人的服务器上），只有前端 js 代码的控制权。\n\n这种场景下，应用是没有持久化存储的能力的。因此，按照 oauth2.0 的规定，这种应用是拿不到 refresh token 的。其整个授权流程如下：\n\n\n\n该模式下，access_token 容易泄露且不可刷新\n\n（1）资源拥有者打开客户端，客户端要求资源拥有者给予授权，它将浏览器被重定向到授权服务器，重定向时会附加客户端的身份信息。如：\n\n/oauth/authorize?client_id=simplify&response_type=token&scope=server&redirect_uri=https://www.ivoov.com\n\n\n参数描述同授权码模式 ，注意response_type=token，说明是简化模式。\n\n（2）浏览器出现向授权服务器授权页面，之后将用户同意授权。\n\n（3）授权服务器将授权码将令牌（access_token）以hash的形式存放在重定向uri的fargment中发送给浏览器。\n\n提示\n\nfragment 主要是用来标识 uri 所标识资源里的某个资源，在 uri 的末尾通过 （#）作为 fragment 的开头，其中 # 不属于 fragment 的值。如https://domain/index#l18这个 uri 中 l18 就是 fragment 的值。大家只需要知道js通过响应浏览器地址栏变化的方式能获取到fragment 就行了。一般来说，简化模式用于没有服务器端的第三方单页面应用，因为没有服务器端就无法接收授权码。\n\n\n# 测试\n\n浏览器访问认证页面：\n\nhttp://localhost:3000/oauth/authorize?client_id=simplify&response_type=token&scope=server&redirect_uri=https://www.ivoov.com\n\n\n\n\n然后输入模拟的账号和密码点登陆之后进入授权页面：\n\n\n\n确认授权后，浏览器会重定向到指定路径（oauth_client_details表中的web_server_redirect_uri）并以hash的形式存放在重定向uri的fargment中,如：\n\nhttps://www.ivoov.com/#access_token=273b7f10-3b4d-495e-9b15-b5bc801ff265&token_type=bearer&expires_in=43199&scope=server\n\n\n\n# 授权码模式\n\n授权码模式适用于有自己的服务器的应用，它是一个一次性的临时凭证，用来换取 access_token 和 refresh_token。认证服务器提供了一个类似这样的接口：\n\n/oauth/authorize?response_type=code&client_id=ci&scope=server&redirect_uri=https://www.ivoov.com\n\n\n需要传入 code、client_id 以及 client_secret。验证通过后，返回 access_token 和 refresh_token。一旦换取成功，code 立即作废，不能再使用第二次。流程图如下：\n\n\n\n（1）资源拥有者打开客户端，客户端要求资源拥有者给予授权，它将浏览器被重定向到授权服务器，重定向时会附加客户端的身份信息。如：\n\n/oauth/authorize?client_id=simplify&response_type=code&scope=server&redirect_uri=https://www.ivoov.com\n\n\n参数列表如下：\n\n * client_id：客户端准入标识。\n * response_type：授权码模式固定为code。\n * scope：客户端权限。\n * redirect_uri：跳转uri，当授权码申请成功后会跳转到此地址，并在后边带上code参数（授权码）。\n\n（2）浏览器出现向授权服务器授权页面，之后将用户同意授权。\n\n（3）授权服务器将授权码（authorizationcode）转经浏览器发送给client(通过redirect_uri)。\n\n（4）客户端拿着授权码向授权服务器索要访问access_token，请求如下：\n\n/oauth/token?client_id=simplify&client_secret=secret&grant_type=authorization_code&code=5pgfcd&redirect_uri=https://www.ivoov.com\n\n\n参数列表如下\n\n * client_id：客户端准入标识。\n * client_secret：客户端秘钥。\n * grant_type：授权类型，填写authorization_code，表示授权码模式\n * code：授权码，就是刚刚获取的授权码，注意：授权码只使用一次就无效了，需要重新申请。\n * redirect_uri：申请授权码时的跳转url，一定和申请授权码时用的redirect_uri一致。\n\n（5）授权服务器返回令牌(access_token)\n\n这种模式是四种模式中最安全的一种模式。一般用于client是web服务器端应用或第三方的原生app调用资源服务的时候。因为在这种模式中access_token不会经过浏览器或移动端的app，而是直接从服务端去交换，这样就最大限度的减小了令牌泄漏的风险。\n\n这个 code 的作用是保护 token 的安全性。上一节说到，简单模式下，token 是不安全的。这是因为在第 4 步当中直接把 token 返回给应用。而这一步容易被拦截、窃听。引入了 code 之后，即使攻击者能够窃取到 code，但是由于他无法获得应用保存在服务器的 client_secret，因此也无法通过 code 换取 token。而第 5 步，为什么不容易被拦截、窃听呢？这是因为，首先，这是一个从服务器到服务器的访问，黑客比较难捕捉到；其次，这个请求通常要求是 https 的实现。即使能窃听到数据包也无法解析出内容。\n\n有了这个 code，token 的安全性大大提高。因此，oauth2.0 鼓励使用这种方式进行授权，而简单模式则是在不得已情况下才会使用。\n\n\n# 测试\n\n浏览器访问认证页面：\n\n/oauth/authorize?client_id=simplify&response_type=code&scope=server&redirect_uri=https://www.ivoov.com\n\n\n\n\n然后输入模拟的账号和密码点登陆之后进入授权页面：\n\n\n\n确认授权后，浏览器会重定向到指定路径（oauth_client_details表中的web_server_redirect_uri）并附加验证码?code=hzzmj7（每次不一样），最后使用该验证码获取token。\n\n使用postman请求授权服务器。\n\n\n\n\n# 密码模式\n\n密码模式中，用户向客户端提供自己的用户名和密码。客户端使用这些信息，向 "服务商提供商" 索要授权。在这种模式中，用户必须把自己的密码给客户端，但是客户端不得储存密码。这通常用在用户对客户端高度信任的情况下，比如客户端是操作系统的一部分。\n\n一个典型的例子是同一个企业内部的不同产品要使用本企业的 oauth2.0 体系。在有些情况下，产品希望能够定制化授权页面。由于是同个企业，不需要向用户展示“xxx将获取以下权限”等字样并询问用户的授权意向，而只需进行用户的身份认证即可。这个时候，由具体的产品团队开发定制化的授权界面，接收用户输入账号密码，并直接传递给鉴权服务器进行授权即可。\n\n\n\n（1）资源拥有者将用户名、密码发送给客户端\n\n（2）客户端拿着资源拥有者的用户名、密码向授权服务器请求令牌（access_token），请求如下：\n\n/oauth/token?client_id=simplify&client_secret=simplify&username=admin&grant_type=password&password=123456\n\n\n参数列表如下：\n\n * client_id：客户端准入标识。\n * client_secret：客户端秘钥。\n * grant_type：授权类型，填写password表示密码模式\n * username：资源拥有者用户名。\n * password：资源拥有者密码。\n\n（3）授权服务器将令牌（access_token）发送给client\n\n这种模式十分简单，但是却意味着直接将用户敏感信息泄漏给了client，因此这就说明这种模式只能用于client是我们自己开发的情况下。因此密码模式一般用于我们自己开发的，第一方原生app或第一方单页面应用。\n\n有一点需要特别注意的是，在第 2 步中，认证服务器需要对客户端的身份进行验证，确保是受信任的客户端。\n\n\n# 测试\n\n使用postman请求授权服务器。\n\n\n\n\n# 客户端模式\n\n如果信任关系再进一步，或者调用者是一个后端的模块，没有用户界面的时候，可以使用客户端模式。鉴权服务器直接对客户端进行身份验证，验证通过后，返回 token。\n\n\n\n（1）客户端向授权服务器发送自己的身份信息，并请求令牌（access_token） （2）确认客户端身份无误后，将令牌（access_token）发送给client，请求如下：\n\n/oauth/token?client_id=simplify&client_secret=simplify&grant_type=client_credentials\n\n\n参数列表如下：\n\n * client_id：客户端准入标识。\n * client_secret：客户端秘钥。\n * grant_type：授权类型，填写client_credentials表示客户端模式\n\n这种模式是最方便但最不安全的模式。因此这就要求我们对client完全的信任，而client本身也是安全的。因此这种模式一般用来提供给我们完全信任的服务器端服务。比如，合作方系统对接，拉取一组用户信息。\n\n\n# 测试\n\n使用postman请求授权服务器。\n\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"创建案例工程项目",frontmatter:{title:"创建案例工程项目",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/00c356/"},regularPath:"/19.Spring%20Security%20OAuth2/02.%E5%88%9B%E5%BB%BA%E6%A1%88%E4%BE%8B%E5%B7%A5%E7%A8%8B/01.%E5%88%9B%E5%BB%BA%E6%A1%88%E4%BE%8B%E5%B7%A5%E7%A8%8B%E9%A1%B9%E7%9B%AE.html",relativePath:"19.Spring Security OAuth2/02.创建案例工程/01.创建案例工程项目.md",key:"v-2d7230e8",path:"/pages/00c356/",headers:[{level:2,title:"pom",slug:"pom",normalizedTitle:"pom",charIndex:15}],headersStr:"pom",content:'# 创建案例工程项目\n\n\n# pom\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.ieooc</groupId>\n    <artifactId>simplify</artifactId>\n    <packaging>pom</packaging>\n    <name>${project.artifactId}</name>\n    <version>1.0.0-SNAPSHOT</version>\n    <organization>\n        <name>simplify</name>\n        <url>https://www.ieooc.com</url>\n    </organization>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <maven.compiler.encoding>UTF-8</maven.compiler.encoding>\n        <maven.compiler.source>1.8</maven.compiler.source>\n        <maven.compiler.target>1.8</maven.compiler.target>\n        <spring-boot.version>2.3.2.RELEASE</spring-boot.version>\n        <spring-cloud.version>Hoxton.SR8</spring-cloud.version>\n        <spring-cloud-alibaba.version>2.2.3.RELEASE</spring-cloud-alibaba.version>\n        <hutool.version>5.4.5</hutool.version>\n    </properties>\n\n    <modules>\n\n        <module>simplify-common-bom</module>\n    </modules>\n\n    <dependencies>\n        \x3c!-- 配置文件注释处理器 --\x3e\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-configuration-processor</artifactId>\n            <optional>true</optional>\n        </dependency>\n        \x3c!--监控--\x3e\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-actuator</artifactId>\n        </dependency>\n        \x3c!--lombok敏捷开发工具包--\x3e\n        <dependency>\n            <groupId>org.projectlombok</groupId>\n            <artifactId>lombok</artifactId>\n            <scope>provided</scope>\n        </dependency>\n        \x3c!--测试依赖--\x3e\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-dependencies</artifactId>\n                <version>${spring-boot.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>${spring-cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-bom</artifactId>\n                <version>1.0.0-SNAPSHOT</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            \x3c!--spring cloud alibaba--\x3e\n            <dependency>\n                <groupId>com.alibaba.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-alibaba.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            \x3c!--web 模块--\x3e\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-starter-web</artifactId>\n                <version>${spring-boot.version}</version>\n                <exclusions>\n                    \x3c!--排除tomcat依赖--\x3e\n                    <exclusion>\n                        <artifactId>spring-boot-starter-tomcat</artifactId>\n                        <groupId>org.springframework.boot</groupId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    \x3c!--分发构件到远程仓库--\x3e\n    \x3c!-- 定义snapshots库和releases库的nexus地址 --\x3e\n    <distributionManagement>\n        <repository>\n            <id>nexus</id>\n            <name>Nexus Release Repository</name>\n            <url>http://192.168.199.111:8081/repository/maven-releases/</url>\n        </repository>\n        <snapshotRepository>\n            <id>nexus</id>\n            <name>Nexus Snapshot Repository</name>\n            <url>http://192.168.199.111:8081/repository/maven-snapshots/</url>\n        </snapshotRepository>\n    </distributionManagement>\n\n    <build>\n        <finalName>${project.name}</finalName>\n        <resources>\n            <resource>\n                <directory>src/main/resources</directory>\n                <filtering>true</filtering>\n            </resource>\n        </resources>\n        <pluginManagement>\n            <plugins>\n                <plugin>\n                    <groupId>org.springframework.boot</groupId>\n                    <artifactId>spring-boot-maven-plugin</artifactId>\n                    <version>${spring-boot.version}</version>\n                    <executions>\n                        \x3c!-- 在 mvn package 执行之后，这个命令再次打包生成可执行的 jar，\n                        同时将 mvn package 生成的 jar 重命名为 *.origin --\x3e\n                        <execution>\n                            <id>repackage</id>\n                            <goals>\n                                <goal>repackage</goal>\n                            </goals>\n                        </execution>\n                    </executions>\n                </plugin>\n            </plugins>\n        </pluginManagement>\n        <plugins>\n            <plugin>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.8.0</version>\n                <configuration>\n                    <target>${maven.compiler.target}</target>\n                    <source>${maven.compiler.source}</source>\n                    <encoding>UTF-8</encoding>\n                    <skip>true</skip>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n    <repositories>\n        \x3c!--阿里云代理--\x3e\n        <repository>\n            <id>aliyun</id>\n            <name>aliyun</name>\n            <url>http://maven.aliyun.com/nexus/content/groups/public</url>\n        </repository>\n    </repositories>\n\n    <profiles>\n        <profile>\n            <id>dev</id>\n            <properties>\n                \x3c!-- 环境标识，需要与配置文件的名称相对应 --\x3e\n                <profiles.active>dev</profiles.active>\n            </properties>\n            <activation>\n                \x3c!-- 默认环境 --\x3e\n                <activeByDefault>true</activeByDefault>\n            </activation>\n        </profile>\n        <profile>\n            <id>test</id>\n            <properties>\n                <profiles.active>test</profiles.active>\n            </properties>\n        </profile>\n        <profile>\n            <id>prod</id>\n            <properties>\n                <profiles.active>prod</profiles.active>\n            </properties>\n        </profile>\n    </profiles>\n</project>\n',normalizedContent:'# 创建案例工程项目\n\n\n# pom\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <groupid>com.ieooc</groupid>\n    <artifactid>simplify</artifactid>\n    <packaging>pom</packaging>\n    <name>${project.artifactid}</name>\n    <version>1.0.0-snapshot</version>\n    <organization>\n        <name>simplify</name>\n        <url>https://www.ieooc.com</url>\n    </organization>\n\n    <properties>\n        <project.build.sourceencoding>utf-8</project.build.sourceencoding>\n        <project.reporting.outputencoding>utf-8</project.reporting.outputencoding>\n        <maven.compiler.encoding>utf-8</maven.compiler.encoding>\n        <maven.compiler.source>1.8</maven.compiler.source>\n        <maven.compiler.target>1.8</maven.compiler.target>\n        <spring-boot.version>2.3.2.release</spring-boot.version>\n        <spring-cloud.version>hoxton.sr8</spring-cloud.version>\n        <spring-cloud-alibaba.version>2.2.3.release</spring-cloud-alibaba.version>\n        <hutool.version>5.4.5</hutool.version>\n    </properties>\n\n    <modules>\n\n        <module>simplify-common-bom</module>\n    </modules>\n\n    <dependencies>\n        \x3c!-- 配置文件注释处理器 --\x3e\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-configuration-processor</artifactid>\n            <optional>true</optional>\n        </dependency>\n        \x3c!--监控--\x3e\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-actuator</artifactid>\n        </dependency>\n        \x3c!--lombok敏捷开发工具包--\x3e\n        <dependency>\n            <groupid>org.projectlombok</groupid>\n            <artifactid>lombok</artifactid>\n            <scope>provided</scope>\n        </dependency>\n        \x3c!--测试依赖--\x3e\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-test</artifactid>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <dependencymanagement>\n        <dependencies>\n            <dependency>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-dependencies</artifactid>\n                <version>${spring-boot.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            <dependency>\n                <groupid>org.springframework.cloud</groupid>\n                <artifactid>spring-cloud-dependencies</artifactid>\n                <version>${spring-cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-bom</artifactid>\n                <version>1.0.0-snapshot</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            \x3c!--spring cloud alibaba--\x3e\n            <dependency>\n                <groupid>com.alibaba.cloud</groupid>\n                <artifactid>spring-cloud-alibaba-dependencies</artifactid>\n                <version>${spring-cloud-alibaba.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            \x3c!--web 模块--\x3e\n            <dependency>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-starter-web</artifactid>\n                <version>${spring-boot.version}</version>\n                <exclusions>\n                    \x3c!--排除tomcat依赖--\x3e\n                    <exclusion>\n                        <artifactid>spring-boot-starter-tomcat</artifactid>\n                        <groupid>org.springframework.boot</groupid>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n        </dependencies>\n    </dependencymanagement>\n\n    \x3c!--分发构件到远程仓库--\x3e\n    \x3c!-- 定义snapshots库和releases库的nexus地址 --\x3e\n    <distributionmanagement>\n        <repository>\n            <id>nexus</id>\n            <name>nexus release repository</name>\n            <url>http://192.168.199.111:8081/repository/maven-releases/</url>\n        </repository>\n        <snapshotrepository>\n            <id>nexus</id>\n            <name>nexus snapshot repository</name>\n            <url>http://192.168.199.111:8081/repository/maven-snapshots/</url>\n        </snapshotrepository>\n    </distributionmanagement>\n\n    <build>\n        <finalname>${project.name}</finalname>\n        <resources>\n            <resource>\n                <directory>src/main/resources</directory>\n                <filtering>true</filtering>\n            </resource>\n        </resources>\n        <pluginmanagement>\n            <plugins>\n                <plugin>\n                    <groupid>org.springframework.boot</groupid>\n                    <artifactid>spring-boot-maven-plugin</artifactid>\n                    <version>${spring-boot.version}</version>\n                    <executions>\n                        \x3c!-- 在 mvn package 执行之后，这个命令再次打包生成可执行的 jar，\n                        同时将 mvn package 生成的 jar 重命名为 *.origin --\x3e\n                        <execution>\n                            <id>repackage</id>\n                            <goals>\n                                <goal>repackage</goal>\n                            </goals>\n                        </execution>\n                    </executions>\n                </plugin>\n            </plugins>\n        </pluginmanagement>\n        <plugins>\n            <plugin>\n                <artifactid>maven-compiler-plugin</artifactid>\n                <version>3.8.0</version>\n                <configuration>\n                    <target>${maven.compiler.target}</target>\n                    <source>${maven.compiler.source}</source>\n                    <encoding>utf-8</encoding>\n                    <skip>true</skip>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n    <repositories>\n        \x3c!--阿里云代理--\x3e\n        <repository>\n            <id>aliyun</id>\n            <name>aliyun</name>\n            <url>http://maven.aliyun.com/nexus/content/groups/public</url>\n        </repository>\n    </repositories>\n\n    <profiles>\n        <profile>\n            <id>dev</id>\n            <properties>\n                \x3c!-- 环境标识，需要与配置文件的名称相对应 --\x3e\n                <profiles.active>dev</profiles.active>\n            </properties>\n            <activation>\n                \x3c!-- 默认环境 --\x3e\n                <activebydefault>true</activebydefault>\n            </activation>\n        </profile>\n        <profile>\n            <id>test</id>\n            <properties>\n                <profiles.active>test</profiles.active>\n            </properties>\n        </profile>\n        <profile>\n            <id>prod</id>\n            <properties>\n                <profiles.active>prod</profiles.active>\n            </properties>\n        </profile>\n    </profiles>\n</project>\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"创建统一的依赖管理模块",frontmatter:{title:"创建统一的依赖管理模块",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/c3686c/"},regularPath:"/19.Spring%20Security%20OAuth2/02.%E5%88%9B%E5%BB%BA%E6%A1%88%E4%BE%8B%E5%B7%A5%E7%A8%8B/02.%E5%88%9B%E5%BB%BA%E7%BB%9F%E4%B8%80%E7%9A%84%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86%E6%A8%A1%E5%9D%97.html",relativePath:"19.Spring Security OAuth2/02.创建案例工程/02.创建统一的依赖管理模块.md",key:"v-2fa8ca6b",path:"/pages/c3686c/",headers:[{level:2,title:"pom",slug:"pom",normalizedTitle:"pom",charIndex:18}],headersStr:"pom",content:'# 创建统一的依赖管理模块\n\n\n# pom\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <parent>\n        <groupId>com.ieooc.cloud</groupId>\n        <artifactId>simplify-cloud-dependencies-parent</artifactId>\n        <version>1.0.0.RELEASE</version>\n        <relativePath/>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.ieooc</groupId>\n    <artifactId>simplify-common-bom</artifactId>\n    <packaging>pom</packaging>\n    <version>1.0.0-SNAPSHOT</version>\n    <description>simplify 公共版本控制</description>\n\n    <properties>\n        <simplify.version>1.0.0-SNAPSHOT</simplify.version>\n        <mybatis-plus.version>3.4.0</mybatis-plus.version>\n        <druid.version>1.1.24</druid.version>\n        <mysql-connector.version>8.0.21</mysql-connector.version>\n        <swagger.core.version>1.5.20</swagger.core.version>\n        <swagger.fox.version>3.0.0</swagger.fox.version>\n    </properties>\n\n    <dependencyManagement>\n        <dependencies>\n            \x3c!--公共工具核心包--\x3e\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-core</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--数据操作相关--\x3e\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-data</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--安全工具类--\x3e\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-security</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--网关配置--\x3e\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-gateway</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-datasource</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-job</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-log</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-minio</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-sentinel</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--Sentinel 服务降级熔断、限流组件--\x3e\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-sequence</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--swagger构建成RESTful API文档--\x3e\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-swagger</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-common-transaction</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--通用用户权限管理系统公共api模块--\x3e\n            <dependency>\n                <groupId>com.ieooc</groupId>\n                <artifactId>simplify-upms-api</artifactId>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--mybatis plus extension,包含mybatis plus core--\x3e\n            <dependency>\n                <groupId>com.baomidou</groupId>\n                <artifactId>mybatis-plus-extension</artifactId>\n                <version>${mybatis-plus.version}</version>\n            </dependency>\n            \x3c!--mybatis-plus--\x3e\n            <dependency>\n                <groupId>com.baomidou</groupId>\n                <artifactId>mybatis-plus-boot-starter</artifactId>\n                <version>${mybatis-plus.version}</version>\n            </dependency>\n            \x3c!--mybatis-plus代码生成器--\x3e\n            <dependency>\n                <groupId>com.baomidou</groupId>\n                <artifactId>mybatis-plus-generator</artifactId>\n                <version>${mybatis-plus.version}</version>\n            </dependency>\n            \x3c!--druid数据库连接池和监控--\x3e\n            <dependency>\n                <groupId>com.alibaba</groupId>\n                <artifactId>druid-spring-boot-starter</artifactId>\n                <version>${druid.version}</version>\n            </dependency>\n            \x3c!--swagger 核心组件--\x3e\n            <dependency>\n                <groupId>io.swagger</groupId>\n                <artifactId>swagger-annotations</artifactId>\n                <version>${swagger.core.version}</version>\n            </dependency>\n            \x3c!--swagger 依赖--\x3e\n            <dependency>\n                <groupId>io.springfox</groupId>\n                <artifactId>springfox-swagger2</artifactId>\n                <version>${swagger.fox.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>io.springfox</groupId>\n                <artifactId>springfox-boot-starter</artifactId>\n                <version>${swagger.fox.version}</version>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n</project>\n',normalizedContent:'# 创建统一的依赖管理模块\n\n\n# pom\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <parent>\n        <groupid>com.ieooc.cloud</groupid>\n        <artifactid>simplify-cloud-dependencies-parent</artifactid>\n        <version>1.0.0.release</version>\n        <relativepath/>\n    </parent>\n    <modelversion>4.0.0</modelversion>\n\n    <groupid>com.ieooc</groupid>\n    <artifactid>simplify-common-bom</artifactid>\n    <packaging>pom</packaging>\n    <version>1.0.0-snapshot</version>\n    <description>simplify 公共版本控制</description>\n\n    <properties>\n        <simplify.version>1.0.0-snapshot</simplify.version>\n        <mybatis-plus.version>3.4.0</mybatis-plus.version>\n        <druid.version>1.1.24</druid.version>\n        <mysql-connector.version>8.0.21</mysql-connector.version>\n        <swagger.core.version>1.5.20</swagger.core.version>\n        <swagger.fox.version>3.0.0</swagger.fox.version>\n    </properties>\n\n    <dependencymanagement>\n        <dependencies>\n            \x3c!--公共工具核心包--\x3e\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-core</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--数据操作相关--\x3e\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-data</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--安全工具类--\x3e\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-security</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--网关配置--\x3e\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-gateway</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-datasource</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-job</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-log</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-minio</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-sentinel</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--sentinel 服务降级熔断、限流组件--\x3e\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-sequence</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--swagger构建成restful api文档--\x3e\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-swagger</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-common-transaction</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--通用用户权限管理系统公共api模块--\x3e\n            <dependency>\n                <groupid>com.ieooc</groupid>\n                <artifactid>simplify-upms-api</artifactid>\n                <version>${simplify.version}</version>\n            </dependency>\n            \x3c!--mybatis plus extension,包含mybatis plus core--\x3e\n            <dependency>\n                <groupid>com.baomidou</groupid>\n                <artifactid>mybatis-plus-extension</artifactid>\n                <version>${mybatis-plus.version}</version>\n            </dependency>\n            \x3c!--mybatis-plus--\x3e\n            <dependency>\n                <groupid>com.baomidou</groupid>\n                <artifactid>mybatis-plus-boot-starter</artifactid>\n                <version>${mybatis-plus.version}</version>\n            </dependency>\n            \x3c!--mybatis-plus代码生成器--\x3e\n            <dependency>\n                <groupid>com.baomidou</groupid>\n                <artifactid>mybatis-plus-generator</artifactid>\n                <version>${mybatis-plus.version}</version>\n            </dependency>\n            \x3c!--druid数据库连接池和监控--\x3e\n            <dependency>\n                <groupid>com.alibaba</groupid>\n                <artifactid>druid-spring-boot-starter</artifactid>\n                <version>${druid.version}</version>\n            </dependency>\n            \x3c!--swagger 核心组件--\x3e\n            <dependency>\n                <groupid>io.swagger</groupid>\n                <artifactid>swagger-annotations</artifactid>\n                <version>${swagger.core.version}</version>\n            </dependency>\n            \x3c!--swagger 依赖--\x3e\n            <dependency>\n                <groupid>io.springfox</groupid>\n                <artifactid>springfox-swagger2</artifactid>\n                <version>${swagger.fox.version}</version>\n            </dependency>\n            <dependency>\n                <groupid>io.springfox</groupid>\n                <artifactid>springfox-boot-starter</artifactid>\n                <version>${swagger.fox.version}</version>\n            </dependency>\n        </dependencies>\n    </dependencymanagement>\n</project>\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"授权服务器配置",frontmatter:{title:"授权服务器配置",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/5eb137/"},regularPath:"/19.Spring%20Security%20OAuth2/03.%E5%88%9B%E5%BB%BA%E8%AE%A4%E8%AF%81%E6%9C%8D%E5%8A%A1%E5%99%A8/01.%E6%8E%88%E6%9D%83%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE.html",relativePath:"19.Spring Security OAuth2/03.创建认证服务器/01.授权服务器配置.md",key:"v-ca52ce64",path:"/pages/5eb137/",headers:[{level:2,title:"pom.xml",slug:"pom-xml",normalizedTitle:"pom.xml",charIndex:14},{level:2,title:"Application",slug:"application",normalizedTitle:"application",charIndex:1461},{level:2,title:"EnableAuthorizationServer",slug:"enableauthorizationserver",normalizedTitle:"enableauthorizationserver",charIndex:1855},{level:2,title:"配置客户端详细信息",slug:"配置客户端详细信息",normalizedTitle:"配置客户端详细信息",charIndex:2949},{level:2,title:"管理令牌",slug:"管理令牌",normalizedTitle:"管理令牌",charIndex:4202},{level:2,title:"令牌访问端点配置",slug:"令牌访问端点配置",normalizedTitle:"令牌访问端点配置",charIndex:6019},{level:3,title:"配置授权类型（Grant Types）",slug:"配置授权类型-grant-types",normalizedTitle:"配置授权类型（grant types）",charIndex:6103},{level:3,title:"配置授权端点的URL（Endpoint URLs）：",slug:"配置授权端点的url-endpoint-urls",normalizedTitle:"配置授权端点的url（endpoint urls）：",charIndex:6821},{level:3,title:"令牌端点的安全约束",slug:"令牌端点的安全约束",normalizedTitle:"令牌端点的安全约束",charIndex:2934},{level:3,title:"web安全配置",slug:"web安全配置",normalizedTitle:"web安全配置",charIndex:8371}],headersStr:"pom.xml Application EnableAuthorizationServer 配置客户端详细信息 管理令牌 令牌访问端点配置 配置授权类型（Grant Types） 配置授权端点的URL（Endpoint URLs）： 令牌端点的安全约束 web安全配置",content:'# 授权服务器配置\n\n\n# pom.xml\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <parent>\n        <artifactId>simplify-samples</artifactId>\n        <groupId>com.ieooc.simplify.samples</groupId>\n        <version>0.0.1-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>simplify-auth</artifactId>\n    <packaging>jar</packaging>\n    <description>认证授权服务</description>\n\n    <dependencies>\n        \x3c!--web 模块--\x3e\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        \x3c!--undertow容器--\x3e\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-undertow</artifactId>\n        </dependency>\n        \x3c!--集成OAuth2--\x3e\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-oauth2</artifactId>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n\n\n\n# Application\n\npackage com.ieooc.simplify.samples;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n/**\n * @author heyuqiang\n * @date 2020/11/10\n */\n@SpringBootApplication\npublic class AuthApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(AuthApplication.class, args);\n    }\n}\n\n\n\n\n# EnableAuthorizationServer\n\n可以用 @EnableAuthorizationServer 注解并继承AuthorizationServerConfigurerAdapter来配置OAuth2.0 授权服务器。\n\n在Config包下创建AuthorizationServer：\n\n@Configuration\n@EnableAuthorizationServer\npublic class AuthorizationServer extends AuthorizationServerConfigurerAdapter {\n    //略...\n}\n\n\nAuthorizationServerConfigurerAdapter要求配置以下几个类，这几个类是由Spring创建的独立的配置对象，它们会被Spring传入AuthorizationServerConfigurer中进行配置。\n\npublic class AuthorizationServerConfigurerAdapter implements AuthorizationServerConfigurer {\n    public AuthorizationServerConfigurerAdapter() {}\n    public void configure(AuthorizationServerSecurityConfigurer security) throws Exception {}\n    public void configure(ClientDetailsServiceConfigurer clients) throws Exception {}\n    public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception {}\n}\n\n\n * ClientDetailsServiceConfigurer：用来配置客户端详情服务（ClientDetailsService），客户端详情信息在这里进行初始化，你能够把客户端详情信息写死在这里或者是通过数据库来存储调取详情信息。\n * AuthorizationServerEndpointsConfigurer：用来配置令牌（token）的访问端点和令牌服务(tokenservices)。\n * AuthorizationServerSecurityConfigurer：用来配置令牌端点的安全约束.\n\n\n# 配置客户端详细信息\n\nClientDetailsServiceConfigurer 能够使用内存或者JDBC来实现客户端详情服务（ClientDetailsService），ClientDetailsService负责查找ClientDetails，而ClientDetails有几个重要的属性如下列表：\n\n * clientId：（必须的）用来标识客户的Id。\n * secret：（需要值得信任的客户端）客户端安全码，如果有的话。\n * scope：用来限制客户端的访问范围，如果为空（默认）的话，那么客户端拥有全部的访问范围。\n * authorizedGrantTypes：此客户端可以使用的授权类型，默认为空。\n * authorities：此客户端可以使用的权限（基于Spring Security authorities）。\n\n客户端详情（Client Details）能够在应用程序运行的时候进行更新，可以通过访问底层的存储服务（例如将客户端详情存储在一个关系数据库的表中，就可以使用 JdbcClientDetailsService）或者通过自己实现ClientRegistrationService接口（同时你也可以实现 ClientDetailsService 接口）来进行管理。\n\n@Override\npublic void configure(ClientDetailsServiceConfigurer clients) throws Exception {\n    // clients.withClientDetails(clientDetailsService);\n    clients.inMemory()// 使用in‐memory存储\n            .withClient("c1")// client_id\n            .secret(new BCryptPasswordEncoder().encode("secret"))\n            .resourceIds("res1")\n            // 该client允许的授权类型 authorization_code, password, refresh_token, implicit, client_credentials\n            .authorizedGrantTypes("authorization_code", "password", "client_credentials", "implicit", "refresh_token")\n            .scopes("all")// 允许的授权范围\n            .autoApprove(false)\n            //加上验证回调地址\n            .redirectUris("http://www.baidu.com");\n}\n\n\n\n# 管理令牌\n\nAuthorizationServerTokenServices 接口定义了一些操作使得你可以对令牌进行一些必要的管理，令牌可以被用来加载身份信息，里面包含了这个令牌的相关权限。\n\n自己可以创建 AuthorizationServerTokenServices 这个接口的实现，则需要继承 DefaultTokenServices 这个类，里面包含了一些有用实现，你可以使用它来修改令牌的格式和令牌的存储。默认的，当它尝试创建一个令牌的时候，是使用随机值来进行填充的，除了持久化令牌是委托一个 TokenStore 接口来实现以外，这个类几乎帮你做了所有的事情。并且 TokenStore 这个接口有一个默认的实现，它就是 InMemoryTokenStore ，如其命名，所有的令牌是被保存在了内存中。除了使用这个类以外，你还可以使用一些其他的预定义实现，下面有几个版本，它们都实现了TokenStore接口：\n\n * InMemoryTokenStore：这个版本的实现是被默认采用的，它可以完美的工作在单服务器上（即访问并发量 压力不大的情况下，并且它在失败的时候不会进行备份），大多数的项目都可以使用这个版本的实现来进行 尝试，你可以在开发的时候使用它来进行管理，因为不会被保存到磁盘中，所以更易于调试。\n\n * JdbcTokenStore：这是一个基于JDBC的实现版本，令牌会被保存进关系型数据库。使用这个版本的实现时， 你可以在不同的服务器之间共享令牌信息，使用这个版本的时候请注意把"spring-jdbc"这个依赖加入到你的 classpath当中。\n\n * JwtTokenStore：这个版本的全称是 JSON Web Token（JWT），它可以把令牌相关的数据进行编码（因此对 于后端服务来说，它不需要进行存储，这将是一个重大优势），但是它有一个缺点，那就是撤销一个已经授 权令牌将会非常困难，所以它通常用来处理一个生命周期较短的令牌以及撤销刷新令牌（refresh_token）。 另外一个缺点就是这个令牌占用的空间会比较大，如果你加入了比较多用户凭证信息。JwtTokenStore 不会保\n\n存任何数据，但是它在转换令牌值以及授权信息方面与 DefaultTokenServices 所扮演的角色是一样的。\n\n1、定义TokenConfig\n\n在config包下定义TokenConfig，我们暂时先使用InMemoryTokenStore，生成一个普通的令牌。\n\n@Configuration\npublic class TokenConfig {\n    @Bean\n    public TokenStore tokenStore() {\n        return new InMemoryTokenStore();\n    }\n}\n\n\n2、定义AuthorizationServerTokenServices\n\n在AuthorizationServer中定义AuthorizationServerTokenServices\n\n@Autowired\nprivate TokenStore tokenStore;\n@Autowired\nprivate ClientDetailsService clientDetailsService;\n\n@Bean\npublic AuthorizationServerTokenServices tokenService() {\n    DefaultTokenServices service=new DefaultTokenServices();\n    service.setClientDetailsService(clientDetailsService);\n    service.setSupportRefreshToken(true);\n    service.setTokenStore(tokenStore);\n    service.setAccessTokenValiditySeconds(7200); // 令牌默认有效期2小时\n    service.setRefreshTokenValiditySeconds(259200); // 刷新令牌默认有效期3天\n    return service;\n}\n\n\n\n# 令牌访问端点配置\n\nAuthorizationServerEndpointsConfigurer 这个对象的实例可以完成令牌服务以及令牌endpoint配置。\n\n\n# 配置授权类型（Grant Types）\n\nAuthorizationServerEndpointsConfigurer 通过设定以下属性决定支持的授权类型（Grant Types）:\n\n * authenticationManager：认证管理器，当你选择了资源所有者密码（password）授权类型的时候，请设置 这个属性注入一个 AuthenticationManager 对象。\n * userDetailsService：如果你设置了这个属性的话，那说明你有一个自己的 UserDetailsService 接口的实现， 或者你可以把这个东西设置到全局域上面去（例如 GlobalAuthenticationManagerConfigurer 这个配置对 象），当你设置了这个之后，那么 "refresh_token" 即刷新令牌授权类型模式的流程中就会包含一个检查，用 来确保这个账号是否仍然有效，假如说你禁用了这个账户的话。\n * authorizationCodeServices：这个属性是用来设置授权码服务的（即 AuthorizationCodeServices 的实例对 象），主要用于 "authorization_code" 授权码类型模式。\n * implicitGrantService：这个属性用于设置隐式授权模式，用来管理隐式授权模式的状态。 tokenGranter：当你设置了这个东西（即 TokenGranter 接口实现），那么授权将会交由你来完全掌控，并 且会忽略掉上面的这几个属性，这个属性一般是用作拓展用途的，即标准的四种授权模式已经满足不了你的 需求的时候，才会考虑使用这个。\n\n\n# 配置授权端点的URL（Endpoint URLs）：\n\nAuthorizationServerEndpointsConfigurer 这个配置对象有一个叫做 pathMapping() 的方法用来配置端点URL链接，它有两个参数：\n\n * 第一个参数：String 类型的，这个端点URL的默认链接。\n * 第二个参数：String 类型的，你要进行替代的URL链接。\n\n以上的参数都将以 "/" 字符为开始的字符串，框架的默认URL链接如下列表，可以作为这个 pathMapping() 方法的第一个参数：\n\n * /oauth/authorize：授权端点。\n * /oauth/token：令牌端点。\n * /oauth/confirm_access：用户确认授权提交端点。\n * /oauth/error：授权服务错误信息端点。\n * /oauth/check_token：用于资源服务访问的令牌解析端点。\n * /oauth/token_key：提供公有密匙的端点，如果你使用JWT令牌的话。\n\n需要注意的是授权端点这个URL应该被Spring Security保护起来只供授权用户访问.在AuthorizationServer配置令牌访问端点\n\n@Autowired\nprivate AuthorizationCodeServices authorizationCodeServices;\n@Autowired\nprivate AuthenticationManager authenticationManager;\n@Override\n\npublic void configure(AuthorizationServerEndpointsConfigurer endpoints) {\n    endpoints\n        .authenticationManager(authenticationManager)\n        .authorizationCodeServices(authorizationCodeServices)\n        .tokenServices(tokenService())\n        .allowedTokenEndpointRequestMethods(HttpMethod.POST);\n}\n@Bean\npublic AuthorizationCodeServices authorizationCodeServices() { \n    //设置授权码模式的授权码如何存取，暂时采用内存方式\n    return new InMemoryAuthorizationCodeServices();\n}\n\n\n\n# 令牌端点的安全约束\n\nAuthorizationServerSecurityConfigurer：用来配置令牌端点(Token Endpoint)的安全约束，在AuthorizationServer中配置如下.\n\n（1）tokenkey这个endpoint当使用JwtToken且使用非对称加密时，资源服务用于获取公钥而开放的，这里指这个endpoint完全公开。\n（2）checkToken这个endpoint完全公开\n（3）允许表单认证\n\n\n授权服务配置总结：授权服务配置分成三大块，可以关联记忆。\n\n既然要完成认证，它首先得知道客户端信息从哪儿读取，因此要进行客户端详情配置。\n\n既然要颁发token，那必须得定义token的相关endpoint，以及token如何存取，以及客户端支持哪些类型的token。\n\n既然暴露除了一些endpoint，那对这些endpoint可以定义一些安全上的约束等。\n\n\n# web安全配置\n\n将Spring-Boot工程中的WebSecurityConfig拷贝到simplify-auth工程中。\n\n@Configuration\n@EnableGlobalMethodSecurity(securedEnabled = true,prePostEnabled = true)\npublic class WebSecurityConfig extends WebSecurityConfigurerAdapter {\n    @Bean\n    public PasswordEncoder passwordEncoder() {\n        return new BCryptPasswordEncoder();\n    } \n    @Bean\n    public AuthenticationManager authenticationManagerBean() throws Exception {\n        return super.authenticationManagerBean();\n    }\n    // 安全拦截机制（最重要）\n    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http.csrf().disable()\n            .authorizeRequests()\n            .antMatchers("/r/r1").hasAnyAuthority("p1")\n            .antMatchers("/login*").permitAll()\n            .anyRequest().authenticated()\n            .and()\n            .formLogin();\n    }\n}\n',normalizedContent:'# 授权服务器配置\n\n\n# pom.xml\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <parent>\n        <artifactid>simplify-samples</artifactid>\n        <groupid>com.ieooc.simplify.samples</groupid>\n        <version>0.0.1-snapshot</version>\n    </parent>\n    <modelversion>4.0.0</modelversion>\n\n    <artifactid>simplify-auth</artifactid>\n    <packaging>jar</packaging>\n    <description>认证授权服务</description>\n\n    <dependencies>\n        \x3c!--web 模块--\x3e\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-web</artifactid>\n        </dependency>\n        \x3c!--undertow容器--\x3e\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-undertow</artifactid>\n        </dependency>\n        \x3c!--集成oauth2--\x3e\n        <dependency>\n            <groupid>org.springframework.cloud</groupid>\n            <artifactid>spring-cloud-starter-oauth2</artifactid>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-maven-plugin</artifactid>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n\n\n\n# application\n\npackage com.ieooc.simplify.samples;\n\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\n\n/**\n * @author heyuqiang\n * @date 2020/11/10\n */\n@springbootapplication\npublic class authapplication {\n\n    public static void main(string[] args) {\n        springapplication.run(authapplication.class, args);\n    }\n}\n\n\n\n\n# enableauthorizationserver\n\n可以用 @enableauthorizationserver 注解并继承authorizationserverconfigureradapter来配置oauth2.0 授权服务器。\n\n在config包下创建authorizationserver：\n\n@configuration\n@enableauthorizationserver\npublic class authorizationserver extends authorizationserverconfigureradapter {\n    //略...\n}\n\n\nauthorizationserverconfigureradapter要求配置以下几个类，这几个类是由spring创建的独立的配置对象，它们会被spring传入authorizationserverconfigurer中进行配置。\n\npublic class authorizationserverconfigureradapter implements authorizationserverconfigurer {\n    public authorizationserverconfigureradapter() {}\n    public void configure(authorizationserversecurityconfigurer security) throws exception {}\n    public void configure(clientdetailsserviceconfigurer clients) throws exception {}\n    public void configure(authorizationserverendpointsconfigurer endpoints) throws exception {}\n}\n\n\n * clientdetailsserviceconfigurer：用来配置客户端详情服务（clientdetailsservice），客户端详情信息在这里进行初始化，你能够把客户端详情信息写死在这里或者是通过数据库来存储调取详情信息。\n * authorizationserverendpointsconfigurer：用来配置令牌（token）的访问端点和令牌服务(tokenservices)。\n * authorizationserversecurityconfigurer：用来配置令牌端点的安全约束.\n\n\n# 配置客户端详细信息\n\nclientdetailsserviceconfigurer 能够使用内存或者jdbc来实现客户端详情服务（clientdetailsservice），clientdetailsservice负责查找clientdetails，而clientdetails有几个重要的属性如下列表：\n\n * clientid：（必须的）用来标识客户的id。\n * secret：（需要值得信任的客户端）客户端安全码，如果有的话。\n * scope：用来限制客户端的访问范围，如果为空（默认）的话，那么客户端拥有全部的访问范围。\n * authorizedgranttypes：此客户端可以使用的授权类型，默认为空。\n * authorities：此客户端可以使用的权限（基于spring security authorities）。\n\n客户端详情（client details）能够在应用程序运行的时候进行更新，可以通过访问底层的存储服务（例如将客户端详情存储在一个关系数据库的表中，就可以使用 jdbcclientdetailsservice）或者通过自己实现clientregistrationservice接口（同时你也可以实现 clientdetailsservice 接口）来进行管理。\n\n@override\npublic void configure(clientdetailsserviceconfigurer clients) throws exception {\n    // clients.withclientdetails(clientdetailsservice);\n    clients.inmemory()// 使用in‐memory存储\n            .withclient("c1")// client_id\n            .secret(new bcryptpasswordencoder().encode("secret"))\n            .resourceids("res1")\n            // 该client允许的授权类型 authorization_code, password, refresh_token, implicit, client_credentials\n            .authorizedgranttypes("authorization_code", "password", "client_credentials", "implicit", "refresh_token")\n            .scopes("all")// 允许的授权范围\n            .autoapprove(false)\n            //加上验证回调地址\n            .redirecturis("http://www.baidu.com");\n}\n\n\n\n# 管理令牌\n\nauthorizationservertokenservices 接口定义了一些操作使得你可以对令牌进行一些必要的管理，令牌可以被用来加载身份信息，里面包含了这个令牌的相关权限。\n\n自己可以创建 authorizationservertokenservices 这个接口的实现，则需要继承 defaulttokenservices 这个类，里面包含了一些有用实现，你可以使用它来修改令牌的格式和令牌的存储。默认的，当它尝试创建一个令牌的时候，是使用随机值来进行填充的，除了持久化令牌是委托一个 tokenstore 接口来实现以外，这个类几乎帮你做了所有的事情。并且 tokenstore 这个接口有一个默认的实现，它就是 inmemorytokenstore ，如其命名，所有的令牌是被保存在了内存中。除了使用这个类以外，你还可以使用一些其他的预定义实现，下面有几个版本，它们都实现了tokenstore接口：\n\n * inmemorytokenstore：这个版本的实现是被默认采用的，它可以完美的工作在单服务器上（即访问并发量 压力不大的情况下，并且它在失败的时候不会进行备份），大多数的项目都可以使用这个版本的实现来进行 尝试，你可以在开发的时候使用它来进行管理，因为不会被保存到磁盘中，所以更易于调试。\n\n * jdbctokenstore：这是一个基于jdbc的实现版本，令牌会被保存进关系型数据库。使用这个版本的实现时， 你可以在不同的服务器之间共享令牌信息，使用这个版本的时候请注意把"spring-jdbc"这个依赖加入到你的 classpath当中。\n\n * jwttokenstore：这个版本的全称是 json web token（jwt），它可以把令牌相关的数据进行编码（因此对 于后端服务来说，它不需要进行存储，这将是一个重大优势），但是它有一个缺点，那就是撤销一个已经授 权令牌将会非常困难，所以它通常用来处理一个生命周期较短的令牌以及撤销刷新令牌（refresh_token）。 另外一个缺点就是这个令牌占用的空间会比较大，如果你加入了比较多用户凭证信息。jwttokenstore 不会保\n\n存任何数据，但是它在转换令牌值以及授权信息方面与 defaulttokenservices 所扮演的角色是一样的。\n\n1、定义tokenconfig\n\n在config包下定义tokenconfig，我们暂时先使用inmemorytokenstore，生成一个普通的令牌。\n\n@configuration\npublic class tokenconfig {\n    @bean\n    public tokenstore tokenstore() {\n        return new inmemorytokenstore();\n    }\n}\n\n\n2、定义authorizationservertokenservices\n\n在authorizationserver中定义authorizationservertokenservices\n\n@autowired\nprivate tokenstore tokenstore;\n@autowired\nprivate clientdetailsservice clientdetailsservice;\n\n@bean\npublic authorizationservertokenservices tokenservice() {\n    defaulttokenservices service=new defaulttokenservices();\n    service.setclientdetailsservice(clientdetailsservice);\n    service.setsupportrefreshtoken(true);\n    service.settokenstore(tokenstore);\n    service.setaccesstokenvalidityseconds(7200); // 令牌默认有效期2小时\n    service.setrefreshtokenvalidityseconds(259200); // 刷新令牌默认有效期3天\n    return service;\n}\n\n\n\n# 令牌访问端点配置\n\nauthorizationserverendpointsconfigurer 这个对象的实例可以完成令牌服务以及令牌endpoint配置。\n\n\n# 配置授权类型（grant types）\n\nauthorizationserverendpointsconfigurer 通过设定以下属性决定支持的授权类型（grant types）:\n\n * authenticationmanager：认证管理器，当你选择了资源所有者密码（password）授权类型的时候，请设置 这个属性注入一个 authenticationmanager 对象。\n * userdetailsservice：如果你设置了这个属性的话，那说明你有一个自己的 userdetailsservice 接口的实现， 或者你可以把这个东西设置到全局域上面去（例如 globalauthenticationmanagerconfigurer 这个配置对 象），当你设置了这个之后，那么 "refresh_token" 即刷新令牌授权类型模式的流程中就会包含一个检查，用 来确保这个账号是否仍然有效，假如说你禁用了这个账户的话。\n * authorizationcodeservices：这个属性是用来设置授权码服务的（即 authorizationcodeservices 的实例对 象），主要用于 "authorization_code" 授权码类型模式。\n * implicitgrantservice：这个属性用于设置隐式授权模式，用来管理隐式授权模式的状态。 tokengranter：当你设置了这个东西（即 tokengranter 接口实现），那么授权将会交由你来完全掌控，并 且会忽略掉上面的这几个属性，这个属性一般是用作拓展用途的，即标准的四种授权模式已经满足不了你的 需求的时候，才会考虑使用这个。\n\n\n# 配置授权端点的url（endpoint urls）：\n\nauthorizationserverendpointsconfigurer 这个配置对象有一个叫做 pathmapping() 的方法用来配置端点url链接，它有两个参数：\n\n * 第一个参数：string 类型的，这个端点url的默认链接。\n * 第二个参数：string 类型的，你要进行替代的url链接。\n\n以上的参数都将以 "/" 字符为开始的字符串，框架的默认url链接如下列表，可以作为这个 pathmapping() 方法的第一个参数：\n\n * /oauth/authorize：授权端点。\n * /oauth/token：令牌端点。\n * /oauth/confirm_access：用户确认授权提交端点。\n * /oauth/error：授权服务错误信息端点。\n * /oauth/check_token：用于资源服务访问的令牌解析端点。\n * /oauth/token_key：提供公有密匙的端点，如果你使用jwt令牌的话。\n\n需要注意的是授权端点这个url应该被spring security保护起来只供授权用户访问.在authorizationserver配置令牌访问端点\n\n@autowired\nprivate authorizationcodeservices authorizationcodeservices;\n@autowired\nprivate authenticationmanager authenticationmanager;\n@override\n\npublic void configure(authorizationserverendpointsconfigurer endpoints) {\n    endpoints\n        .authenticationmanager(authenticationmanager)\n        .authorizationcodeservices(authorizationcodeservices)\n        .tokenservices(tokenservice())\n        .allowedtokenendpointrequestmethods(httpmethod.post);\n}\n@bean\npublic authorizationcodeservices authorizationcodeservices() { \n    //设置授权码模式的授权码如何存取，暂时采用内存方式\n    return new inmemoryauthorizationcodeservices();\n}\n\n\n\n# 令牌端点的安全约束\n\nauthorizationserversecurityconfigurer：用来配置令牌端点(token endpoint)的安全约束，在authorizationserver中配置如下.\n\n（1）tokenkey这个endpoint当使用jwttoken且使用非对称加密时，资源服务用于获取公钥而开放的，这里指这个endpoint完全公开。\n（2）checktoken这个endpoint完全公开\n（3）允许表单认证\n\n\n授权服务配置总结：授权服务配置分成三大块，可以关联记忆。\n\n既然要完成认证，它首先得知道客户端信息从哪儿读取，因此要进行客户端详情配置。\n\n既然要颁发token，那必须得定义token的相关endpoint，以及token如何存取，以及客户端支持哪些类型的token。\n\n既然暴露除了一些endpoint，那对这些endpoint可以定义一些安全上的约束等。\n\n\n# web安全配置\n\n将spring-boot工程中的websecurityconfig拷贝到simplify-auth工程中。\n\n@configuration\n@enableglobalmethodsecurity(securedenabled = true,prepostenabled = true)\npublic class websecurityconfig extends websecurityconfigureradapter {\n    @bean\n    public passwordencoder passwordencoder() {\n        return new bcryptpasswordencoder();\n    } \n    @bean\n    public authenticationmanager authenticationmanagerbean() throws exception {\n        return super.authenticationmanagerbean();\n    }\n    // 安全拦截机制（最重要）\n    @override\n    protected void configure(httpsecurity http) throws exception {\n        http.csrf().disable()\n            .authorizerequests()\n            .antmatchers("/r/r1").hasanyauthority("p1")\n            .antmatchers("/login*").permitall()\n            .anyrequest().authenticated()\n            .and()\n            .formlogin();\n    }\n}\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"基于内存存储令牌",frontmatter:{title:"基于内存存储令牌",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/2279cf/"},regularPath:"/19.Spring%20Security%20OAuth2/03.%E5%88%9B%E5%BB%BA%E8%AE%A4%E8%AF%81%E6%9C%8D%E5%8A%A1%E5%99%A8/02.%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AD%98%E5%AD%98%E5%82%A8%E4%BB%A4%E7%89%8C.html",relativePath:"19.Spring Security OAuth2/03.创建认证服务器/02.基于内存存储令牌.md",key:"v-620b25ca",path:"/pages/2279cf/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:15},{level:2,title:"配置认证服务器",slug:"配置认证服务器",normalizedTitle:"配置认证服务器",charIndex:104},{level:2,title:"服务器安全配置",slug:"服务器安全配置",normalizedTitle:"服务器安全配置",charIndex:2938},{level:2,title:"application.yml",slug:"application-yml",normalizedTitle:"application.yml",charIndex:4507},{level:2,title:"访问获取授权码",slug:"访问获取授权码",normalizedTitle:"访问获取授权码",charIndex:4655},{level:2,title:"通过授权码向服务器申请令牌",slug:"通过授权码向服务器申请令牌",normalizedTitle:"通过授权码向服务器申请令牌",charIndex:4956}],headersStr:"概述 配置认证服务器 服务器安全配置 application.yml 访问获取授权码 通过授权码向服务器申请令牌",content:'# 基于内存存储令牌\n\n\n# 概述\n\n本章节基于 内存存储令牌 的模式用于演示最基本的操作，帮助大家快速理解 oAuth2 认证服务器中 "认证"、"授权"、"访问令牌” 的基本概念\n\n操作流程\n\n\n\n * 配置认证服务器\n   * 配置客户端信息：ClientDetailsServiceConfigurer\n     * inMemory：内存配置\n     * withClient：客户端标识\n     * secret：客户端安全码\n     * authorizedGrantTypes：客户端授权类型\n     * scopes：客户端授权范围\n     * redirectUris：注册回调地址\n * 配置 Web 安全\n * 通过 GET 请求访问认证服务器获取授权码\n   * 端点：/oauth/authorize\n * 通过 POST 请求利用授权码访问认证服务器获取令牌\n   * 端点：/oauth/token\n\n附：默认的端点 URL\n\n * /oauth/authorize：授权端点\n * /oauth/token：令牌端点\n * /oauth/confirm_access：用户确认授权提交端点\n * /oauth/error：授权服务错误信息端点\n * /oauth/check_token：用于资源服务访问的令牌解析端点\n * /oauth/token_key：提供公有密匙的端点，如果你使用 JWT 令牌的话\n\n\n# 配置认证服务器\n\n创建一个类继承 AuthorizationServerConfigurerAdapter 并添加相关注解：\n\n * @Configuration\n * @EnableAuthorizationServer\n\npackage com.ieooc.simplify.samples.config;\n\nimport lombok.AllArgsConstructor;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.security.crypto.password.PasswordEncoder;\nimport org.springframework.security.oauth2.config.annotation.configurers.ClientDetailsServiceConfigurer;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.AuthorizationServerConfigurerAdapter;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.EnableAuthorizationServer;\nimport org.springframework.security.oauth2.config.annotation.web.configurers.AuthorizationServerSecurityConfigurer;\n\n/**\n * @author heyuqiang\n * @date 2020/11/10\n */\n@Configuration\n@EnableAuthorizationServer\n@AllArgsConstructor\npublic class AuthorizationServerConfig extends AuthorizationServerConfigurerAdapter {\n\n\n    private final PasswordEncoder passwordEncoder;\n\n    /**\n     * 配置客户端\n     */\n    @Override\n    public void configure(ClientDetailsServiceConfigurer clients) throws Exception {\n        // 使用in‐memory存储\n        clients.inMemory()\n                // client_id\n                .withClient("simplify")\n                .secret(passwordEncoder.encode("simplify"))\n                .resourceIds("res1")\n                // 该client允许的授权类型authorization_code, password, refresh_token, implicit, client_credentials\n                .authorizedGrantTypes("authorization_code", "password", "client_credentials", "implicit", "refresh_token")\n                // 允许的授权范围\n                .scopes("all")\n                .autoApprove(false)\n                //加上验证回调地址\n                .redirectUris("https://www.ivoov.com");\n\n    }\n\n    /**\n     * 安全配置\n     */\n    @Override\n    public void configure(AuthorizationServerSecurityConfigurer security) throws Exception {\n        security\n                // 检查令牌 /oauth/token_key\n                .tokenKeyAccess("permitAll()")\n                // 检查令牌 /oauth/check_token\n                .checkTokenAccess("permitAll()")\n                // 开启后请求需要带上 client_id client_secret 不需要 Basic 请求头\n                // 默认请求头 Basic Base64(client_id+client_secret)\n                .allowFormAuthenticationForClients();\n    }\n}\n\n\n\n# 服务器安全配置\n\n创建一个类继承 WebSecurityConfigurerAdapter 并添加相关注解：\n\n * @Configuration\n * @EnableGlobalMethodSecurity(prePostEnabled = true, securedEnabled = true, jsr250Enabled = true)：全局方法拦截\n\npackage com.ieooc.simplify.samples.config;\n\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;\nimport org.springframework.security.config.annotation.web.builders.HttpSecurity;\nimport org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;\nimport org.springframework.security.crypto.factory.PasswordEncoderFactories;\nimport org.springframework.security.crypto.password.PasswordEncoder;\n\n/**\n * @author heyuqiang\n * @date 2020/11/10\n */\n@Configuration\npublic class WebSecurityConfig extends WebSecurityConfigurerAdapter {\n\n    /**\n     * 授权密码编码器\n     */\n    @Bean\n    public PasswordEncoder passwordEncoder() {\n        return PasswordEncoderFactories.createDelegatingPasswordEncoder();\n    }\n\n    @Override\n    protected void configure(AuthenticationManagerBuilder auth) throws Exception {\n\n        auth.inMemoryAuthentication()\n                .passwordEncoder(passwordEncoder())\n                .withUser("admin")\n                .password(passwordEncoder().encode("123456"))\n                .roles("ROOT", "USER")\n                .and()\n                .withUser("user")\n                .password(passwordEncoder().encode("123456"))\n                .roles("USER");\n    }\n}\n\n\n\n# application.yml\n\nserver:\n  port: 3000\nspring:\n  application:\n    name: simplify-auth\nlogging:\n  level:\n    org.springframework.security: debug\n\n\n\n# 访问获取授权码\n\n打开浏览器，输入地址：\n\nhttp://localhost:3000/oauth/authorize?response_type=code&client_id=simplify&scope=all&redirect_uri=https://www.ivoov.com\n\n\n第一次访问会跳转到登录页面\n\n\n\n验证成功后会询问用户是否授权客户端\n\n\n\n选择授权后会跳转到我的博客，浏览器地址上还会包含一个授权码（code=jB00SK），浏览器地址栏会显示如下地址：\n\nhttps://www.ivoov.com/?code=jB00SK\n\n\n有了这个授权码就可以获取访问令牌了\n\n\n# 通过授权码向服务器申请令牌\n\n通过 CURL 或是 Postman 请求\n\ncurl --location --request POST \'http://localhost:3000/oauth/token?grant_type=authorization_code&code=jB00SK&client_id=simplify&client_secret=simplify&redirect_uri=https://www.ivoov.com\'\n\n\n',normalizedContent:'# 基于内存存储令牌\n\n\n# 概述\n\n本章节基于 内存存储令牌 的模式用于演示最基本的操作，帮助大家快速理解 oauth2 认证服务器中 "认证"、"授权"、"访问令牌” 的基本概念\n\n操作流程\n\n\n\n * 配置认证服务器\n   * 配置客户端信息：clientdetailsserviceconfigurer\n     * inmemory：内存配置\n     * withclient：客户端标识\n     * secret：客户端安全码\n     * authorizedgranttypes：客户端授权类型\n     * scopes：客户端授权范围\n     * redirecturis：注册回调地址\n * 配置 web 安全\n * 通过 get 请求访问认证服务器获取授权码\n   * 端点：/oauth/authorize\n * 通过 post 请求利用授权码访问认证服务器获取令牌\n   * 端点：/oauth/token\n\n附：默认的端点 url\n\n * /oauth/authorize：授权端点\n * /oauth/token：令牌端点\n * /oauth/confirm_access：用户确认授权提交端点\n * /oauth/error：授权服务错误信息端点\n * /oauth/check_token：用于资源服务访问的令牌解析端点\n * /oauth/token_key：提供公有密匙的端点，如果你使用 jwt 令牌的话\n\n\n# 配置认证服务器\n\n创建一个类继承 authorizationserverconfigureradapter 并添加相关注解：\n\n * @configuration\n * @enableauthorizationserver\n\npackage com.ieooc.simplify.samples.config;\n\nimport lombok.allargsconstructor;\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.security.crypto.password.passwordencoder;\nimport org.springframework.security.oauth2.config.annotation.configurers.clientdetailsserviceconfigurer;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.authorizationserverconfigureradapter;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.enableauthorizationserver;\nimport org.springframework.security.oauth2.config.annotation.web.configurers.authorizationserversecurityconfigurer;\n\n/**\n * @author heyuqiang\n * @date 2020/11/10\n */\n@configuration\n@enableauthorizationserver\n@allargsconstructor\npublic class authorizationserverconfig extends authorizationserverconfigureradapter {\n\n\n    private final passwordencoder passwordencoder;\n\n    /**\n     * 配置客户端\n     */\n    @override\n    public void configure(clientdetailsserviceconfigurer clients) throws exception {\n        // 使用in‐memory存储\n        clients.inmemory()\n                // client_id\n                .withclient("simplify")\n                .secret(passwordencoder.encode("simplify"))\n                .resourceids("res1")\n                // 该client允许的授权类型authorization_code, password, refresh_token, implicit, client_credentials\n                .authorizedgranttypes("authorization_code", "password", "client_credentials", "implicit", "refresh_token")\n                // 允许的授权范围\n                .scopes("all")\n                .autoapprove(false)\n                //加上验证回调地址\n                .redirecturis("https://www.ivoov.com");\n\n    }\n\n    /**\n     * 安全配置\n     */\n    @override\n    public void configure(authorizationserversecurityconfigurer security) throws exception {\n        security\n                // 检查令牌 /oauth/token_key\n                .tokenkeyaccess("permitall()")\n                // 检查令牌 /oauth/check_token\n                .checktokenaccess("permitall()")\n                // 开启后请求需要带上 client_id client_secret 不需要 basic 请求头\n                // 默认请求头 basic base64(client_id+client_secret)\n                .allowformauthenticationforclients();\n    }\n}\n\n\n\n# 服务器安全配置\n\n创建一个类继承 websecurityconfigureradapter 并添加相关注解：\n\n * @configuration\n * @enableglobalmethodsecurity(prepostenabled = true, securedenabled = true, jsr250enabled = true)：全局方法拦截\n\npackage com.ieooc.simplify.samples.config;\n\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.security.config.annotation.authentication.builders.authenticationmanagerbuilder;\nimport org.springframework.security.config.annotation.web.builders.httpsecurity;\nimport org.springframework.security.config.annotation.web.configuration.websecurityconfigureradapter;\nimport org.springframework.security.crypto.factory.passwordencoderfactories;\nimport org.springframework.security.crypto.password.passwordencoder;\n\n/**\n * @author heyuqiang\n * @date 2020/11/10\n */\n@configuration\npublic class websecurityconfig extends websecurityconfigureradapter {\n\n    /**\n     * 授权密码编码器\n     */\n    @bean\n    public passwordencoder passwordencoder() {\n        return passwordencoderfactories.createdelegatingpasswordencoder();\n    }\n\n    @override\n    protected void configure(authenticationmanagerbuilder auth) throws exception {\n\n        auth.inmemoryauthentication()\n                .passwordencoder(passwordencoder())\n                .withuser("admin")\n                .password(passwordencoder().encode("123456"))\n                .roles("root", "user")\n                .and()\n                .withuser("user")\n                .password(passwordencoder().encode("123456"))\n                .roles("user");\n    }\n}\n\n\n\n# application.yml\n\nserver:\n  port: 3000\nspring:\n  application:\n    name: simplify-auth\nlogging:\n  level:\n    org.springframework.security: debug\n\n\n\n# 访问获取授权码\n\n打开浏览器，输入地址：\n\nhttp://localhost:3000/oauth/authorize?response_type=code&client_id=simplify&scope=all&redirect_uri=https://www.ivoov.com\n\n\n第一次访问会跳转到登录页面\n\n\n\n验证成功后会询问用户是否授权客户端\n\n\n\n选择授权后会跳转到我的博客，浏览器地址上还会包含一个授权码（code=jb00sk），浏览器地址栏会显示如下地址：\n\nhttps://www.ivoov.com/?code=jb00sk\n\n\n有了这个授权码就可以获取访问令牌了\n\n\n# 通过授权码向服务器申请令牌\n\n通过 curl 或是 postman 请求\n\ncurl --location --request post \'http://localhost:3000/oauth/token?grant_type=authorization_code&code=jb00sk&client_id=simplify&client_secret=simplify&redirect_uri=https://www.ivoov.com\'\n\n\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"基于 JDBC 存储令牌",frontmatter:{title:"基于 JDBC 存储令牌",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/9b9a7b/"},regularPath:"/19.Spring%20Security%20OAuth2/03.%E5%88%9B%E5%BB%BA%E8%AE%A4%E8%AF%81%E6%9C%8D%E5%8A%A1%E5%99%A8/03.%E5%9F%BA%E4%BA%8E%20JDBC%20%E5%AD%98%E5%82%A8%E4%BB%A4%E7%89%8C.html",relativePath:"19.Spring Security OAuth2/03.创建认证服务器/03.基于 JDBC 存储令牌.md",key:"v-d70c8d9c",path:"/pages/9b9a7b/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:19},{level:2,title:"初始化 OAuth2 数据库 schema",slug:"初始化-oauth2-数据库-schema",normalizedTitle:"初始化 oauth2 数据库 schema",charIndex:891},{level:2,title:"在数据库中配置客户端",slug:"在数据库中配置客户端",normalizedTitle:"在数据库中配置客户端",charIndex:130},{level:2,title:"POM",slug:"pom",normalizedTitle:"pom",charIndex:7252},{level:2,title:"配置认证服务器",slug:"配置认证服务器",normalizedTitle:"配置认证服务器",charIndex:144},{level:2,title:"服务器安全配置",slug:"服务器安全配置",normalizedTitle:"服务器安全配置",charIndex:12064},{level:2,title:"application.yml",slug:"application-yml",normalizedTitle:"application.yml",charIndex:13844},{level:2,title:"访问获取授权码",slug:"访问获取授权码",normalizedTitle:"访问获取授权码",charIndex:14416},{level:2,title:"通过授权码向服务器申请令牌",slug:"通过授权码向服务器申请令牌",normalizedTitle:"通过授权码向服务器申请令牌",charIndex:14717}],headersStr:"概述 初始化 OAuth2 数据库 schema 在数据库中配置客户端 POM 配置认证服务器 服务器安全配置 application.yml 访问获取授权码 通过授权码向服务器申请令牌",content:"# 基于 JDBC 存储令牌\n\n\n# 概述\n\n本章节 基于 JDBC 存储令牌 的模式用于演示最基本的操作，帮助大家快速理解 oAuth2 认证服务器中 \"认证\"、\"授权\"、\"访问令牌” 的基本概念\n\n操作流程\n\n\n\n * 初始化 oAuth2 相关表\n * 在数据库中配置客户端\n * 配置认证服务器\n   * 配置数据源：DataSource\n   * 配置令牌存储方式：TokenStore -> JdbcTokenStore\n   * 配置客户端读取方式：ClientDetailsService -> JdbcClientDetailsService\n   * 配置服务端点信息：\n     \n     AuthorizationServerEndpointsConfigurer\n     \n     \n     * tokenStore：设置令牌存储方式\n   * 配置客户端信息：\n     \n     ClientDetailsServiceConfigurer\n     \n     \n     * withClientDetails：设置客户端配置读取方式\n * 配置 Web 安全\n   * 配置密码加密方式：BCryptPasswordEncoder\n   * 配置认证信息：AuthenticationManagerBuilder\n * 通过 GET 请求访问认证服务器获取授权码\n   * 端点：/oauth/authorize\n * 通过 POST 请求利用授权码访问认证服务器获取令牌\n   * 端点：/oauth/token\n\n附：默认的端点 URL\n\n * /oauth/authorize：授权端点\n * /oauth/token：令牌端点\n * /oauth/confirm_access：用户确认授权提交端点\n * /oauth/error：授权服务错误信息端点\n * /oauth/check_token：用于资源服务访问的令牌解析端点\n * /oauth/token_key：提供公有密匙的端点，如果你使用 JWT 令牌的话\n\n\n# 初始化 OAuth2 数据库 schema\n\n使用spring-security-oauth2官方提供的建表脚本初始化OAuth2相关表，地址如下：\n\nhttps://github.com/spring-projects/spring-security-oauth/blob/master/spring-security-oauth2/src/test/resources/schema.sql\n\n\nOAuth2相关的5张表：\n\n * oauth_access_token：访问令牌\n * oauth_refresh_token：更新令牌\n * oauth_client_details：客户端信息\n * oauth_code：授权码\n * oauth_approvals：授权记录\n * oauth_client_token: 客户端用来记录token信息\n\n> 只以密码模式来进行测试，不考虑管理功能，只用到了了oauth_client_details，oauth_access_token，oauth_refresh_token 三张表\n\n由于我们使用的是 MySQL 8 数据库，默认建表语句中主键为 VARCHAR(256)，这超过了最大的主键长度，请手动修改为 255，并用 BLOB 替换语句中的 LONGVARBINARY 类型(官方文档)，修改后的建表脚本如下：\n\nSET NAMES utf8mb4;\nSET FOREIGN_KEY_CHECKS = 0;\n\n-- ----------------------------\n-- Table structure for oauth_access_token\n-- ----------------------------\nDROP TABLE IF EXISTS `oauth_access_token`;\nCREATE TABLE `oauth_access_token` (\n  `token_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '加密的access_token的值',\n  `token` blob COMMENT 'OAuth2AccessToken.java对象序列化后的二进制数据',\n  `authentication_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '加密过的username,client_id,scope',\n  `user_name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '登录的用户名',\n  `client_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '客户端ID',\n  `authentication` blob COMMENT 'OAuth2Authentication.java对象序列化后的二进制数据',\n  `refresh_token` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '加密的refresh_token的值'\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci ROW_FORMAT=DYNAMIC;\n\n-- ----------------------------\n-- Table structure for oauth_approvals\n-- ----------------------------\nDROP TABLE IF EXISTS `oauth_approvals`;\nCREATE TABLE `oauth_approvals` (\n  `userId` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '登录的用户名',\n  `clientId` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '客户端ID',\n  `scope` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '申请的权限范围',\n  `status` varchar(10) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '状态（Approve或Deny）',\n  `expiresAt` datetime DEFAULT NULL COMMENT '过期时间',\n  `lastModifiedAt` datetime DEFAULT NULL COMMENT '最终修改时间'\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci ROW_FORMAT=DYNAMIC COMMENT='已授权客户端存储记录';\n\n-- ----------------------------\n-- Table structure for oauth_client_details\n-- ----------------------------\nDROP TABLE IF EXISTS `oauth_client_details`;\nCREATE TABLE `oauth_client_details` (\n  `client_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL COMMENT '客户端ID',\n  `resource_ids` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '资源ID集合,多个资源时用逗号(,)分隔',\n  `client_secret` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '客户端密匙',\n  `scope` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '客户端申请的权限范围',\n  `authorized_grant_types` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '客户端支持的grant_type',\n  `web_server_redirect_uri` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '重定向URI',\n  `authorities` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '客户端所拥有的Spring Security的权限值，多个用逗号(,)分隔',\n  `access_token_validity` int DEFAULT NULL COMMENT '访问令牌有效时间值(单位:秒)',\n  `refresh_token_validity` int DEFAULT NULL COMMENT '更新令牌有效时间值(单位:秒)',\n  `additional_information` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '预留字段',\n  `autoapprove` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '用户是否自动Approval操作'\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci ROW_FORMAT=DYNAMIC;\n\n-- ----------------------------\n-- Table structure for oauth_client_token\n-- ----------------------------\nDROP TABLE IF EXISTS `oauth_client_token`;\nCREATE TABLE `oauth_client_token` (\n  `token_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '加密的access_token值',\n  `token` blob COMMENT 'OAuth2AccessToken.java对象序列化后的二进制数据',\n  `authentication_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '加密过的username,client_id,scope',\n  `user_name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '登录的用户名',\n  `client_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '客户端ID'\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci ROW_FORMAT=DYNAMIC;\n\n-- ----------------------------\n-- Table structure for oauth_code\n-- ----------------------------\nDROP TABLE IF EXISTS `oauth_code`;\nCREATE TABLE `oauth_code` (\n  `code` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '授权码(未加密)',\n  `authentication` blob COMMENT 'AuthorizationRequestHolder.java对象序列化后的二进制数据'\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci ROW_FORMAT=DYNAMIC;\n\n-- ----------------------------\n-- Table structure for oauth_refresh_token\n-- ----------------------------\nDROP TABLE IF EXISTS `oauth_refresh_token`;\nCREATE TABLE `oauth_refresh_token` (\n  `token_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '加密过的refresh_token的值',\n  `token` blob COMMENT 'OAuth2RefreshToken.java对象序列化后的二进制数据 ',\n  `authentication` blob COMMENT 'OAuth2Authentication.java对象序列化后的二进制数据'\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci ROW_FORMAT=DYNAMIC;\n\nSET FOREIGN_KEY_CHECKS = 1;\n\n\n\n# 在数据库中配置客户端\n\n在表 oauth_client_details 中增加一条客户端配置记录，需要设置的字段如下：\n\n * client_id：客户端标识\n * client_secret：客户端安全码，此处不能是明文，需要加密\n * scope：客户端授权范围\n * authorized_grant_types：客户端授权类型\n * web_server_redirect_uri：服务器回调地址\n\n使用 BCryptPasswordEncoder 为客户端安全码加密，代码如下：\n\nSystem.out.println(new BCryptPasswordEncoder().encode(\"simplify\"));\n\n\n执行如下sql脚本创建一条客户端信息：\n\nINSERT INTO `oauth_client_details` VALUES ('simplify', NULL, '{bcrypt}$2a$10$7X.MyK2rDC2SsZQsYFYczOeuc0AX.FaUNam0IlMXL.YkcWNP1t/Ge', 'read,write', 'password,refresh_token,authorization_code,client_credentials,implicit', 'https://www.ivoov.com', NULL, NULL, NULL, NULL, 'false');\n\n\n\n# POM\n\n由于使用了 JDBC 存储，我们需要增加相关依赖\n\n<dependencies>              \n    \x3c!--spring jdbc--\x3e\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-jdbc</artifactId>\n    </dependency>\n    \x3c!--mysql驱动--\x3e\n    <dependency>\n        <groupId>mysql</groupId>\n        <artifactId>mysql-connector-java</artifactId>\n    </dependency>\n</dependencies>\n\n\n\n# 配置认证服务器\n\n创建一个类继承 AuthorizationServerConfigurerAdapter 并添加相关注解：\n\n * @Configuration\n * @EnableAuthorizationServer\n\npackage com.ieooc.simplify.samples.config;\n\nimport lombok.AllArgsConstructor;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.security.crypto.password.PasswordEncoder;\nimport org.springframework.security.oauth2.config.annotation.configurers.ClientDetailsServiceConfigurer;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.AuthorizationServerConfigurerAdapter;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.EnableAuthorizationServer;\nimport org.springframework.security.oauth2.config.annotation.web.configurers.AuthorizationServerEndpointsConfigurer;\nimport org.springframework.security.oauth2.config.annotation.web.configurers.AuthorizationServerSecurityConfigurer;\nimport org.springframework.security.oauth2.provider.approval.ApprovalStore;\nimport org.springframework.security.oauth2.provider.approval.JdbcApprovalStore;\nimport org.springframework.security.oauth2.provider.client.JdbcClientDetailsService;\nimport org.springframework.security.oauth2.provider.code.AuthorizationCodeServices;\nimport org.springframework.security.oauth2.provider.code.JdbcAuthorizationCodeServices;\nimport org.springframework.security.oauth2.provider.token.TokenStore;\nimport org.springframework.security.oauth2.provider.token.store.JdbcTokenStore;\n\nimport javax.sql.DataSource;\n\n/**\n * @author heyuqiang\n * @date 2020/11/10\n */\n@Configuration\n@EnableAuthorizationServer\n@AllArgsConstructor\npublic class AuthorizationServerConfig extends AuthorizationServerConfigurerAdapter {\n\n    private final DataSource dataSource;\n    private final AuthenticationManager authenticationManagerBean;\n\n    /**\n     * 已授权客户端存储记录 采用mysql缓存\n     * <p>\n     * 1.数据库表名：oauth_approvals\n     * 2.如果不配置默认使用TokenApprovalStore从token信息中提取\n     *\n     * @return\n     */\n    @Bean\n    public ApprovalStore approvalStore() {\n        return new JdbcApprovalStore(dataSource);\n    }\n\n    /**\n     * 设置授权码模式的授权码如何存取 采用mysql缓存\n     *\n     * @return\n     */\n    @Bean\n    public AuthorizationCodeServices authorizationCodeServices() {\n        return new JdbcAuthorizationCodeServices(dataSource);\n    }\n\n    /**\n     * 访问令牌 采用Jdbc实现用mysql存储\n     *\n     * @return\n     */\n    @Bean\n    public TokenStore tokenStore() {\n        return new JdbcTokenStore(dataSource);\n    }\n\n    /**\n     * 用来配置令牌（token）的访问端点和令牌服务(token services)。\n     * <p>\n     * 1、tokenKey这个endpoint当使用JwtToken且使用非对称加密时，资源服务用于获取公钥而开放的，这里指这个endpoint完全公开。\n     * 2、checkToken这个endpoint完全公开\n     * 3、允许使用ClientDetailsUserDetailsService来进行client端登录的验证\n     *\n     * @param security j\n     * @throws Exception hg\n     */\n    @Override\n    public void configure(AuthorizationServerSecurityConfigurer security) throws Exception {\n        security\n                // 检查令牌 /oauth/token_key\n                .tokenKeyAccess(\"permitAll()\")\n                // 检查令牌 /oauth/check_token\n                .checkTokenAccess(\"permitAll()\")\n                // 开启后请求需要带上 client_id client_secret 不需要 Basic 请求头\n                // 默认请求头 Basic Base64(client_id+client_secret)\n                .allowFormAuthenticationForClients();\n    }\n\n    /**\n     * 基于JDBC - 用来配置客户端详情服务（ClientDetailsService），\n     * 客户端详情信息在这里进行初始化，你能够把客户端详情信息写死在这里或者是通过数据库来存储调取详情信息。\n     *\n     * @param clients\n     * @throws Exception\n     */\n    @Override\n    public void configure(ClientDetailsServiceConfigurer clients) throws Exception {\n        // 设置客户端的配置从数据库中读取，存储在oauth_client_details表\n        JdbcClientDetailsService jdbcClientDetailsService = new JdbcClientDetailsService(dataSource);\n        clients.withClientDetails(jdbcClientDetailsService);\n    }\n\n    /**\n     * 配置授权服务器端点信息\n     *\n     * @param endpoints\n     * @throws Exception\n     */\n    @Override\n    public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception {\n        endpoints\n                // 设置已授权客户端存储记录存储方式\n                .approvalStore(approvalStore())\n                // 设置访问令牌存储方式\n                .tokenStore(tokenStore())\n                // 设置授权码模式的授权码存储方式\n                .authorizationCodeServices(authorizationCodeServices())\n                // 开启密码验证，来源于 WebSecurityConfigurerAdapter\n                .authenticationManager(authenticationManagerBean)\n        ;\n\n    }\n}\n\n\n\n# 服务器安全配置\n\n创建一个类继承 WebSecurityConfigurerAdapter 并添加相关注解：\n\n * @Configuration\n * @EnableGlobalMethodSecurity(prePostEnabled = true, securedEnabled = true, jsr250Enabled = true)：全局方法拦截\n\npackage com.ieooc.simplify.samples.config;\n\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;\nimport org.springframework.security.config.annotation.web.builders.HttpSecurity;\nimport org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;\nimport org.springframework.security.crypto.factory.PasswordEncoderFactories;\nimport org.springframework.security.crypto.password.PasswordEncoder;\n\n/**\n * @author heyuqiang\n * @date 2020/11/10\n */\n@Configuration\npublic class WebSecurityConfig extends WebSecurityConfigurerAdapter {\n\n    /**\n     * 授权密码编码器\n     */\n    @Bean\n    public PasswordEncoder passwordEncoder() {\n        return PasswordEncoderFactories.createDelegatingPasswordEncoder();\n    }\n\n    /**\n     * 不定义没有password grant_type模式\n     */\n    @Bean\n    @Override\n    public AuthenticationManager authenticationManagerBean() throws Exception {\n        return super.authenticationManagerBean();\n    }\n\n    @Override\n    protected void configure(AuthenticationManagerBuilder auth) throws Exception {\n\n        auth.inMemoryAuthentication()\n                .passwordEncoder(passwordEncoder())\n                .withUser(\"admin\")\n                .password(passwordEncoder().encode(\"123456\"))\n                .roles(\"ROOT\", \"USER\")\n                .and()\n                .withUser(\"user\")\n                .password(passwordEncoder().encode(\"123456\"))\n                .roles(\"USER\");\n    }\n}\n\n\n\n# application.yml\n\nserver:\n  port: 3000\nspring:\n  application:\n    name: simplify-auth\n  datasource:\n    type: com.alibaba.druid.pool.DruidDataSource\n    druid:\n      driver-class-name: com.mysql.cj.jdbc.Driver\n      username: root\n      password: root\n      url: jdbc:mysql://localhost:3306/simplify?characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&useSSL=false&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=GMT%2B8&allowMultiQueries=true&allowPublicKeyRetrieval=true\n\nlogging:\n  level:\n    org.springframework.security: debug\n\n\n\n# 访问获取授权码\n\n打开浏览器，输入地址：\n\nhttp://localhost:3000/oauth/authorize?response_type=code&client_id=simplify&scope=all&redirect_uri=https://www.ivoov.com\n\n\n第一次访问会跳转到登录页面\n\n\n\n验证成功后会询问用户是否授权客户端\n\n\n\n选择授权后会跳转到我的博客，浏览器地址上还会包含一个授权码（code=jB00SK），浏览器地址栏会显示如下地址：\n\nhttps://www.ivoov.com/?code=jB00SK\n\n\n有了这个授权码就可以获取访问令牌了\n\n\n# 通过授权码向服务器申请令牌\n\n通过 CURL 或是 Postman 请求\n\ncurl --location --request POST 'http://localhost:3000/oauth/token?grant_type=authorization_code&code=jB00SK&client_id=simplify&client_secret=simplify&redirect_uri=https://www.ivoov.com'\n\n\n\n\n操作成功后数据库 oauth_access_token 表中会增加一笔记录，效果图如下：\n\n",normalizedContent:"# 基于 jdbc 存储令牌\n\n\n# 概述\n\n本章节 基于 jdbc 存储令牌 的模式用于演示最基本的操作，帮助大家快速理解 oauth2 认证服务器中 \"认证\"、\"授权\"、\"访问令牌” 的基本概念\n\n操作流程\n\n\n\n * 初始化 oauth2 相关表\n * 在数据库中配置客户端\n * 配置认证服务器\n   * 配置数据源：datasource\n   * 配置令牌存储方式：tokenstore -> jdbctokenstore\n   * 配置客户端读取方式：clientdetailsservice -> jdbcclientdetailsservice\n   * 配置服务端点信息：\n     \n     authorizationserverendpointsconfigurer\n     \n     \n     * tokenstore：设置令牌存储方式\n   * 配置客户端信息：\n     \n     clientdetailsserviceconfigurer\n     \n     \n     * withclientdetails：设置客户端配置读取方式\n * 配置 web 安全\n   * 配置密码加密方式：bcryptpasswordencoder\n   * 配置认证信息：authenticationmanagerbuilder\n * 通过 get 请求访问认证服务器获取授权码\n   * 端点：/oauth/authorize\n * 通过 post 请求利用授权码访问认证服务器获取令牌\n   * 端点：/oauth/token\n\n附：默认的端点 url\n\n * /oauth/authorize：授权端点\n * /oauth/token：令牌端点\n * /oauth/confirm_access：用户确认授权提交端点\n * /oauth/error：授权服务错误信息端点\n * /oauth/check_token：用于资源服务访问的令牌解析端点\n * /oauth/token_key：提供公有密匙的端点，如果你使用 jwt 令牌的话\n\n\n# 初始化 oauth2 数据库 schema\n\n使用spring-security-oauth2官方提供的建表脚本初始化oauth2相关表，地址如下：\n\nhttps://github.com/spring-projects/spring-security-oauth/blob/master/spring-security-oauth2/src/test/resources/schema.sql\n\n\noauth2相关的5张表：\n\n * oauth_access_token：访问令牌\n * oauth_refresh_token：更新令牌\n * oauth_client_details：客户端信息\n * oauth_code：授权码\n * oauth_approvals：授权记录\n * oauth_client_token: 客户端用来记录token信息\n\n> 只以密码模式来进行测试，不考虑管理功能，只用到了了oauth_client_details，oauth_access_token，oauth_refresh_token 三张表\n\n由于我们使用的是 mysql 8 数据库，默认建表语句中主键为 varchar(256)，这超过了最大的主键长度，请手动修改为 255，并用 blob 替换语句中的 longvarbinary 类型(官方文档)，修改后的建表脚本如下：\n\nset names utf8mb4;\nset foreign_key_checks = 0;\n\n-- ----------------------------\n-- table structure for oauth_access_token\n-- ----------------------------\ndrop table if exists `oauth_access_token`;\ncreate table `oauth_access_token` (\n  `token_id` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '加密的access_token的值',\n  `token` blob comment 'oauth2accesstoken.java对象序列化后的二进制数据',\n  `authentication_id` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '加密过的username,client_id,scope',\n  `user_name` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '登录的用户名',\n  `client_id` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '客户端id',\n  `authentication` blob comment 'oauth2authentication.java对象序列化后的二进制数据',\n  `refresh_token` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '加密的refresh_token的值'\n) engine=innodb default charset=utf8mb4 collate=utf8mb4_0900_ai_ci row_format=dynamic;\n\n-- ----------------------------\n-- table structure for oauth_approvals\n-- ----------------------------\ndrop table if exists `oauth_approvals`;\ncreate table `oauth_approvals` (\n  `userid` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '登录的用户名',\n  `clientid` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '客户端id',\n  `scope` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '申请的权限范围',\n  `status` varchar(10) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '状态（approve或deny）',\n  `expiresat` datetime default null comment '过期时间',\n  `lastmodifiedat` datetime default null comment '最终修改时间'\n) engine=innodb default charset=utf8mb4 collate=utf8mb4_0900_ai_ci row_format=dynamic comment='已授权客户端存储记录';\n\n-- ----------------------------\n-- table structure for oauth_client_details\n-- ----------------------------\ndrop table if exists `oauth_client_details`;\ncreate table `oauth_client_details` (\n  `client_id` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci not null comment '客户端id',\n  `resource_ids` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '资源id集合,多个资源时用逗号(,)分隔',\n  `client_secret` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '客户端密匙',\n  `scope` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '客户端申请的权限范围',\n  `authorized_grant_types` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '客户端支持的grant_type',\n  `web_server_redirect_uri` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '重定向uri',\n  `authorities` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '客户端所拥有的spring security的权限值，多个用逗号(,)分隔',\n  `access_token_validity` int default null comment '访问令牌有效时间值(单位:秒)',\n  `refresh_token_validity` int default null comment '更新令牌有效时间值(单位:秒)',\n  `additional_information` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '预留字段',\n  `autoapprove` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '用户是否自动approval操作'\n) engine=innodb default charset=utf8mb4 collate=utf8mb4_0900_ai_ci row_format=dynamic;\n\n-- ----------------------------\n-- table structure for oauth_client_token\n-- ----------------------------\ndrop table if exists `oauth_client_token`;\ncreate table `oauth_client_token` (\n  `token_id` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '加密的access_token值',\n  `token` blob comment 'oauth2accesstoken.java对象序列化后的二进制数据',\n  `authentication_id` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '加密过的username,client_id,scope',\n  `user_name` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '登录的用户名',\n  `client_id` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '客户端id'\n) engine=innodb default charset=utf8mb4 collate=utf8mb4_0900_ai_ci row_format=dynamic;\n\n-- ----------------------------\n-- table structure for oauth_code\n-- ----------------------------\ndrop table if exists `oauth_code`;\ncreate table `oauth_code` (\n  `code` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '授权码(未加密)',\n  `authentication` blob comment 'authorizationrequestholder.java对象序列化后的二进制数据'\n) engine=innodb default charset=utf8mb4 collate=utf8mb4_0900_ai_ci row_format=dynamic;\n\n-- ----------------------------\n-- table structure for oauth_refresh_token\n-- ----------------------------\ndrop table if exists `oauth_refresh_token`;\ncreate table `oauth_refresh_token` (\n  `token_id` varchar(255) character set utf8mb4 collate utf8mb4_0900_ai_ci default null comment '加密过的refresh_token的值',\n  `token` blob comment 'oauth2refreshtoken.java对象序列化后的二进制数据 ',\n  `authentication` blob comment 'oauth2authentication.java对象序列化后的二进制数据'\n) engine=innodb default charset=utf8mb4 collate=utf8mb4_0900_ai_ci row_format=dynamic;\n\nset foreign_key_checks = 1;\n\n\n\n# 在数据库中配置客户端\n\n在表 oauth_client_details 中增加一条客户端配置记录，需要设置的字段如下：\n\n * client_id：客户端标识\n * client_secret：客户端安全码，此处不能是明文，需要加密\n * scope：客户端授权范围\n * authorized_grant_types：客户端授权类型\n * web_server_redirect_uri：服务器回调地址\n\n使用 bcryptpasswordencoder 为客户端安全码加密，代码如下：\n\nsystem.out.println(new bcryptpasswordencoder().encode(\"simplify\"));\n\n\n执行如下sql脚本创建一条客户端信息：\n\ninsert into `oauth_client_details` values ('simplify', null, '{bcrypt}$2a$10$7x.myk2rdc2sszqsyfyczoeuc0ax.faunam0ilmxl.ykcwnp1t/ge', 'read,write', 'password,refresh_token,authorization_code,client_credentials,implicit', 'https://www.ivoov.com', null, null, null, null, 'false');\n\n\n\n# pom\n\n由于使用了 jdbc 存储，我们需要增加相关依赖\n\n<dependencies>              \n    \x3c!--spring jdbc--\x3e\n    <dependency>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-jdbc</artifactid>\n    </dependency>\n    \x3c!--mysql驱动--\x3e\n    <dependency>\n        <groupid>mysql</groupid>\n        <artifactid>mysql-connector-java</artifactid>\n    </dependency>\n</dependencies>\n\n\n\n# 配置认证服务器\n\n创建一个类继承 authorizationserverconfigureradapter 并添加相关注解：\n\n * @configuration\n * @enableauthorizationserver\n\npackage com.ieooc.simplify.samples.config;\n\nimport lombok.allargsconstructor;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.security.crypto.password.passwordencoder;\nimport org.springframework.security.oauth2.config.annotation.configurers.clientdetailsserviceconfigurer;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.authorizationserverconfigureradapter;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.enableauthorizationserver;\nimport org.springframework.security.oauth2.config.annotation.web.configurers.authorizationserverendpointsconfigurer;\nimport org.springframework.security.oauth2.config.annotation.web.configurers.authorizationserversecurityconfigurer;\nimport org.springframework.security.oauth2.provider.approval.approvalstore;\nimport org.springframework.security.oauth2.provider.approval.jdbcapprovalstore;\nimport org.springframework.security.oauth2.provider.client.jdbcclientdetailsservice;\nimport org.springframework.security.oauth2.provider.code.authorizationcodeservices;\nimport org.springframework.security.oauth2.provider.code.jdbcauthorizationcodeservices;\nimport org.springframework.security.oauth2.provider.token.tokenstore;\nimport org.springframework.security.oauth2.provider.token.store.jdbctokenstore;\n\nimport javax.sql.datasource;\n\n/**\n * @author heyuqiang\n * @date 2020/11/10\n */\n@configuration\n@enableauthorizationserver\n@allargsconstructor\npublic class authorizationserverconfig extends authorizationserverconfigureradapter {\n\n    private final datasource datasource;\n    private final authenticationmanager authenticationmanagerbean;\n\n    /**\n     * 已授权客户端存储记录 采用mysql缓存\n     * <p>\n     * 1.数据库表名：oauth_approvals\n     * 2.如果不配置默认使用tokenapprovalstore从token信息中提取\n     *\n     * @return\n     */\n    @bean\n    public approvalstore approvalstore() {\n        return new jdbcapprovalstore(datasource);\n    }\n\n    /**\n     * 设置授权码模式的授权码如何存取 采用mysql缓存\n     *\n     * @return\n     */\n    @bean\n    public authorizationcodeservices authorizationcodeservices() {\n        return new jdbcauthorizationcodeservices(datasource);\n    }\n\n    /**\n     * 访问令牌 采用jdbc实现用mysql存储\n     *\n     * @return\n     */\n    @bean\n    public tokenstore tokenstore() {\n        return new jdbctokenstore(datasource);\n    }\n\n    /**\n     * 用来配置令牌（token）的访问端点和令牌服务(token services)。\n     * <p>\n     * 1、tokenkey这个endpoint当使用jwttoken且使用非对称加密时，资源服务用于获取公钥而开放的，这里指这个endpoint完全公开。\n     * 2、checktoken这个endpoint完全公开\n     * 3、允许使用clientdetailsuserdetailsservice来进行client端登录的验证\n     *\n     * @param security j\n     * @throws exception hg\n     */\n    @override\n    public void configure(authorizationserversecurityconfigurer security) throws exception {\n        security\n                // 检查令牌 /oauth/token_key\n                .tokenkeyaccess(\"permitall()\")\n                // 检查令牌 /oauth/check_token\n                .checktokenaccess(\"permitall()\")\n                // 开启后请求需要带上 client_id client_secret 不需要 basic 请求头\n                // 默认请求头 basic base64(client_id+client_secret)\n                .allowformauthenticationforclients();\n    }\n\n    /**\n     * 基于jdbc - 用来配置客户端详情服务（clientdetailsservice），\n     * 客户端详情信息在这里进行初始化，你能够把客户端详情信息写死在这里或者是通过数据库来存储调取详情信息。\n     *\n     * @param clients\n     * @throws exception\n     */\n    @override\n    public void configure(clientdetailsserviceconfigurer clients) throws exception {\n        // 设置客户端的配置从数据库中读取，存储在oauth_client_details表\n        jdbcclientdetailsservice jdbcclientdetailsservice = new jdbcclientdetailsservice(datasource);\n        clients.withclientdetails(jdbcclientdetailsservice);\n    }\n\n    /**\n     * 配置授权服务器端点信息\n     *\n     * @param endpoints\n     * @throws exception\n     */\n    @override\n    public void configure(authorizationserverendpointsconfigurer endpoints) throws exception {\n        endpoints\n                // 设置已授权客户端存储记录存储方式\n                .approvalstore(approvalstore())\n                // 设置访问令牌存储方式\n                .tokenstore(tokenstore())\n                // 设置授权码模式的授权码存储方式\n                .authorizationcodeservices(authorizationcodeservices())\n                // 开启密码验证，来源于 websecurityconfigureradapter\n                .authenticationmanager(authenticationmanagerbean)\n        ;\n\n    }\n}\n\n\n\n# 服务器安全配置\n\n创建一个类继承 websecurityconfigureradapter 并添加相关注解：\n\n * @configuration\n * @enableglobalmethodsecurity(prepostenabled = true, securedenabled = true, jsr250enabled = true)：全局方法拦截\n\npackage com.ieooc.simplify.samples.config;\n\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.security.config.annotation.authentication.builders.authenticationmanagerbuilder;\nimport org.springframework.security.config.annotation.web.builders.httpsecurity;\nimport org.springframework.security.config.annotation.web.configuration.websecurityconfigureradapter;\nimport org.springframework.security.crypto.factory.passwordencoderfactories;\nimport org.springframework.security.crypto.password.passwordencoder;\n\n/**\n * @author heyuqiang\n * @date 2020/11/10\n */\n@configuration\npublic class websecurityconfig extends websecurityconfigureradapter {\n\n    /**\n     * 授权密码编码器\n     */\n    @bean\n    public passwordencoder passwordencoder() {\n        return passwordencoderfactories.createdelegatingpasswordencoder();\n    }\n\n    /**\n     * 不定义没有password grant_type模式\n     */\n    @bean\n    @override\n    public authenticationmanager authenticationmanagerbean() throws exception {\n        return super.authenticationmanagerbean();\n    }\n\n    @override\n    protected void configure(authenticationmanagerbuilder auth) throws exception {\n\n        auth.inmemoryauthentication()\n                .passwordencoder(passwordencoder())\n                .withuser(\"admin\")\n                .password(passwordencoder().encode(\"123456\"))\n                .roles(\"root\", \"user\")\n                .and()\n                .withuser(\"user\")\n                .password(passwordencoder().encode(\"123456\"))\n                .roles(\"user\");\n    }\n}\n\n\n\n# application.yml\n\nserver:\n  port: 3000\nspring:\n  application:\n    name: simplify-auth\n  datasource:\n    type: com.alibaba.druid.pool.druiddatasource\n    druid:\n      driver-class-name: com.mysql.cj.jdbc.driver\n      username: root\n      password: root\n      url: jdbc:mysql://localhost:3306/simplify?characterencoding=utf8&zerodatetimebehavior=converttonull&usessl=false&usejdbccomplianttimezoneshift=true&uselegacydatetimecode=false&servertimezone=gmt%2b8&allowmultiqueries=true&allowpublickeyretrieval=true\n\nlogging:\n  level:\n    org.springframework.security: debug\n\n\n\n# 访问获取授权码\n\n打开浏览器，输入地址：\n\nhttp://localhost:3000/oauth/authorize?response_type=code&client_id=simplify&scope=all&redirect_uri=https://www.ivoov.com\n\n\n第一次访问会跳转到登录页面\n\n\n\n验证成功后会询问用户是否授权客户端\n\n\n\n选择授权后会跳转到我的博客，浏览器地址上还会包含一个授权码（code=jb00sk），浏览器地址栏会显示如下地址：\n\nhttps://www.ivoov.com/?code=jb00sk\n\n\n有了这个授权码就可以获取访问令牌了\n\n\n# 通过授权码向服务器申请令牌\n\n通过 curl 或是 postman 请求\n\ncurl --location --request post 'http://localhost:3000/oauth/token?grant_type=authorization_code&code=jb00sk&client_id=simplify&client_secret=simplify&redirect_uri=https://www.ivoov.com'\n\n\n\n\n操作成功后数据库 oauth_access_token 表中会增加一笔记录，效果图如下：\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"RBAC 基于角色的权限控制",frontmatter:{title:"RBAC 基于角色的权限控制",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/cee981/"},regularPath:"/19.Spring%20Security%20OAuth2/03.%E5%88%9B%E5%BB%BA%E8%AE%A4%E8%AF%81%E6%9C%8D%E5%8A%A1%E5%99%A8/04.RBAC%20%E5%9F%BA%E4%BA%8E%E8%A7%92%E8%89%B2%E7%9A%84%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6.html",relativePath:"19.Spring Security OAuth2/03.创建认证服务器/04.RBAC 基于角色的权限控制.md",key:"v-8dc9cdca",path:"/pages/cee981/",headersStr:null,content:"# RBAC 基于角色的权限控制",normalizedContent:"# rbac 基于角色的权限控制",charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"基于 RBAC 的自定义认证",frontmatter:{title:"基于 RBAC 的自定义认证",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/4bff79/"},regularPath:"/19.Spring%20Security%20OAuth2/03.%E5%88%9B%E5%BB%BA%E8%AE%A4%E8%AF%81%E6%9C%8D%E5%8A%A1%E5%99%A8/05.%E5%9F%BA%E4%BA%8E%20RBAC%20%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AE%A4%E8%AF%81.html",relativePath:"19.Spring Security OAuth2/03.创建认证服务器/05.基于 RBAC 的自定义认证.md",key:"v-994030a6",path:"/pages/4bff79/",headersStr:null,content:"# 基于 RBAC 的自定义认证",normalizedContent:"# 基于 rbac 的自定义认证",charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"JWT令牌",frontmatter:{title:"JWT令牌",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/8dc90c/"},regularPath:"/19.Spring%20Security%20OAuth2/03.%E5%88%9B%E5%BB%BA%E8%AE%A4%E8%AF%81%E6%9C%8D%E5%8A%A1%E5%99%A8/06.JWT%E4%BB%A4%E7%89%8C.html",relativePath:"19.Spring Security OAuth2/03.创建认证服务器/06.JWT令牌.md",key:"v-c4e4c304",path:"/pages/8dc90c/",headers:[{level:2,title:"JWT介绍",slug:"jwt介绍",normalizedTitle:"jwt介绍",charIndex:12},{level:3,title:"什么是JWT？",slug:"什么是jwt",normalizedTitle:"什么是jwt？",charIndex:233},{level:3,title:"JWT令牌结构",slug:"jwt令牌结构",normalizedTitle:"jwt令牌结构",charIndex:593},{level:2,title:"配置JWT令牌服务",slug:"配置jwt令牌服务",normalizedTitle:"配置jwt令牌服务",charIndex:1411},{level:2,title:"生成jwt令牌",slug:"生成jwt令牌",normalizedTitle:"生成jwt令牌",charIndex:2703},{level:2,title:"校验jwt令牌",slug:"校验jwt令牌",normalizedTitle:"校验jwt令牌",charIndex:2722}],headersStr:"JWT介绍 什么是JWT？ JWT令牌结构 配置JWT令牌服务 生成jwt令牌 校验jwt令牌",content:'# JWT令牌\n\n\n# JWT介绍\n\n通过上边的测试我们发现，当资源服务和授权服务不在一起时资源服务使用RemoteTokenServices 远程请求授权服务验证token，如果访问量较大将会影响系统的性能 。\n\n解决上边问题：\n\n令牌采用JWT格式即可解决上边的问题，用户认证通过会得到一个JWT令牌，JWT令牌中已经包括了用户相关的信息，客户端只需要携带JWT访问资源服务，资源服务根据事先约定的算法自行完成令牌校验，无需每次都请求认证服务完成授权。\n\n\n# 什么是JWT？\n\nJSON Web Token（JWT）是一个开放的行业标准（RFC 7519），它定义了一种简介的、自包含的协议格式，用于在通信双方传递json对象，传递的信息经过数字签名可以被验证和信任。JWT可以使用HMAC算法或使用RSA的公钥/私钥对来签名，防止被篡改。\n\n官网：https://jwt.io/\n\n标准：https://tools.ietf.org/html/rfc7519\n\nJWT令牌的优点：\n\n * 1）jwt基于json，非常方便解析。\n * 2）可以在令牌中自定义丰富的内容，易扩展。\n * 3）通过非对称加密算法及数字签名技术，JWT防止篡改，安全性高。\n * 4）资源服务使用JWT可不依赖认证服务即可完成授权。\n\n缺点：\n\n * 1）JWT令牌较长，占存储空间比较大。\n\n\n# JWT令牌结构\n\n通过学习JWT令牌结构为自定义jwt令牌打好基础。\n\nJWT令牌由三部分组成，每部分中间使用点（.）分隔，比如：xxxxx.yyyyy.zzzzz\n\n * Header\n   \n   头部包括令牌的类型（即JWT）及使用的哈希算法（如HMAC SHA256或RSA）\n   \n   一个例子如下：\n   \n   下边是Header部分的内容\n\n{\n    "alg": "HS256",\n    "typ": "JWT"\n}\n\n\n将上边的内容使用Base64Url编码，得到一个字符串就是JWT令牌的第一部分。\n\n * Payload\n\n第二部分是负载，内容也是一个json对象，它是存放有效信息的地方，它可以存放jwt提供的现成字段，比如：iss（签发者）,exp（过期时间戳）, sub（面向的用户）等，也可自定义字段。此部分不建议存放敏感信息，因为此部分可以解码还原原始内容。\n\n最后将第二部分负载使用Base64Url编码，得到一个字符串就是JWT令牌的第二部分。\n\n一个例子：\n\n{\n    "sub": "1234567890",\n    "name": "456",\n    "admin": true\n}\n\n\n * Signature\n\n第三部分是签名，此部分用于防止jwt内容被篡改。\n\n这个部分使用base64url将前两部分进行编码，编码后使用点（.）连接组成字符串，最后使用header中声明签名算法进行签名。\n\n一个例子：\n\nHMACSHA256(\nbase64UrlEncode(header) + "." +\nbase64UrlEncode(payload),\nsecret)\n\n\n * base64UrlEncode(header)：jwt令牌的第一部分。\n * base64UrlEncode(payload)：jwt令牌的第二部分。\n * secret：签名所使用的密钥。\n\n\n# 配置JWT令牌服务\n\n在simplify-auth中配置jwt令牌服务，即可实现生成jwt格式的令牌。\n\n1、TokenConfig\n\n@Configuration\npublic class TokenConfig {\n    private String SIGNING_KEY = "simplify_key";\n\n    @Bean\n    public TokenStore tokenStore() {\n        return new JwtTokenStore(accessTokenConverter());\n    } \n    \n    @Bean\n    public JwtAccessTokenConverter accessTokenConverter() {\n        JwtAccessTokenConverter converter = new JwtAccessTokenConverter();\n        converter.setSigningKey(SIGNING_KEY); //对称秘钥，资源服务器使用该秘钥来验证\n        return converter;\n    }\n}\n\n\n2、定义JWT令牌服务\n\n@Autowired\nprivate JwtAccessTokenConverter accessTokenConverter;\n    @Bean\n    public AuthorizationServerTokenServices tokenService() {\n        DefaultTokenServices service=new DefaultTokenServices();\n        service.setClientDetailsService(clientDetailsService);\n        service.setSupportRefreshToken(true);\n        service.setTokenStore(tokenStore);\n        TokenEnhancerChain tokenEnhancerChain = new TokenEnhancerChain();\n        tokenEnhancerChain.setTokenEnhancers(Arrays.asList(accessTokenConverter));\n        service.setTokenEnhancer(tokenEnhancerChain);\n        //令牌默认有效期2小时\n        service.setAccessTokenValiditySeconds(7200);\n        //刷新令牌默认有效期3天\n        service.setRefreshTokenValiditySeconds(259200); \n        return service;\n    }\n}\n\n\n\n# 生成jwt令牌\n\ntodo\n\n\n\n# 校验jwt令牌\n\n资源服务需要和授权服务拥有一致的签字、令牌服务等：\n\n1、将授权服务中的TokenConfig类拷贝到资源 服务中\n\n2、屏蔽资源 服务原来的令牌服务类\n\ntodo\n\n\n3、测试\n\n1）申请jwt令牌\n\n2）使用令牌请求资源',normalizedContent:'# jwt令牌\n\n\n# jwt介绍\n\n通过上边的测试我们发现，当资源服务和授权服务不在一起时资源服务使用remotetokenservices 远程请求授权服务验证token，如果访问量较大将会影响系统的性能 。\n\n解决上边问题：\n\n令牌采用jwt格式即可解决上边的问题，用户认证通过会得到一个jwt令牌，jwt令牌中已经包括了用户相关的信息，客户端只需要携带jwt访问资源服务，资源服务根据事先约定的算法自行完成令牌校验，无需每次都请求认证服务完成授权。\n\n\n# 什么是jwt？\n\njson web token（jwt）是一个开放的行业标准（rfc 7519），它定义了一种简介的、自包含的协议格式，用于在通信双方传递json对象，传递的信息经过数字签名可以被验证和信任。jwt可以使用hmac算法或使用rsa的公钥/私钥对来签名，防止被篡改。\n\n官网：https://jwt.io/\n\n标准：https://tools.ietf.org/html/rfc7519\n\njwt令牌的优点：\n\n * 1）jwt基于json，非常方便解析。\n * 2）可以在令牌中自定义丰富的内容，易扩展。\n * 3）通过非对称加密算法及数字签名技术，jwt防止篡改，安全性高。\n * 4）资源服务使用jwt可不依赖认证服务即可完成授权。\n\n缺点：\n\n * 1）jwt令牌较长，占存储空间比较大。\n\n\n# jwt令牌结构\n\n通过学习jwt令牌结构为自定义jwt令牌打好基础。\n\njwt令牌由三部分组成，每部分中间使用点（.）分隔，比如：xxxxx.yyyyy.zzzzz\n\n * header\n   \n   头部包括令牌的类型（即jwt）及使用的哈希算法（如hmac sha256或rsa）\n   \n   一个例子如下：\n   \n   下边是header部分的内容\n\n{\n    "alg": "hs256",\n    "typ": "jwt"\n}\n\n\n将上边的内容使用base64url编码，得到一个字符串就是jwt令牌的第一部分。\n\n * payload\n\n第二部分是负载，内容也是一个json对象，它是存放有效信息的地方，它可以存放jwt提供的现成字段，比如：iss（签发者）,exp（过期时间戳）, sub（面向的用户）等，也可自定义字段。此部分不建议存放敏感信息，因为此部分可以解码还原原始内容。\n\n最后将第二部分负载使用base64url编码，得到一个字符串就是jwt令牌的第二部分。\n\n一个例子：\n\n{\n    "sub": "1234567890",\n    "name": "456",\n    "admin": true\n}\n\n\n * signature\n\n第三部分是签名，此部分用于防止jwt内容被篡改。\n\n这个部分使用base64url将前两部分进行编码，编码后使用点（.）连接组成字符串，最后使用header中声明签名算法进行签名。\n\n一个例子：\n\nhmacsha256(\nbase64urlencode(header) + "." +\nbase64urlencode(payload),\nsecret)\n\n\n * base64urlencode(header)：jwt令牌的第一部分。\n * base64urlencode(payload)：jwt令牌的第二部分。\n * secret：签名所使用的密钥。\n\n\n# 配置jwt令牌服务\n\n在simplify-auth中配置jwt令牌服务，即可实现生成jwt格式的令牌。\n\n1、tokenconfig\n\n@configuration\npublic class tokenconfig {\n    private string signing_key = "simplify_key";\n\n    @bean\n    public tokenstore tokenstore() {\n        return new jwttokenstore(accesstokenconverter());\n    } \n    \n    @bean\n    public jwtaccesstokenconverter accesstokenconverter() {\n        jwtaccesstokenconverter converter = new jwtaccesstokenconverter();\n        converter.setsigningkey(signing_key); //对称秘钥，资源服务器使用该秘钥来验证\n        return converter;\n    }\n}\n\n\n2、定义jwt令牌服务\n\n@autowired\nprivate jwtaccesstokenconverter accesstokenconverter;\n    @bean\n    public authorizationservertokenservices tokenservice() {\n        defaulttokenservices service=new defaulttokenservices();\n        service.setclientdetailsservice(clientdetailsservice);\n        service.setsupportrefreshtoken(true);\n        service.settokenstore(tokenstore);\n        tokenenhancerchain tokenenhancerchain = new tokenenhancerchain();\n        tokenenhancerchain.settokenenhancers(arrays.aslist(accesstokenconverter));\n        service.settokenenhancer(tokenenhancerchain);\n        //令牌默认有效期2小时\n        service.setaccesstokenvalidityseconds(7200);\n        //刷新令牌默认有效期3天\n        service.setrefreshtokenvalidityseconds(259200); \n        return service;\n    }\n}\n\n\n\n# 生成jwt令牌\n\ntodo\n\n\n\n# 校验jwt令牌\n\n资源服务需要和授权服务拥有一致的签字、令牌服务等：\n\n1、将授权服务中的tokenconfig类拷贝到资源 服务中\n\n2、屏蔽资源 服务原来的令牌服务类\n\ntodo\n\n\n3、测试\n\n1）申请jwt令牌\n\n2）使用令牌请求资源',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"资源服务器配置",frontmatter:{title:"资源服务器配置",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/3414a7/"},regularPath:"/19.Spring%20Security%20OAuth2/04.%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E6%9C%8D%E5%8A%A1%E5%99%A8/01.%E8%B5%84%E6%BA%90%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE.html",relativePath:"19.Spring Security OAuth2/04.创建资源服务器/01.资源服务器配置.md",key:"v-c4de77d8",path:"/pages/3414a7/",headers:[{level:2,title:"pom.xml",slug:"pom-xml",normalizedTitle:"pom.xml",charIndex:14},{level:2,title:"资源服务器配置",slug:"资源服务器配置-2",normalizedTitle:"资源服务器配置",charIndex:2},{level:2,title:"验证token",slug:"验证token",normalizedTitle:"验证token",charIndex:5679},{level:2,title:"编写资源",slug:"编写资源",normalizedTitle:"编写资源",charIndex:7034},{level:2,title:"添加安全访问控制",slug:"添加安全访问控制",normalizedTitle:"添加安全访问控制",charIndex:7278},{level:2,title:"测试",slug:"测试",normalizedTitle:"测试",charIndex:7848}],headersStr:"pom.xml 资源服务器配置 验证token 编写资源 添加安全访问控制 测试",content:'# 资源服务器配置\n\n\n# pom.xml\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <parent>\n        <artifactId>simplify-upms</artifactId>\n        <groupId>com.ieooc</groupId>\n        <version>1.0.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>simplify-upms-biz</artifactId>\n    <packaging>jar</packaging>\n    <description>simplify 通用用户权限管理系统业务处理模块</description>\n\n    <dependencies>\n        \x3c!--upms api、model 模块--\x3e\n        <dependency>\n            <groupId>com.ieooc</groupId>\n            <artifactId>simplify-upms-api</artifactId>\n        </dependency>\n        \x3c!--Swagger 接口文档--\x3e\n        <dependency>\n            <groupId>com.ieooc</groupId>\n            <artifactId>simplify-common-swagger</artifactId>\n        </dependency>\n        \x3c!--数据操作相关--\x3e\n        <dependency>\n            <groupId>com.ieooc</groupId>\n            <artifactId>simplify-common-data</artifactId>\n        </dependency>\n        \x3c!--安全工具类--\x3e\n        <dependency>\n            <groupId>com.ieooc</groupId>\n            <artifactId>simplify-common-security</artifactId>\n        </dependency>\n        \x3c!--注册中心客户端--\x3e\n        <dependency>\n            <groupId>com.alibaba.cloud</groupId>\n            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n        </dependency>\n        \x3c!--配置中心客户端--\x3e\n        <dependency>\n            <groupId>com.alibaba.cloud</groupId>\n            <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>\n        </dependency>\n        \x3c!--web 模块--\x3e\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        \x3c!--undertow容器--\x3e\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-undertow</artifactId>\n        </dependency>\n        \x3c!-- TODO 临时解决问题：https://blog.csdn.net/qq_24585103/article/details/103126411 --\x3e\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n        </dependency>\n        \x3c!--mybatis-plus代码生成器--\x3e\n        <dependency>\n            <groupId>com.baomidou</groupId>\n            <artifactId>mybatis-plus-generator</artifactId>\n        </dependency>\n        \x3c!--freemarker这里只用作代码生成--\x3e\n        <dependency>\n            <groupId>org.freemarker</groupId>\n            <artifactId>freemarker</artifactId>\n        </dependency>\n        \x3c!--druid数据库连接池和监控--\x3e\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid-spring-boot-starter</artifactId>\n        </dependency>\n        \x3c!--mysql 驱动--\x3e\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <scope>runtime</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n\n\n\n# 资源服务器配置\n\n@EnableResourceServer 注解到一个 @Configuration 配置类上，并且必须使用 ResourceServerConfigurer 这个配置对象来进行配置（可以选择继承自 ResourceServerConfigurerAdapter 然后覆写其中的方法，参数就是这个对象的实例），下面是一些可以配置的属性：\n\nResourceServerSecurityConfigurer中主要包括：\n\n * tokenServices：ResourceServerTokenServices 类的实例，用来实现令牌服务。\n * tokenStore：TokenStore类的实例，指定令牌如何访问，与tokenServices配置可选\n * resourceId：这个资源服务的ID，这个属性是可选的，但是推荐设置并在授权服务中进行验证。\n\n其他的拓展属性例如 tokenExtractor 令牌提取器用来提取请求中的令牌。\n\nHttpSecurity配置这个与Spring Security类似：\n\n * 请求匹配器，用来设置需要进行保护的资源路径，默认的情况下是保护资源服务的全部路径。\n * 通过http.authorizeRequests()来设置受保护资源的访问规则\n * 其他的自定义权限保护规则通过 HttpSecurity 来进行配置。\n\n@EnableResourceServer 注解自动增加了一个类型为 OAuth2AuthenticationProcessingFilter 的过滤器链编写ResouceServerConfig：\n\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;\nimport org.springframework.security.config.annotation.web.builders.HttpSecurity;\nimport org.springframework.security.config.http.SessionCreationPolicy;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.EnableResourceServer;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.ResourceServerConfigurerAdapter;\nimport org.springframework.security.oauth2.config.annotation.web.configurers.ResourceServerSecurityConfigurer;\nimport org.springframework.security.oauth2.provider.token.RemoteTokenServices;\nimport org.springframework.security.oauth2.provider.token.ResourceServerTokenServices;\n\n@Configuration\n@EnableResourceServer\n@EnableGlobalMethodSecurity(prePostEnabled = true)\npublic class ResouceServerConfig extends ResourceServerConfigurerAdapter {\n    public static final String RESOURCE_ID = "res1";\n\n    @Override\n    public void configure(ResourceServerSecurityConfigurer resources) {\n            resources.resourceId(RESOURCE_ID)\n                .tokenServices(tokenService())\n                .stateless(true);\n    } \n    \n    @Override\n    public void configure(HttpSecurity http) throws Exception {\n            http\n                .authorizeRequests()\n                .antMatchers("/**").access("#oauth2.hasScope(\'all\')")\n                .and().csrf().disable()\n                .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS);\n    }\n}\n\n\n\n# 验证token\n\nResourceServerTokenServices 是组成授权服务的另一半，如果你的授权服务和资源服务在同一个应用程序上的话，你可以使用 DefaultTokenServices ，这样的话，你就不用考虑关于实现所有必要的接口的一致性问题。如果你的资源服务器是分离开的，那么你就必须要确保能够有匹配授权服务提供的 ResourceServerTokenServices，它知道如何对令牌进行解码。\n\n令牌解析方法： 使用 DefaultTokenServices 在资源服务器本地配置令牌存储、解码、解析方式 使用RemoteTokenServices 资源服务器通过 HTTP 请求来解码令牌，每次都请求授权服务器端点 /oauth/check_token\n\n使用授权服务的 /oauth/check_token 端点你需要在授权服务将这个端点暴露出去，以便资源服务可以进行访问，这在咱们授权服务配置中已经提到了，下面是一个例子,在这个例子中，我们在授权服务中配置了/oauth/check_token 和 /oauth/token_key 这两个端点：\n\n@Override\npublic void configure(AuthorizationServerSecurityConfigurer security) throws Exception {\n        security\n        .tokenKeyAccess("permitAll()")// /oauth/token_key 安全配置\n        .checkTokenAccess("permitAll()") // /oauth/check_token 安全配置\n}\n\n\n在资源 服务配置RemoteTokenServices ，在ResouceServerConfig中配置：\n\n//资源服务令牌解析服务\n@Bean\npublic ResourceServerTokenServices tokenService() {\n    //使用远程服务请求授权服务器校验token,必须指定校验token 的url、client_id，client_secret\n    RemoteTokenServices service=new RemoteTokenServices();\n    service.setCheckTokenEndpointUrl("http://localhost:53020/uaa/oauth/check_token");\n    service.setClientId("c1");\n    service.setClientSecret("secret");\n    return service;\n} \n\n@Override\npublic void configure(ResourceServerSecurityConfigurer resources) {\n    resources.resourceId(RESOURCE_ID)\n    .tokenServices(tokenService())\n    .stateless(true);\n}\n\n\n\n# 编写资源\n\n在controller包下编写OrderController，此controller表示订单资源的访问类：\n\n@RestController\npublic class OrderController {\n\n    @GetMapping(value = "/r1")\n    @PreAuthorize("hasAnyAuthority(\'p1\')")\n    public String r1(){\n        return "访问资源1";\n    }\n}\n\n\n\n# 添加安全访问控制\n\n@Configuration\n@EnableGlobalMethodSecurity(securedEnabled = true,prePostEnabled = true)\npublic class WebSecurityConfig extends WebSecurityConfigurerAdapter {\n    //安全拦截机制（最重要）\n    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http.csrf().disable()\n        .authorizeRequests()\n        // .antMatchers("/r/r1").hasAuthority("p2")\n        // .antMatchers("/r/r2").hasAuthority("p2")\n        .antMatchers("/r/**").authenticated()//所有/r/**的请求必须认证通过\n        .anyRequest().permitAll()//除了/r/**，其它的请求可以访问\n        ;\n    }\n}\n\n\n\n# 测试\n\n1、申请令牌\n\n采用密码模式，使用Postman请求授权服务器。\n\n\n\n{\n    "access_token": "273b7f10-3b4d-495e-9b15-b5bc801ff265",\n    "token_type": "bearer",\n    "refresh_token": "6c9c8d31-cd8b-41e5-8199-6fbd0a08ae31",\n    "expires_in": 41353,\n    "scope": "server"\n}\n\n\n2、请求资源\n\n按照oauth2.0协议要求，请求资源需要携带token，如下： token的参数名称为：Authorization，值为：Bearer token值\n\n\n\n{\n    "success": true,\n    "code": 200,\n    "message": "操作成功",\n    "data": {\n        "userId": 1,\n        "username": "admin",\n        "password": "{bcrypt}$2a$10$IVzj1Wd.ZQdOIWdb1htQjexU94uoNeuk1crlQ9ExVupPi0Iy1uv.C",\n        "createTime": "2020-09-28T07:15:18",\n        "updateTime": "2020-11-05T19:29:05"\n    }\n}\n\n\n如果token错误，则授权失败，如下：\n\n{\n    "error": "unauthorized",\n    "error_description": "Full authentication is required to access this resource"\n}\n',normalizedContent:'# 资源服务器配置\n\n\n# pom.xml\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <parent>\n        <artifactid>simplify-upms</artifactid>\n        <groupid>com.ieooc</groupid>\n        <version>1.0.0-snapshot</version>\n    </parent>\n    <modelversion>4.0.0</modelversion>\n\n    <artifactid>simplify-upms-biz</artifactid>\n    <packaging>jar</packaging>\n    <description>simplify 通用用户权限管理系统业务处理模块</description>\n\n    <dependencies>\n        \x3c!--upms api、model 模块--\x3e\n        <dependency>\n            <groupid>com.ieooc</groupid>\n            <artifactid>simplify-upms-api</artifactid>\n        </dependency>\n        \x3c!--swagger 接口文档--\x3e\n        <dependency>\n            <groupid>com.ieooc</groupid>\n            <artifactid>simplify-common-swagger</artifactid>\n        </dependency>\n        \x3c!--数据操作相关--\x3e\n        <dependency>\n            <groupid>com.ieooc</groupid>\n            <artifactid>simplify-common-data</artifactid>\n        </dependency>\n        \x3c!--安全工具类--\x3e\n        <dependency>\n            <groupid>com.ieooc</groupid>\n            <artifactid>simplify-common-security</artifactid>\n        </dependency>\n        \x3c!--注册中心客户端--\x3e\n        <dependency>\n            <groupid>com.alibaba.cloud</groupid>\n            <artifactid>spring-cloud-starter-alibaba-nacos-discovery</artifactid>\n        </dependency>\n        \x3c!--配置中心客户端--\x3e\n        <dependency>\n            <groupid>com.alibaba.cloud</groupid>\n            <artifactid>spring-cloud-starter-alibaba-nacos-config</artifactid>\n        </dependency>\n        \x3c!--web 模块--\x3e\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-web</artifactid>\n        </dependency>\n        \x3c!--undertow容器--\x3e\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-undertow</artifactid>\n        </dependency>\n        \x3c!-- todo 临时解决问题：https://blog.csdn.net/qq_24585103/article/details/103126411 --\x3e\n        <dependency>\n            <groupid>org.springframework.cloud</groupid>\n            <artifactid>spring-cloud-starter-netflix-hystrix</artifactid>\n        </dependency>\n        \x3c!--mybatis-plus代码生成器--\x3e\n        <dependency>\n            <groupid>com.baomidou</groupid>\n            <artifactid>mybatis-plus-generator</artifactid>\n        </dependency>\n        \x3c!--freemarker这里只用作代码生成--\x3e\n        <dependency>\n            <groupid>org.freemarker</groupid>\n            <artifactid>freemarker</artifactid>\n        </dependency>\n        \x3c!--druid数据库连接池和监控--\x3e\n        <dependency>\n            <groupid>com.alibaba</groupid>\n            <artifactid>druid-spring-boot-starter</artifactid>\n        </dependency>\n        \x3c!--mysql 驱动--\x3e\n        <dependency>\n            <groupid>mysql</groupid>\n            <artifactid>mysql-connector-java</artifactid>\n            <scope>runtime</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-maven-plugin</artifactid>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n\n\n\n# 资源服务器配置\n\n@enableresourceserver 注解到一个 @configuration 配置类上，并且必须使用 resourceserverconfigurer 这个配置对象来进行配置（可以选择继承自 resourceserverconfigureradapter 然后覆写其中的方法，参数就是这个对象的实例），下面是一些可以配置的属性：\n\nresourceserversecurityconfigurer中主要包括：\n\n * tokenservices：resourceservertokenservices 类的实例，用来实现令牌服务。\n * tokenstore：tokenstore类的实例，指定令牌如何访问，与tokenservices配置可选\n * resourceid：这个资源服务的id，这个属性是可选的，但是推荐设置并在授权服务中进行验证。\n\n其他的拓展属性例如 tokenextractor 令牌提取器用来提取请求中的令牌。\n\nhttpsecurity配置这个与spring security类似：\n\n * 请求匹配器，用来设置需要进行保护的资源路径，默认的情况下是保护资源服务的全部路径。\n * 通过http.authorizerequests()来设置受保护资源的访问规则\n * 其他的自定义权限保护规则通过 httpsecurity 来进行配置。\n\n@enableresourceserver 注解自动增加了一个类型为 oauth2authenticationprocessingfilter 的过滤器链编写resouceserverconfig：\n\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.security.config.annotation.method.configuration.enableglobalmethodsecurity;\nimport org.springframework.security.config.annotation.web.builders.httpsecurity;\nimport org.springframework.security.config.http.sessioncreationpolicy;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.enableresourceserver;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.resourceserverconfigureradapter;\nimport org.springframework.security.oauth2.config.annotation.web.configurers.resourceserversecurityconfigurer;\nimport org.springframework.security.oauth2.provider.token.remotetokenservices;\nimport org.springframework.security.oauth2.provider.token.resourceservertokenservices;\n\n@configuration\n@enableresourceserver\n@enableglobalmethodsecurity(prepostenabled = true)\npublic class resouceserverconfig extends resourceserverconfigureradapter {\n    public static final string resource_id = "res1";\n\n    @override\n    public void configure(resourceserversecurityconfigurer resources) {\n            resources.resourceid(resource_id)\n                .tokenservices(tokenservice())\n                .stateless(true);\n    } \n    \n    @override\n    public void configure(httpsecurity http) throws exception {\n            http\n                .authorizerequests()\n                .antmatchers("/**").access("#oauth2.hasscope(\'all\')")\n                .and().csrf().disable()\n                .sessionmanagement().sessioncreationpolicy(sessioncreationpolicy.stateless);\n    }\n}\n\n\n\n# 验证token\n\nresourceservertokenservices 是组成授权服务的另一半，如果你的授权服务和资源服务在同一个应用程序上的话，你可以使用 defaulttokenservices ，这样的话，你就不用考虑关于实现所有必要的接口的一致性问题。如果你的资源服务器是分离开的，那么你就必须要确保能够有匹配授权服务提供的 resourceservertokenservices，它知道如何对令牌进行解码。\n\n令牌解析方法： 使用 defaulttokenservices 在资源服务器本地配置令牌存储、解码、解析方式 使用remotetokenservices 资源服务器通过 http 请求来解码令牌，每次都请求授权服务器端点 /oauth/check_token\n\n使用授权服务的 /oauth/check_token 端点你需要在授权服务将这个端点暴露出去，以便资源服务可以进行访问，这在咱们授权服务配置中已经提到了，下面是一个例子,在这个例子中，我们在授权服务中配置了/oauth/check_token 和 /oauth/token_key 这两个端点：\n\n@override\npublic void configure(authorizationserversecurityconfigurer security) throws exception {\n        security\n        .tokenkeyaccess("permitall()")// /oauth/token_key 安全配置\n        .checktokenaccess("permitall()") // /oauth/check_token 安全配置\n}\n\n\n在资源 服务配置remotetokenservices ，在resouceserverconfig中配置：\n\n//资源服务令牌解析服务\n@bean\npublic resourceservertokenservices tokenservice() {\n    //使用远程服务请求授权服务器校验token,必须指定校验token 的url、client_id，client_secret\n    remotetokenservices service=new remotetokenservices();\n    service.setchecktokenendpointurl("http://localhost:53020/uaa/oauth/check_token");\n    service.setclientid("c1");\n    service.setclientsecret("secret");\n    return service;\n} \n\n@override\npublic void configure(resourceserversecurityconfigurer resources) {\n    resources.resourceid(resource_id)\n    .tokenservices(tokenservice())\n    .stateless(true);\n}\n\n\n\n# 编写资源\n\n在controller包下编写ordercontroller，此controller表示订单资源的访问类：\n\n@restcontroller\npublic class ordercontroller {\n\n    @getmapping(value = "/r1")\n    @preauthorize("hasanyauthority(\'p1\')")\n    public string r1(){\n        return "访问资源1";\n    }\n}\n\n\n\n# 添加安全访问控制\n\n@configuration\n@enableglobalmethodsecurity(securedenabled = true,prepostenabled = true)\npublic class websecurityconfig extends websecurityconfigureradapter {\n    //安全拦截机制（最重要）\n    @override\n    protected void configure(httpsecurity http) throws exception {\n        http.csrf().disable()\n        .authorizerequests()\n        // .antmatchers("/r/r1").hasauthority("p2")\n        // .antmatchers("/r/r2").hasauthority("p2")\n        .antmatchers("/r/**").authenticated()//所有/r/**的请求必须认证通过\n        .anyrequest().permitall()//除了/r/**，其它的请求可以访问\n        ;\n    }\n}\n\n\n\n# 测试\n\n1、申请令牌\n\n采用密码模式，使用postman请求授权服务器。\n\n\n\n{\n    "access_token": "273b7f10-3b4d-495e-9b15-b5bc801ff265",\n    "token_type": "bearer",\n    "refresh_token": "6c9c8d31-cd8b-41e5-8199-6fbd0a08ae31",\n    "expires_in": 41353,\n    "scope": "server"\n}\n\n\n2、请求资源\n\n按照oauth2.0协议要求，请求资源需要携带token，如下： token的参数名称为：authorization，值为：bearer token值\n\n\n\n{\n    "success": true,\n    "code": 200,\n    "message": "操作成功",\n    "data": {\n        "userid": 1,\n        "username": "admin",\n        "password": "{bcrypt}$2a$10$ivzj1wd.zqdoiwdb1htqjexu94uoneuk1crlq9exvuppi0iy1uv.c",\n        "createtime": "2020-09-28t07:15:18",\n        "updatetime": "2020-11-05t19:29:05"\n    }\n}\n\n\n如果token错误，则授权失败，如下：\n\n{\n    "error": "unauthorized",\n    "error_description": "full authentication is required to access this resource"\n}\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"什么是分布式系统",frontmatter:{title:"什么是分布式系统",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/d62ff2/"},regularPath:"/19.Spring%20Security%20OAuth2/05.%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%AE%A4%E8%AF%81%E6%96%B9%E6%A1%88/01.%E4%BB%80%E4%B9%88%E6%98%AF%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F.html",relativePath:"19.Spring Security OAuth2/05.分布式系统认证方案/01.什么是分布式系统.md",key:"v-8851eab0",path:"/pages/d62ff2/",headersStr:null,content:"# 什么是分布式系统\n\n随着软件环境和需求的变化，软件的架构由单体结构演变为分布式架构，具有分布式架构的系统叫分布式系统，分布式系统的运行通常依赖网络，它将单体结构的系统分为若干服务，服务之间通过网络交互来完成用户的业务处理，当前流行的微服务架构就是分布式系统架构，如下图：\n\n\n\n分布式系统具体如下基本特点:\n\n * 分布性：每个部分都可以独立部署，服务之间交互通过网络进行通信，比如:订单服务、商品服务。\n\n * 伸缩性：每个部分都可以集群方式部署，并可针对部分结点进行硬件及软件扩容，具有一定的伸缩能力。\n\n * 共享性：每个部分都可以作为共享资源对外提供服务，多个部分可能有操作共享资源的情况。\n\n * 开放性：每个部分根据需求都可以对外发布共享资源的访问接口，并可允许第三方系统访问。",normalizedContent:"# 什么是分布式系统\n\n随着软件环境和需求的变化，软件的架构由单体结构演变为分布式架构，具有分布式架构的系统叫分布式系统，分布式系统的运行通常依赖网络，它将单体结构的系统分为若干服务，服务之间通过网络交互来完成用户的业务处理，当前流行的微服务架构就是分布式系统架构，如下图：\n\n\n\n分布式系统具体如下基本特点:\n\n * 分布性：每个部分都可以独立部署，服务之间交互通过网络进行通信，比如:订单服务、商品服务。\n\n * 伸缩性：每个部分都可以集群方式部署，并可针对部分结点进行硬件及软件扩容，具有一定的伸缩能力。\n\n * 共享性：每个部分都可以作为共享资源对外提供服务，多个部分可能有操作共享资源的情况。\n\n * 开放性：每个部分根据需求都可以对外发布共享资源的访问接口，并可允许第三方系统访问。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"分布式认证需求",frontmatter:{title:"分布式认证需求",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/16b31e/"},regularPath:"/19.Spring%20Security%20OAuth2/05.%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%AE%A4%E8%AF%81%E6%96%B9%E6%A1%88/02.%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A4%E8%AF%81%E9%9C%80%E6%B1%82.html",relativePath:"19.Spring Security OAuth2/05.分布式系统认证方案/02.分布式认证需求.md",key:"v-f80986b8",path:"/pages/16b31e/",headers:[{level:2,title:"统一认证授权",slug:"统一认证授权",normalizedTitle:"统一认证授权",charIndex:153},{level:2,title:"应用接入认证",slug:"应用接入认证",normalizedTitle:"应用接入认证",charIndex:317}],headersStr:"统一认证授权 应用接入认证",content:"# 分布式认证需求\n\n分布式系统的每个服务都会有认证、授权的需求，如果每个服务都实现一套认证授权逻辑会非常冗余，考虑分布式 系统共享性的特点，需要由独立的认证服务处理系统认证授权的请求;考虑分布式系统开放性的特点，不仅对系统 内部服务提供认证，对第三方系统也要提供认证。分布式认证的需求总结如下:\n\n\n# 统一认证授权\n\n提供独立的认证服务，统一处理认证授权。\n\n无论是不同类型的用户，还是不同种类的客户端(web端，H5、APP)，均采用一致的认证、权限、会话机制，实现 统一认证授权。\n\n要实现统一则认证方式必须可扩展，支持各种认证需求，比如:用户名密码认证、短信验证码、二维码、人脸识等认证方式，并可以非常灵活的切换。\n\n\n# 应用接入认证\n\n应提供扩展和开放能力，提供安全的系统对接机制，并可开放部分API给接入第三方使用，一方应用(内部 系统服 务)和三方应用(第三方应用)均采用统一机制接入。",normalizedContent:"# 分布式认证需求\n\n分布式系统的每个服务都会有认证、授权的需求，如果每个服务都实现一套认证授权逻辑会非常冗余，考虑分布式 系统共享性的特点，需要由独立的认证服务处理系统认证授权的请求;考虑分布式系统开放性的特点，不仅对系统 内部服务提供认证，对第三方系统也要提供认证。分布式认证的需求总结如下:\n\n\n# 统一认证授权\n\n提供独立的认证服务，统一处理认证授权。\n\n无论是不同类型的用户，还是不同种类的客户端(web端，h5、app)，均采用一致的认证、权限、会话机制，实现 统一认证授权。\n\n要实现统一则认证方式必须可扩展，支持各种认证需求，比如:用户名密码认证、短信验证码、二维码、人脸识等认证方式，并可以非常灵活的切换。\n\n\n# 应用接入认证\n\n应提供扩展和开放能力，提供安全的系统对接机制，并可开放部分api给接入第三方使用，一方应用(内部 系统服 务)和三方应用(第三方应用)均采用统一机制接入。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"分布式认证方案",frontmatter:{title:"分布式认证方案",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/d6bec0/"},regularPath:"/19.Spring%20Security%20OAuth2/05.%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%AE%A4%E8%AF%81%E6%96%B9%E6%A1%88/03.%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A4%E8%AF%81%E6%96%B9%E6%A1%88.html",relativePath:"19.Spring Security OAuth2/05.分布式系统认证方案/03.分布式认证方案.md",key:"v-7afa71b5",path:"/pages/d6bec0/",headers:[{level:2,title:"基于session的认证方式",slug:"基于session的认证方式",normalizedTitle:"基于session的认证方式",charIndex:14},{level:2,title:"基于token的认证方式",slug:"基于token的认证方式",normalizedTitle:"基于token的认证方式",charIndex:451}],headersStr:"基于session的认证方式 基于token的认证方式",content:"# 分布式认证方案\n\n\n# 基于session的认证方式\n\n在分布式的环境下，基于session的认证会出现一个问题，每个应用服务都需要在session中存储用户身份信息，通 过负载均衡将本地的请求分配到另一个应用服务需要将session信息带过去，否则会重新认证。\n\n\n\n这个时候，通常的做法有下面几种:\n\n * Session复制：多台应用服务器之间同步session，使session保持一致，对外透明。\n * Session黏贴：当用户访问集群中某台服务器后，强制指定后续所有请求均落到此机器上。\n * Session集中存储：将Session存入分布式缓存中，所有服务器应用实例统一从分布式缓存中存取Session。\n\n总体来讲，基于session认证的认证方式，可以更好的在服务端对会话进行控制，且安全性较高。但是，session机 制方式基于cookie，在复杂多样的移动客户端上不能有效的使用，并且无法跨域，另外随着系统的扩展需提高 session的复制、黏贴及存储的容错性。\n\n\n# 基于token的认证方式\n\n基于token的认证方式，服务端不用存储认证数据，易维护扩展性强， 客户端可以把token 存在任意地方，并且可 以实现web和app统一认证机制。其缺点也很明显，token由于自包含信息，因此一般数据量较大，而且每次请求 都需要传递，因此比较占带宽。另外，token的签名验签操作也会给cpu带来额外的处理负担。\n\n",normalizedContent:"# 分布式认证方案\n\n\n# 基于session的认证方式\n\n在分布式的环境下，基于session的认证会出现一个问题，每个应用服务都需要在session中存储用户身份信息，通 过负载均衡将本地的请求分配到另一个应用服务需要将session信息带过去，否则会重新认证。\n\n\n\n这个时候，通常的做法有下面几种:\n\n * session复制：多台应用服务器之间同步session，使session保持一致，对外透明。\n * session黏贴：当用户访问集群中某台服务器后，强制指定后续所有请求均落到此机器上。\n * session集中存储：将session存入分布式缓存中，所有服务器应用实例统一从分布式缓存中存取session。\n\n总体来讲，基于session认证的认证方式，可以更好的在服务端对会话进行控制，且安全性较高。但是，session机 制方式基于cookie，在复杂多样的移动客户端上不能有效的使用，并且无法跨域，另外随着系统的扩展需提高 session的复制、黏贴及存储的容错性。\n\n\n# 基于token的认证方式\n\n基于token的认证方式，服务端不用存储认证数据，易维护扩展性强， 客户端可以把token 存在任意地方，并且可 以实现web和app统一认证机制。其缺点也很明显，token由于自包含信息，因此一般数据量较大，而且每次请求 都需要传递，因此比较占带宽。另外，token的签名验签操作也会给cpu带来额外的处理负担。\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"分布式认证实战",frontmatter:{title:"分布式认证实战",date:"2022-02-09T11:52:11.000Z",permalink:"/pages/7b210e/"},regularPath:"/19.Spring%20Security%20OAuth2/05.%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%AE%A4%E8%AF%81%E6%96%B9%E6%A1%88/04.%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A4%E8%AF%81%E5%AE%9E%E6%88%98.html",relativePath:"19.Spring Security OAuth2/05.分布式系统认证方案/04.分布式认证实战.md",key:"v-bee7b4c4",path:"/pages/7b210e/",headers:[{level:2,title:"技术方案",slug:"技术方案",normalizedTitle:"技术方案",charIndex:14},{level:2,title:"统一认证服务",slug:"统一认证服务",normalizedTitle:"统一认证服务",charIndex:245},{level:2,title:"API网关",slug:"api网关",normalizedTitle:"api网关",charIndex:452}],headersStr:"技术方案 统一认证服务 API网关",content:"# 分布式认证实战\n\n\n# 技术方案\n\n根据选型的分析，决定采用基于token的认证方式，它的优点是:\n\n * 适合统一认证的机制，客户端、一方应用、三方应用都遵循一致的认证机制。\n\n * token认证方式对第三方应用接入更适合，因为它更开放，可使用当前有流行的开放协议Oauth2.0、JWT等。\n\n * 一般情况服务端无需存储会话信息，减轻了服务端的压力。\n\n分布式系统认证技术方案见下图:\n\n\n\n流程描述:\n\n * (1)用户通过接入方(应用)登录，接入方采取OAuth2.0方式在统一认证服务(simplify-auth)中认证。\n\n * (2)认证服务(simplify-auth)调用验证该用户的身份是否合法，并获取用户权限信息。\n\n * (3)认证服务(simplify-auth)获取接入方权限信息，并验证接入方是否合法。\n\n * (4)若登录用户以及接入方都合法，认证服务生成jwt令牌返回给接入方，其中jwt中包含了用户权限及接入方权限。\n\n * (5)后续，接入方携带jwt令牌对API网关内的微服务资源进行访问。\n\n * (6)API网关对令牌解析、并验证接入方的权限是否能够访问本次请求的微服务。\n\n * (7)如果接入方的权限没问题，API网关将原请求header中附加解析后的明文Token，并将请求转发至微服务。\n\n * (8)微服务收到请求，明文token中包含登录用户的身份和权限信息。因此后续微服务自己可以干两件事:\n   \n   * 1、用户授权拦截(看当前用户是否有权访问该资源)\n   \n   * 2、将用户信息存储进当前线程上下文(有利于后续业务逻辑随时获取当前用户信息)\n\n流程所涉及到UAA服务、API网关这三个组件职责如下:\n\n\n# 统一认证服务\n\n它承载了OAuth2.0接入方认证、登入用户的认证、授权以及生成令牌的职责，完成实际的用户认证、授权功能。\n\n\n# API网关\n\n作为系统的唯一入口，API网关为接入方提供定制的API集合，它可能还具有其它职责，如身份验证、监控、负载均 衡、缓存等。API网关方式的核心要点是，所有的接入方和消费端都通过统一的网关接入微服务，在网关层处理所 有的非业务功能。",normalizedContent:"# 分布式认证实战\n\n\n# 技术方案\n\n根据选型的分析，决定采用基于token的认证方式，它的优点是:\n\n * 适合统一认证的机制，客户端、一方应用、三方应用都遵循一致的认证机制。\n\n * token认证方式对第三方应用接入更适合，因为它更开放，可使用当前有流行的开放协议oauth2.0、jwt等。\n\n * 一般情况服务端无需存储会话信息，减轻了服务端的压力。\n\n分布式系统认证技术方案见下图:\n\n\n\n流程描述:\n\n * (1)用户通过接入方(应用)登录，接入方采取oauth2.0方式在统一认证服务(simplify-auth)中认证。\n\n * (2)认证服务(simplify-auth)调用验证该用户的身份是否合法，并获取用户权限信息。\n\n * (3)认证服务(simplify-auth)获取接入方权限信息，并验证接入方是否合法。\n\n * (4)若登录用户以及接入方都合法，认证服务生成jwt令牌返回给接入方，其中jwt中包含了用户权限及接入方权限。\n\n * (5)后续，接入方携带jwt令牌对api网关内的微服务资源进行访问。\n\n * (6)api网关对令牌解析、并验证接入方的权限是否能够访问本次请求的微服务。\n\n * (7)如果接入方的权限没问题，api网关将原请求header中附加解析后的明文token，并将请求转发至微服务。\n\n * (8)微服务收到请求，明文token中包含登录用户的身份和权限信息。因此后续微服务自己可以干两件事:\n   \n   * 1、用户授权拦截(看当前用户是否有权访问该资源)\n   \n   * 2、将用户信息存储进当前线程上下文(有利于后续业务逻辑随时获取当前用户信息)\n\n流程所涉及到uaa服务、api网关这三个组件职责如下:\n\n\n# 统一认证服务\n\n它承载了oauth2.0接入方认证、登入用户的认证、授权以及生成令牌的职责，完成实际的用户认证、授权功能。\n\n\n# api网关\n\n作为系统的唯一入口，api网关为接入方提供定制的api集合，它可能还具有其它职责，如身份验证、监控、负载均 衡、缓存等。api网关方式的核心要点是，所有的接入方和消费端都通过统一的网关接入微服务，在网关层处理所 有的非业务功能。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"软件工程介绍",frontmatter:{title:"软件工程介绍",date:"2022-02-09T13:05:09.000Z",permalink:"/pages/11f8a5/"},regularPath:"/20.DevOps/01.%E5%85%A5%E9%97%A8/01.%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E4%BB%8B%E7%BB%8D.html",relativePath:"20.DevOps/01.入门/01.软件工程介绍.md",key:"v-7bad6842",path:"/pages/11f8a5/",headers:[{level:2,title:"软件开发生命周期 (SDLC)",slug:"软件开发生命周期-sdlc",normalizedTitle:"软件开发生命周期 (sdlc)",charIndex:13},{level:3,title:"计划和需求分析 (Planning and Requirement Analysis)",slug:"计划和需求分析-planning-and-requirement-analysis",normalizedTitle:"计划和需求分析 (planning and requirement analysis)",charIndex:116},{level:3,title:"设计项目架构 (Project Architecture)",slug:"设计项目架构-project-architecture",normalizedTitle:"设计项目架构 (project architecture)",charIndex:296},{level:3,title:"开发和编程 (Development and Coding)",slug:"开发和编程-development-and-coding",normalizedTitle:"开发和编程 (development and coding)",charIndex:447},{level:3,title:"测试 (Testing)",slug:"测试-testing",normalizedTitle:"测试 (testing)",charIndex:617},{level:3,title:"部署 (Deployment)",slug:"部署-deployment",normalizedTitle:"部署 (deployment)",charIndex:715},{level:2,title:"SDLC模型 (Software Development Lifecycle Model)",slug:"sdlc模型-software-development-lifecycle-model",normalizedTitle:"sdlc模型 (software development lifecycle model)",charIndex:852},{level:3,title:"瀑布SDLC模型",slug:"瀑布sdlc模型",normalizedTitle:"瀑布sdlc模型",charIndex:1177},{level:3,title:"迭代SDLC模型",slug:"迭代sdlc模型",normalizedTitle:"迭代sdlc模型",charIndex:1709},{level:3,title:"螺旋SDLC模型",slug:"螺旋sdlc模型",normalizedTitle:"螺旋sdlc模型",charIndex:2325},{level:3,title:"V形SDLC模型",slug:"v形sdlc模型",normalizedTitle:"v形sdlc模型",charIndex:2865},{level:3,title:"敏捷SDLC模型",slug:"敏捷sdlc模型",normalizedTitle:"敏捷sdlc模型",charIndex:3204}],headersStr:"软件开发生命周期 (SDLC) 计划和需求分析 (Planning and Requirement Analysis) 设计项目架构 (Project Architecture) 开发和编程 (Development and Coding) 测试 (Testing) 部署 (Deployment) SDLC模型 (Software Development Lifecycle Model) 瀑布SDLC模型 迭代SDLC模型 螺旋SDLC模型 V形SDLC模型 敏捷SDLC模型",content:"# 软件工程介绍\n\n\n# 软件开发生命周期 (SDLC)\n\n软件开发生命周期又叫做SDLC(Software Development Life Cycle)，它是集合了计划、开发、测试 和部署过程的集合。如下图所示 :\n\n\n\n\n# 计划和需求分析 (Planning and Requirement Analysis)\n\n每个软件开发生命周期模型都从分析开始，过程的利益相关者讨论对最终产品的要求。此阶段的目标是系统要求的详细定义。此外，还需要确保所有流程参与者都清楚地了解任务以及每个需求将如何实施。通常，讨论涉及质量保证专家，如果有必要，他们甚至可以在开发阶段干预过程中的添加。\n\n\n# 设计项目架构 (Project Architecture)\n\n在软件开发生命周期的第二阶段，开发人员实际上正在设计架构。所有利益相关者（包括客户）都会讨论此阶段可能出现的所有不同技术问题。此外，还定义了项目中使用的技术，团队负载，限制，时间范围和预算。最合适的项目决策是根据定义的要求做出的。\n\n\n# 开发和编程 (Development and Coding)\n\n在批准要求后，该过程进入下一阶段 - 实际开发。程序员从这里开始编写源代码，同时牢记先前定义的需求。系统管理员调整软件环境，前端程序员开发程序的用户界面以及与服务器交互的逻辑。 编程本身假设有四个阶段\n\n * 算法开发\n * 源代码编写\n * 汇编\n * 测试和调试\n\n\n# 测试 (Testing)\n\n测试阶段包括调试过程。开发过程中遗漏的所有代码缺陷都会在此处检测到，记录下来并传回给开发人员进行修复。重复测试过程，直到删除所有关键问题并且软件工作流程稳定。\n\n\n# 部署 (Deployment)\n\n当程序最终确定并且没有关键问题时 - 是时候为最终用户启动它了。新程序版本发布后，技术支持团队加入。该部门提供用户反馈; 在利用期间咨询和支持用户。此外，此阶段还包括所选组件的更新，以确保软件是最新的，并且不会受到安全漏洞的影响。\n\n\n# SDLC模型 (Software Development Lifecycle Model)\n\n从第一个也是最古老的“瀑布式”SDLC模型演变而来，它们的种类显着扩大。SDLC模型的多样性由众多产品类型预先确定 - 从简单的网站到复杂的医疗软件。如果你采用下面提到的SDLC模型之一作为基础 - 无论如何，它应该根据产品，项目和公司的特征进行调整。下面给出了最常用，最受欢迎和最重要的SDLC模型：\n\n * 瀑布模型 (Waterfall Model)\n * 迭代模型 (Iterative Model)\n * 螺旋模型 (Spiral Model)\n * V形模型 (V-Shape Model)\n * 敏捷模型 (Agile Model)\n\n\n# 瀑布SDLC模型\n\n瀑布 - 是一个级联SDLC模型，其中开发过程看起来像流程，一步一步地进行分析，预测，实现，测试，实施和支持阶段。该SDLC模型包括完全逐步执行每个阶段。该过程严格记录并预定义，具有该软件开发生命周期模型的每个阶段所期望的功能。\n\n\n\n好处                            劣势\n简单易用和理解                       只有在最后一个阶段结束后，软件才会准备就绪\n由于其刚性，管理简单：每个阶段都有明确的结果和流程审查   高风险和不确定性\n发展阶段逐一进行                      不是复杂和面向对象项目的最佳选择\n适用于要求明确且不模棱两可的小型或中型项目         不适合长期项目\n易于确定开发周期中的关键点                 阶段的进展很难衡量，但仍处于发展阶段\n易于分类和确定任务的优先级                 集成在最后完成，不提供预先识别问题的选项\n\nWaterfall SDLC模型的用例：\n\n * 准确记录了这些要求\n * 产品定义稳定\n * 技术堆栈是预定义的，这使得它不是动态的\n * 没有模棱两可的要求\n * 该项目很短\n\n\n# 迭代SDLC模型\n\n在项目开始之前，迭代SDLC模型不需要完整的需求列表。开发过程可以从对功能部件的要求开始，可以在以后扩展。该过程是重复的，允许为每个循环制作新版本的产品。每次迭代（持续两到六周）都包括开发系统的单独组件，然后，将此组件添加到之前开发的功能中。说到数学术语，迭代模型是顺序逼近方法的实现; 这意味着逐渐接近计划的最终产品形状。\n\n\n\n好处                              劣势\n某些功能可以在开发生命周期的开始阶段快速开发          迭代模型比瀑布模型需要更多资源\n可以应用并行开发                        需要持续管理\n进展很容易衡量                         可能会出现架构或设计问题，因为在短期规划阶段并未预见到所有要求\n较短的迭代是 - 更容易的测试和调试阶段            小项目的糟糕选择\n由于首先完成高风险任务，因此更容易控制风险           这个过程很难管理\n在下一个sprint中可以防止在一次迭代中定义的问题和风险   即使在项目的最后阶段，风险也可能无法完全确定\n灵活性和准备变化的要求                     风险分析需要高素质专家的参与\n\n迭代模型的用例：\n\n * 最终产品的要求是严格预定义的\n * 适用于大型项目\n * 主要任务是预定义的，但细节可能随着时间而推进\n\n\n# 螺旋SDLC模型\n\n螺旋模型 - 是SDLC模型，它分阶段结合了架构和原型。它是Iterative和Waterfall SDLC模型的组合，具有重要的风险分析重点。螺旋模型的主要问题是确定进入下一阶段的正确时机。建议将初步设定的时间范围作为此问题的解决方案。即使前一阶段的工作尚未完成，也将根据计划完成向下一阶段的转变。该计划是根据统计数据引入的，即使从个人开发人员的经验来看，也可以在之前的项目中收到。\n\n\n\n好处                                   劣势\n生命周期分为小部分，如果风险集中度较高，则可以提前完成阶段以解决问题   可能相当昂贵\n开发过程准确记录，可根据变化进行扩展                   风险控制需要高技能专业人员的参与\n可伸缩性允许在相对较晚的阶段进行更改并添加新功能             对小项目可能无效\n早期的工作原型已经完成 - 用户可以更快地指出这些缺陷          大量的中间阶段需要过多的文档\n\n旋模型的用例\n\n * 客户不确定要求\n * 预计在开发周期中会进行重大编辑\n * 具有中级或高级风险的项目，防止这些风险非常重要\n * 应该在几个阶段发布的新产品，以获得足够的客户反馈\n\n\n# V形SDLC模型\n\nV形SDLC模型是经典瀑布模型的扩展，它基于每个开发阶段的相关测试阶段。这是一个非常严格的模型，下一阶段仅在前一阶段之后开始。这也称为“验证和验证”模型。每个阶段都有当前的过程控制，以确保可以转换到下一个阶段。\n\n\n\n好处                         劣势\nV形模型的每个阶段都有严格的结果，因此很容易控制   缺乏灵活性\n测试和验证在早期阶段进行               小项目的糟糕选择\n适用于需求稳定且清晰的小型项目            相对较大的风险\n\nV形模型的用例：\n\n * 对于需要进行准确产品测试的项目\n * 适用于中小型项目，其中严格要求严格要求\n * 获得所需资格的工程师，特别是测试人员，可轻松到达。\n\n\n# 敏捷SDLC模型\n\n在每次开发迭代之后的敏捷方法中，客户能够看到结果并理解他是否满意或不满意。这是敏捷软件开发生命周期模型的优势之一。其缺点之一是，由于缺乏明确的要求，很难估计资源和开发成本。极限编程是敏捷模型的实际应用之一。这种模型的基础包括每周短暂的会议 - Sprint是Scrum方法的一部分。\n\n\n\n好处                       劣势\n功能需求的更正被实施到开发过程中以提供竞争力   由于永久性变化而无法衡量最终成本\n项目按短而透明的迭代划分             团队应该是高度专业和客户导向的\n灵活的变更过程使风险最小化            新要求可能与现有架构冲突\n快速发布第一个产品版本              通过所有更正和更改，项目可能会超出预期时间\n\n敏捷模型的用例：\n\n * 用户需求动态变化\n * 由于许多迭代，实施的更改的价格更低\n * 与瀑布模型不同，它只需要初始计划来启动项目",normalizedContent:"# 软件工程介绍\n\n\n# 软件开发生命周期 (sdlc)\n\n软件开发生命周期又叫做sdlc(software development life cycle)，它是集合了计划、开发、测试 和部署过程的集合。如下图所示 :\n\n\n\n\n# 计划和需求分析 (planning and requirement analysis)\n\n每个软件开发生命周期模型都从分析开始，过程的利益相关者讨论对最终产品的要求。此阶段的目标是系统要求的详细定义。此外，还需要确保所有流程参与者都清楚地了解任务以及每个需求将如何实施。通常，讨论涉及质量保证专家，如果有必要，他们甚至可以在开发阶段干预过程中的添加。\n\n\n# 设计项目架构 (project architecture)\n\n在软件开发生命周期的第二阶段，开发人员实际上正在设计架构。所有利益相关者（包括客户）都会讨论此阶段可能出现的所有不同技术问题。此外，还定义了项目中使用的技术，团队负载，限制，时间范围和预算。最合适的项目决策是根据定义的要求做出的。\n\n\n# 开发和编程 (development and coding)\n\n在批准要求后，该过程进入下一阶段 - 实际开发。程序员从这里开始编写源代码，同时牢记先前定义的需求。系统管理员调整软件环境，前端程序员开发程序的用户界面以及与服务器交互的逻辑。 编程本身假设有四个阶段\n\n * 算法开发\n * 源代码编写\n * 汇编\n * 测试和调试\n\n\n# 测试 (testing)\n\n测试阶段包括调试过程。开发过程中遗漏的所有代码缺陷都会在此处检测到，记录下来并传回给开发人员进行修复。重复测试过程，直到删除所有关键问题并且软件工作流程稳定。\n\n\n# 部署 (deployment)\n\n当程序最终确定并且没有关键问题时 - 是时候为最终用户启动它了。新程序版本发布后，技术支持团队加入。该部门提供用户反馈; 在利用期间咨询和支持用户。此外，此阶段还包括所选组件的更新，以确保软件是最新的，并且不会受到安全漏洞的影响。\n\n\n# sdlc模型 (software development lifecycle model)\n\n从第一个也是最古老的“瀑布式”sdlc模型演变而来，它们的种类显着扩大。sdlc模型的多样性由众多产品类型预先确定 - 从简单的网站到复杂的医疗软件。如果你采用下面提到的sdlc模型之一作为基础 - 无论如何，它应该根据产品，项目和公司的特征进行调整。下面给出了最常用，最受欢迎和最重要的sdlc模型：\n\n * 瀑布模型 (waterfall model)\n * 迭代模型 (iterative model)\n * 螺旋模型 (spiral model)\n * v形模型 (v-shape model)\n * 敏捷模型 (agile model)\n\n\n# 瀑布sdlc模型\n\n瀑布 - 是一个级联sdlc模型，其中开发过程看起来像流程，一步一步地进行分析，预测，实现，测试，实施和支持阶段。该sdlc模型包括完全逐步执行每个阶段。该过程严格记录并预定义，具有该软件开发生命周期模型的每个阶段所期望的功能。\n\n\n\n好处                            劣势\n简单易用和理解                       只有在最后一个阶段结束后，软件才会准备就绪\n由于其刚性，管理简单：每个阶段都有明确的结果和流程审查   高风险和不确定性\n发展阶段逐一进行                      不是复杂和面向对象项目的最佳选择\n适用于要求明确且不模棱两可的小型或中型项目         不适合长期项目\n易于确定开发周期中的关键点                 阶段的进展很难衡量，但仍处于发展阶段\n易于分类和确定任务的优先级                 集成在最后完成，不提供预先识别问题的选项\n\nwaterfall sdlc模型的用例：\n\n * 准确记录了这些要求\n * 产品定义稳定\n * 技术堆栈是预定义的，这使得它不是动态的\n * 没有模棱两可的要求\n * 该项目很短\n\n\n# 迭代sdlc模型\n\n在项目开始之前，迭代sdlc模型不需要完整的需求列表。开发过程可以从对功能部件的要求开始，可以在以后扩展。该过程是重复的，允许为每个循环制作新版本的产品。每次迭代（持续两到六周）都包括开发系统的单独组件，然后，将此组件添加到之前开发的功能中。说到数学术语，迭代模型是顺序逼近方法的实现; 这意味着逐渐接近计划的最终产品形状。\n\n\n\n好处                              劣势\n某些功能可以在开发生命周期的开始阶段快速开发          迭代模型比瀑布模型需要更多资源\n可以应用并行开发                        需要持续管理\n进展很容易衡量                         可能会出现架构或设计问题，因为在短期规划阶段并未预见到所有要求\n较短的迭代是 - 更容易的测试和调试阶段            小项目的糟糕选择\n由于首先完成高风险任务，因此更容易控制风险           这个过程很难管理\n在下一个sprint中可以防止在一次迭代中定义的问题和风险   即使在项目的最后阶段，风险也可能无法完全确定\n灵活性和准备变化的要求                     风险分析需要高素质专家的参与\n\n迭代模型的用例：\n\n * 最终产品的要求是严格预定义的\n * 适用于大型项目\n * 主要任务是预定义的，但细节可能随着时间而推进\n\n\n# 螺旋sdlc模型\n\n螺旋模型 - 是sdlc模型，它分阶段结合了架构和原型。它是iterative和waterfall sdlc模型的组合，具有重要的风险分析重点。螺旋模型的主要问题是确定进入下一阶段的正确时机。建议将初步设定的时间范围作为此问题的解决方案。即使前一阶段的工作尚未完成，也将根据计划完成向下一阶段的转变。该计划是根据统计数据引入的，即使从个人开发人员的经验来看，也可以在之前的项目中收到。\n\n\n\n好处                                   劣势\n生命周期分为小部分，如果风险集中度较高，则可以提前完成阶段以解决问题   可能相当昂贵\n开发过程准确记录，可根据变化进行扩展                   风险控制需要高技能专业人员的参与\n可伸缩性允许在相对较晚的阶段进行更改并添加新功能             对小项目可能无效\n早期的工作原型已经完成 - 用户可以更快地指出这些缺陷          大量的中间阶段需要过多的文档\n\n旋模型的用例\n\n * 客户不确定要求\n * 预计在开发周期中会进行重大编辑\n * 具有中级或高级风险的项目，防止这些风险非常重要\n * 应该在几个阶段发布的新产品，以获得足够的客户反馈\n\n\n# v形sdlc模型\n\nv形sdlc模型是经典瀑布模型的扩展，它基于每个开发阶段的相关测试阶段。这是一个非常严格的模型，下一阶段仅在前一阶段之后开始。这也称为“验证和验证”模型。每个阶段都有当前的过程控制，以确保可以转换到下一个阶段。\n\n\n\n好处                         劣势\nv形模型的每个阶段都有严格的结果，因此很容易控制   缺乏灵活性\n测试和验证在早期阶段进行               小项目的糟糕选择\n适用于需求稳定且清晰的小型项目            相对较大的风险\n\nv形模型的用例：\n\n * 对于需要进行准确产品测试的项目\n * 适用于中小型项目，其中严格要求严格要求\n * 获得所需资格的工程师，特别是测试人员，可轻松到达。\n\n\n# 敏捷sdlc模型\n\n在每次开发迭代之后的敏捷方法中，客户能够看到结果并理解他是否满意或不满意。这是敏捷软件开发生命周期模型的优势之一。其缺点之一是，由于缺乏明确的要求，很难估计资源和开发成本。极限编程是敏捷模型的实际应用之一。这种模型的基础包括每周短暂的会议 - sprint是scrum方法的一部分。\n\n\n\n好处                       劣势\n功能需求的更正被实施到开发过程中以提供竞争力   由于永久性变化而无法衡量最终成本\n项目按短而透明的迭代划分             团队应该是高度专业和客户导向的\n灵活的变更过程使风险最小化            新要求可能与现有架构冲突\n快速发布第一个产品版本              通过所有更正和更改，项目可能会超出预期时间\n\n敏捷模型的用例：\n\n * 用户需求动态变化\n * 由于许多迭代，实施的更改的价格更低\n * 与瀑布模型不同，它只需要初始计划来启动项目",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"持续继承流程说明",frontmatter:{title:"持续继承流程说明",date:"2022-02-09T13:05:09.000Z",permalink:"/pages/5b4493/"},regularPath:"/20.DevOps/01.%E5%85%A5%E9%97%A8/02.%E6%8C%81%E7%BB%AD%E7%BB%A7%E6%89%BF%E6%B5%81%E7%A8%8B%E8%AF%B4%E6%98%8E.html",relativePath:"20.DevOps/01.入门/02.持续继承流程说明.md",key:"v-d8f92f9e",path:"/pages/5b4493/",headersStr:null,content:"# 持续继承流程说明\n\n\n\n1 )首先，开发人员每天进行代码提交，提交到Git仓库\n\n2 )然后，Jenkins作为持续集成工具，使用Git工具到Git仓库拉取代码到集成服务器，再配合JDK， Maven等软件完成代码编译，代码测试与审查，测试，打包等工作，在这个过程中每一步出错，都重新 再执行一次整个流程。\n\n3 )最后，Jenkins把生成的jar或war包分发到测试服务器或者生产服务器，测试人员或用户就可以访问 应用。",normalizedContent:"# 持续继承流程说明\n\n\n\n1 )首先，开发人员每天进行代码提交，提交到git仓库\n\n2 )然后，jenkins作为持续集成工具，使用git工具到git仓库拉取代码到集成服务器，再配合jdk， maven等软件完成代码编译，代码测试与审查，测试，打包等工作，在这个过程中每一步出错，都重新 再执行一次整个流程。\n\n3 )最后，jenkins把生成的jar或war包分发到测试服务器或者生产服务器，测试人员或用户就可以访问 应用。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Gitlab代码托管服务器安装",frontmatter:{title:"Gitlab代码托管服务器安装",date:"2022-02-09T13:05:09.000Z",permalink:"/pages/eaeddc/"},regularPath:"/20.DevOps/01.%E5%85%A5%E9%97%A8/03.Gitlab%E4%BB%A3%E7%A0%81%E6%89%98%E7%AE%A1%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85.html",relativePath:"20.DevOps/01.入门/03.Gitlab代码托管服务器安装.md",key:"v-f6cd3ec8",path:"/pages/eaeddc/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:22},{level:2,title:"硬件要求",slug:"硬件要求",normalizedTitle:"硬件要求",charIndex:117},{level:2,title:"安装依赖",slug:"安装依赖",normalizedTitle:"安装依赖",charIndex:237},{level:2,title:"下载",slug:"下载",normalizedTitle:"下载",charIndex:758},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:15},{level:2,title:"配置",slug:"配置",normalizedTitle:"配置",charIndex:145},{level:2,title:"重启",slug:"重启",normalizedTitle:"重启",charIndex:3096},{level:2,title:"首次访问",slug:"首次访问",normalizedTitle:"首次访问",charIndex:3177}],headersStr:"概述 硬件要求 安装依赖 下载 安装 配置 重启 首次访问",content:"# Gitlab代码托管服务器安装\n\n\n# 概述\n\n\n\nGitLab 是一个用于仓库管理系统的开源项目，使用Git作为代码管理工具，并在此基础上搭建起来的Web服务。安装方法是参考GitLab在GitHub上的Wiki页面。\n\n\n# 硬件要求\n\n下表列出了部署Gitlab的最低和建议的硬件配置。\n\n资源     最小配置    推荐配置\nCPU    4 CPU   8 CPU\nMem    4 GB    8 GB\nDisk   40 GB   160 GB\n\n\n# 安装依赖\n\n * CentOS\n   \n   在 CentOS 7/8 上，下面的命令也会在系统防火墙中打开 HTTP、HTTPS 和 SSH 访问。 这是一个可选步骤，如果您打算仅从本地网络访问 GitLab 或 关闭防火墙，则可以跳过它。\n   \n   sudo yum install -y curl policycoreutils-python openssh-server perl\n   sudo systemctl enable sshd\n   sudo systemctl start sshd\n   sudo firewall-cmd --permanent --add-service=http\n   sudo firewall-cmd --permanent --add-service=https\n   sudo systemctl reload firewalld\n   \n\n * Ubuntu\n   \n   sudo apt-get update\n   sudo apt-get install -y curl openssh-server ca-certificates tzdata perl\n   \n\n\n# 下载\n\n官方社区版本: GitLab CE\n\nNAME                                    DISTRO/VERSION   OS\ngitlab-ce-13.12.4-ce.0.el7.x86_64.rpm   el/7             CentOS 7\ngitlab-ce-13.12.4-ce.0.el8.x86_64.rpm   el/8             CentOS 8\ngitlab-ce_13.12.4-ce.0_amd64.deb        ubuntu/bionic    Ubuntu 18.04 LTS\ngitlab-ce_13.12.4-ce.0_amd64.deb        ubuntu/focal     Ubuntu 20.04 LTS\n\n根据操作系统版本选择对应软件包版本，点击右上角Download按钮下载软件包\n\n\n# 安装\n\n下载 GitLab 包后，使用以下命令安装它：\n\n# GitLab Community Edition\n# Debian/Ubuntu\nsudo dpkg -i gitlab-ce_<version>.deb\n\n# CentOS/RHEL\nsudo rpm -Uvh gitlab-ce-<version>.rpm\n\n\n安装过程需要些时间，如果出现日志，则说明安装成功。\n\nubuntu@ubuntu:~$ sudo dpkg -i gitlab-ce_13.12.4-ce.0_amd64.deb \nSelecting previously unselected package gitlab-ce.\n(Reading database ... 71349 files and directories currently installed.)\nPreparing to unpack gitlab-ce_13.12.4-ce.0_amd64.deb ...\nUnpacking gitlab-ce (13.12.4-ce.0) ...\nSetting up gitlab-ce (13.12.4-ce.0) ...\nIt looks like GitLab has not been configured yet; skipping the upgrade script.\n\n       *.                  *.\n      ***                 ***\n     *****               *****\n    .******             *******\n    ********            ********\n   ,,,,,,,,,***********,,,,,,,,,\n  ,,,,,,,,,,,*********,,,,,,,,,,,\n  .,,,,,,,,,,,*******,,,,,,,,,,,,\n      ,,,,,,,,,*****,,,,,,,,,.\n         ,,,,,,,****,,,,,,\n            .,,,***,,,,\n                ,*,.\n  \n\n\n     _______ __  __          __\n    / ____(_) /_/ /   ____ _/ /_\n   / / __/ / __/ /   / __ `/ __ \\\n  / /_/ / / /_/ /___/ /_/ / /_/ /\n  \\____/_/\\__/_____/\\__,_/_.___/\n  \n\nThank you for installing GitLab!\nGitLab was unable to detect a valid hostname for your instance.\nPlease configure a URL for your GitLab instance by setting `external_url`\nconfiguration in /etc/gitlab/gitlab.rb file.\nThen, you can start your GitLab instance by running the following command:\n  sudo gitlab-ctl reconfigure\n\nFor a comprehensive list of configuration options please see the Omnibus GitLab readme\nhttps://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/01.软件工程介绍.md\n\nHelp us improve the installation experience, let us know how we did with a 1 minute survey:\nhttps://gitlab.fra1.qualtrics.com/jfe/form/SV_6kVqZANThUQ1bZb?installation=omnibus&release=13-12\n\n\n\n# 配置\n\n修改gitlab配置文件指定服务器ip和自定义端口：\n\nsudo vi /etc/gitlab/gitlab.rb\n\n\n找到external_url 'http://gitlab.example.com'并修改为当前服务器IP或域名地址。\n\n\n# 重启\n\nsudo gitlab-ctl reconfigure\n\n\n提示\n\n重新配置gitlab可能消耗几分钟的时间，根据硬件配置有关，请耐心等待即可。\n\n\n# 首次访问\n\n在您第一次访问时，您将被重定向到密码重置屏幕。 提供初始管理员帐户的密码，您将被重定向回登录页面。 使用默认帐户的用户名 root 登录。\n\n\n\n有关安装和配置的详细说明，请参阅我们的文档。",normalizedContent:"# gitlab代码托管服务器安装\n\n\n# 概述\n\n\n\ngitlab 是一个用于仓库管理系统的开源项目，使用git作为代码管理工具，并在此基础上搭建起来的web服务。安装方法是参考gitlab在github上的wiki页面。\n\n\n# 硬件要求\n\n下表列出了部署gitlab的最低和建议的硬件配置。\n\n资源     最小配置    推荐配置\ncpu    4 cpu   8 cpu\nmem    4 gb    8 gb\ndisk   40 gb   160 gb\n\n\n# 安装依赖\n\n * centos\n   \n   在 centos 7/8 上，下面的命令也会在系统防火墙中打开 http、https 和 ssh 访问。 这是一个可选步骤，如果您打算仅从本地网络访问 gitlab 或 关闭防火墙，则可以跳过它。\n   \n   sudo yum install -y curl policycoreutils-python openssh-server perl\n   sudo systemctl enable sshd\n   sudo systemctl start sshd\n   sudo firewall-cmd --permanent --add-service=http\n   sudo firewall-cmd --permanent --add-service=https\n   sudo systemctl reload firewalld\n   \n\n * ubuntu\n   \n   sudo apt-get update\n   sudo apt-get install -y curl openssh-server ca-certificates tzdata perl\n   \n\n\n# 下载\n\n官方社区版本: gitlab ce\n\nname                                    distro/version   os\ngitlab-ce-13.12.4-ce.0.el7.x86_64.rpm   el/7             centos 7\ngitlab-ce-13.12.4-ce.0.el8.x86_64.rpm   el/8             centos 8\ngitlab-ce_13.12.4-ce.0_amd64.deb        ubuntu/bionic    ubuntu 18.04 lts\ngitlab-ce_13.12.4-ce.0_amd64.deb        ubuntu/focal     ubuntu 20.04 lts\n\n根据操作系统版本选择对应软件包版本，点击右上角download按钮下载软件包\n\n\n# 安装\n\n下载 gitlab 包后，使用以下命令安装它：\n\n# gitlab community edition\n# debian/ubuntu\nsudo dpkg -i gitlab-ce_<version>.deb\n\n# centos/rhel\nsudo rpm -uvh gitlab-ce-<version>.rpm\n\n\n安装过程需要些时间，如果出现日志，则说明安装成功。\n\nubuntu@ubuntu:~$ sudo dpkg -i gitlab-ce_13.12.4-ce.0_amd64.deb \nselecting previously unselected package gitlab-ce.\n(reading database ... 71349 files and directories currently installed.)\npreparing to unpack gitlab-ce_13.12.4-ce.0_amd64.deb ...\nunpacking gitlab-ce (13.12.4-ce.0) ...\nsetting up gitlab-ce (13.12.4-ce.0) ...\nit looks like gitlab has not been configured yet; skipping the upgrade script.\n\n       *.                  *.\n      ***                 ***\n     *****               *****\n    .******             *******\n    ********            ********\n   ,,,,,,,,,***********,,,,,,,,,\n  ,,,,,,,,,,,*********,,,,,,,,,,,\n  .,,,,,,,,,,,*******,,,,,,,,,,,,\n      ,,,,,,,,,*****,,,,,,,,,.\n         ,,,,,,,****,,,,,,\n            .,,,***,,,,\n                ,*,.\n  \n\n\n     _______ __  __          __\n    / ____(_) /_/ /   ____ _/ /_\n   / / __/ / __/ /   / __ `/ __ \\\n  / /_/ / / /_/ /___/ /_/ / /_/ /\n  \\____/_/\\__/_____/\\__,_/_.___/\n  \n\nthank you for installing gitlab!\ngitlab was unable to detect a valid hostname for your instance.\nplease configure a url for your gitlab instance by setting `external_url`\nconfiguration in /etc/gitlab/gitlab.rb file.\nthen, you can start your gitlab instance by running the following command:\n  sudo gitlab-ctl reconfigure\n\nfor a comprehensive list of configuration options please see the omnibus gitlab readme\nhttps://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/01.软件工程介绍.md\n\nhelp us improve the installation experience, let us know how we did with a 1 minute survey:\nhttps://gitlab.fra1.qualtrics.com/jfe/form/sv_6kvqzanthuq1bzb?installation=omnibus&release=13-12\n\n\n\n# 配置\n\n修改gitlab配置文件指定服务器ip和自定义端口：\n\nsudo vi /etc/gitlab/gitlab.rb\n\n\n找到external_url 'http://gitlab.example.com'并修改为当前服务器ip或域名地址。\n\n\n# 重启\n\nsudo gitlab-ctl reconfigure\n\n\n提示\n\n重新配置gitlab可能消耗几分钟的时间，根据硬件配置有关，请耐心等待即可。\n\n\n# 首次访问\n\n在您第一次访问时，您将被重定向到密码重置屏幕。 提供初始管理员帐户的密码，您将被重定向回登录页面。 使用默认帐户的用户名 root 登录。\n\n\n\n有关安装和配置的详细说明，请参阅我们的文档。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Jenkins自动化服务器安装",frontmatter:{title:"Jenkins自动化服务器安装",date:"2022-02-09T13:05:09.000Z",permalink:"/pages/605fde/"},regularPath:"/20.DevOps/01.%E5%85%A5%E9%97%A8/04.Jenkins%E8%87%AA%E5%8A%A8%E5%8C%96%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85.html",relativePath:"20.DevOps/01.入门/04.Jenkins自动化服务器安装.md",key:"v-d7bef6c8",path:"/pages/605fde/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:22},{level:2,title:"硬件要求",slug:"硬件要求",normalizedTitle:"硬件要求",charIndex:462},{level:2,title:"安装 Java",slug:"安装-java",normalizedTitle:"安装 java",charIndex:582},{level:2,title:"安装 Jenkins",slug:"安装-jenkins",normalizedTitle:"安装 jenkins",charIndex:1274},{level:3,title:"Ubuntu",slug:"ubuntu",normalizedTitle:"ubuntu",charIndex:1031},{level:3,title:"CentOS",slug:"centos",normalizedTitle:"centos",charIndex:1608},{level:2,title:"启动 Jenkins",slug:"启动-jenkins",normalizedTitle:"启动 jenkins",charIndex:2365},{level:2,title:"安装后设置向导",slug:"安装后设置向导",normalizedTitle:"安装后设置向导",charIndex:3160},{level:3,title:"解锁 Jenkins",slug:"解锁-jenkins",normalizedTitle:"解锁 jenkins",charIndex:3172},{level:3,title:"使用插件自定义 Jenkins",slug:"使用插件自定义-jenkins",normalizedTitle:"使用插件自定义 jenkins",charIndex:4614},{level:3,title:"创建第一个管理员用户",slug:"创建第一个管理员用户",normalizedTitle:"创建第一个管理员用户",charIndex:4966},{level:2,title:"Jenkins 欢迎页面",slug:"jenkins-欢迎页面",normalizedTitle:"jenkins 欢迎页面",charIndex:5258}],headersStr:"概述 硬件要求 安装 Java 安装 Jenkins Ubuntu CentOS 启动 Jenkins 安装后设置向导 解锁 Jenkins 使用插件自定义 Jenkins 创建第一个管理员用户 Jenkins 欢迎页面",content:'# Jenkins自动化服务器安装\n\n\n# 概述\n\n\n\nJenkins 是一款流行的开源持续集成(Continuous Integration)工具，广泛用于项目开发，具有自动 化构建、测试和部署等功能。官网: http://jenkins-ci.org/。\n\nJenkins的特征:\n\n * 开源的Java语言开发持续集成工具，支持持续集成，持续部署。\n * 易于安装部署配置:可通过yum安装,或下载war包以及通过docker容器等快速实现安装部署，可方便web界面配置管理。\n * 消息通知及测试报告:集成RSS/E-mail通过RSS发布构建结果或当构建完成时通过e-mail通知，生成JUnit/TestNG测试报告。\n * 分布式构建:支持Jenkins能够让多台计算机一起构建/测试。\n * 文件识别:Jenkins能够跟踪哪次构建生成哪些jar，哪次构建使用哪个版本的jar等。\n * 丰富的插件支持:支持扩展插件，你可以开发适合自己团队使用的工具，如git，svn，maven，docker等。\n\n\n# 硬件要求\n\n下表列出了部署Jenkins的最低和建议的硬件配置。\n\n资源     最小配置    推荐配置\nCPU    2 CPU   4 CPU\nMem    1 GB    4 GB\nDisk   10 GB   40 GB\n\n\n# 安装 Java\n\nJenkins需要Java才能运行，但某些发行版默认不包含此功能，并且某些 Java 版本与 Jenkins 不兼容。\n\n您可以使用多种 Java 实现。 OpenJDK 是目前最流行的，我们将在本指南中使用它。\n\n要安装 Open Java Development Kit (OpenJDK)，请运行以下命令：\n\n * 更新存储库:\n\nsudo apt update\n\n\n * 搜索所有可用的包 :\n\nsudo apt search openjdk\n\n\n * 选择一个选项并安装它 :\n\nsudo apt install openjdk-8-jdk\n\n\n * 确认安装:\n\nsudo apt install openjdk-8-jdk\n\n\n * 检查安装 :\n\njava -version\n\n\n * 检查结果:\n\nopenjdk version "11.0.11" 2021-04-20\nOpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04)\nOpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n\n\n提示\n\n为什么使用 apt 而不是 apt-get 或其他命令？ apt 命令自 2014 年开始可用。它具有类似于 apt-get 的命令结构，但旨在为典型用户提供更愉快的体验。 使用 apt 可以更轻松地执行简单的软件管理任务，例如安装、搜索和删除。\n\n\n# 安装 Jenkins\n\n\n# Ubuntu\n\n在 Debian 和基于 Debian 的发行版（如 Ubuntu）上，您可以通过 apt 安装 Jenkins。\n\nwget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add -\nsudo sh -c \'echo deb https://pkg.jenkins.io/debian-stable binary/ > \\\n    /etc/apt/sources.list.d/jenkins.list\'\nsudo apt-get update\nsudo apt-get install jenkins\n\n\n\n# CentOS\n\n您可以通过 yum 在 Red Hat Enterprise Linux、CentOS 和其他基于 Red Hat 的发行版上安装 Jenkins。\n\nsudo wget -O /etc/yum.repos.d/jenkins.repo \\\n    https://pkg.jenkins.io/redhat-stable/jenkins.repo\nsudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key\nsudo yum upgrade\nsudo yum install jenkins java-11-openjdk-devel\nsudo systemctl daemon-reload\n\n\n此软件包安装将：\n\n * 将Jenkins设置为启动时启动的守护进程。有关更多详细信息，请参阅 /etc/init.d/jenkins。\n * 创建一个jenkins用户来运行这个服务。\n * 直接控制台日志输出到文件 /var/log/jenkins/jenkins.log。如果您正在对Jenkins进行故障排除，请检查此文件。\n * 使用启动的配置参数填充 /etc/default/jenkins，例如JENKINS_HOME\n * 将Jenkins设置为侦听端口8080。使用浏览器访问此端口以开始配置。\n\n如何修改默认服务端口？\n\n如果您的/etc/init.d/jenkins文件无法启动Jenkins，请编辑/etc/default/jenkins将----HTTP_PORT=8080----行替换为----HTTP_PORT=8081---此处选择了8081，但您可以设置另一个可用端口。\n\n\n# 启动 Jenkins\n\n使用命令注册 Jenkins 服务:\n\nsudo systemctl daemon-reload\n\n\n您可以使用以下命令启动 Jenkins 服务:\n\nsudo systemctl start jenkins\n\n\n您可以使用命令检查 Jenkins 服务的状态:\n\nsudo systemctl status jenkins\n\n\n如果一切都设置正确，你应该看到这样的输出:\n\nLoaded: loaded (/etc/rc.d/init.d/jenkins; generated)\nActive: active (running) since Tue 2018-11-13 16:19:01 +03; 4min 57s ago\n\n\n注意\n\n如果您安装了防火墙，则必须将 Jenkins 添加为例外。 您必须将下面脚本中的 YOURPORT 更改为要使用的端口。 端口 8080 是最常见的。\n\nYOURPORT=8080\nPERM="--permanent"\nSERV="$PERM --service=jenkins"\n\nfirewall-cmd $PERM --new-service=jenkins\nfirewall-cmd $SERV --set-short="Jenkins ports"\nfirewall-cmd $SERV --set-description="Jenkins port exceptions"\nfirewall-cmd $SERV --add-port=$YOURPORT/tcp\nfirewall-cmd $PERM --add-service=jenkins\nfirewall-cmd --zone=public --add-service=http --permanent\nfirewall-cmd --reload\n\n\n\n# 安装后设置向导\n\n\n# 解锁 Jenkins\n\n当您第一次访问一个新的 Jenkins 实例时，系统会要求您使用自动生成的密码将其解锁。\n\n1.浏览到 http://localhost:8080（或您在安装时为 Jenkins 配置的任何端口）并等待 Unlock Jenkins 页面出现。\n\n2.从 Jenkins 控制台日志输出中，复制自动生成的字母数字密码（在 2 组星号之间）。\n\ncat /var/log/jenkins/jenkins.log\n\n\nJenkins 启动日志如下：\n\n...\n2021-06-15 10:36:55.529+0000 [id=28]    INFO    jenkins.install.SetupWizard#init: \n\n*************************************************************\n*************************************************************\n*************************************************************\n\nJenkins initial setup is required. An admin user has been created and a password generated.\nPlease use the following password to proceed to installation:\n\n7400569b100b47c2b86c2e674c7123f3\n\nThis may also be found at: /var/lib/jenkins/secrets/initialAdminPassword\n\n*************************************************************\n*************************************************************\n*************************************************************\n\n2021-06-15 10:37:27.106+0000 [id=28]    INFO    jenkins.InitReactorRunner$1#onAttained: Completed initialization\n2021-06-15 10:37:27.153+0000 [id=22]    INFO    hudson.WebAppMain$3#run: Jenkins is fully up and running\n...\n\n\n提示\n\n执行命令： sudo cat /var/lib/jenkins/secrets/initialAdminPassword 将在控制台打印密码。\n\n如果您使用官方 jenkins/jenkins 映像在 Docker 中运行 Jenkins，则可以使用 sudo docker exec ${CONTAINER_ID or CONTAINER_NAME} cat /var/jenkins_home/secrets/initialAdminPassword 在控制台中打印密码，而无需执行到容器中。\n\n\n# 使用插件自定义 Jenkins\n\n解锁Jenkins后，会出现Customize Jenkins页面。 作为初始设置的一部分，您可以在此处安装任意数量的有用插件。\n\n单击显示的两个选项之一：\n\n * 安装建议的插件 - 安装推荐的插件集，这些插件基于最常见的用例。\n\n * 选择要安装的插件 - 选择最初安装的插件集。 当您第一次访问插件选择页面时，默认情况下会选择建议的插件。\n\n提示\n\n如果您不确定需要哪些插件，请选择安装建议的插件。 您可以稍后通过Manage Jenkins > Manage Plugins 页面安装（或删除）其他 Jenkins 插件。\n\n\n\n设置向导显示正在配置的 Jenkins 的进度以及正在安装的您选择的 Jenkins 插件集。 此过程可能需要几分钟时间。\n\n\n# 创建第一个管理员用户\n\n最后，在使用插件自定义 Jenkins 之后，Jenkins 要求您创建您的第一个管理员用户。\n\n1.当“创建第一个管理员用户”页面出现时，在相应字段中指定管理员用户的详细信息，然后单击保存并完成。\n\n\n\n2.当 Jenkins 准备就绪页面出现时，单击开始使用 Jenkins。\n\n提示\n\n这个页面可能表明jenkins几乎准备好了！ 相反，如果是这样，请单击重新启动。\n\n如果页面在一分钟后没有自动刷新，请使用 Web 浏览器手动刷新页面。\n\n3.如果需要，请使用您刚刚创建的用户的凭据登录 Jenkins，您就可以开始使用 Jenkins 了！\n\n\n# Jenkins 欢迎页面\n\n\n\n有关安装和配置的详细说明，请参阅官方文档。',normalizedContent:'# jenkins自动化服务器安装\n\n\n# 概述\n\n\n\njenkins 是一款流行的开源持续集成(continuous integration)工具，广泛用于项目开发，具有自动 化构建、测试和部署等功能。官网: http://jenkins-ci.org/。\n\njenkins的特征:\n\n * 开源的java语言开发持续集成工具，支持持续集成，持续部署。\n * 易于安装部署配置:可通过yum安装,或下载war包以及通过docker容器等快速实现安装部署，可方便web界面配置管理。\n * 消息通知及测试报告:集成rss/e-mail通过rss发布构建结果或当构建完成时通过e-mail通知，生成junit/testng测试报告。\n * 分布式构建:支持jenkins能够让多台计算机一起构建/测试。\n * 文件识别:jenkins能够跟踪哪次构建生成哪些jar，哪次构建使用哪个版本的jar等。\n * 丰富的插件支持:支持扩展插件，你可以开发适合自己团队使用的工具，如git，svn，maven，docker等。\n\n\n# 硬件要求\n\n下表列出了部署jenkins的最低和建议的硬件配置。\n\n资源     最小配置    推荐配置\ncpu    2 cpu   4 cpu\nmem    1 gb    4 gb\ndisk   10 gb   40 gb\n\n\n# 安装 java\n\njenkins需要java才能运行，但某些发行版默认不包含此功能，并且某些 java 版本与 jenkins 不兼容。\n\n您可以使用多种 java 实现。 openjdk 是目前最流行的，我们将在本指南中使用它。\n\n要安装 open java development kit (openjdk)，请运行以下命令：\n\n * 更新存储库:\n\nsudo apt update\n\n\n * 搜索所有可用的包 :\n\nsudo apt search openjdk\n\n\n * 选择一个选项并安装它 :\n\nsudo apt install openjdk-8-jdk\n\n\n * 确认安装:\n\nsudo apt install openjdk-8-jdk\n\n\n * 检查安装 :\n\njava -version\n\n\n * 检查结果:\n\nopenjdk version "11.0.11" 2021-04-20\nopenjdk runtime environment (build 11.0.11+9-ubuntu-0ubuntu2.18.04)\nopenjdk 64-bit server vm (build 11.0.11+9-ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n\n\n提示\n\n为什么使用 apt 而不是 apt-get 或其他命令？ apt 命令自 2014 年开始可用。它具有类似于 apt-get 的命令结构，但旨在为典型用户提供更愉快的体验。 使用 apt 可以更轻松地执行简单的软件管理任务，例如安装、搜索和删除。\n\n\n# 安装 jenkins\n\n\n# ubuntu\n\n在 debian 和基于 debian 的发行版（如 ubuntu）上，您可以通过 apt 安装 jenkins。\n\nwget -q -o - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add -\nsudo sh -c \'echo deb https://pkg.jenkins.io/debian-stable binary/ > \\\n    /etc/apt/sources.list.d/jenkins.list\'\nsudo apt-get update\nsudo apt-get install jenkins\n\n\n\n# centos\n\n您可以通过 yum 在 red hat enterprise linux、centos 和其他基于 red hat 的发行版上安装 jenkins。\n\nsudo wget -o /etc/yum.repos.d/jenkins.repo \\\n    https://pkg.jenkins.io/redhat-stable/jenkins.repo\nsudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key\nsudo yum upgrade\nsudo yum install jenkins java-11-openjdk-devel\nsudo systemctl daemon-reload\n\n\n此软件包安装将：\n\n * 将jenkins设置为启动时启动的守护进程。有关更多详细信息，请参阅 /etc/init.d/jenkins。\n * 创建一个jenkins用户来运行这个服务。\n * 直接控制台日志输出到文件 /var/log/jenkins/jenkins.log。如果您正在对jenkins进行故障排除，请检查此文件。\n * 使用启动的配置参数填充 /etc/default/jenkins，例如jenkins_home\n * 将jenkins设置为侦听端口8080。使用浏览器访问此端口以开始配置。\n\n如何修改默认服务端口？\n\n如果您的/etc/init.d/jenkins文件无法启动jenkins，请编辑/etc/default/jenkins将----http_port=8080----行替换为----http_port=8081---此处选择了8081，但您可以设置另一个可用端口。\n\n\n# 启动 jenkins\n\n使用命令注册 jenkins 服务:\n\nsudo systemctl daemon-reload\n\n\n您可以使用以下命令启动 jenkins 服务:\n\nsudo systemctl start jenkins\n\n\n您可以使用命令检查 jenkins 服务的状态:\n\nsudo systemctl status jenkins\n\n\n如果一切都设置正确，你应该看到这样的输出:\n\nloaded: loaded (/etc/rc.d/init.d/jenkins; generated)\nactive: active (running) since tue 2018-11-13 16:19:01 +03; 4min 57s ago\n\n\n注意\n\n如果您安装了防火墙，则必须将 jenkins 添加为例外。 您必须将下面脚本中的 yourport 更改为要使用的端口。 端口 8080 是最常见的。\n\nyourport=8080\nperm="--permanent"\nserv="$perm --service=jenkins"\n\nfirewall-cmd $perm --new-service=jenkins\nfirewall-cmd $serv --set-short="jenkins ports"\nfirewall-cmd $serv --set-description="jenkins port exceptions"\nfirewall-cmd $serv --add-port=$yourport/tcp\nfirewall-cmd $perm --add-service=jenkins\nfirewall-cmd --zone=public --add-service=http --permanent\nfirewall-cmd --reload\n\n\n\n# 安装后设置向导\n\n\n# 解锁 jenkins\n\n当您第一次访问一个新的 jenkins 实例时，系统会要求您使用自动生成的密码将其解锁。\n\n1.浏览到 http://localhost:8080（或您在安装时为 jenkins 配置的任何端口）并等待 unlock jenkins 页面出现。\n\n2.从 jenkins 控制台日志输出中，复制自动生成的字母数字密码（在 2 组星号之间）。\n\ncat /var/log/jenkins/jenkins.log\n\n\njenkins 启动日志如下：\n\n...\n2021-06-15 10:36:55.529+0000 [id=28]    info    jenkins.install.setupwizard#init: \n\n*************************************************************\n*************************************************************\n*************************************************************\n\njenkins initial setup is required. an admin user has been created and a password generated.\nplease use the following password to proceed to installation:\n\n7400569b100b47c2b86c2e674c7123f3\n\nthis may also be found at: /var/lib/jenkins/secrets/initialadminpassword\n\n*************************************************************\n*************************************************************\n*************************************************************\n\n2021-06-15 10:37:27.106+0000 [id=28]    info    jenkins.initreactorrunner$1#onattained: completed initialization\n2021-06-15 10:37:27.153+0000 [id=22]    info    hudson.webappmain$3#run: jenkins is fully up and running\n...\n\n\n提示\n\n执行命令： sudo cat /var/lib/jenkins/secrets/initialadminpassword 将在控制台打印密码。\n\n如果您使用官方 jenkins/jenkins 映像在 docker 中运行 jenkins，则可以使用 sudo docker exec ${container_id or container_name} cat /var/jenkins_home/secrets/initialadminpassword 在控制台中打印密码，而无需执行到容器中。\n\n\n# 使用插件自定义 jenkins\n\n解锁jenkins后，会出现customize jenkins页面。 作为初始设置的一部分，您可以在此处安装任意数量的有用插件。\n\n单击显示的两个选项之一：\n\n * 安装建议的插件 - 安装推荐的插件集，这些插件基于最常见的用例。\n\n * 选择要安装的插件 - 选择最初安装的插件集。 当您第一次访问插件选择页面时，默认情况下会选择建议的插件。\n\n提示\n\n如果您不确定需要哪些插件，请选择安装建议的插件。 您可以稍后通过manage jenkins > manage plugins 页面安装（或删除）其他 jenkins 插件。\n\n\n\n设置向导显示正在配置的 jenkins 的进度以及正在安装的您选择的 jenkins 插件集。 此过程可能需要几分钟时间。\n\n\n# 创建第一个管理员用户\n\n最后，在使用插件自定义 jenkins 之后，jenkins 要求您创建您的第一个管理员用户。\n\n1.当“创建第一个管理员用户”页面出现时，在相应字段中指定管理员用户的详细信息，然后单击保存并完成。\n\n\n\n2.当 jenkins 准备就绪页面出现时，单击开始使用 jenkins。\n\n提示\n\n这个页面可能表明jenkins几乎准备好了！ 相反，如果是这样，请单击重新启动。\n\n如果页面在一分钟后没有自动刷新，请使用 web 浏览器手动刷新页面。\n\n3.如果需要，请使用您刚刚创建的用户的凭据登录 jenkins，您就可以开始使用 jenkins 了！\n\n\n# jenkins 欢迎页面\n\n\n\n有关安装和配置的详细说明，请参阅官方文档。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Jenkins插件管理",frontmatter:{title:"Jenkins插件管理",date:"2022-02-09T13:05:09.000Z",permalink:"/pages/6519b5/"},regularPath:"/20.DevOps/01.%E5%85%A5%E9%97%A8/05.Jenkins%E6%8F%92%E4%BB%B6%E7%AE%A1%E7%90%86.html",relativePath:"20.DevOps/01.入门/05.Jenkins插件管理.md",key:"v-4988c303",path:"/pages/6519b5/",headers:[{level:2,title:"修改Jenkins插件下载地址",slug:"修改jenkins插件下载地址",normalizedTitle:"修改jenkins插件下载地址",charIndex:105}],headersStr:"修改Jenkins插件下载地址",content:"# Jenkins插件管理\n\nJenkins本身不提供很多功能，我们可以通过使用插件来满足我们的使用。例如从Gitlab拉取代码，使用Maven构建项目等功能需要依靠插件完成。接下来演示如何下载插件。\n\n\n# 修改Jenkins插件下载地址\n\nJenkins国外官方插件地址下载速度非常慢，所以可以修改为国内插件地址： 系统管理->插件管理，点击可选插件\n\n\n\n这样做是为了把Jenkins官方的插件列表下载到本地，接着修改地址文件，替换为国内插件地址\n\n# 进入Jenkins配置目录\ncd /var/lib/jenkins/updates\n# 备份默认配置文件\ncp default.json default.json.bak\n# 执行替换操作\nsed -i 's/http:\\/\\/updates.jenkinsci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g' default.json\nsed -i 's/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g' default.json\n\n\n最后，插件管理点击高级，把升级站点改为国内插件下载地址\n\nhttps://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\n\n\n\n\n点击提交后，重启Jenkins",normalizedContent:"# jenkins插件管理\n\njenkins本身不提供很多功能，我们可以通过使用插件来满足我们的使用。例如从gitlab拉取代码，使用maven构建项目等功能需要依靠插件完成。接下来演示如何下载插件。\n\n\n# 修改jenkins插件下载地址\n\njenkins国外官方插件地址下载速度非常慢，所以可以修改为国内插件地址： 系统管理->插件管理，点击可选插件\n\n\n\n这样做是为了把jenkins官方的插件列表下载到本地，接着修改地址文件，替换为国内插件地址\n\n# 进入jenkins配置目录\ncd /var/lib/jenkins/updates\n# 备份默认配置文件\ncp default.json default.json.bak\n# 执行替换操作\nsed -i 's/http:\\/\\/updates.jenkinsci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g' default.json\nsed -i 's/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g' default.json\n\n\n最后，插件管理点击高级，把升级站点改为国内插件下载地址\n\nhttps://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\n\n\n\n\n点击提交后，重启jenkins",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Jenkins用户权限管理",frontmatter:{title:"Jenkins用户权限管理",date:"2022-02-09T13:05:09.000Z",permalink:"/pages/61d667/"},regularPath:"/20.DevOps/01.%E5%85%A5%E9%97%A8/06.Jenkins%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86.html",relativePath:"20.DevOps/01.入门/06.Jenkins用户权限管理.md",key:"v-7073a58e",path:"/pages/61d667/",headers:[{level:2,title:"安装Role-based Authorization Strategy插件",slug:"安装role-based-authorization-strategy插件",normalizedTitle:"安装role-based authorization strategy插件",charIndex:78},{level:2,title:"开启权限全局安全配置",slug:"开启权限全局安全配置",normalizedTitle:"开启权限全局安全配置",charIndex:136},{level:2,title:"创建角色",slug:"创建角色",normalizedTitle:"创建角色",charIndex:207},{level:2,title:"创建用户",slug:"创建用户",normalizedTitle:"创建用户",charIndex:653},{level:3,title:"给用户分配角色",slug:"给用户分配角色",normalizedTitle:"给用户分配角色",charIndex:720},{level:2,title:"创建项目测试权限",slug:"创建项目测试权限",normalizedTitle:"创建项目测试权限",charIndex:865}],headersStr:"安装Role-based Authorization Strategy插件 开启权限全局安全配置 创建角色 创建用户 给用户分配角色 创建项目测试权限",content:'# Jenkins用户权限管理\n\n我们可以利用Role-based Authorization Strategy 插件来管理Jenkins用户权限\n\n\n# 安装Role-based Authorization Strategy插件\n\n系统管理 -> 插件管理\n\n\n\n\n# 开启权限全局安全配置\n\n系统管理 -> 全局安全配置\n\n\n\n授权策略切换为Role-Based Strategy，点击"保存"。\n\n\n\n\n# 创建角色\n\n在系统管理页面进入 Manage and Assign Roles\n\n\n\n点击Manage Roles\n\n\n\n * Global roles（全局角色）：管理员等高级用户可以创建基于全局的角色\n * Item roles（项目角色）： 针对某个或者某些项目的角色\n * Node roles（从节点）：节点相关的权限\n\n我们添加以下三个角色：\n\n * baseRole：该角色为全局角色。这个角色需要绑定Overall下面的Read权限，是为了给所有用户绑定最基本的Jenkins访问权限。注意：如果不给后续用户绑定这个角色，会报错误：用户名 is missing the Overall/Read permission\n * role1：该角色为项目角色。使用正则表达式绑定project1.*，意思是只能操作project1开头的项目。\n * role2：该角色也为项目角色。绑定project2.*，意思是只能操作project2开头的项目。\n\n\n\n点击"保存"。\n\n\n# 创建用户\n\n在系统管理页面进入 管理用户\n\n\n\n管理用户 -> 新建用户\n\n\n\n分别创建两个用户：jack和jasper\n\n\n\n\n# 给用户分配角色\n\n系统管理页面进入Manage and Assign Roles，点击Assign Roles\n\n\n\n绑定规则如下：\n\n * jack用户分别绑定baseRole和role1角色\n * jasper用户分别绑定baseRole和role2角色\n\n\n\n点击"保存"。\n\n\n# 创建项目测试权限\n\n以admin管理员账户创建两个项目，分别为project1_demo和project2_demo\n\n结果为：\n\n * jack用户登录，只能看到project1_demo项目\n   \n   \n\n * jsaper用户登录，只能看到project2_demo项目\n   \n   \n\n至此，基于角色的权限配置就完成了，实际使用中根据项目名称匹配用户管理项目即可。',normalizedContent:'# jenkins用户权限管理\n\n我们可以利用role-based authorization strategy 插件来管理jenkins用户权限\n\n\n# 安装role-based authorization strategy插件\n\n系统管理 -> 插件管理\n\n\n\n\n# 开启权限全局安全配置\n\n系统管理 -> 全局安全配置\n\n\n\n授权策略切换为role-based strategy，点击"保存"。\n\n\n\n\n# 创建角色\n\n在系统管理页面进入 manage and assign roles\n\n\n\n点击manage roles\n\n\n\n * global roles（全局角色）：管理员等高级用户可以创建基于全局的角色\n * item roles（项目角色）： 针对某个或者某些项目的角色\n * node roles（从节点）：节点相关的权限\n\n我们添加以下三个角色：\n\n * baserole：该角色为全局角色。这个角色需要绑定overall下面的read权限，是为了给所有用户绑定最基本的jenkins访问权限。注意：如果不给后续用户绑定这个角色，会报错误：用户名 is missing the overall/read permission\n * role1：该角色为项目角色。使用正则表达式绑定project1.*，意思是只能操作project1开头的项目。\n * role2：该角色也为项目角色。绑定project2.*，意思是只能操作project2开头的项目。\n\n\n\n点击"保存"。\n\n\n# 创建用户\n\n在系统管理页面进入 管理用户\n\n\n\n管理用户 -> 新建用户\n\n\n\n分别创建两个用户：jack和jasper\n\n\n\n\n# 给用户分配角色\n\n系统管理页面进入manage and assign roles，点击assign roles\n\n\n\n绑定规则如下：\n\n * jack用户分别绑定baserole和role1角色\n * jasper用户分别绑定baserole和role2角色\n\n\n\n点击"保存"。\n\n\n# 创建项目测试权限\n\n以admin管理员账户创建两个项目，分别为project1_demo和project2_demo\n\n结果为：\n\n * jack用户登录，只能看到project1_demo项目\n   \n   \n\n * jsaper用户登录，只能看到project2_demo项目\n   \n   \n\n至此，基于角色的权限配置就完成了，实际使用中根据项目名称匹配用户管理项目即可。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Jenkins凭证管理",frontmatter:{title:"Jenkins凭证管理",date:"2022-02-09T13:05:09.000Z",permalink:"/pages/a1fe86/"},regularPath:"/20.DevOps/01.%E5%85%A5%E9%97%A8/07.Jenkins%E5%87%AD%E8%AF%81%E7%AE%A1%E7%90%86.html",relativePath:"20.DevOps/01.入门/07.Jenkins凭证管理.md",key:"v-60dca1e3",path:"/pages/a1fe86/",headers:[{level:2,title:"安装Credentials Binding Plugin插件",slug:"安装credentials-binding-plugin插件",normalizedTitle:"安装credentials binding plugin插件",charIndex:92},{level:2,title:"安装Git插件和Git工具",slug:"安装git插件和git工具",normalizedTitle:"安装git插件和git工具",charIndex:668},{level:3,title:"Jenkins Git plugin插件安装",slug:"jenkins-git-plugin插件安装",normalizedTitle:"jenkins git plugin插件安装",charIndex:740},{level:3,title:"CentOS7上安装Git工具",slug:"centos7上安装git工具",normalizedTitle:"centos7上安装git工具",charIndex:719},{level:3,title:"用户密码类型",slug:"用户密码类型",normalizedTitle:"用户密码类型",charIndex:878},{level:3,title:"SSH密钥类型",slug:"ssh密钥类型",normalizedTitle:"ssh密钥类型",charIndex:1548}],headersStr:"安装Credentials Binding Plugin插件 安装Git插件和Git工具 Jenkins Git plugin插件安装 CentOS7上安装Git工具 用户密码类型 SSH密钥类型",content:'# Jenkins凭证管理\n\n凭据可以用来存储需要密文保护的数据库密码、Gitlab密码信息、Docker私有仓库密码等，以便 Jenkins可以和这些第三方的应用进行交互。\n\n\n# 安装Credentials Binding Plugin插件\n\n要在Jenkins使用凭证管理功能，需要安装Credentials Binding Plugin插件\n\n\n\n安装插件后，系统管理 -> Manage Credentials，在这里管理所有凭证\n\n\n\n点击 全局 -> 添加凭证 ，可以添加的凭证有6种:\n\n\n\n * Username with password：用户名和密码\n * Github App：对接Github应用\n * SSH Username with private key：使用SSH用户和密钥\n * Secret file：需要保密的文本文件，使用时Jenkins会将文件复制到一个临时目录中，再将文件路径 设置到一个变量中，等构建结束后，所复制的Secret file就会被删除。\n * Secret text：需要保存的一个加密的文本串，如钉钉机器人或Github的api token\n * Certificate：通过上传证书文件的方式\n\n常用的凭证类型有: Username with password(用户密码) 和 SSH Username with private key(SSH 密钥)\n\n接下来以使用Git工具到Gitlab拉取项目源码为例，演示Jenkins的如何管理Gitlab的凭证。\n\n\n# 安装Git插件和Git工具\n\n为了让Jenkins支持从Gitlab拉取源码，需要安装Git插件以及在CentOS7上安装Git工具。\n\n\n# Jenkins Git plugin插件安装\n\n\n\n\n# CentOS7上安装Git工具\n\n# 安装\nyum install git -y\n# 安装后查看版本\ngit --version\n\n\n提示\n\nubuntu server 20.04系统自带git工具无需安装。\n\n\n# 用户密码类型\n\n1)创建凭证 jenkins-gitlab-checkout-creds\n\nJenkins->凭证->系统->全局凭证->添加凭证\n\n\n\n选择Username with password，输入Gitlab的用户名和密码，点击"确定"。\n\n\n\n2)测试凭证是否可用\n\n创建一个FreeStyle项目：新建任务->自由风格项目->确定\n\n\n\n找到"源码管理"->"Git"，在Repository URL复制Gitlab中的项目URL\n\n\n\n\n\n这时会报错说无法连接仓库!在Credentials选择刚刚添加的凭证就不报错啦\n\n\n\n保存配置后，点击"构建图标" 开始构建项目\n\n\n\n查看控制台输出日志\n\n\n\n查看/var/lib/jenkins/workspace/目录，发现已经从Gitlab成功拉取了代码到Jenkins中。\n\n\n\n\n\n\n \n \n\n\nubuntu@jenkins:/var/lib/jenkins/workspace$ ls -al\ntotal 24\ndrwxr-xr-x  6 jenkins jenkins 4096 Jun 20 17:24 .\ndrwxr-xr-x 18 jenkins jenkins 4096 Jun 20 17:25 ..\ndrwxr-xr-x  3 jenkins jenkins 4096 Jun 20 14:52 my-project\ndrwxr-xr-x  2 jenkins jenkins 4096 Jun 20 14:52 my-project@tmp\n\n\n\n# SSH密钥类型\n\nSSH免密登录示意图\n\n\n\n1)登录jenkins服务器，执行如下命令生成公钥和私钥，-C参数指定密钥注释内容，可以填入邮箱地址。\n\nssh-keygen -t rsa -b 2048 -C "<comment>"\n\n\n例子：\n\nubuntu@jenkins:~$ ssh-keygen -t rsa -b 2048 -C "<jenkins@ieooc.com>"\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/ubuntu/.ssh/id_rsa):\nCreated directory \'/home/ubuntu/.ssh\'.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/ubuntu/.ssh/id_rsa\nYour public key has been saved in /home/ubuntu/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:azN4tuxLMbWM5Hd25GaXRaN4Kgttgee8Yg3/8uFS8M8 <jenkins@ieooc.com>\nThe key\'s randomart image is:\n+---[RSA 2048]----+\n|               ..|\n|        .   . ...|\n|       ..o.. o. .|\n|       o==..oo  o|\n|       oS**.o =..|\n|       .*=+= + . |\n|      .oO=..o    |\n|      .*.*o .E   |\n|       .=.++     |\n+----[SHA256]-----+\n\n\n注意\n\n为了实现免密登录效果，生成证书时要求输入证书密码步骤时直接回车跳过无需设置密码。\n\n在/home/ubuntu/.ssh/目录保存了公钥和使用\n\nubuntu@jenkins:~/.ssh$ ls\nid_rsa  id_rsa.pub\n\n\n * id_rsa：私钥文件\n * id_rsa.pub：公钥文件\n\n2)把生成的公钥放在Gitlab中\n\n以root账户登录Gitlab->点击头像->Edit profile->SSH Keys 复制刚才id_rsa.pub文件的内容到这里，点击"Add Key"\n\n\n\n3)在Jenkins中添加凭证，配置私钥\n\n在Jenkins添加一个新的凭证，类型为SSH Username with private key，把刚才生成私有文件内容复制过来\n\n\n\n\n\n4)测试凭证是否可用 新建my-project-ssh项目->源码管理->Git，这次要使用Gitlab的SSH连接，并且选择SSH凭证\n\n\n\n\n\n\n\n同样尝试构建项目，如果代码可以正常拉取，代表凭证配置成功!\n\n\n\n\n\n\n\n \n \n\n\n\nubuntu@jenkins:/var/lib/jenkins/workspace$ ls -al\ntotal 24\ndrwxr-xr-x  6 jenkins jenkins 4096 Jun 20 17:24 .\ndrwxr-xr-x 18 jenkins jenkins 4096 Jun 20 17:25 ..\ndrwxr-xr-x  3 jenkins jenkins 4096 Jun 20 14:52 my-project\ndrwxr-xr-x  3 jenkins jenkins 4096 Jun 20 17:24 my-project-ssh\ndrwxr-xr-x  2 jenkins jenkins 4096 Jun 20 17:24 my-project-ssh@tmp\ndrwxr-xr-x  2 jenkins jenkins 4096 Jun 20 14:52 my-project@tmp\n',normalizedContent:'# jenkins凭证管理\n\n凭据可以用来存储需要密文保护的数据库密码、gitlab密码信息、docker私有仓库密码等，以便 jenkins可以和这些第三方的应用进行交互。\n\n\n# 安装credentials binding plugin插件\n\n要在jenkins使用凭证管理功能，需要安装credentials binding plugin插件\n\n\n\n安装插件后，系统管理 -> manage credentials，在这里管理所有凭证\n\n\n\n点击 全局 -> 添加凭证 ，可以添加的凭证有6种:\n\n\n\n * username with password：用户名和密码\n * github app：对接github应用\n * ssh username with private key：使用ssh用户和密钥\n * secret file：需要保密的文本文件，使用时jenkins会将文件复制到一个临时目录中，再将文件路径 设置到一个变量中，等构建结束后，所复制的secret file就会被删除。\n * secret text：需要保存的一个加密的文本串，如钉钉机器人或github的api token\n * certificate：通过上传证书文件的方式\n\n常用的凭证类型有: username with password(用户密码) 和 ssh username with private key(ssh 密钥)\n\n接下来以使用git工具到gitlab拉取项目源码为例，演示jenkins的如何管理gitlab的凭证。\n\n\n# 安装git插件和git工具\n\n为了让jenkins支持从gitlab拉取源码，需要安装git插件以及在centos7上安装git工具。\n\n\n# jenkins git plugin插件安装\n\n\n\n\n# centos7上安装git工具\n\n# 安装\nyum install git -y\n# 安装后查看版本\ngit --version\n\n\n提示\n\nubuntu server 20.04系统自带git工具无需安装。\n\n\n# 用户密码类型\n\n1)创建凭证 jenkins-gitlab-checkout-creds\n\njenkins->凭证->系统->全局凭证->添加凭证\n\n\n\n选择username with password，输入gitlab的用户名和密码，点击"确定"。\n\n\n\n2)测试凭证是否可用\n\n创建一个freestyle项目：新建任务->自由风格项目->确定\n\n\n\n找到"源码管理"->"git"，在repository url复制gitlab中的项目url\n\n\n\n\n\n这时会报错说无法连接仓库!在credentials选择刚刚添加的凭证就不报错啦\n\n\n\n保存配置后，点击"构建图标" 开始构建项目\n\n\n\n查看控制台输出日志\n\n\n\n查看/var/lib/jenkins/workspace/目录，发现已经从gitlab成功拉取了代码到jenkins中。\n\n\n\n\n\n\n \n \n\n\nubuntu@jenkins:/var/lib/jenkins/workspace$ ls -al\ntotal 24\ndrwxr-xr-x  6 jenkins jenkins 4096 jun 20 17:24 .\ndrwxr-xr-x 18 jenkins jenkins 4096 jun 20 17:25 ..\ndrwxr-xr-x  3 jenkins jenkins 4096 jun 20 14:52 my-project\ndrwxr-xr-x  2 jenkins jenkins 4096 jun 20 14:52 my-project@tmp\n\n\n\n# ssh密钥类型\n\nssh免密登录示意图\n\n\n\n1)登录jenkins服务器，执行如下命令生成公钥和私钥，-c参数指定密钥注释内容，可以填入邮箱地址。\n\nssh-keygen -t rsa -b 2048 -c "<comment>"\n\n\n例子：\n\nubuntu@jenkins:~$ ssh-keygen -t rsa -b 2048 -c "<jenkins@ieooc.com>"\ngenerating public/private rsa key pair.\nenter file in which to save the key (/home/ubuntu/.ssh/id_rsa):\ncreated directory \'/home/ubuntu/.ssh\'.\nenter passphrase (empty for no passphrase):\nenter same passphrase again:\nyour identification has been saved in /home/ubuntu/.ssh/id_rsa\nyour public key has been saved in /home/ubuntu/.ssh/id_rsa.pub\nthe key fingerprint is:\nsha256:azn4tuxlmbwm5hd25gaxran4kgttgee8yg3/8ufs8m8 <jenkins@ieooc.com>\nthe key\'s randomart image is:\n+---[rsa 2048]----+\n|               ..|\n|        .   . ...|\n|       ..o.. o. .|\n|       o==..oo  o|\n|       os**.o =..|\n|       .*=+= + . |\n|      .oo=..o    |\n|      .*.*o .e   |\n|       .=.++     |\n+----[sha256]-----+\n\n\n注意\n\n为了实现免密登录效果，生成证书时要求输入证书密码步骤时直接回车跳过无需设置密码。\n\n在/home/ubuntu/.ssh/目录保存了公钥和使用\n\nubuntu@jenkins:~/.ssh$ ls\nid_rsa  id_rsa.pub\n\n\n * id_rsa：私钥文件\n * id_rsa.pub：公钥文件\n\n2)把生成的公钥放在gitlab中\n\n以root账户登录gitlab->点击头像->edit profile->ssh keys 复制刚才id_rsa.pub文件的内容到这里，点击"add key"\n\n\n\n3)在jenkins中添加凭证，配置私钥\n\n在jenkins添加一个新的凭证，类型为ssh username with private key，把刚才生成私有文件内容复制过来\n\n\n\n\n\n4)测试凭证是否可用 新建my-project-ssh项目->源码管理->git，这次要使用gitlab的ssh连接，并且选择ssh凭证\n\n\n\n\n\n\n\n同样尝试构建项目，如果代码可以正常拉取，代表凭证配置成功!\n\n\n\n\n\n\n\n \n \n\n\n\nubuntu@jenkins:/var/lib/jenkins/workspace$ ls -al\ntotal 24\ndrwxr-xr-x  6 jenkins jenkins 4096 jun 20 17:24 .\ndrwxr-xr-x 18 jenkins jenkins 4096 jun 20 17:25 ..\ndrwxr-xr-x  3 jenkins jenkins 4096 jun 20 14:52 my-project\ndrwxr-xr-x  3 jenkins jenkins 4096 jun 20 17:24 my-project-ssh\ndrwxr-xr-x  2 jenkins jenkins 4096 jun 20 17:24 my-project-ssh@tmp\ndrwxr-xr-x  2 jenkins jenkins 4096 jun 20 14:52 my-project@tmp\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Maven安装和配置",frontmatter:{title:"Maven安装和配置",date:"2022-02-09T13:05:09.000Z",permalink:"/pages/c15beb/"},regularPath:"/20.DevOps/01.%E5%85%A5%E9%97%A8/08.Maven%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE.html",relativePath:"20.DevOps/01.入门/08.Maven安装和配置.md",key:"v-69282982",path:"/pages/c15beb/",headers:[{level:2,title:"安装Maven",slug:"安装maven",normalizedTitle:"安装maven",charIndex:33},{level:2,title:"全局工具配置关联JDK和Maven",slug:"全局工具配置关联jdk和maven",normalizedTitle:"全局工具配置关联jdk和maven",charIndex:1087},{level:2,title:"添加Jenkins全局变量",slug:"添加jenkins全局变量",normalizedTitle:"添加jenkins全局变量",charIndex:1185},{level:2,title:"修改Maven的settings.xml",slug:"修改maven的settings-xml",normalizedTitle:"修改maven的settings.xml",charIndex:1271},{level:2,title:"测试Maven是否配置成功",slug:"测试maven是否配置成功",normalizedTitle:"测试maven是否配置成功",charIndex:1618}],headersStr:"安装Maven 全局工具配置关联JDK和Maven 添加Jenkins全局变量 修改Maven的settings.xml 测试Maven是否配置成功",content:'# Maven安装和配置\n\n在Jenkins集成服务器上，我们需要安装Maven来编译和打包项目。\n\n\n# 安装Maven\n\n下载maven\n\nwget https://mirrors.bfsu.edu.cn/apache/maven/maven-3/3.8.1/binaries/apache-maven-3.8.1-bin.tar.gz\n\n\n解压maven\n\nsudo tar -zxvf apache-maven-3.8.1-bin.tar.gz -C /usr/local/\n\n\n配置环境变量\n\nsudo vi /etc/profile\n\n\n文档最后面追加如下内容：\n\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\nexport MAVEN_HOME=/usr/local/apache-maven-3.8.1\nexport PATH=$JAVA_HOME/bin:$MAVEN_HOME/bin:$PATH:.\n\n\n检查配置是否正确\n\n# 配置生效\nubuntu@jenkins:~$ source /etc/profile   \n# 检查java \nubuntu@jenkins:~$ java -version \nopenjdk version "11.0.11" 2021-04-20\nOpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.20.04)\nOpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.20.04, mixed mode, sharing)\n# 检查maven\nubuntu@jenkins:~$ mvn -v \nApache Maven 3.8.1 (05c21c65bdfed0f71a2f2ada8b84da59348c4c5d)\nMaven home: /usr/local/apache-maven-3.8.1\nJava version: 11.0.11, vendor: Ubuntu, runtime: /usr/lib/jvm/java-11-openjdk-amd64\nDefault locale: en_US, platform encoding: UTF-8\nOS name: "linux", version: "5.4.0-74-generic", arch: "amd64", family: "unix"\n\n\n\n# 全局工具配置关联JDK和Maven\n\nJenkins->全局工具配置->JDK->新增JDK，配置如下:\n\n\n\nJenkins->全局工具配置->JDK->新增Maven，配置如下:\n\n\n\n\n# 添加Jenkins全局变量\n\n系统管理->系统配置->全局属性\n\n添加三个全局变量 JAVA_HOME、M2_HOME、PATH+EXTRA\n\n\n\n点击"保存"。\n\n\n# 修改Maven的settings.xml\n\n打开 maven 的配置文件（maven 安装目录的）\n\nsudo vi /usr/local/apache-maven-3.8.1/conf/settings.xml\n\n\n在<mirrors></mirrors>标签中添加 mirror 子节点:\n\n<mirror>\n  <id>aliyunmaven</id>\n  <mirrorOf>*</mirrorOf>\n  <name>阿里云公共仓库</name>\n  <url>https://maven.aliyun.com/repository/public</url>\n</mirror>\n\n\n提示\n\n阿里云仓库服务：https://maven.aliyun.com/mvn/guide\n\n\n# 测试Maven是否配置成功\n\n修改my-project项目配置\n\n\n\n构建->增加构建步骤->执行 Shell\n\n\n\n输入构建命令：mvn clean package\n\n\n\n再次构建，如果可以把项目打成war包，代表maven环境配置成功啦!\n\n...\nDownloaded from aliyunmaven: https://maven.aliyun.com/repository/public/com/thoughtworks/xstream/xstream/1.4.10/xstream-1.4.10.jar (590 kB at 516 kB/s)\nDownloaded from aliyunmaven: https://maven.aliyun.com/repository/public/org/codehaus/plexus/plexus-utils/3.1.0/plexus-utils-3.1.0.jar (262 kB at 216 kB/s)\n[INFO] Packaging webapp\n[INFO] Assembling webapp [my-project] in [/var/lib/jenkins/workspace/my-project/target/my-project]\n[INFO] Processing war project\n[INFO] Copying webapp resources [/var/lib/jenkins/workspace/my-project/src/main/webapp]\n[INFO] Webapp assembled in [44 msecs]\n[INFO] Building war: /var/lib/jenkins/workspace/my-project/target/my-project.war\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  11.555 s\n[INFO] Finished at: 2021-06-20T20:21:58+08:00\n[INFO] ------------------------------------------------------------------------\nFinished: SUCCESS\n',normalizedContent:'# maven安装和配置\n\n在jenkins集成服务器上，我们需要安装maven来编译和打包项目。\n\n\n# 安装maven\n\n下载maven\n\nwget https://mirrors.bfsu.edu.cn/apache/maven/maven-3/3.8.1/binaries/apache-maven-3.8.1-bin.tar.gz\n\n\n解压maven\n\nsudo tar -zxvf apache-maven-3.8.1-bin.tar.gz -c /usr/local/\n\n\n配置环境变量\n\nsudo vi /etc/profile\n\n\n文档最后面追加如下内容：\n\nexport java_home=/usr/lib/jvm/java-11-openjdk-amd64\nexport maven_home=/usr/local/apache-maven-3.8.1\nexport path=$java_home/bin:$maven_home/bin:$path:.\n\n\n检查配置是否正确\n\n# 配置生效\nubuntu@jenkins:~$ source /etc/profile   \n# 检查java \nubuntu@jenkins:~$ java -version \nopenjdk version "11.0.11" 2021-04-20\nopenjdk runtime environment (build 11.0.11+9-ubuntu-0ubuntu2.20.04)\nopenjdk 64-bit server vm (build 11.0.11+9-ubuntu-0ubuntu2.20.04, mixed mode, sharing)\n# 检查maven\nubuntu@jenkins:~$ mvn -v \napache maven 3.8.1 (05c21c65bdfed0f71a2f2ada8b84da59348c4c5d)\nmaven home: /usr/local/apache-maven-3.8.1\njava version: 11.0.11, vendor: ubuntu, runtime: /usr/lib/jvm/java-11-openjdk-amd64\ndefault locale: en_us, platform encoding: utf-8\nos name: "linux", version: "5.4.0-74-generic", arch: "amd64", family: "unix"\n\n\n\n# 全局工具配置关联jdk和maven\n\njenkins->全局工具配置->jdk->新增jdk，配置如下:\n\n\n\njenkins->全局工具配置->jdk->新增maven，配置如下:\n\n\n\n\n# 添加jenkins全局变量\n\n系统管理->系统配置->全局属性\n\n添加三个全局变量 java_home、m2_home、path+extra\n\n\n\n点击"保存"。\n\n\n# 修改maven的settings.xml\n\n打开 maven 的配置文件（maven 安装目录的）\n\nsudo vi /usr/local/apache-maven-3.8.1/conf/settings.xml\n\n\n在<mirrors></mirrors>标签中添加 mirror 子节点:\n\n<mirror>\n  <id>aliyunmaven</id>\n  <mirrorof>*</mirrorof>\n  <name>阿里云公共仓库</name>\n  <url>https://maven.aliyun.com/repository/public</url>\n</mirror>\n\n\n提示\n\n阿里云仓库服务：https://maven.aliyun.com/mvn/guide\n\n\n# 测试maven是否配置成功\n\n修改my-project项目配置\n\n\n\n构建->增加构建步骤->执行 shell\n\n\n\n输入构建命令：mvn clean package\n\n\n\n再次构建，如果可以把项目打成war包，代表maven环境配置成功啦!\n\n...\ndownloaded from aliyunmaven: https://maven.aliyun.com/repository/public/com/thoughtworks/xstream/xstream/1.4.10/xstream-1.4.10.jar (590 kb at 516 kb/s)\ndownloaded from aliyunmaven: https://maven.aliyun.com/repository/public/org/codehaus/plexus/plexus-utils/3.1.0/plexus-utils-3.1.0.jar (262 kb at 216 kb/s)\n[info] packaging webapp\n[info] assembling webapp [my-project] in [/var/lib/jenkins/workspace/my-project/target/my-project]\n[info] processing war project\n[info] copying webapp resources [/var/lib/jenkins/workspace/my-project/src/main/webapp]\n[info] webapp assembled in [44 msecs]\n[info] building war: /var/lib/jenkins/workspace/my-project/target/my-project.war\n[info] ------------------------------------------------------------------------\n[info] build success\n[info] ------------------------------------------------------------------------\n[info] total time:  11.555 s\n[info] finished at: 2021-06-20t20:21:58+08:00\n[info] ------------------------------------------------------------------------\nfinished: success\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Tomcat安装和配置",frontmatter:{title:"Tomcat安装和配置",date:"2022-02-09T13:05:09.000Z",permalink:"/pages/9d384b/"},regularPath:"/20.DevOps/01.%E5%85%A5%E9%97%A8/09.Tomcat%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE.html",relativePath:"20.DevOps/01.入门/09.Tomcat安装和配置.md",key:"v-fae7c1bc",path:"/pages/9d384b/",headers:[{level:2,title:"安装tomcat",slug:"安装tomcat",normalizedTitle:"安装tomcat",charIndex:18},{level:2,title:"配置Tomcat用户角色权限",slug:"配置tomcat用户角色权限",normalizedTitle:"配置tomcat用户角色权限",charIndex:340}],headersStr:"安装tomcat 配置Tomcat用户角色权限",content:'# Tomcat安装和配置\n\n\n# 安装tomcat\n\n安装jdk\n\nsudo apt install openjdk-11-jdk\n\n\n下载\n\nwget https://mirrors.bfsu.edu.cn/apache/tomcat/tomcat-9/v9.0.48/bin/apache-tomcat-9.0.48.tar.gz\n\n\n解压\n\ntar -zxvf apache-tomcat-9.0.48.tar.gz\n\n\n启动\n\n/home/ubuntu/apache-tomcat-9.0.48/bin/startup.sh\n\n\n访问地址：http://192.168.50.103:8080\n\n\n\n提示\n\n如果是centos请关闭防火墙，或者打开8080端口\n\n\n# 配置Tomcat用户角色权限\n\n默认情况下Tomcat是没有配置用户角色权限，点击manager webapp页面显示403。\n\n\n\n修改tomcat以下配置tomcat-users.xml，添加用户及权限\n\nsudo vi /home/ubuntu/apache-tomcat-9.0.48/conf/tomcat-users.xml\n\n\n内容如下:\n\n\n\n \n\n\n\n\n\n\n\n\n\n<tomcat-users>\n    <role rolename="role1"/>\n    <role rolename="manager-script"/>\n    <role rolename="manager-gui"/>\n    <role rolename="manager-status"/>\n    <role rolename="admin-gui"/>\n    <role rolename="admin-script"/>\n    <user username="tomcat" password="tomcat" roles="manager-gui,manager-script,tomcat,admin-gui,admin-script"/>\n</tomcat-users>\n\n\n用户和密码都是:tomcat\n\n注意\n\n为了能够刚才配置的用户登录到Tomcat，还需要修改以下配置allow正则表达式，允许本地ip\n\nvi /home/ubuntu/apache-tomcat-9.0.48/webapps/manager/META-INF/context.xml\n\n\n\n\n \n\n\n  <Valve className="org.apache.catalina.valves.RemoteAddrValve"\n         allow="192\\.\\168\\.\\50\\.\\d+|::1|0:0:0:0:0:0:0:1" />\n\n\n重启tomcat\n\n# 停止\n/home/ubuntu/apache-tomcat-9.0.48/bin/shutdown.sh\n# 启动\n/home/ubuntu/apache-tomcat-9.0.48/bin/startup.sh\n\n\n访问: http://192.168.50.103:8080/manager/html ，输入tomcat和tomcat，看到以下页面代表成功啦\n\n',normalizedContent:'# tomcat安装和配置\n\n\n# 安装tomcat\n\n安装jdk\n\nsudo apt install openjdk-11-jdk\n\n\n下载\n\nwget https://mirrors.bfsu.edu.cn/apache/tomcat/tomcat-9/v9.0.48/bin/apache-tomcat-9.0.48.tar.gz\n\n\n解压\n\ntar -zxvf apache-tomcat-9.0.48.tar.gz\n\n\n启动\n\n/home/ubuntu/apache-tomcat-9.0.48/bin/startup.sh\n\n\n访问地址：http://192.168.50.103:8080\n\n\n\n提示\n\n如果是centos请关闭防火墙，或者打开8080端口\n\n\n# 配置tomcat用户角色权限\n\n默认情况下tomcat是没有配置用户角色权限，点击manager webapp页面显示403。\n\n\n\n修改tomcat以下配置tomcat-users.xml，添加用户及权限\n\nsudo vi /home/ubuntu/apache-tomcat-9.0.48/conf/tomcat-users.xml\n\n\n内容如下:\n\n\n\n \n\n\n\n\n\n\n\n\n\n<tomcat-users>\n    <role rolename="role1"/>\n    <role rolename="manager-script"/>\n    <role rolename="manager-gui"/>\n    <role rolename="manager-status"/>\n    <role rolename="admin-gui"/>\n    <role rolename="admin-script"/>\n    <user username="tomcat" password="tomcat" roles="manager-gui,manager-script,tomcat,admin-gui,admin-script"/>\n</tomcat-users>\n\n\n用户和密码都是:tomcat\n\n注意\n\n为了能够刚才配置的用户登录到tomcat，还需要修改以下配置allow正则表达式，允许本地ip\n\nvi /home/ubuntu/apache-tomcat-9.0.48/webapps/manager/meta-inf/context.xml\n\n\n\n\n \n\n\n  <valve classname="org.apache.catalina.valves.remoteaddrvalve"\n         allow="192\\.\\168\\.\\50\\.\\d+|::1|0:0:0:0:0:0:0:1" />\n\n\n重启tomcat\n\n# 停止\n/home/ubuntu/apache-tomcat-9.0.48/bin/shutdown.sh\n# 启动\n/home/ubuntu/apache-tomcat-9.0.48/bin/startup.sh\n\n\n访问: http://192.168.50.103:8080/manager/html ，输入tomcat和tomcat，看到以下页面代表成功啦\n\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Jenkins构建Maven项目",frontmatter:{title:"Jenkins构建Maven项目",date:"2022-02-09T13:05:17.000Z",permalink:"/pages/5448da/"},regularPath:"/20.DevOps/02.%E5%AE%9E%E6%88%98/01.Jenkins%E6%9E%84%E5%BB%BAMaven%E9%A1%B9%E7%9B%AE.html",relativePath:"20.DevOps/02.实战/01.Jenkins构建Maven项目.md",key:"v-7c133ad0",path:"/pages/5448da/",headers:[{level:2,title:"Jenkins构建的项目类型介绍",slug:"jenkins构建的项目类型介绍",normalizedTitle:"jenkins构建的项目类型介绍",charIndex:23},{level:2,title:"自由风格项目构建",slug:"自由风格项目构建",normalizedTitle:"自由风格项目构建",charIndex:255},{level:3,title:"拉取代码",slug:"拉取代码",normalizedTitle:"拉取代码",charIndex:296},{level:3,title:"编译打包",slug:"编译打包",normalizedTitle:"编译打包",charIndex:360},{level:3,title:"部署",slug:"部署",normalizedTitle:"部署",charIndex:310},{level:3,title:"演示改动代码后的持续集成",slug:"演示改动代码后的持续集成",normalizedTitle:"演示改动代码后的持续集成",charIndex:2093},{level:2,title:"Maven项目构建",slug:"maven项目构建",normalizedTitle:"maven项目构建",charIndex:2164},{level:2,title:"Pipeline流水线项目构建",slug:"pipeline流水线项目构建",normalizedTitle:"pipeline流水线项目构建",charIndex:2263},{level:3,title:"Pipeline简介",slug:"pipeline简介",normalizedTitle:"pipeline简介",charIndex:2283},{level:2,title:"安装Pipeline插件",slug:"安装pipeline插件",normalizedTitle:"安装pipeline插件",charIndex:2964},{level:2,title:"Pipeline语法快速入门",slug:"pipeline语法快速入门",normalizedTitle:"pipeline语法快速入门",charIndex:3021},{level:3,title:"Declarative声明式-Pipeline",slug:"declarative声明式-pipeline",normalizedTitle:"declarative声明式-pipeline",charIndex:3040},{level:3,title:"Scripted Pipeline脚本式-Pipeline",slug:"scripted-pipeline脚本式-pipeline",normalizedTitle:"scripted pipeline脚本式-pipeline",charIndex:3818},{level:3,title:"Pipeline Script from SCM",slug:"pipeline-script-from-scm",normalizedTitle:"pipeline script from scm",charIndex:6770}],headersStr:"Jenkins构建的项目类型介绍 自由风格项目构建 拉取代码 编译打包 部署 演示改动代码后的持续集成 Maven项目构建 Pipeline流水线项目构建 Pipeline简介 安装Pipeline插件 Pipeline语法快速入门 Declarative声明式-Pipeline Scripted Pipeline脚本式-Pipeline Pipeline Script from SCM",content:"# Jenkins构建Maven项目\n\n\n# Jenkins构建的项目类型介绍\n\nJenkins中自动构建项目的类型有很多，常用的有以下三种:\n\n * 自由风格软件项目(FreeStyle Project)\n * Maven项目(Maven Project)\n * 流水线项目(Pipeline Project)\n\n每种类型的构建其实都可以完成一样的构建过程与结果，只是在操作方式、灵活度等方面有所区别，在 实际开发中可以根据自己的需求和习惯来选择。(PS:个人推荐使用流水线类型，因为灵活度非常高)\n\n\n# 自由风格项目构建\n\n下面演示创建一个自由风格项目来完成项目的集成过程:\n\n提示\n\n拉取代码->编译->打包->部署\n\n\n# 拉取代码\n\n1)创建项目\n\n\n\n2)配置源码管理，从gitlab拉取代码\n\n\n\n\n# 编译打包\n\n提示\n\n构建->添加构建步骤->执行 Shell\n\necho \"开始编译和打包\" \nmvn clean package \necho \"编译和打包结束\"\n\n\n\n\n\n# 部署\n\n把项目部署到远程的Tomcat里面\n\n1)安装 Deploy to container插件\n\n提示\n\nJenkins本身无法实现远程部署到Tomcat的功能，需要安装Deploy to container插件实现\n\n\n\n2)添加Tomcat用户凭证 jenkins-tomcat-deploy-creds\n\n\n\n3)添加构建后操作\n\n\n\n\n\n点击\"立即构建\"，开始构建过程\n\n[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ web-demo ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ web-demo ---\n[INFO] No tests to run.\n[INFO] \n[INFO] --- maven-war-plugin:3.2.2:war (default-war) @ web-demo ---\n[INFO] Packaging webapp\n[INFO] Assembling webapp [web-demo] in [/var/lib/jenkins/workspace/web-demo/target/web-demo]\n[INFO] Processing war project\n[INFO] Copying webapp resources [/var/lib/jenkins/workspace/web-demo/src/main/webapp]\n[INFO] Webapp assembled in [61 msecs]\n[INFO] Building war: /var/lib/jenkins/workspace/web-demo/target/web-demo.war\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  4.462 s\n[INFO] Finished at: 2021-06-24T23:05:22+08:00\n[INFO] ------------------------------------------------------------------------\n+ echo 编译和打包结束\n编译和打包结束\n[DeployPublisher][INFO] Attempting to deploy 1 war file(s)\n[DeployPublisher][INFO] Deploying /var/lib/jenkins/workspace/web-demo/target/web-demo.war to container Tomcat 9.x Remote with context null\n  [/var/lib/jenkins/workspace/web-demo/target/web-demo.war] is not deployed. Doing a fresh deployment.\n  Deploying [/var/lib/jenkins/workspace/web-demo/target/web-demo.war]\nFinished: SUCCESS\n\n\n4)部署成功后，访问项目 http://192.168.50.103:8080/web-demo/\n\n\n\n\n# 演示改动代码后的持续集成\n\n1)IDEA中源码修改并提交到gitlab\n\n2)在Jenkins中项目重新构建\n\n3)访问Tomcat\n\n\n# Maven项目构建\n\n1)安装Maven Integration插件\n\n\n\n2)创建Maven项目\n\n\n\n3)配置项目 拉取代码和远程部署的过程和自由风格项目一样，只是\"构建\"部分不同\n\n\n\n\n# Pipeline流水线项目构建\n\n\n# Pipeline简介\n\n1)概念\n\nPipeline，简单来说就是一套运行在 Jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。\n\n2)使用Pipeline有以下好处(来自翻译自官方文档):\n\n * 代码:Pipeline以代码的形式实现，通常被检入源代码控制，使团队能够编辑，审查和迭代其传送流程。\n * 持久:无论是计划内的还是计划外的服务器重启，Pipeline都是可恢复的。\n * 可停止:Pipeline可接 收交互式输入，以确定是否继续执行Pipeline。\n * 多功能:Pipeline支持现实世界中复杂的持续交付要求。它支持fork/join、循环执行，并行执行任务的功能。\n * 可扩展:Pipeline插件支持其DSL的自定义扩展 以及与其他插件集成的多个选项。\n\n3)如何创建 Jenkins Pipeline呢?\n\n * Pipeline 脚本是由 Groovy 语言实现的，但是我们没必要单独去学习 Groovy\n * Pipeline 支持两种语法:Declarative(声明式)和 Scripted Pipeline(脚本式)语法\n * Pipeline 也有两种创建方法:可以直接在 Jenkins 的 Web UI 界面中输入脚本;也可以通过创建一 个 Jenkinsfile 脚本文件放入项目源码库中(一般我们都推荐在 Jenkins 中直接从源代码控制(SCM) 中直接载入 Jenkinsfile Pipeline 这种方法)。\n\n\n# 安装Pipeline插件\n\n插件管理->可选插件\n\n\n\n安装插件后，创建项目的时候多了“流水线”类型\n\n\n\n\n# Pipeline语法快速入门\n\n\n# Declarative声明式-Pipeline\n\n创建项目\n\n\n\n流水线->选择HelloWorld模板\n\n\n\n生成内容如下:\n\npipeline {\n    agent any\n\n    stages {\n        stage('Hello') {\n            steps {\n                echo 'Hello World'\n            }\n        }\n    }\n}\n\n\nstages:代表整个流水线的所有执行阶段。通常stages只有1个，里面包含多个stage\n\nstage:代表流水线中的某个阶段，可能出现n个。一般分为拉取代码，编译构建，部署等阶段。\n\nsteps:代表一个阶段内需要执行的逻辑。steps里面是shell脚本，git拉取代码，ssh远程发布等任意内容。 编写一个简单声明式Pipeline:\n\n编写一个简单声明式Pipeline:\n\npipeline {\n    agent any\n\n    stages {\n        stage('Checkout') {\n            steps {\n                echo '拉取代码'\n            }\n        }\n        stage('Build') {\n            steps {\n                echo '编译构建'\n            }\n        }\n        stage('Deoloy') {\n            steps {\n                echo '项目部署'\n            }\n        }\n    }\n}\n\n\n点击构建，可以看到整个构建过程\n\n\n\n\n# Scripted Pipeline脚本式-Pipeline\n\n创建项目\n\n\n\n这次选择\"Scripted Pipeline\"\n\nnode {\n    def mvnHome\n    stage('Preparation') { // for display purposes\n        // Get some code from a GitHub repository\n        git 'https://github.com/jglick/simple-maven-project-with-tests.git'\n        // Get the Maven tool.\n        // ** NOTE: This 'M3' Maven tool must be configured\n        // **       in the global configuration.\n        mvnHome = tool 'M3'\n    }\n    stage('Build') {\n        // Run the maven build\n        withEnv([\"MVN_HOME=$mvnHome\"]) {\n            if (isUnix()) {\n                sh '\"$MVN_HOME/bin/mvn\" -Dmaven.test.failure.ignore clean package'\n            } else {\n                bat(/\"%MVN_HOME%\\bin\\mvn\" -Dmaven.test.failure.ignore clean package/)\n            }\n        }\n    }\n    stage('Results') {\n        junit '**/target/surefire-reports/TEST-*.xml'\n        archiveArtifacts 'target/*.jar'\n    }\n}\n\n\n * Node:节点，一个 Node 就是一个 Jenkins 节点，Master 或者 Agent，是执行 Step 的具体运行 环境，后续讲到Jenkins的Master-Slave架构的时候用到。\n * Stage:阶段，一个 Pipeline 可以划分为若干个 Stage，每个 Stage 代表一组操作，比如: Build、Test、Deploy，Stage 是一个逻辑分组的概念。\n * Step:步骤，Step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 Docker 镜像， 由各类 Jenkins 插件提供，比如命令:sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令 一样。\n\n编写一个简单的脚本式Pipeline\n\nnode {\n    def mvnHome\n    stage('Checkout') { // for display purposes\n        echo '拉取代码'\n    }\n    stage('Build') {\n        echo '编译构建'\n    }\n    stage('Deoloy') {\n        echo '项目部署'\n    }\n}\n\n\n构建结果和声明式一样!\n\n拉取代码\n\npipeline {\n    agent any\n\n    stages {\n        stage('Checkout') {\n            steps {\n                checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'adf6d5d5-d16c-4665-b85b-ed720f9641ad', url: 'git@192.168.50.101:jack/web-demo.git']]])\n            }\n        }\n    }\n}\n\n\n编译打包\n\npipeline {\n    agent any\n\n    stages {\n        stage('Checkout') {\n            steps {\n                checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'adf6d5d5-d16c-4665-b85b-ed720f9641ad', url: 'git@192.168.50.101:jack/web-demo.git']]])\n            }\n        }\n        stage('Build') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n    }\n}\n\n\n部署\n\npipeline {\n    agent any\n\n    stages {\n        stage('Checkout') {\n            steps {\n                checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'adf6d5d5-d16c-4665-b85b-ed720f9641ad', url: 'git@192.168.50.101:jack/web-demo.git']]])\n            }\n        }\n        stage('Build') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n        stage('Deoloy') {\n            steps {\n                deploy adapters: [tomcat9(credentialsId: 'dd2801ae-50d3-472c-98ee-bfc1265fef97', path: '', url: 'http://192.168.50.103:8080')], contextPath: null, war: 'target/*.war'\n            }\n        }\n    }\n}\n\n\n\n# Pipeline Script from SCM\n\n刚才我们都是直接在Jenkins的UI界面编写Pipeline代码，这样不方便脚本维护，建议把Pipeline脚本放在项目中(一起进行版本控制)\n\n1)在项目根目录建立Jenkinsfile文件，把内容复制到该文件中\n\n\n\n把Jenkinsfile上传到Gitlab\n\n2)在项目中引用该文件\n\n",normalizedContent:"# jenkins构建maven项目\n\n\n# jenkins构建的项目类型介绍\n\njenkins中自动构建项目的类型有很多，常用的有以下三种:\n\n * 自由风格软件项目(freestyle project)\n * maven项目(maven project)\n * 流水线项目(pipeline project)\n\n每种类型的构建其实都可以完成一样的构建过程与结果，只是在操作方式、灵活度等方面有所区别，在 实际开发中可以根据自己的需求和习惯来选择。(ps:个人推荐使用流水线类型，因为灵活度非常高)\n\n\n# 自由风格项目构建\n\n下面演示创建一个自由风格项目来完成项目的集成过程:\n\n提示\n\n拉取代码->编译->打包->部署\n\n\n# 拉取代码\n\n1)创建项目\n\n\n\n2)配置源码管理，从gitlab拉取代码\n\n\n\n\n# 编译打包\n\n提示\n\n构建->添加构建步骤->执行 shell\n\necho \"开始编译和打包\" \nmvn clean package \necho \"编译和打包结束\"\n\n\n\n\n\n# 部署\n\n把项目部署到远程的tomcat里面\n\n1)安装 deploy to container插件\n\n提示\n\njenkins本身无法实现远程部署到tomcat的功能，需要安装deploy to container插件实现\n\n\n\n2)添加tomcat用户凭证 jenkins-tomcat-deploy-creds\n\n\n\n3)添加构建后操作\n\n\n\n\n\n点击\"立即构建\"，开始构建过程\n\n[info] --- maven-compiler-plugin:3.8.0:testcompile (default-testcompile) @ web-demo ---\n[info] no sources to compile\n[info] \n[info] --- maven-surefire-plugin:2.22.1:test (default-test) @ web-demo ---\n[info] no tests to run.\n[info] \n[info] --- maven-war-plugin:3.2.2:war (default-war) @ web-demo ---\n[info] packaging webapp\n[info] assembling webapp [web-demo] in [/var/lib/jenkins/workspace/web-demo/target/web-demo]\n[info] processing war project\n[info] copying webapp resources [/var/lib/jenkins/workspace/web-demo/src/main/webapp]\n[info] webapp assembled in [61 msecs]\n[info] building war: /var/lib/jenkins/workspace/web-demo/target/web-demo.war\n[info] ------------------------------------------------------------------------\n[info] build success\n[info] ------------------------------------------------------------------------\n[info] total time:  4.462 s\n[info] finished at: 2021-06-24t23:05:22+08:00\n[info] ------------------------------------------------------------------------\n+ echo 编译和打包结束\n编译和打包结束\n[deploypublisher][info] attempting to deploy 1 war file(s)\n[deploypublisher][info] deploying /var/lib/jenkins/workspace/web-demo/target/web-demo.war to container tomcat 9.x remote with context null\n  [/var/lib/jenkins/workspace/web-demo/target/web-demo.war] is not deployed. doing a fresh deployment.\n  deploying [/var/lib/jenkins/workspace/web-demo/target/web-demo.war]\nfinished: success\n\n\n4)部署成功后，访问项目 http://192.168.50.103:8080/web-demo/\n\n\n\n\n# 演示改动代码后的持续集成\n\n1)idea中源码修改并提交到gitlab\n\n2)在jenkins中项目重新构建\n\n3)访问tomcat\n\n\n# maven项目构建\n\n1)安装maven integration插件\n\n\n\n2)创建maven项目\n\n\n\n3)配置项目 拉取代码和远程部署的过程和自由风格项目一样，只是\"构建\"部分不同\n\n\n\n\n# pipeline流水线项目构建\n\n\n# pipeline简介\n\n1)概念\n\npipeline，简单来说就是一套运行在 jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。\n\n2)使用pipeline有以下好处(来自翻译自官方文档):\n\n * 代码:pipeline以代码的形式实现，通常被检入源代码控制，使团队能够编辑，审查和迭代其传送流程。\n * 持久:无论是计划内的还是计划外的服务器重启，pipeline都是可恢复的。\n * 可停止:pipeline可接 收交互式输入，以确定是否继续执行pipeline。\n * 多功能:pipeline支持现实世界中复杂的持续交付要求。它支持fork/join、循环执行，并行执行任务的功能。\n * 可扩展:pipeline插件支持其dsl的自定义扩展 以及与其他插件集成的多个选项。\n\n3)如何创建 jenkins pipeline呢?\n\n * pipeline 脚本是由 groovy 语言实现的，但是我们没必要单独去学习 groovy\n * pipeline 支持两种语法:declarative(声明式)和 scripted pipeline(脚本式)语法\n * pipeline 也有两种创建方法:可以直接在 jenkins 的 web ui 界面中输入脚本;也可以通过创建一 个 jenkinsfile 脚本文件放入项目源码库中(一般我们都推荐在 jenkins 中直接从源代码控制(scm) 中直接载入 jenkinsfile pipeline 这种方法)。\n\n\n# 安装pipeline插件\n\n插件管理->可选插件\n\n\n\n安装插件后，创建项目的时候多了“流水线”类型\n\n\n\n\n# pipeline语法快速入门\n\n\n# declarative声明式-pipeline\n\n创建项目\n\n\n\n流水线->选择helloworld模板\n\n\n\n生成内容如下:\n\npipeline {\n    agent any\n\n    stages {\n        stage('hello') {\n            steps {\n                echo 'hello world'\n            }\n        }\n    }\n}\n\n\nstages:代表整个流水线的所有执行阶段。通常stages只有1个，里面包含多个stage\n\nstage:代表流水线中的某个阶段，可能出现n个。一般分为拉取代码，编译构建，部署等阶段。\n\nsteps:代表一个阶段内需要执行的逻辑。steps里面是shell脚本，git拉取代码，ssh远程发布等任意内容。 编写一个简单声明式pipeline:\n\n编写一个简单声明式pipeline:\n\npipeline {\n    agent any\n\n    stages {\n        stage('checkout') {\n            steps {\n                echo '拉取代码'\n            }\n        }\n        stage('build') {\n            steps {\n                echo '编译构建'\n            }\n        }\n        stage('deoloy') {\n            steps {\n                echo '项目部署'\n            }\n        }\n    }\n}\n\n\n点击构建，可以看到整个构建过程\n\n\n\n\n# scripted pipeline脚本式-pipeline\n\n创建项目\n\n\n\n这次选择\"scripted pipeline\"\n\nnode {\n    def mvnhome\n    stage('preparation') { // for display purposes\n        // get some code from a github repository\n        git 'https://github.com/jglick/simple-maven-project-with-tests.git'\n        // get the maven tool.\n        // ** note: this 'm3' maven tool must be configured\n        // **       in the global configuration.\n        mvnhome = tool 'm3'\n    }\n    stage('build') {\n        // run the maven build\n        withenv([\"mvn_home=$mvnhome\"]) {\n            if (isunix()) {\n                sh '\"$mvn_home/bin/mvn\" -dmaven.test.failure.ignore clean package'\n            } else {\n                bat(/\"%mvn_home%\\bin\\mvn\" -dmaven.test.failure.ignore clean package/)\n            }\n        }\n    }\n    stage('results') {\n        junit '**/target/surefire-reports/test-*.xml'\n        archiveartifacts 'target/*.jar'\n    }\n}\n\n\n * node:节点，一个 node 就是一个 jenkins 节点，master 或者 agent，是执行 step 的具体运行 环境，后续讲到jenkins的master-slave架构的时候用到。\n * stage:阶段，一个 pipeline 可以划分为若干个 stage，每个 stage 代表一组操作，比如: build、test、deploy，stage 是一个逻辑分组的概念。\n * step:步骤，step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 docker 镜像， 由各类 jenkins 插件提供，比如命令:sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令 一样。\n\n编写一个简单的脚本式pipeline\n\nnode {\n    def mvnhome\n    stage('checkout') { // for display purposes\n        echo '拉取代码'\n    }\n    stage('build') {\n        echo '编译构建'\n    }\n    stage('deoloy') {\n        echo '项目部署'\n    }\n}\n\n\n构建结果和声明式一样!\n\n拉取代码\n\npipeline {\n    agent any\n\n    stages {\n        stage('checkout') {\n            steps {\n                checkout([$class: 'gitscm', branches: [[name: '*/master']], extensions: [], userremoteconfigs: [[credentialsid: 'adf6d5d5-d16c-4665-b85b-ed720f9641ad', url: 'git@192.168.50.101:jack/web-demo.git']]])\n            }\n        }\n    }\n}\n\n\n编译打包\n\npipeline {\n    agent any\n\n    stages {\n        stage('checkout') {\n            steps {\n                checkout([$class: 'gitscm', branches: [[name: '*/master']], extensions: [], userremoteconfigs: [[credentialsid: 'adf6d5d5-d16c-4665-b85b-ed720f9641ad', url: 'git@192.168.50.101:jack/web-demo.git']]])\n            }\n        }\n        stage('build') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n    }\n}\n\n\n部署\n\npipeline {\n    agent any\n\n    stages {\n        stage('checkout') {\n            steps {\n                checkout([$class: 'gitscm', branches: [[name: '*/master']], extensions: [], userremoteconfigs: [[credentialsid: 'adf6d5d5-d16c-4665-b85b-ed720f9641ad', url: 'git@192.168.50.101:jack/web-demo.git']]])\n            }\n        }\n        stage('build') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n        stage('deoloy') {\n            steps {\n                deploy adapters: [tomcat9(credentialsid: 'dd2801ae-50d3-472c-98ee-bfc1265fef97', path: '', url: 'http://192.168.50.103:8080')], contextpath: null, war: 'target/*.war'\n            }\n        }\n    }\n}\n\n\n\n# pipeline script from scm\n\n刚才我们都是直接在jenkins的ui界面编写pipeline代码，这样不方便脚本维护，建议把pipeline脚本放在项目中(一起进行版本控制)\n\n1)在项目根目录建立jenkinsfile文件，把内容复制到该文件中\n\n\n\n把jenkinsfile上传到gitlab\n\n2)在项目中引用该文件\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Jenkins常用构建触发器",frontmatter:{title:"Jenkins常用构建触发器",date:"2022-02-09T13:05:17.000Z",permalink:"/pages/bf228f/"},regularPath:"/20.DevOps/02.%E5%AE%9E%E6%88%98/02.Jenkins%E5%B8%B8%E7%94%A8%E6%9E%84%E5%BB%BA%E8%A7%A6%E5%8F%91%E5%99%A8.html",relativePath:"20.DevOps/02.实战/02.Jenkins常用构建触发器.md",key:"v-0f2a3d88",path:"/pages/bf228f/",headers:[{level:2,title:"Jenkins内置建触发器",slug:"jenkins内置建触发器",normalizedTitle:"jenkins内置建触发器",charIndex:21},{level:3,title:"触发远程构建",slug:"触发远程构建",normalizedTitle:"触发远程构建",charIndex:58},{level:3,title:"其他工程构建后触发",slug:"其他工程构建后触发",normalizedTitle:"其他工程构建后触发",charIndex:68},{level:3,title:"定时构建",slug:"定时构建",normalizedTitle:"定时构建",charIndex:119},{level:3,title:"轮询SCM",slug:"轮询scm",normalizedTitle:"轮询scm",charIndex:147},{level:2,title:"Git hook自动触发构建",slug:"git-hook自动触发构建",normalizedTitle:"git hook自动触发构建",charIndex:815},{level:3,title:"安装Gitlab Hook插件",slug:"安装gitlab-hook插件",normalizedTitle:"安装gitlab hook插件",charIndex:956},{level:3,title:"Jenkins设置自动构建",slug:"jenkins设置自动构建",normalizedTitle:"jenkins设置自动构建",charIndex:1010},{level:3,title:"Gitlab配置webhook",slug:"gitlab配置webhook",normalizedTitle:"gitlab配置webhook",charIndex:1225},{level:3,title:"Jenkins添加凭证",slug:"jenkins添加凭证",normalizedTitle:"jenkins添加凭证",charIndex:1694},{level:3,title:"测试Webhooks",slug:"测试webhooks",normalizedTitle:"测试webhooks",charIndex:1838}],headersStr:"Jenkins内置建触发器 触发远程构建 其他工程构建后触发 定时构建 轮询SCM Git hook自动触发构建 安装Gitlab Hook插件 Jenkins设置自动构建 Gitlab配置webhook Jenkins添加凭证 测试Webhooks",content:'# Jenkins常用构建触发器\n\n\n# Jenkins内置建触发器\n\nJenkins内置4种构建触发器:\n\n * 触发远程构建\n * 其他工程构建后触发(Build after other projects are build)\n * 定时构建(Build periodically)\n * 轮询SCM(Poll SCM)\n\n\n# 触发远程构建\n\n提示\n\n身份验证令牌一般是加密字符串\n\n\n\n触发构建url: http://192.168.50.102:8080/job/web-demo-pipeline02/build?token=123456\n\n\n# 其他工程构建后触发\n\n1)创建pre_project流水线工程\n\n提示\n\n一般在多模块项目存在依赖关系时使用\n\n\n\n2)配置需要触发的工程\n\n提示\n\n配置多个依赖项目时使用,分割\n\n\n\n\n# 定时构建\n\n\n\n定时字符串从左往右分别为: 分 时 日 月 周\n\n一些定时表达式的例子:\n\n每30分钟构建一次:H代表形参 H/30 * * * * 10:02 10:32\n每2个小时构建一次: H H/2 * * *\n每天的8点，12点，22点，一天构建3次: (多个时间点中间用逗号隔开) 0 8,12,22 * * * \n每天中午12点定时构建一次 H 12 * * *\n每天下午18点定时构建一次 H 18 * * * \n在每个小时的前半个小时内的每10分钟 H(0-29)/10 * * * *\n每两小时一次，每个工作日上午9点到下午5点(也许是上午10:38，下午12:38，下午2:38，下午 4:38) H H(9-16)/2 * * 1-5\n\n\n\n# 轮询SCM\n\n轮询SCM，是指定时扫描本地代码仓库的代码是否有变更，如果代码有变更就触发项目构建。\n\n\n\n注意\n\n轮询SCM，是指定时扫描本地代码仓库的代码是否有变更，如果代码有变更就触发项目构建。\n\n\n# Git hook自动触发构建\n\n刚才我们看到在Jenkins的内置构建触发器中，轮询SCM可以实现Gitlab代码更新，项目自动构建，但是该方案的性能不佳。那有没有更好的方案呢? 有的。就是利用Gitlab的webhook实现代码push到仓库，立即触发项目自动构建。\n\n\n\n\n# 安装Gitlab Hook插件\n\n需要安装两个插件: GitLab 和 Gitlab Hook\n\n\n\n\n# Jenkins设置自动构建\n\n\n\nGitLab webhook URL:\n\nhttp://192.168.50.102:8080/project/web-demo-pipeline02\n\n\nSecret token: 点击"Generate"\n\n726808fb71ce6d9c7ac2610ce718d5a2\n\n\n等会需要把生成的GitLab webhook URL 和 Secret token配置到Gitlab中。\n\n\n# Gitlab配置webhook\n\n1)开启webhook功能\n\n提示\n\n开启webhook功能需要使用Gitlab的root用户操作\n\n使用root账户登录到后台，点击Admin Area -> Settings -> Network 勾选"Allow requests to the local network from web hooks and services"\n\n\n\n2)在项目添加webhook\n\n点击项目->Settings->Integrations\n\n提示\n\n项目添加webhook，如果项目使用普通用户创建（例如：jack）需要使用普通用户进行操作\n\n\n\n3)创建访问令牌\n\nEdit profile -> Access Tokens 点击"Create personal access token"\n\n\n\n提示\n\n注意保存access token,只显示一次，刷新后不再显示。（可以重新创建）\n\n\n\nYour new personal access token\n\nQszazrfXTedjnk-Zw56e\n\n\n\n# Jenkins添加凭证\n\n返回Jenkins，添加类型Gitlab API token , API token填写上一步Gitlab创建的personal access token：QszazrfXTedjnk-Zw56e\n\n\n\n系统管理 -> 系统配置 -> Gitlab\n\n\n\n\n# 测试Webhooks\n\n返回Gitlab，打开web-demo项目 -> Settings -> Webhooks -> Test -> Push events\n\n\n\n提示如下内容表示调用成功，返回Jenkins，项目已经开始构建。\n\nHook executed successfully: HTTP 200\n',normalizedContent:'# jenkins常用构建触发器\n\n\n# jenkins内置建触发器\n\njenkins内置4种构建触发器:\n\n * 触发远程构建\n * 其他工程构建后触发(build after other projects are build)\n * 定时构建(build periodically)\n * 轮询scm(poll scm)\n\n\n# 触发远程构建\n\n提示\n\n身份验证令牌一般是加密字符串\n\n\n\n触发构建url: http://192.168.50.102:8080/job/web-demo-pipeline02/build?token=123456\n\n\n# 其他工程构建后触发\n\n1)创建pre_project流水线工程\n\n提示\n\n一般在多模块项目存在依赖关系时使用\n\n\n\n2)配置需要触发的工程\n\n提示\n\n配置多个依赖项目时使用,分割\n\n\n\n\n# 定时构建\n\n\n\n定时字符串从左往右分别为: 分 时 日 月 周\n\n一些定时表达式的例子:\n\n每30分钟构建一次:h代表形参 h/30 * * * * 10:02 10:32\n每2个小时构建一次: h h/2 * * *\n每天的8点，12点，22点，一天构建3次: (多个时间点中间用逗号隔开) 0 8,12,22 * * * \n每天中午12点定时构建一次 h 12 * * *\n每天下午18点定时构建一次 h 18 * * * \n在每个小时的前半个小时内的每10分钟 h(0-29)/10 * * * *\n每两小时一次，每个工作日上午9点到下午5点(也许是上午10:38，下午12:38，下午2:38，下午 4:38) h h(9-16)/2 * * 1-5\n\n\n\n# 轮询scm\n\n轮询scm，是指定时扫描本地代码仓库的代码是否有变更，如果代码有变更就触发项目构建。\n\n\n\n注意\n\n轮询scm，是指定时扫描本地代码仓库的代码是否有变更，如果代码有变更就触发项目构建。\n\n\n# git hook自动触发构建\n\n刚才我们看到在jenkins的内置构建触发器中，轮询scm可以实现gitlab代码更新，项目自动构建，但是该方案的性能不佳。那有没有更好的方案呢? 有的。就是利用gitlab的webhook实现代码push到仓库，立即触发项目自动构建。\n\n\n\n\n# 安装gitlab hook插件\n\n需要安装两个插件: gitlab 和 gitlab hook\n\n\n\n\n# jenkins设置自动构建\n\n\n\ngitlab webhook url:\n\nhttp://192.168.50.102:8080/project/web-demo-pipeline02\n\n\nsecret token: 点击"generate"\n\n726808fb71ce6d9c7ac2610ce718d5a2\n\n\n等会需要把生成的gitlab webhook url 和 secret token配置到gitlab中。\n\n\n# gitlab配置webhook\n\n1)开启webhook功能\n\n提示\n\n开启webhook功能需要使用gitlab的root用户操作\n\n使用root账户登录到后台，点击admin area -> settings -> network 勾选"allow requests to the local network from web hooks and services"\n\n\n\n2)在项目添加webhook\n\n点击项目->settings->integrations\n\n提示\n\n项目添加webhook，如果项目使用普通用户创建（例如：jack）需要使用普通用户进行操作\n\n\n\n3)创建访问令牌\n\nedit profile -> access tokens 点击"create personal access token"\n\n\n\n提示\n\n注意保存access token,只显示一次，刷新后不再显示。（可以重新创建）\n\n\n\nyour new personal access token\n\nqszazrfxtedjnk-zw56e\n\n\n\n# jenkins添加凭证\n\n返回jenkins，添加类型gitlab api token , api token填写上一步gitlab创建的personal access token：qszazrfxtedjnk-zw56e\n\n\n\n系统管理 -> 系统配置 -> gitlab\n\n\n\n\n# 测试webhooks\n\n返回gitlab，打开web-demo项目 -> settings -> webhooks -> test -> push events\n\n\n\n提示如下内容表示调用成功，返回jenkins，项目已经开始构建。\n\nhook executed successfully: http 200\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Jenkins参数化构建",frontmatter:{title:"Jenkins参数化构建",date:"2022-02-09T13:05:17.000Z",permalink:"/pages/991f2e/"},regularPath:"/20.DevOps/02.%E5%AE%9E%E6%88%98/03.Jenkins%E5%8F%82%E6%95%B0%E5%8C%96%E6%9E%84%E5%BB%BA.html",relativePath:"20.DevOps/02.实战/03.Jenkins参数化构建.md",key:"v-78c2fe6d",path:"/pages/991f2e/",headersStr:null,content:"# Jenkins参数化构建\n\n有时在项目构建的过程中，我们需要根据用户的输入动态传入一些参数，从而影响整个构建结果，这时我们可以使用参数化构建。\n\nJenkins支持非常丰富的参数类型\n\n\n\n接下来演示通过输入gitlab项目的分支名称来部署不同分支项目。\n\n项目创建分支v1，并推送到Gitlab上，这时看到gitlab上有一个两个分支:master和v1\n\n\n\n在Jenkins添加字符串类型参数\n\n\n\n改动pipeline流水线代码，修改*/master替换为*/${branch}\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npipeline {\n    agent any\n\n    stages {\n        stage('Checkout') {\n            steps {\n                checkout([$class: 'GitSCM', branches: [[name: '*/${branch}']], extensions: [], userRemoteConfigs: [[credentialsId: 'adf6d5d5-d16c-4665-b85b-ed720f9641ad', url: 'git@192.168.50.101:jack/web-demo.git']]])\n            }\n        }\n        stage('Build') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n        stage('Deoloy') {\n            steps {\n                deploy adapters: [tomcat9(credentialsId: 'dd2801ae-50d3-472c-98ee-bfc1265fef97', path: '', url: 'http://192.168.50.103:8080')], contextPath: null, war: 'target/*.war'\n            }\n        }\n    }\n}\n\n\n点击Build with Parameters\n\n\n\n输入分支名称，点击\"开始构建\"，构建完成后访问Tomcat查看结果。",normalizedContent:"# jenkins参数化构建\n\n有时在项目构建的过程中，我们需要根据用户的输入动态传入一些参数，从而影响整个构建结果，这时我们可以使用参数化构建。\n\njenkins支持非常丰富的参数类型\n\n\n\n接下来演示通过输入gitlab项目的分支名称来部署不同分支项目。\n\n项目创建分支v1，并推送到gitlab上，这时看到gitlab上有一个两个分支:master和v1\n\n\n\n在jenkins添加字符串类型参数\n\n\n\n改动pipeline流水线代码，修改*/master替换为*/${branch}\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npipeline {\n    agent any\n\n    stages {\n        stage('checkout') {\n            steps {\n                checkout([$class: 'gitscm', branches: [[name: '*/${branch}']], extensions: [], userremoteconfigs: [[credentialsid: 'adf6d5d5-d16c-4665-b85b-ed720f9641ad', url: 'git@192.168.50.101:jack/web-demo.git']]])\n            }\n        }\n        stage('build') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n        stage('deoloy') {\n            steps {\n                deploy adapters: [tomcat9(credentialsid: 'dd2801ae-50d3-472c-98ee-bfc1265fef97', path: '', url: 'http://192.168.50.103:8080')], contextpath: null, war: 'target/*.war'\n            }\n        }\n    }\n}\n\n\n点击build with parameters\n\n\n\n输入分支名称，点击\"开始构建\"，构建完成后访问tomcat查看结果。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Jenkins配置邮箱服务器",frontmatter:{title:"Jenkins配置邮箱服务器",date:"2022-02-09T13:05:17.000Z",permalink:"/pages/bd7581/"},regularPath:"/20.DevOps/02.%E5%AE%9E%E6%88%98/04.Jenkins%E9%85%8D%E7%BD%AE%E9%82%AE%E7%AE%B1%E6%9C%8D%E5%8A%A1%E5%99%A8.html",relativePath:"20.DevOps/02.实战/04.Jenkins配置邮箱服务器.md",key:"v-7183b4d5",path:"/pages/bd7581/",headersStr:null,content:'# Jenkins配置邮箱服务器\n\nJenkins配置邮箱服务器，发送构建结果。\n\n安装Email Extension插件\n\n\n\nJenkins设置邮箱相关参数\n\n系统管理 -> 系统配置\n\n\n\n设置邮件参数\n\n\n\n设置Jenkins默认邮箱信息\n\n\n\n配置完成后点击"Test configuration"按钮发送测试邮件\n\n准备邮件内容\n\n在项目根目录编写email.html，并把文件推送到Gitlab，内容如下:\n\n\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>${ENV, var="JOB_NAME"}-第${BUILD_NUMBER}次构建日志</title>\n</head>\n<body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n<table width="95%" cellpadding="0" cellspacing="0"\n       style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif">\n    <tr>\n        <td>(本邮件是程序自动下发的，请勿回复!)</td>\n    </tr>\n    <tr>\n        <td>\n            <h2>\n                <font color="#0000FF">构建结果 - ${BUILD_STATUS}</font>\n            </h2>\n        </td>\n    </tr>\n    <tr>\n        <td><br/>\n            <b><font color="#0B610B">构建信息</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>项目名称&nbsp;:&nbsp;${PROJECT_NAME}</li>\n                <li>构建编号&nbsp;:&nbsp;第${BUILD_NUMBER}次构建</li>\n                <li>触发原因:&nbsp;${CAUSE}</li>\n                <li>构建日志:&nbsp;<a href="${BUILD_URL}console">${BUILD_URL}console</a></li>\n                <li>构建&nbsp;&nbsp;Url&nbsp;:&nbsp;<a href="${BUILD_URL}">${BUILD_URL}</a></li>\n                <li>工作目录&nbsp;:&nbsp;<a href="${PROJECT_URL}ws">${PROJECT_URL}ws</a></li>\n                <li>项目&nbsp;&nbsp;Url&nbsp;:&nbsp;<a href="${PROJECT_URL}">${PROJECT_URL}</a></li>\n            </ul>\n        </td>\n    </tr>\n    <tr>\n        <td><b>\n            <font color="#0B610B">Changes Since Last Successful Build:</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>历史变更记录 : <a href="${PROJECT_URL}changes">${PROJECT_URL}changes</a></li>\n            </ul> ${CHANGES_SINCE_LAST_SUCCESS,reverse=true, format="Changes for Build #%n:<br />%c<br />",showPaths=true,changesFormat="<pre>[%a]<br />%m</pre>",pathFormat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n        </td>\n    </tr>\n    <tr>\n        <td><b>Failed Test Results</b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td><pre\n                style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif">$FAILED_TESTS</pre>\n            <br/></td>\n    </tr>\n    <tr>\n        <td><b><font color="#0B610B">构建日志 (最后 100行):</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td><textarea cols="80" rows="30" readonly="readonly"\n                      style="font-family: Courier New">${BUILD_LOG,maxLines=100}</textarea>\n        </td>\n    </tr>\n</table>\n</body>\n</html>\n\n\n编写Jenkinsfile添加构建后发送邮件\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\npipeline {\n    agent any\n\n    stages {\n        stage(\'Checkout\') {\n            steps {\n                checkout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']], extensions: [], userRemoteConfigs: [[credentialsId: \'adf6d5d5-d16c-4665-b85b-ed720f9641ad\', url: \'git@192.168.50.101:jack/web-demo.git\']]])\n            }\n        }\n        stage(\'Build\') {\n            steps {\n                sh \'mvn clean package\'\n            }\n        }\n        stage(\'Deoloy\') {\n            steps {\n                deploy adapters: [tomcat9(credentialsId: \'dd2801ae-50d3-472c-98ee-bfc1265fef97\', path: \'\', url: \'http://192.168.50.103:8080\')], contextPath: null, war: \'target/*.war\'\n            }\n        }\n    }\n\n    post {\n      always {\n        // One or more steps need to be included within each condition\'s block.\n        // 发送系统通知邮件\n        emailext body: \'${FILE,path="email.html"}\', subject: \'构建通知:${PROJECT_NAME} - Build # ${BUILD_NUMBER} - ${BUILD_STATUS}!\', to: \'785056500@qq.com\'\n      }\n    }\n}\n\n\n提交代码触发webhooks,jenkins开始自动编译打包发布项目，最后发送构建结果邮件通知。\n\n\n\n邮件相关全局参数参考列表\n\n系统设置->Extended E-mail Notification->Content Token Reference，点击旁边的?号\n\n',normalizedContent:'# jenkins配置邮箱服务器\n\njenkins配置邮箱服务器，发送构建结果。\n\n安装email extension插件\n\n\n\njenkins设置邮箱相关参数\n\n系统管理 -> 系统配置\n\n\n\n设置邮件参数\n\n\n\n设置jenkins默认邮箱信息\n\n\n\n配置完成后点击"test configuration"按钮发送测试邮件\n\n准备邮件内容\n\n在项目根目录编写email.html，并把文件推送到gitlab，内容如下:\n\n\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>${env, var="job_name"}-第${build_number}次构建日志</title>\n</head>\n<body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n<table width="95%" cellpadding="0" cellspacing="0"\n       style="font-size: 11pt; font-family: tahoma, arial, helvetica, sans-serif">\n    <tr>\n        <td>(本邮件是程序自动下发的，请勿回复!)</td>\n    </tr>\n    <tr>\n        <td>\n            <h2>\n                <font color="#0000ff">构建结果 - ${build_status}</font>\n            </h2>\n        </td>\n    </tr>\n    <tr>\n        <td><br/>\n            <b><font color="#0b610b">构建信息</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>项目名称&nbsp;:&nbsp;${project_name}</li>\n                <li>构建编号&nbsp;:&nbsp;第${build_number}次构建</li>\n                <li>触发原因:&nbsp;${cause}</li>\n                <li>构建日志:&nbsp;<a href="${build_url}console">${build_url}console</a></li>\n                <li>构建&nbsp;&nbsp;url&nbsp;:&nbsp;<a href="${build_url}">${build_url}</a></li>\n                <li>工作目录&nbsp;:&nbsp;<a href="${project_url}ws">${project_url}ws</a></li>\n                <li>项目&nbsp;&nbsp;url&nbsp;:&nbsp;<a href="${project_url}">${project_url}</a></li>\n            </ul>\n        </td>\n    </tr>\n    <tr>\n        <td><b>\n            <font color="#0b610b">changes since last successful build:</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>历史变更记录 : <a href="${project_url}changes">${project_url}changes</a></li>\n            </ul> ${changes_since_last_success,reverse=true, format="changes for build #%n:<br />%c<br />",showpaths=true,changesformat="<pre>[%a]<br />%m</pre>",pathformat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n        </td>\n    </tr>\n    <tr>\n        <td><b>failed test results</b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td><pre\n                style="font-size: 11pt; font-family: tahoma, arial, helvetica, sans-serif">$failed_tests</pre>\n            <br/></td>\n    </tr>\n    <tr>\n        <td><b><font color="#0b610b">构建日志 (最后 100行):</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td><textarea cols="80" rows="30" readonly="readonly"\n                      style="font-family: courier new">${build_log,maxlines=100}</textarea>\n        </td>\n    </tr>\n</table>\n</body>\n</html>\n\n\n编写jenkinsfile添加构建后发送邮件\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\npipeline {\n    agent any\n\n    stages {\n        stage(\'checkout\') {\n            steps {\n                checkout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']], extensions: [], userremoteconfigs: [[credentialsid: \'adf6d5d5-d16c-4665-b85b-ed720f9641ad\', url: \'git@192.168.50.101:jack/web-demo.git\']]])\n            }\n        }\n        stage(\'build\') {\n            steps {\n                sh \'mvn clean package\'\n            }\n        }\n        stage(\'deoloy\') {\n            steps {\n                deploy adapters: [tomcat9(credentialsid: \'dd2801ae-50d3-472c-98ee-bfc1265fef97\', path: \'\', url: \'http://192.168.50.103:8080\')], contextpath: null, war: \'target/*.war\'\n            }\n        }\n    }\n\n    post {\n      always {\n        // one or more steps need to be included within each condition\'s block.\n        // 发送系统通知邮件\n        emailext body: \'${file,path="email.html"}\', subject: \'构建通知:${project_name} - build # ${build_number} - ${build_status}!\', to: \'785056500@qq.com\'\n      }\n    }\n}\n\n\n提交代码触发webhooks,jenkins开始自动编译打包发布项目，最后发送构建结果邮件通知。\n\n\n\n邮件相关全局参数参考列表\n\n系统设置->extended e-mail notification->content token reference，点击旁边的?号\n\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"SonarQube自动代码审查工具",frontmatter:{title:"SonarQube自动代码审查工具",date:"2022-02-09T13:05:17.000Z",permalink:"/pages/e2748f/"},regularPath:"/20.DevOps/02.%E5%AE%9E%E6%88%98/05.SonarQube%E8%87%AA%E5%8A%A8%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5%E5%B7%A5%E5%85%B7.html",relativePath:"20.DevOps/02.实战/05.SonarQube自动代码审查工具.md",key:"v-8f10297e",path:"/pages/e2748f/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:24},{level:2,title:"硬件要求",slug:"硬件要求",normalizedTitle:"硬件要求",charIndex:143},{level:2,title:"软件版本",slug:"软件版本",normalizedTitle:"软件版本",charIndex:267},{level:2,title:"安装PostgreSQL",slug:"安装postgresql",normalizedTitle:"安装postgresql",charIndex:453},{level:3,title:"安装PostgreSQL:",slug:"安装postgresql-2",normalizedTitle:"安装postgresql:",charIndex:598},{level:3,title:"PostgreSQL角色和身份认证方式",slug:"postgresql角色和身份认证方式",normalizedTitle:"postgresql角色和身份认证方式",charIndex:1034},{level:3,title:"创建 PostgreSQL 角色和数据库",slug:"创建-postgresql-角色和数据库",normalizedTitle:"创建 postgresql 角色和数据库",charIndex:1675},{level:3,title:"启用远程访问PostgreSQL服务器",slug:"启用远程访问postgresql服务器",normalizedTitle:"启用远程访问postgresql服务器",charIndex:2091},{level:3,title:"将sonarqube用户密码设置为sonarqube",slug:"将sonarqube用户密码设置为sonarqube",normalizedTitle:"将sonarqube用户密码设置为sonarqube",charIndex:3770},{level:2,title:"安装JDK",slug:"安装jdk",normalizedTitle:"安装jdk",charIndex:4429},{level:2,title:"安装SonarQube",slug:"安装sonarqube",normalizedTitle:"安装sonarqube",charIndex:4979}],headersStr:"概述 硬件要求 软件版本 安装PostgreSQL 安装PostgreSQL: PostgreSQL角色和身份认证方式 创建 PostgreSQL 角色和数据库 启用远程访问PostgreSQL服务器 将sonarqube用户密码设置为sonarqube 安装JDK 安装SonarQube",content:'# SonarQube自动代码审查工具\n\n\n# 概述\n\nSonar（SonarQube）是一个开源平台，用于管理源代码的质量。Sonar 不只是一个质量数据报告工具，更是代码质量管理平台。支持的语言包括：Java、PHP、C#、C、Cobol、PL/SQL、Flex 等。\n\n\n\n\n# 硬件要求\n\n下表列出了部署SonarQube的最低和建议的硬件配置。\n\n资源     最小配置    推荐配置\nCPU    2 CPU   8 CPU\nMem    4 GB    16 GB\nDisk   40 GB   160 GB\n\n\n# 软件版本\n\n下表列出了部署SonarQube的依赖软件版本要求。\n\n软件          版本\nJava        Oracle JRE 11 或 OpenJDK 11\nDatabase    PostgreSQL 12\nOS          Ubuntu 20.04 Server LTS\nSonarQube   Community 8.9.1 LTS\n\n\n# 安装PostgreSQL\n\nPostgreSQL 或者 Postgres 是一个开源的，多用途的关系型数据库管理系统。它有很多高级特性，可以允许你构建容错的环境或者复杂的应用。\n\n在写这篇文章的时候，PostgreSQL 在 Ubuntu 官方软件源中的最新可用版本为 12.7。\n\n\n# 安装PostgreSQL:\n\nsudo apt update\nsudo apt install postgresql postgresql-contrib\n\n\n我们同时安装 PostgreSQL contrib 软件包，它可以提供 PostgreSQL 数据库系统的一些额外特性。\n\n一旦安装完成， PostgreSQL 服务将会自动启动。使用psql工具通过连接 PostgreSQL 数据库并且打印它的版本来验证安装：\n\nsudo -u postgres psql -c "SELECT version();"\n\n\n输出：\n\nPostgreSQL 12.7 (Ubuntu 12.7-0ubuntu0.20.04.1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0, 64-bit\n\n\n就这样。PostgreSQL 已经安装好了，你可以开始使用它了。\n\n\n# PostgreSQL角色和身份认证方式\n\nPostgreSQL 数据库访问权限是通过角色来处理的。一个角色代表一个数据库用户或者一个数据库用户组。\n\nPostgreSQL 支持多种身份认证方式。最常用的方法如下：\n\n * Trust - 只要满足pg_hba.conf定义的条件，一个角色就可以不使用密码就能连接服务器\n * Password - 通过密码，一个角色可以连接服务器。密码可以被存储为 scram-sha-256, md5, 和 password(明文)。\n * Ident - 仅仅支持 TCP/IP 连接。它通常通过一个可选的用户名映射表，获取客户端操作系统用户名。\n * Peer - 和 Ident 一样，仅仅支持本地连接。\n\nPostgreSQL 客户端身份验证通常被定义在pg_hba.conf文件中。默认情况下，对于本地连接，PostgreSQL 被设置成身份认证防范 peer。\n\n为了以postgres用户身份登录 PostgreSQL 服务器，首先切换用户，然后使用psql工具访问 PostgreSQL。\n\nsudo su - postgres\npsql\n\n\n从这里开始，你可以与 PostgreSQL 实例交互。退出 PostgreSQL Shell，输入：\n\n\\q\n\n\n你也可以不切换用户，而使用sudo命令访问 PostgreSQL：\n\nsudo -u postgres psql\n\n\n通常，postgres用户仅仅在本地被使用。\n\n\n# 创建 PostgreSQL 角色和数据库\n\n仅仅超级用户和拥有CREATEROLE权限的角色可以创建新角色。\n\n在下面的例子中，我们创建一个名称为sonarqube的角色，一个名称为sonarqube的数据库，并且授予数据库上的权限：\n\n1)创建一个新的 PostgreSQL 角色：\n\nsudo su - postgres -c "createuser sonarqube"\n\n\n2)创建一个新的 PostgreSQL 数据库：\n\nsudo su - postgres -c "createdb sonarqube"\n\n\n想要授权用户操作数据库，连接到 PostgreSQL shell:\n\nsudo -u postgres psql\n\n\n并且运行下面的 query:\n\npostgres=# grant all privileges on database sonarqube to sonarqube;\nGRANT\n\n\n\n# 启用远程访问PostgreSQL服务器\n\n默认情况下，PostgreSQL 服务器仅仅监听本地网络接口：127.0.0.1。\n\n为了允许远程访问你的 PostgreSQL 服务器，打开配置文件postgresql.conf并且在CONNECTIONS AND AUTHENTICATION一节添加listen_addresses = \'*\'。\n\nsudo vim /etc/postgresql/12/main/postgresql.conf\n#------------------------------------------------------------------------------\n# CONNECTIONS AND AUTHENTICATION\n#------------------------------------------------------------------------------\n\n# - Connection Settings -\n\nlisten_addresses = \'*\'     # what IP address(es) to listen on;\n\n\n保存文件并且重启 PostgreSQL 服务：\n\nsudo systemctl restart postgresql\n\n\n使用ss工具验证修改：\n\nss -nlt | grep 5432\n\n\n输出显示 PostgreSQL 服务器正在监听所有的网络接口(0.0.0.0):\n\nLISTEN  0        244              0.0.0.0:5432           0.0.0.0:*              \nLISTEN  0        244                 [::]:5432              [::]:*\n\n\n下一步就是配置服务器接受远程连接，编辑/etc/postgresql/12/main/pg_hba.conf文件。\n\n下面是一些例子，显示不同的用户场景：\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n# TYPE  DATABASE        USER            ADDRESS                 METHOD\n\n# The user jane can access all databases from all locations using md5 password\nhost    all             sonarqube       0.0.0.0/0                md5\n\n# The user jane can access only the janedb from all locations using md5 password\nhost    sonarqube       sonarqube       0.0.0.0/0                md5\n\n# The user jane can access all databases from a trusted location (192.168.1.134) without a password\nhost    all             sonarqube       192.168.50.100             trust\n\n\n重启PostgreSQL服务\n\nsudo systemctl restart postgresql\n\n\n最后一步（可选）就是在你的防火墙上打开端口5432端口。\n\n假设你正在使用UFW来管理你的防火墙，并且你想允许从10.17.10.0/24子网过来的访问，你应该运行下面的命令：\n\nsudo ufw allow proto tcp from 10.17.10.0/24 to any port 5432\n\n\n确保你的防火墙被配置好，并仅仅接受来自受信任 IP 范围的连接。\n\n\n# 将sonarqube用户密码设置为sonarqube\n\nsudo su - postgres\npsql\n\npostgres=# \\password sonarqube\nEnter new password: \nEnter it again: \n\npostgres=# SELECT rolname,rolpassword FROM pg_authid;\n          rolname          |             rolpassword             \n---------------------------+-------------------------------------\n postgres                  | \n pg_monitor                | \n pg_read_all_settings      | \n pg_read_all_stats         | \n pg_stat_scan_tables       | \n pg_read_server_files      | \n pg_write_server_files     | \n pg_execute_server_program | \n pg_signal_backend         | \n sonarqube                 | md5699b64e757e1f31984196747c316e4e3\n(10 rows)\n\n\n\n# 安装JDK\n\n下载 jdk-11.0.11_linux-x64_bin.tar.gz\n\nsudo mkdir /usr/lib/jvm\nsudo tar -zxvf jdk-11.0.11_linux-x64_bin.tar.gz -C /usr/lib/jvm\n\n\n配置环境变量，编辑sudo vi /etc/profile文件，文档最后追加如下内容：\n\nexport ES_JAVA_HOME=/usr/lib/jvm/jdk-11.0.11\nexport CLASSPATH=.:%{ES_JAVA_HOME}/lib\nexport PATH=${ES_JAVA_HOME}/bin:$PATH\n\n\n更新\n\nsource /etc/profile\n\n\n验证\n\nubuntu@sonarqueb:~$ java -version\njava version "11.0.11" 2021-04-20 LTS\nJava(TM) SE Runtime Environment 18.9 (build 11.0.11+9-LTS-194)\nJava HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.11+9-LTS-194, mixed mode)\n\n\n\n# 安装SonarQube\n\n准备工作\n\nsudo sysctl -w vm.max_map_count=524288\nsudo sysctl -w fs.file-max=131072\nulimit -n 131072\nulimit -u 8192\n\n\n官网下载并解压 下载地址: http://www.sonarqube.org/downloads/ 下载SonarQube 8.9.1版本\n\n切换到普通用户\n\n$ cd /home/ubuntu\n$ wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.9.1.44547.zip\n\n\n安装zip\n\nsudo apt install zip unzip\n\n\n解压安装\n\n$ unzip sonarqube-8.9.1.44547.zip \n\n\n配置 SonarQube\n\n$ vi /home/ubuntu/sonarqube-8.9.1.44547/conf/sonar.properties\n\n\n修改如下内容：\n\n# 数据库账户\nsonar.jdbc.username=sonarqube\nsonar.jdbc.password=sonarqube\n# 数据库连接\nsonar.jdbc.url=jdbc:postgresql://localhost/sonarqube?currentSchema=public\nsonar.web.host=0.0.0.0\nsonar.web.port=9000\n\n\n启动测试\n\ncd /home/ubuntu/sonarqube-8.9.1.44547/bin/linux-x86-64\n./sonar.sh console\n\n\n浏览器访问：http://Server_IP:9000，默认管理员账号：admin，密码：admin。首次登录后修改密码。\n\n',normalizedContent:'# sonarqube自动代码审查工具\n\n\n# 概述\n\nsonar（sonarqube）是一个开源平台，用于管理源代码的质量。sonar 不只是一个质量数据报告工具，更是代码质量管理平台。支持的语言包括：java、php、c#、c、cobol、pl/sql、flex 等。\n\n\n\n\n# 硬件要求\n\n下表列出了部署sonarqube的最低和建议的硬件配置。\n\n资源     最小配置    推荐配置\ncpu    2 cpu   8 cpu\nmem    4 gb    16 gb\ndisk   40 gb   160 gb\n\n\n# 软件版本\n\n下表列出了部署sonarqube的依赖软件版本要求。\n\n软件          版本\njava        oracle jre 11 或 openjdk 11\ndatabase    postgresql 12\nos          ubuntu 20.04 server lts\nsonarqube   community 8.9.1 lts\n\n\n# 安装postgresql\n\npostgresql 或者 postgres 是一个开源的，多用途的关系型数据库管理系统。它有很多高级特性，可以允许你构建容错的环境或者复杂的应用。\n\n在写这篇文章的时候，postgresql 在 ubuntu 官方软件源中的最新可用版本为 12.7。\n\n\n# 安装postgresql:\n\nsudo apt update\nsudo apt install postgresql postgresql-contrib\n\n\n我们同时安装 postgresql contrib 软件包，它可以提供 postgresql 数据库系统的一些额外特性。\n\n一旦安装完成， postgresql 服务将会自动启动。使用psql工具通过连接 postgresql 数据库并且打印它的版本来验证安装：\n\nsudo -u postgres psql -c "select version();"\n\n\n输出：\n\npostgresql 12.7 (ubuntu 12.7-0ubuntu0.20.04.1) on x86_64-pc-linux-gnu, compiled by gcc (ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0, 64-bit\n\n\n就这样。postgresql 已经安装好了，你可以开始使用它了。\n\n\n# postgresql角色和身份认证方式\n\npostgresql 数据库访问权限是通过角色来处理的。一个角色代表一个数据库用户或者一个数据库用户组。\n\npostgresql 支持多种身份认证方式。最常用的方法如下：\n\n * trust - 只要满足pg_hba.conf定义的条件，一个角色就可以不使用密码就能连接服务器\n * password - 通过密码，一个角色可以连接服务器。密码可以被存储为 scram-sha-256, md5, 和 password(明文)。\n * ident - 仅仅支持 tcp/ip 连接。它通常通过一个可选的用户名映射表，获取客户端操作系统用户名。\n * peer - 和 ident 一样，仅仅支持本地连接。\n\npostgresql 客户端身份验证通常被定义在pg_hba.conf文件中。默认情况下，对于本地连接，postgresql 被设置成身份认证防范 peer。\n\n为了以postgres用户身份登录 postgresql 服务器，首先切换用户，然后使用psql工具访问 postgresql。\n\nsudo su - postgres\npsql\n\n\n从这里开始，你可以与 postgresql 实例交互。退出 postgresql shell，输入：\n\n\\q\n\n\n你也可以不切换用户，而使用sudo命令访问 postgresql：\n\nsudo -u postgres psql\n\n\n通常，postgres用户仅仅在本地被使用。\n\n\n# 创建 postgresql 角色和数据库\n\n仅仅超级用户和拥有createrole权限的角色可以创建新角色。\n\n在下面的例子中，我们创建一个名称为sonarqube的角色，一个名称为sonarqube的数据库，并且授予数据库上的权限：\n\n1)创建一个新的 postgresql 角色：\n\nsudo su - postgres -c "createuser sonarqube"\n\n\n2)创建一个新的 postgresql 数据库：\n\nsudo su - postgres -c "createdb sonarqube"\n\n\n想要授权用户操作数据库，连接到 postgresql shell:\n\nsudo -u postgres psql\n\n\n并且运行下面的 query:\n\npostgres=# grant all privileges on database sonarqube to sonarqube;\ngrant\n\n\n\n# 启用远程访问postgresql服务器\n\n默认情况下，postgresql 服务器仅仅监听本地网络接口：127.0.0.1。\n\n为了允许远程访问你的 postgresql 服务器，打开配置文件postgresql.conf并且在connections and authentication一节添加listen_addresses = \'*\'。\n\nsudo vim /etc/postgresql/12/main/postgresql.conf\n#------------------------------------------------------------------------------\n# connections and authentication\n#------------------------------------------------------------------------------\n\n# - connection settings -\n\nlisten_addresses = \'*\'     # what ip address(es) to listen on;\n\n\n保存文件并且重启 postgresql 服务：\n\nsudo systemctl restart postgresql\n\n\n使用ss工具验证修改：\n\nss -nlt | grep 5432\n\n\n输出显示 postgresql 服务器正在监听所有的网络接口(0.0.0.0):\n\nlisten  0        244              0.0.0.0:5432           0.0.0.0:*              \nlisten  0        244                 [::]:5432              [::]:*\n\n\n下一步就是配置服务器接受远程连接，编辑/etc/postgresql/12/main/pg_hba.conf文件。\n\n下面是一些例子，显示不同的用户场景：\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n# type  database        user            address                 method\n\n# the user jane can access all databases from all locations using md5 password\nhost    all             sonarqube       0.0.0.0/0                md5\n\n# the user jane can access only the janedb from all locations using md5 password\nhost    sonarqube       sonarqube       0.0.0.0/0                md5\n\n# the user jane can access all databases from a trusted location (192.168.1.134) without a password\nhost    all             sonarqube       192.168.50.100             trust\n\n\n重启postgresql服务\n\nsudo systemctl restart postgresql\n\n\n最后一步（可选）就是在你的防火墙上打开端口5432端口。\n\n假设你正在使用ufw来管理你的防火墙，并且你想允许从10.17.10.0/24子网过来的访问，你应该运行下面的命令：\n\nsudo ufw allow proto tcp from 10.17.10.0/24 to any port 5432\n\n\n确保你的防火墙被配置好，并仅仅接受来自受信任 ip 范围的连接。\n\n\n# 将sonarqube用户密码设置为sonarqube\n\nsudo su - postgres\npsql\n\npostgres=# \\password sonarqube\nenter new password: \nenter it again: \n\npostgres=# select rolname,rolpassword from pg_authid;\n          rolname          |             rolpassword             \n---------------------------+-------------------------------------\n postgres                  | \n pg_monitor                | \n pg_read_all_settings      | \n pg_read_all_stats         | \n pg_stat_scan_tables       | \n pg_read_server_files      | \n pg_write_server_files     | \n pg_execute_server_program | \n pg_signal_backend         | \n sonarqube                 | md5699b64e757e1f31984196747c316e4e3\n(10 rows)\n\n\n\n# 安装jdk\n\n下载 jdk-11.0.11_linux-x64_bin.tar.gz\n\nsudo mkdir /usr/lib/jvm\nsudo tar -zxvf jdk-11.0.11_linux-x64_bin.tar.gz -c /usr/lib/jvm\n\n\n配置环境变量，编辑sudo vi /etc/profile文件，文档最后追加如下内容：\n\nexport es_java_home=/usr/lib/jvm/jdk-11.0.11\nexport classpath=.:%{es_java_home}/lib\nexport path=${es_java_home}/bin:$path\n\n\n更新\n\nsource /etc/profile\n\n\n验证\n\nubuntu@sonarqueb:~$ java -version\njava version "11.0.11" 2021-04-20 lts\njava(tm) se runtime environment 18.9 (build 11.0.11+9-lts-194)\njava hotspot(tm) 64-bit server vm 18.9 (build 11.0.11+9-lts-194, mixed mode)\n\n\n\n# 安装sonarqube\n\n准备工作\n\nsudo sysctl -w vm.max_map_count=524288\nsudo sysctl -w fs.file-max=131072\nulimit -n 131072\nulimit -u 8192\n\n\n官网下载并解压 下载地址: http://www.sonarqube.org/downloads/ 下载sonarqube 8.9.1版本\n\n切换到普通用户\n\n$ cd /home/ubuntu\n$ wget https://binaries.sonarsource.com/distribution/sonarqube/sonarqube-8.9.1.44547.zip\n\n\n安装zip\n\nsudo apt install zip unzip\n\n\n解压安装\n\n$ unzip sonarqube-8.9.1.44547.zip \n\n\n配置 sonarqube\n\n$ vi /home/ubuntu/sonarqube-8.9.1.44547/conf/sonar.properties\n\n\n修改如下内容：\n\n# 数据库账户\nsonar.jdbc.username=sonarqube\nsonar.jdbc.password=sonarqube\n# 数据库连接\nsonar.jdbc.url=jdbc:postgresql://localhost/sonarqube?currentschema=public\nsonar.web.host=0.0.0.0\nsonar.web.port=9000\n\n\n启动测试\n\ncd /home/ubuntu/sonarqube-8.9.1.44547/bin/linux-x86-64\n./sonar.sh console\n\n\n浏览器访问：http://server_ip:9000，默认管理员账号：admin，密码：admin。首次登录后修改密码。\n\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Jenkins整合SonarQube实现代码审查",frontmatter:{title:"Jenkins整合SonarQube实现代码审查",date:"2022-02-09T13:05:17.000Z",permalink:"/pages/848aec/"},regularPath:"/20.DevOps/02.%E5%AE%9E%E6%88%98/06.Jenkins%E6%95%B4%E5%90%88SonarQube%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5.html",relativePath:"20.DevOps/02.实战/06.Jenkins整合SonarQube实现代码审查.md",key:"v-233d0506",path:"/pages/848aec/",headers:[{level:2,title:"流程图",slug:"流程图",normalizedTitle:"流程图",charIndex:31},{level:2,title:"Jenkins安装SonarQube Scanner插件",slug:"jenkins安装sonarqube-scanner插件",normalizedTitle:"jenkins安装sonarqube scanner插件",charIndex:41},{level:2,title:"SonarQube 生成token",slug:"sonarqube-生成token",normalizedTitle:"sonarqube 生成token",charIndex:109},{level:2,title:"Jenkins 添加 SonarQube凭证",slug:"jenkins-添加-sonarqube凭证",normalizedTitle:"jenkins 添加 sonarqube凭证",charIndex:254},{level:2,title:"Jenkins 配置SonarQube",slug:"jenkins-配置sonarqube",normalizedTitle:"jenkins 配置sonarqube",charIndex:336},{level:2,title:"SonaQube关闭审查结果上传到SCM功能",slug:"sonaqube关闭审查结果上传到scm功能",normalizedTitle:"sonaqube关闭审查结果上传到scm功能",charIndex:580},{level:2,title:"非流水线项目",slug:"非流水线项目",normalizedTitle:"非流水线项目",charIndex:644},{level:2,title:"流水线项目",slug:"流水线项目",normalizedTitle:"流水线项目",charIndex:645}],headersStr:"流程图 Jenkins安装SonarQube Scanner插件 SonarQube 生成token Jenkins 添加 SonarQube凭证 Jenkins 配置SonarQube SonaQube关闭审查结果上传到SCM功能 非流水线项目 流水线项目",content:"# Jenkins整合SonarQube实现代码审查\n\n\n# 流程图\n\n\n\n\n# Jenkins安装SonarQube Scanner插件\n\nJenkins -> 系统管理 -> 插件管理 -> 可选插件\n\n\n\n\n# SonarQube 生成token\n\n点击右上角头像 -> My Account -> Security：Generate Tokens 点击 \"Generate\"\n\n\n\n点击\"Copy\"安装复制\n\na691b0c2921eedb7f025a6f95a5147351a520b34\n\n\n# Jenkins 添加 SonarQube凭证\n\nJenkins -> 系统管理 -> Manage Credentials -> 全局 -> 添加凭据\n\n\n\n\n# Jenkins 配置SonarQube\n\nJenkins -> 系统管理 -> 系统配置 -> SonarQube servers\n\n点击\"Add SonarQube\"按钮，添加SonarQube servers\n\n * name：sonarqube-8.9.1\n * Server URL：http://10.17.10.104:9000/\n\n\n\n系统管理 -> 全局工具配置 -> SonarQube Scanner\n\n * Name：sonarqube-scanner\n\n\n\n\n# SonaQube关闭审查结果上传到SCM功能\n\n返回SonarQube，Administration -> SCM\n\n\n\n\n# 非流水线项目\n\n在项目添加SonaQube代码审查\n\n1）项目根目录下，创建sonar-project.properties文件\n\n\n\n# must be unique in a given SonarQube instance\nsonar.projectKey=web-project\n# this is the name and version displayed in the SonarQube UI. Was mandatory prior to SonarQube 9.8.1.\nsonar.projectName=web-project\nsonar.projectVersion=1.0\n# Path is relative to the sonar-project.properties file. Replace \"\\\" by \"/\" on Windows.\n# This property is optional if sonar.modules is set.\nsonar.sources=.\nsonar.exclusions=**/test/**,**/target/**\nsonar.java.source=1.8\nsonar.java.target=1.8\n# Encoding of the source code. Default is default system encoding\nsonar.sourceEncoding=UTF-8\n\n\n2）Jenkins新建任务，选择\"构建一个自由风格的软件项目\"\n\n\n\n3）配置源码管理\n\n\n\n4）添加构建步骤 \"Execute SonarQube Scanner\"\n\n\n\n\n\n 5. 配置完成，点击\"立即构建\"，构建完成后打开SonarQube可以查看构建结果\n\n\n\n\n# 流水线项目\n\n1）修改Jenkinsfile，加入SonarQube代码审查阶段\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npipeline {\n    agent any\n\n    stages {\n        stage('checkout') {\n            steps {\n                checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'b8994da1-c4c7-4d37-b12e-939363a42230', url: 'http://192.168.50.100/demo/web-project.git']]])\n            }\n        }\n        stage('build') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n        stage('sonarqube') {\n            steps{\n                script {\n                    scannerHome = tool 'sonarqube-scanner'\n                }\n                withSonarQubeEnv('sonarqube-8.9.1') {\n                    sh \"${scannerHome}/bin/sonar-scanner\"\n                }\n            }\n        }\n        stage('deploy') {\n            steps {\n                echo '项目部署'\n            }\n        }\n    }\n\n    post {\n          always {\n            emailext body: '${FILE,path=\"email.html\"}', subject: '构建通知:${PROJECT_NAME} - Build # ${BUILD_NUMBER} - ${BUILD_STATUS}!', to: '785056500@qq.com'\n          }\n    }\n}\n\n\n3）到SonarQube的UI界面查看审查结果\n\n",normalizedContent:"# jenkins整合sonarqube实现代码审查\n\n\n# 流程图\n\n\n\n\n# jenkins安装sonarqube scanner插件\n\njenkins -> 系统管理 -> 插件管理 -> 可选插件\n\n\n\n\n# sonarqube 生成token\n\n点击右上角头像 -> my account -> security：generate tokens 点击 \"generate\"\n\n\n\n点击\"copy\"安装复制\n\na691b0c2921eedb7f025a6f95a5147351a520b34\n\n\n# jenkins 添加 sonarqube凭证\n\njenkins -> 系统管理 -> manage credentials -> 全局 -> 添加凭据\n\n\n\n\n# jenkins 配置sonarqube\n\njenkins -> 系统管理 -> 系统配置 -> sonarqube servers\n\n点击\"add sonarqube\"按钮，添加sonarqube servers\n\n * name：sonarqube-8.9.1\n * server url：http://10.17.10.104:9000/\n\n\n\n系统管理 -> 全局工具配置 -> sonarqube scanner\n\n * name：sonarqube-scanner\n\n\n\n\n# sonaqube关闭审查结果上传到scm功能\n\n返回sonarqube，administration -> scm\n\n\n\n\n# 非流水线项目\n\n在项目添加sonaqube代码审查\n\n1）项目根目录下，创建sonar-project.properties文件\n\n\n\n# must be unique in a given sonarqube instance\nsonar.projectkey=web-project\n# this is the name and version displayed in the sonarqube ui. was mandatory prior to sonarqube 9.8.1.\nsonar.projectname=web-project\nsonar.projectversion=1.0\n# path is relative to the sonar-project.properties file. replace \"\\\" by \"/\" on windows.\n# this property is optional if sonar.modules is set.\nsonar.sources=.\nsonar.exclusions=**/test/**,**/target/**\nsonar.java.source=1.8\nsonar.java.target=1.8\n# encoding of the source code. default is default system encoding\nsonar.sourceencoding=utf-8\n\n\n2）jenkins新建任务，选择\"构建一个自由风格的软件项目\"\n\n\n\n3）配置源码管理\n\n\n\n4）添加构建步骤 \"execute sonarqube scanner\"\n\n\n\n\n\n 5. 配置完成，点击\"立即构建\"，构建完成后打开sonarqube可以查看构建结果\n\n\n\n\n# 流水线项目\n\n1）修改jenkinsfile，加入sonarqube代码审查阶段\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npipeline {\n    agent any\n\n    stages {\n        stage('checkout') {\n            steps {\n                checkout([$class: 'gitscm', branches: [[name: '*/master']], extensions: [], userremoteconfigs: [[credentialsid: 'b8994da1-c4c7-4d37-b12e-939363a42230', url: 'http://192.168.50.100/demo/web-project.git']]])\n            }\n        }\n        stage('build') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n        stage('sonarqube') {\n            steps{\n                script {\n                    scannerhome = tool 'sonarqube-scanner'\n                }\n                withsonarqubeenv('sonarqube-8.9.1') {\n                    sh \"${scannerhome}/bin/sonar-scanner\"\n                }\n            }\n        }\n        stage('deploy') {\n            steps {\n                echo '项目部署'\n            }\n        }\n    }\n\n    post {\n          always {\n            emailext body: '${file,path=\"email.html\"}', subject: '构建通知:${project_name} - build # ${build_number} - ${build_status}!', to: '785056500@qq.com'\n          }\n    }\n}\n\n\n3）到sonarqube的ui界面查看审查结果\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"微服务持续集成(上)",frontmatter:{title:"微服务持续集成(上)",date:"2022-02-09T13:05:17.000Z",permalink:"/pages/8e8067/"},regularPath:"/20.DevOps/02.%E5%AE%9E%E6%88%98/07.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90(%E4%B8%8A).html",relativePath:"20.DevOps/02.实战/07.微服务持续集成(上).md",key:"v-64d0cdf1",path:"/pages/8e8067/",headers:[{level:2,title:"Jenkins+Docker+SpringCloud持续集成流程说明",slug:"jenkins-docker-springcloud持续集成流程说明",normalizedTitle:"jenkins+docker+springcloud持续集成流程说明",charIndex:17},{level:2,title:"服务列表",slug:"服务列表",normalizedTitle:"服务列表",charIndex:232},{level:2,title:"Harbor镜像仓库安装及使用",slug:"harbor镜像仓库安装及使用",normalizedTitle:"harbor镜像仓库安装及使用",charIndex:512},{level:2,title:"项目代码上传到Gitlab",slug:"项目代码上传到gitlab",normalizedTitle:"项目代码上传到gitlab",charIndex:558},{level:2,title:"从Gitlab拉取项目源码",slug:"从gitlab拉取项目源码",normalizedTitle:"从gitlab拉取项目源码",charIndex:613},{level:2,title:"提交到SonarQube代码审查",slug:"提交到sonarqube代码审查",normalizedTitle:"提交到sonarqube代码审查",charIndex:1123},{level:2,title:"使用Dockerfile编译、生成镜像，推送镜像到私服",slug:"使用dockerfile编译、生成镜像-推送镜像到私服",normalizedTitle:"使用dockerfile编译、生成镜像，推送镜像到私服",charIndex:2743},{level:2,title:"拉取镜像和发布应用",slug:"拉取镜像和发布应用",normalizedTitle:"拉取镜像和发布应用",charIndex:3508},{level:2,title:"设置SSH通过密钥登录",slug:"设置ssh通过密钥登录",normalizedTitle:"设置ssh通过密钥登录",charIndex:3562},{level:2,title:"安装 Publish Over SSH 插件",slug:"安装-publish-over-ssh-插件",normalizedTitle:"安装 publish over ssh 插件",charIndex:4648},{level:2,title:"修改Jenkinsfile构建脚本",slug:"修改jenkinsfile构建脚本",normalizedTitle:"修改jenkinsfile构建脚本",charIndex:1867},{level:2,title:"Jenkins中Pipeline没有Docker执行权限解决办法",slug:"jenkins中pipeline没有docker执行权限解决办法",normalizedTitle:"jenkins中pipeline没有docker执行权限解决办法",charIndex:8166},{level:2,title:"指定jdk版本",slug:"指定jdk版本",normalizedTitle:"指定jdk版本",charIndex:8567},{level:2,title:"编写deploy.sh部署脚本",slug:"编写deploy-sh部署脚本",normalizedTitle:"编写deploy.sh部署脚本",charIndex:8782},{level:2,title:"部署前端静态web网站",slug:"部署前端静态web网站",normalizedTitle:"部署前端静态web网站",charIndex:9542},{level:3,title:"安装Nginx服务器",slug:"安装nginx服务器",normalizedTitle:"安装nginx服务器",charIndex:9560},{level:2,title:"安装NodeJS插件",slug:"安装nodejs插件",normalizedTitle:"安装nodejs插件",charIndex:9814},{level:2,title:"Jenkins配置Nodejs",slug:"jenkins配置nodejs",normalizedTitle:"jenkins配置nodejs",charIndex:9831},{level:2,title:"创建前端流水线项目",slug:"创建前端流水线项目",normalizedTitle:"创建前端流水线项目",charIndex:9896},{level:2,title:"建立Jenkinsfile构建脚本",slug:"建立jenkinsfile构建脚本",normalizedTitle:"建立jenkinsfile构建脚本",charIndex:9916}],headersStr:"Jenkins+Docker+SpringCloud持续集成流程说明 服务列表 Harbor镜像仓库安装及使用 项目代码上传到Gitlab 从Gitlab拉取项目源码 提交到SonarQube代码审查 使用Dockerfile编译、生成镜像，推送镜像到私服 拉取镜像和发布应用 设置SSH通过密钥登录 安装 Publish Over SSH 插件 修改Jenkinsfile构建脚本 Jenkins中Pipeline没有Docker执行权限解决办法 指定jdk版本 编写deploy.sh部署脚本 部署前端静态web网站 安装Nginx服务器 安装NodeJS插件 Jenkins配置Nodejs 创建前端流水线项目 建立Jenkinsfile构建脚本",content:"# 微服务持续集成(上)\n\n\n# Jenkins+Docker+SpringCloud持续集成流程说明\n\n\n\n大致流程说明：\n\n1）开发人员每天把代码提交到Gitlab代码仓库\n\n2）Jenkins从Gitlab中拉取项目源码，编译并打成jar包，然后构建成Docker镜像，将镜像上传到 Harbor私有仓库。\n\n3）Jenkins发送SSH远程命令，让生产部署服务器到Harbor私有仓库拉取镜像到本地，然后创建容器。\n\n4）最后，用户可以访问到容器\n\n\n# 服务列表\n\n服务器名称         IP地址             安装的软件\n代码托管服务器       192.168.50.100   GitLab Community Edition 14.0.5\n持续集成服务器       192.168.50.101   Jenkins，Maven，Docker18.06.1-ce\nDocker仓库服务器   192.168.50.102   Docker18.06.1-ce，Harbor1.9.2\n生产部署服务器       192.168.50.103   Docker18.06.1-ce\n\n\n# Harbor镜像仓库安装及使用\n\n部署 Harbor 企业级Registry服务器\n\n\n# 项目代码上传到Gitlab\n\n在IDEA操作即可，参考之前的步骤。包括后台微服务和前端web网站代码\n\n\n# 从Gitlab拉取项目源码\n\n1）创建Jenkinsfile文件\n\npipeline {\n    agent any\n\n    stages {\n        stage('Checkout') {\n            steps {\n                checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'jenkins-gitlab-checkout-creds', url: 'git@192.168.50.100:ieooc/simplify.git']]])\n            }\n        }\n        stage('Build') {\n            steps {\n                sh 'mvn -B -DskipTests clean package'\n            }\n        }\n    }\n}\n\n\n2）拉取Jenkinsfile文件\n\n\n# 提交到SonarQube代码审查\n\n1）创建项目，并设置参数\n\n创建simplify项目，添加两个参数\n\n2）每个项目的根目录下添加sonar-project.properties\n\n\n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n# must be unique in a given SonarQube instance\nsonar.projectKey=simplify\n# this is the name and version displayed in the SonarQube UI.\nsonar.projectName=simplify\nsonar.projectVersion=1.0\n# Path is relative to the sonar-project.properties file. Replace \"\\\" by \"/\" on Windows.\n# This property is optional if sonar.modules is set.\nsonar.sources=.\nsonar.exclusions=**/test/**,**/target/**\nsonar.java.binaries=.\nsonar.java.source=1.8\nsonar.java.target=1.8\nsonar.java.libraries=**/target/classes/**\n# Encoding of the source code. Default is default system encoding\nsonar.sourceEncoding=UTF-8\n\n\n注意\n\n修改sonar.projectKey和sonar.projectName\n\n3）修改Jenkinsfile构建脚本\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\npipeline {\n    agent any\n\n    stages {\n        stage('Checkout') {\n            steps {\n                checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'jenkins-gitlab-checkout-creds', url: 'git@192.168.50.100:ieooc/simplify.git']]])\n            }\n        }\n        stage('Build') {\n            steps {\n                sh 'mvn -B -DskipTests clean package'\n            }\n        }\n        stage('Code Quality') {\n            steps{\n                script {\n                    scannerHome = tool 'sonarqube-scanner'\n                }\n                withSonarQubeEnv('sonarqube-8.9.1') {\n                    sh \"\"\"\n                        ${scannerHome}/bin/sonar-scanner\n                    \"\"\"\n                }\n            }\n        }\n    }\n}\n\n\n\n# 使用Dockerfile编译、生成镜像，推送镜像到私服\n\n1）Ubuntu 安装 Docker\n\n2）自己制作一个java11的docker镜像\n\n3）利用docker-maven-plugin插件构建Docker镜像\n\n4）修改Jenkinsfile构建脚本\n\npipeline {\n    agent any\n\n    stages {\n        stage('Checkout') {\n            steps {\n                checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'jenkins-gitlab-checkout-creds', url: 'http://192.168.50.100/simplify-cloud/simplify.git']]])\n            }\n        }\n        stage('Build') {\n            steps {\n                // 编译并安装公共聚合模块\n                sh 'mvn -f simplify-common clean install'\n                // 编译构建docker镜像并推送至Harbor镜像私服\n                sh 'mvn -f ${project_name} clean package docker:build docker:push'\n            }\n        }\n    }\n}\n\n\n\n# 拉取镜像和发布应用\n\n\n\n提示\n\n注意：192.168.50.103服务已经安装Docker并启动\n\n\n# 设置SSH通过密钥登录\n\n提示\n\n理论上只需要在生产服务器配置SSH密钥登录，最后拷贝Jenkins服务器上的公钥即可。\n\n1）生成密钥，并通过-m PEM指定格式\n\nssh-keygen -m PEM -t rsa -b 4096\n\n\n参数说明：\n\n * -m 参数指定密钥的格式，PEM是rsa之前使用的旧格式\n * -b 指定密钥长度。对于RSA密钥，最小要求768位，默认是2048位。\n\n查看密钥格式\n\n\n\n \n\n\n\n\nubuntu@jenkins:~/.ssh$ more /home/ubuntu/.ssh/id_rsa\n-----BEGIN RSA PRIVATE KEY-----\nMIIJKQIBAAKCAgEAuL2pM5NMsPPR9LhvNbfjx1uVMSR8z98ID/s1qMQ5bYVbYTIg\n...\n\n\n可以看到密钥的首行是：\n\n-----BEGIN RSA PRIVATE KEY-----\n\n\n而我们生成密钥时如果不指定格式，默认生成的密钥首行是：\n\n-----BEGIN OPENSSH PRIVATE KEY-----\n\n\n而 Jenkins 2.289.1 版本在检验密钥时还不支持这种格式，会出现如下错误：\n\njenkins.plugins.publish_over.BapPublisherException: Failed to add SSH key. Message [invalid privatekey: [B@60373f7]\n\n\n键入以下命令，在服务器上安装公钥：\n\ncd ~/.ssh\ncat id_rsa.pub >> authorized_keys\n\n\n如此便完成了公钥的安装。为了确保连接成功，请保证以下文件权限正确：\n\nchmod 600 authorized_keys\nchmod 700 ~/.ssh\n\n\n设置SSH，打开密钥登录功能\n\n编辑 vi /etc/ssh/sshd_config 文件，进行如下设置：\n\nPubkeyAuthentication yes\n\n当你完成全部设置，并以密钥方式登录成功后，再禁用密码登录：\n\nPasswordAuthentication no\n\n最后，将私钥文件 id_rsa 下载到客户端机器上，重启 SSH 服务：\n\nsystemctl restart sshd.service\n\n\n登录jenkins服务器，拷贝ssh公钥到生产服务器\n\nubuntu@jenkins:~$ ssh-copy-id ubuntu@10.17.10.103\n\n\n\n# 安装 Publish Over SSH 插件\n\n安装以下插件，可以实现远程发送Shell命令\n\n\n\n提示\n\n在配置一下内容时，要首先在jenkins服务器上生成私钥，然后将公钥推送到生成服务器上。\n\n登录jenkins服务器，拷贝私钥文件id_rsa到jenkins工作目录。\n\n# 切换root用户\nubuntu@jenkins:~$ su\n# 拷贝私钥文件\nroot@jenkins:/home/ubuntu# cp /home/ubuntu/.ssh/id_rsa /var/lib/jenkins/.ssh\n# 修改私钥文件访问权限\nroot@jenkins:/home/ubuntu# chown jenkins:jenkins /var/lib/jenkins/.ssh/id_rsa\n\n\n系统管理 -> 系统配置 -> Publish over SSH\n\n\n\n * Passphrase： 如果私钥设置了密码就是私钥的密码，私钥没设置密码可以不填\n * Path to key： 私钥的位置(实际文件所在位置/var/lib/jenkins/.ssh/id_rsa)\n * Key： 私钥的内容。如果此处填入了值，则以此处的值为准，会忽略掉Path to key的配置\n\n\n\nSSH Servers：服务器的配置\n\n * Name： 名称（自定义）\n * Hostname： 服务器地址\n * Username： 用户名\n * Remote Directory： 默认远程服务器的地址\n\n\n# 修改Jenkinsfile构建脚本\n\n生成远程调用模板代码\n\n\n\nsshPublisher(publishers: [sshPublisherDesc(configName: '10.17.10.107_deploy-simplify', transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: \"/home/ubuntu/jenkins_shell/deploy.sh ${HARBOR_URL} ${HARBOR_PROJECT_NAME} ${project_name} ${TAG}\", execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])\n\n\n新增构建参数\n\n * 项目模块名称\n * 启动服务端口\n\n\n\n完整的Jenkinsfile内容日下：\n\npipeline {\n    agent any\n    \n    environment {\n        HARBOR_URL = '10.17.10.110'\n        HARBOR_PROJECT_NAME = 'library'\n        TAG = \"1.0.0-SNAPSHOT\"\n        HARBOR_COMMON_CREDS = credentials('jenkins-harbor-auth-creds')\n    }\n    \n    stages {\n        stage('Checkout') {\n            steps {\n                checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'b8994da1-c4c7-4d37-b12e-939363a42230', url: 'http://10.17.10.101/simplify-cloud/simplify.git']]])\n            }\n        }\n        stage('Install') {\n            steps {\n                // Compile and install common modules\n                sh 'mvn -f simplify-common clean install'\n            }\n        }\n        stage('Test') {\n            steps {\n                echo \"do test\"\n            }\n        }\n        stage('Build') {\n            steps {\n                // withCredentials([usernamePassword(credentialsId: 'jenkins-harbor-auth-creds', passwordVariable: 'password', usernameVariable: 'username')]) {\n                //     // Harbor Login\n                //     sh \"docker login -u ${docker_username} -p ${docker_password} ${HARBOR_URL}\"\n                // }\n\n                // Harbor Login\n                sh  \"docker login -u ${HARBOR_COMMON_CREDS_USR} -p ${HARBOR_COMMON_CREDS_PSW} ${HARBOR_URL}\"\n                // Compile and build the docker image and push it to the harbor image private server\n                sh \"mvn -f ${PROJECT_NAME} clean package docker:build docker:push\"\n            }\n        }\n        stage('Deploy') {\n            steps {\n                sshPublisher(publishers: [sshPublisherDesc(configName: '10.17.10.107_deploy-simplify', transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: \"/home/ubuntu/jenkins_shell/deploy.sh ${HARBOR_URL} ${HARBOR_PROJECT_NAME} ${project_name} ${TAG} ${PORT} ${HARBOR_COMMON_CREDS_USR} ${HARBOR_COMMON_CREDS_PSW}\", execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])\n            }\n        }\n    }\n}\n\n\n\n# Jenkins中Pipeline没有Docker执行权限解决办法\n\nvi /etc/init.d/jenkins\n\n\n添加如下内容到脚本顶部\n\nDOCKER_SOCKET=/var/run/docker.sock\nDOCKER_GROUP=docker\nJENKINS_USER=jenkins\n\nif [ -S ${DOCKER_SOCKET} ]; then\n    DOCKER_GID=$(stat -c '%g' ${DOCKER_SOCKET})\n    sudo groupadd -for -g ${DOCKER_GID} ${DOCKER_GROUP}\n    sudo usermod -aG ${DOCKER_GROUP} ${JENKINS_USER}\nfi\n\n# Start Jenkins service\nsudo service jenkins restart\n\n\n\n# 指定jdk版本\n\nvi /etc/init.d/jenkins\n\n\n添加如下内容到脚本顶部\n\nexport JAVA_HOME=/usr/lib/jvm/jdk1.8.0_291\nexport JRE_HOME=${JAVA_HOME}/jre\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\nexport PATH=${JAVA_HOME}/bin:$PATH\n\n\n\n# 编写deploy.sh部署脚本\n\n上传deploy.sh文件到/opt/jenkins_shell目录下，且文件至少有执行权限！\n\n#!/bin/bash\n  \nharbor_url=$1\nharbor_project_name=$2\nproject_name=$3\ntag=$4\nport=$5\nharbor_username=$6\nharbor_password=$7\n\nimage_name=\"${harbor_url}/${harbor_project_name}/${project_name}:${tag}\"\n\n\ncontainerId=`docker ps -a | grep -w ${project_name}:${tag} | awk '{print $1}'`\n\nif [ \"$containerId\" != \"\" ]; then\n    docker stop $containerId\n    docker rm $containerId\n    echo \"Container deleted successfully\"\nfi\n\ndocker login -u ${harbor_username} -p ${harbor_password} ${harbor_url}\n\ndocker pull $image_name\n\ndocker run -itd -p ${port}:${port} --name ${project_name} ${image_name}\n\ndocker rmi $(docker images -q -f dangling=true)\n\necho \"Deployment complete\"\n\n\n添加执行权限\n\nchmod +x deploy.sh \n\n\n\n# 部署前端静态web网站\n\n\n\n\n# 安装Nginx服务器\n\n关闭SELinux\n\n# 临时关闭selinux\nsetenforce 0\n# 永久关闭selinux\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\n\n\n启动Nginx\n\nsystemctl enable nginx 设置开机启动\nsystemctl start nginx 启动\nsystemctl stop nginx 停止\nsystemctl restart nginx 重启\n\n\n\n# 安装NodeJS插件\n\n\n\n\n# Jenkins配置Nodejs\n\nManage Jenkins->Global Tool Configuration\n\n\n\n\n# 创建前端流水线项目\n\n\n\n\n\n\n\n\n# 建立Jenkinsfile构建脚本\n\n//gitlab的凭证\ndef git_auth = \"68f2087f-a034-4d39-a9ff-1f776dd3dfa8\"\nnode {\nstage('Checkout') {\ncheckout([$class: 'GitSCM', branches: [[name: '*/${branch}']],\ndoGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [],\nuserRemoteConfigs: [[credentialsId: \"${git_auth}\", url:\n'git@192.168.66.100:itheima_group/tensquare_front.git']]])\n} s\ntage('打包，部署网站') {\n//使用NodeJS的npm进行打包\nnodejs('nodejs12'){\nsh '''\nnpm install\nnpm run build\n'''\n} /\n/=====以下为远程调用进行项目部署========\nsshPublisher(publishers: [sshPublisherDesc(configName: 'master_server',\ntransfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: '',\nexecTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes:\nfalse, patternSeparator: '[, ]+', remoteDirectory: '/usr/share/nginx/html',\nremoteDirectorySDF: false, removePrefix: 'dist', sourceFiles: 'dist/**')],\nusePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])\n}\n}\n\n\n完成后，访问：http://192.168.66.103:9090 进行测试。",normalizedContent:"# 微服务持续集成(上)\n\n\n# jenkins+docker+springcloud持续集成流程说明\n\n\n\n大致流程说明：\n\n1）开发人员每天把代码提交到gitlab代码仓库\n\n2）jenkins从gitlab中拉取项目源码，编译并打成jar包，然后构建成docker镜像，将镜像上传到 harbor私有仓库。\n\n3）jenkins发送ssh远程命令，让生产部署服务器到harbor私有仓库拉取镜像到本地，然后创建容器。\n\n4）最后，用户可以访问到容器\n\n\n# 服务列表\n\n服务器名称         ip地址             安装的软件\n代码托管服务器       192.168.50.100   gitlab community edition 14.0.5\n持续集成服务器       192.168.50.101   jenkins，maven，docker18.06.1-ce\ndocker仓库服务器   192.168.50.102   docker18.06.1-ce，harbor1.9.2\n生产部署服务器       192.168.50.103   docker18.06.1-ce\n\n\n# harbor镜像仓库安装及使用\n\n部署 harbor 企业级registry服务器\n\n\n# 项目代码上传到gitlab\n\n在idea操作即可，参考之前的步骤。包括后台微服务和前端web网站代码\n\n\n# 从gitlab拉取项目源码\n\n1）创建jenkinsfile文件\n\npipeline {\n    agent any\n\n    stages {\n        stage('checkout') {\n            steps {\n                checkout([$class: 'gitscm', branches: [[name: '*/master']], extensions: [], userremoteconfigs: [[credentialsid: 'jenkins-gitlab-checkout-creds', url: 'git@192.168.50.100:ieooc/simplify.git']]])\n            }\n        }\n        stage('build') {\n            steps {\n                sh 'mvn -b -dskiptests clean package'\n            }\n        }\n    }\n}\n\n\n2）拉取jenkinsfile文件\n\n\n# 提交到sonarqube代码审查\n\n1）创建项目，并设置参数\n\n创建simplify项目，添加两个参数\n\n2）每个项目的根目录下添加sonar-project.properties\n\n\n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n# must be unique in a given sonarqube instance\nsonar.projectkey=simplify\n# this is the name and version displayed in the sonarqube ui.\nsonar.projectname=simplify\nsonar.projectversion=1.0\n# path is relative to the sonar-project.properties file. replace \"\\\" by \"/\" on windows.\n# this property is optional if sonar.modules is set.\nsonar.sources=.\nsonar.exclusions=**/test/**,**/target/**\nsonar.java.binaries=.\nsonar.java.source=1.8\nsonar.java.target=1.8\nsonar.java.libraries=**/target/classes/**\n# encoding of the source code. default is default system encoding\nsonar.sourceencoding=utf-8\n\n\n注意\n\n修改sonar.projectkey和sonar.projectname\n\n3）修改jenkinsfile构建脚本\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\npipeline {\n    agent any\n\n    stages {\n        stage('checkout') {\n            steps {\n                checkout([$class: 'gitscm', branches: [[name: '*/master']], extensions: [], userremoteconfigs: [[credentialsid: 'jenkins-gitlab-checkout-creds', url: 'git@192.168.50.100:ieooc/simplify.git']]])\n            }\n        }\n        stage('build') {\n            steps {\n                sh 'mvn -b -dskiptests clean package'\n            }\n        }\n        stage('code quality') {\n            steps{\n                script {\n                    scannerhome = tool 'sonarqube-scanner'\n                }\n                withsonarqubeenv('sonarqube-8.9.1') {\n                    sh \"\"\"\n                        ${scannerhome}/bin/sonar-scanner\n                    \"\"\"\n                }\n            }\n        }\n    }\n}\n\n\n\n# 使用dockerfile编译、生成镜像，推送镜像到私服\n\n1）ubuntu 安装 docker\n\n2）自己制作一个java11的docker镜像\n\n3）利用docker-maven-plugin插件构建docker镜像\n\n4）修改jenkinsfile构建脚本\n\npipeline {\n    agent any\n\n    stages {\n        stage('checkout') {\n            steps {\n                checkout([$class: 'gitscm', branches: [[name: '*/master']], extensions: [], userremoteconfigs: [[credentialsid: 'jenkins-gitlab-checkout-creds', url: 'http://192.168.50.100/simplify-cloud/simplify.git']]])\n            }\n        }\n        stage('build') {\n            steps {\n                // 编译并安装公共聚合模块\n                sh 'mvn -f simplify-common clean install'\n                // 编译构建docker镜像并推送至harbor镜像私服\n                sh 'mvn -f ${project_name} clean package docker:build docker:push'\n            }\n        }\n    }\n}\n\n\n\n# 拉取镜像和发布应用\n\n\n\n提示\n\n注意：192.168.50.103服务已经安装docker并启动\n\n\n# 设置ssh通过密钥登录\n\n提示\n\n理论上只需要在生产服务器配置ssh密钥登录，最后拷贝jenkins服务器上的公钥即可。\n\n1）生成密钥，并通过-m pem指定格式\n\nssh-keygen -m pem -t rsa -b 4096\n\n\n参数说明：\n\n * -m 参数指定密钥的格式，pem是rsa之前使用的旧格式\n * -b 指定密钥长度。对于rsa密钥，最小要求768位，默认是2048位。\n\n查看密钥格式\n\n\n\n \n\n\n\n\nubuntu@jenkins:~/.ssh$ more /home/ubuntu/.ssh/id_rsa\n-----begin rsa private key-----\nmiijkqibaakcageaul2pm5nmsppr9lhvnbfjx1uvmsr8z98id/s1qmq5byvbytig\n...\n\n\n可以看到密钥的首行是：\n\n-----begin rsa private key-----\n\n\n而我们生成密钥时如果不指定格式，默认生成的密钥首行是：\n\n-----begin openssh private key-----\n\n\n而 jenkins 2.289.1 版本在检验密钥时还不支持这种格式，会出现如下错误：\n\njenkins.plugins.publish_over.bappublisherexception: failed to add ssh key. message [invalid privatekey: [b@60373f7]\n\n\n键入以下命令，在服务器上安装公钥：\n\ncd ~/.ssh\ncat id_rsa.pub >> authorized_keys\n\n\n如此便完成了公钥的安装。为了确保连接成功，请保证以下文件权限正确：\n\nchmod 600 authorized_keys\nchmod 700 ~/.ssh\n\n\n设置ssh，打开密钥登录功能\n\n编辑 vi /etc/ssh/sshd_config 文件，进行如下设置：\n\npubkeyauthentication yes\n\n当你完成全部设置，并以密钥方式登录成功后，再禁用密码登录：\n\npasswordauthentication no\n\n最后，将私钥文件 id_rsa 下载到客户端机器上，重启 ssh 服务：\n\nsystemctl restart sshd.service\n\n\n登录jenkins服务器，拷贝ssh公钥到生产服务器\n\nubuntu@jenkins:~$ ssh-copy-id ubuntu@10.17.10.103\n\n\n\n# 安装 publish over ssh 插件\n\n安装以下插件，可以实现远程发送shell命令\n\n\n\n提示\n\n在配置一下内容时，要首先在jenkins服务器上生成私钥，然后将公钥推送到生成服务器上。\n\n登录jenkins服务器，拷贝私钥文件id_rsa到jenkins工作目录。\n\n# 切换root用户\nubuntu@jenkins:~$ su\n# 拷贝私钥文件\nroot@jenkins:/home/ubuntu# cp /home/ubuntu/.ssh/id_rsa /var/lib/jenkins/.ssh\n# 修改私钥文件访问权限\nroot@jenkins:/home/ubuntu# chown jenkins:jenkins /var/lib/jenkins/.ssh/id_rsa\n\n\n系统管理 -> 系统配置 -> publish over ssh\n\n\n\n * passphrase： 如果私钥设置了密码就是私钥的密码，私钥没设置密码可以不填\n * path to key： 私钥的位置(实际文件所在位置/var/lib/jenkins/.ssh/id_rsa)\n * key： 私钥的内容。如果此处填入了值，则以此处的值为准，会忽略掉path to key的配置\n\n\n\nssh servers：服务器的配置\n\n * name： 名称（自定义）\n * hostname： 服务器地址\n * username： 用户名\n * remote directory： 默认远程服务器的地址\n\n\n# 修改jenkinsfile构建脚本\n\n生成远程调用模板代码\n\n\n\nsshpublisher(publishers: [sshpublisherdesc(configname: '10.17.10.107_deploy-simplify', transfers: [sshtransfer(cleanremote: false, excludes: '', execcommand: \"/home/ubuntu/jenkins_shell/deploy.sh ${harbor_url} ${harbor_project_name} ${project_name} ${tag}\", exectimeout: 120000, flatten: false, makeemptydirs: false, nodefaultexcludes: false, patternseparator: '[, ]+', remotedirectory: '', remotedirectorysdf: false, removeprefix: '', sourcefiles: '')], usepromotiontimestamp: false, useworkspaceinpromotion: false, verbose: false)])\n\n\n新增构建参数\n\n * 项目模块名称\n * 启动服务端口\n\n\n\n完整的jenkinsfile内容日下：\n\npipeline {\n    agent any\n    \n    environment {\n        harbor_url = '10.17.10.110'\n        harbor_project_name = 'library'\n        tag = \"1.0.0-snapshot\"\n        harbor_common_creds = credentials('jenkins-harbor-auth-creds')\n    }\n    \n    stages {\n        stage('checkout') {\n            steps {\n                checkout([$class: 'gitscm', branches: [[name: '*/master']], extensions: [], userremoteconfigs: [[credentialsid: 'b8994da1-c4c7-4d37-b12e-939363a42230', url: 'http://10.17.10.101/simplify-cloud/simplify.git']]])\n            }\n        }\n        stage('install') {\n            steps {\n                // compile and install common modules\n                sh 'mvn -f simplify-common clean install'\n            }\n        }\n        stage('test') {\n            steps {\n                echo \"do test\"\n            }\n        }\n        stage('build') {\n            steps {\n                // withcredentials([usernamepassword(credentialsid: 'jenkins-harbor-auth-creds', passwordvariable: 'password', usernamevariable: 'username')]) {\n                //     // harbor login\n                //     sh \"docker login -u ${docker_username} -p ${docker_password} ${harbor_url}\"\n                // }\n\n                // harbor login\n                sh  \"docker login -u ${harbor_common_creds_usr} -p ${harbor_common_creds_psw} ${harbor_url}\"\n                // compile and build the docker image and push it to the harbor image private server\n                sh \"mvn -f ${project_name} clean package docker:build docker:push\"\n            }\n        }\n        stage('deploy') {\n            steps {\n                sshpublisher(publishers: [sshpublisherdesc(configname: '10.17.10.107_deploy-simplify', transfers: [sshtransfer(cleanremote: false, excludes: '', execcommand: \"/home/ubuntu/jenkins_shell/deploy.sh ${harbor_url} ${harbor_project_name} ${project_name} ${tag} ${port} ${harbor_common_creds_usr} ${harbor_common_creds_psw}\", exectimeout: 120000, flatten: false, makeemptydirs: false, nodefaultexcludes: false, patternseparator: '[, ]+', remotedirectory: '', remotedirectorysdf: false, removeprefix: '', sourcefiles: '')], usepromotiontimestamp: false, useworkspaceinpromotion: false, verbose: false)])\n            }\n        }\n    }\n}\n\n\n\n# jenkins中pipeline没有docker执行权限解决办法\n\nvi /etc/init.d/jenkins\n\n\n添加如下内容到脚本顶部\n\ndocker_socket=/var/run/docker.sock\ndocker_group=docker\njenkins_user=jenkins\n\nif [ -s ${docker_socket} ]; then\n    docker_gid=$(stat -c '%g' ${docker_socket})\n    sudo groupadd -for -g ${docker_gid} ${docker_group}\n    sudo usermod -ag ${docker_group} ${jenkins_user}\nfi\n\n# start jenkins service\nsudo service jenkins restart\n\n\n\n# 指定jdk版本\n\nvi /etc/init.d/jenkins\n\n\n添加如下内容到脚本顶部\n\nexport java_home=/usr/lib/jvm/jdk1.8.0_291\nexport jre_home=${java_home}/jre\nexport classpath=.:${java_home}/lib:${jre_home}/lib\nexport path=${java_home}/bin:$path\n\n\n\n# 编写deploy.sh部署脚本\n\n上传deploy.sh文件到/opt/jenkins_shell目录下，且文件至少有执行权限！\n\n#!/bin/bash\n  \nharbor_url=$1\nharbor_project_name=$2\nproject_name=$3\ntag=$4\nport=$5\nharbor_username=$6\nharbor_password=$7\n\nimage_name=\"${harbor_url}/${harbor_project_name}/${project_name}:${tag}\"\n\n\ncontainerid=`docker ps -a | grep -w ${project_name}:${tag} | awk '{print $1}'`\n\nif [ \"$containerid\" != \"\" ]; then\n    docker stop $containerid\n    docker rm $containerid\n    echo \"container deleted successfully\"\nfi\n\ndocker login -u ${harbor_username} -p ${harbor_password} ${harbor_url}\n\ndocker pull $image_name\n\ndocker run -itd -p ${port}:${port} --name ${project_name} ${image_name}\n\ndocker rmi $(docker images -q -f dangling=true)\n\necho \"deployment complete\"\n\n\n添加执行权限\n\nchmod +x deploy.sh \n\n\n\n# 部署前端静态web网站\n\n\n\n\n# 安装nginx服务器\n\n关闭selinux\n\n# 临时关闭selinux\nsetenforce 0\n# 永久关闭selinux\nsed -i 's#selinux=enforcing#selinux=disabled#g' /etc/selinux/config\n\n\n启动nginx\n\nsystemctl enable nginx 设置开机启动\nsystemctl start nginx 启动\nsystemctl stop nginx 停止\nsystemctl restart nginx 重启\n\n\n\n# 安装nodejs插件\n\n\n\n\n# jenkins配置nodejs\n\nmanage jenkins->global tool configuration\n\n\n\n\n# 创建前端流水线项目\n\n\n\n\n\n\n\n\n# 建立jenkinsfile构建脚本\n\n//gitlab的凭证\ndef git_auth = \"68f2087f-a034-4d39-a9ff-1f776dd3dfa8\"\nnode {\nstage('checkout') {\ncheckout([$class: 'gitscm', branches: [[name: '*/${branch}']],\ndogeneratesubmoduleconfigurations: false, extensions: [], submodulecfg: [],\nuserremoteconfigs: [[credentialsid: \"${git_auth}\", url:\n'git@192.168.66.100:itheima_group/tensquare_front.git']]])\n} s\ntage('打包，部署网站') {\n//使用nodejs的npm进行打包\nnodejs('nodejs12'){\nsh '''\nnpm install\nnpm run build\n'''\n} /\n/=====以下为远程调用进行项目部署========\nsshpublisher(publishers: [sshpublisherdesc(configname: 'master_server',\ntransfers: [sshtransfer(cleanremote: false, excludes: '', execcommand: '',\nexectimeout: 120000, flatten: false, makeemptydirs: false, nodefaultexcludes:\nfalse, patternseparator: '[, ]+', remotedirectory: '/usr/share/nginx/html',\nremotedirectorysdf: false, removeprefix: 'dist', sourcefiles: 'dist/**')],\nusepromotiontimestamp: false, useworkspaceinpromotion: false, verbose: false)])\n}\n}\n\n\n完成后，访问：http://192.168.66.103:9090 进行测试。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"微服务持续集成(下)",frontmatter:{title:"微服务持续集成(下)",date:"2022-02-09T13:05:17.000Z",permalink:"/pages/b7044d/"},regularPath:"/20.DevOps/02.%E5%AE%9E%E6%88%98/08.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90(%E4%B8%8B).html",relativePath:"20.DevOps/02.实战/08.微服务持续集成(下).md",key:"v-6dbf4a31",path:"/pages/b7044d/",headers:[{level:2,title:"Jenkins+Docker+SpringCloud部署方案优化",slug:"jenkins-docker-springcloud部署方案优化",normalizedTitle:"jenkins+docker+springcloud部署方案优化",charIndex:17},{level:2,title:"Jenkins+Docker+SpringCloud集群部署流程说明",slug:"jenkins-docker-springcloud集群部署流程说明",normalizedTitle:"jenkins+docker+springcloud集群部署流程说明",charIndex:211},{level:2,title:"修改所有微服务配置",slug:"修改所有微服务配置",normalizedTitle:"修改所有微服务配置",charIndex:252},{level:2,title:"设计Jenkins集群项目的构建参数",slug:"设计jenkins集群项目的构建参数",normalizedTitle:"设计jenkins集群项目的构建参数",charIndex:277},{level:2,title:"完成微服务多服务器远程发布",slug:"完成微服务多服务器远程发布",normalizedTitle:"完成微服务多服务器远程发布",charIndex:3123},{level:2,title:"Nginx+Zuul集群实现高可用网关",slug:"nginx-zuul集群实现高可用网关",normalizedTitle:"nginx+zuul集群实现高可用网关",charIndex:7192}],headersStr:"Jenkins+Docker+SpringCloud部署方案优化 Jenkins+Docker+SpringCloud集群部署流程说明 修改所有微服务配置 设计Jenkins集群项目的构建参数 完成微服务多服务器远程发布 Nginx+Zuul集群实现高可用网关",content:'# 微服务持续集成(下)\n\n\n# Jenkins+Docker+SpringCloud部署方案优化\n\n上面部署方案存在的问题：\n\n1）一次只能选择一个微服务部署\n\n2）只有一台生产者部署服务器\n\n3）每个微服务只有一个实例，容错率低\n\n优化方案：\n\n1）在一个Jenkins工程中可以选择多个微服务同时发布\n\n2）在一个Jenkins工程中可以选择多台生产服务器同时部署\n\n3）每个微服务都是以集群高可用形式部署\n\n\n# Jenkins+Docker+SpringCloud集群部署流程说明\n\n\n\n\n# 修改所有微服务配置\n\n注册中心配置(*)\n\n\n# 设计Jenkins集群项目的构建参数\n\n1）安装Extended Choice Parameter插件\n\n\n\n支持多选框\n\n2）创建流水线项目\n\n\n\n3）添加参数\n\n字符串参数：分支名称\n\n\n\n多选框：项目名称\n\n\n\n\n\ntensquare_eureka_server@10086,tensquare_zuul@10020,tensquare_admin_service@9001,\ntensquare_gathering@9002\n\n\n\n\n最后效果：\n\n\n\n完成微服务构建镜像，上传私服\n\n//gitlab的凭证\ndef git_auth = "68f2087f-a034-4d39-a9ff-1f776dd3dfa8"\n//构建版本的名称\ndef tag = "latest"\n//Harbor私服地址\ndef harbor_url = "192.168.66.102:85"\n//Harbor的项目名称\ndef harbor_project_name = "tensquare"\n//Harbor的凭证\ndef harbor_auth = "ef499f29-f138-44dd-975e-ff1ca1d8c933"\nnode {\n//把选择的项目信息转为数组\ndef selectedProjects = "${project_name}".split(\',\')\nstage(\'Checkout\') {\ncheckout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']],\ndoGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [],\nuserRemoteConfigs: [[credentialsId: \'${git_auth}\', url:\n\'git@192.168.66.100:itheima_group/tensquare_back_cluster.git\']]])\n} s\ntage(\'代码审查\') {\ndef scannerHome = tool \'sonarqube-scanner\'\nwithSonarQubeEnv(\'sonarqube6.7.4\') {\nfor(int i=0;i<selectedProjects.size();i++){\n//取出每个项目的名称和端口\ndef currentProject = selectedProjects[i];\n//项目名称\ndef currentProjectName = currentProject.split(\'@\')[0]\n//项目启动端口\ndef currentProjectPort = currentProject.split(\'@\')[1]\nsh """\ncd ${currentProjectName}\n${scannerHome}/bin/sonar-scanner\n"""\necho "${currentProjectName}完成代码审查"\n}\n}\n} s\ntage(\'编译，构建镜像，部署服务\') {\n//编译并安装公共工程\nsh "mvn -f tensquare_common clean install"\nfor(int i=0;i<selectedProjects.size();i++){\n//取出每个项目的名称和端口\ndef currentProject = selectedProjects[i];\n//项目名称\ndef currentProjectName = currentProject.split(\'@\')[0]\n//项目启动端口\ndef currentProjectPort = currentProject.split(\'@\')[1]\n//定义镜像名称\ndef imageName = "${currentProjectName}:${tag}"\n//编译，构建本地镜像\nsh "mvn -f ${currentProjectName} clean package\ndockerfile:build"\n//给镜像打标签\nsh "docker tag ${imageName}\n${harbor_url}/${harbor_project_name}/${imageName}"\n//登录Harbor，并上传镜像\nwithCredentials([usernamePassword(credentialsId:\n"${harbor_auth}", passwordVariable: \'password\', usernameVariable: \'username\')])\n{\n//登录\nsh "docker login -u ${username} -p ${password}\n${harbor_url}"\n//上传镜像\nsh "docker push\n${harbor_url}/${harbor_project_name}/${imageName}"\n} /\n/删除本地镜像\nsh "docker rmi -f ${imageName}"\nsh "docker rmi -f\n${harbor_url}/${harbor_project_name}/${imageName}"\n//=====以下为远程调用进行项目部署========\n//sshPublisher(publishers: [sshPublisherDesc(configName:\n\'master_server\', transfers: [sshTransfer(cleanRemote: false, excludes: \'\',\nexecCommand: "/opt/jenkins_shell/deployCluster.sh $harbor_url\n$harbor_project_name $currentProjectName $tag $currentProjectPort", execTimeout:\n120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false,\npatternSeparator: \'[, ]+\', remoteDirectory: \'\', remoteDirectorySDF: false,\nremovePrefix: \'\', sourceFiles: \'\')], usePromotionTimestamp: false,\nuseWorkspaceInPromotion: false, verbose: false)])\necho "${currentProjectName}完成编译，构建镜像"\n}\n}\n}\n\n\n\n# 完成微服务多服务器远程发布\n\n1）配置远程部署服务器\n\n拷贝公钥到远程服务器\n\nssh-copy-id 192.168.66.104\n\n系统配置->添加远程服务器\n\n\n\n2）修改Docker配置信任Harbor私服地址\n\n{ "\nregistry-mirrors": ["https://zydiol88.mirror.aliyuncs.com"],\n"insecure-registries": ["192.168.66.102:85"]\n}\n\n\n重启Docker\n\n3）添加参数\n\n多选框：部署服务器\n\n\n\n\n\n最终效果：\n\n\n\n4）修改Jenkinsfile构建脚本\n\n//gitlab的凭证\ndef git_auth = "68f2087f-a034-4d39-a9ff-1f776dd3dfa8"\n//构建版本的名称\ndef tag = "latest"\n//Harbor私服地址\ndef harbor_url = "192.168.66.102:85"\n//Harbor的项目名称\ndef harbor_project_name = "tensquare"\n//Harbor的凭证\ndef harbor_auth = "ef499f29-f138-44dd-975e-ff1ca1d8c933"\nnode {\n//把选择的项目信息转为数组\ndef selectedProjects = "${project_name}".split(\',\')\n//把选择的服务区信息转为数组\ndef selectedServers = "${publish_server}".split(\',\')\nstage(\'Checkout\') {\ncheckout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']],\ndoGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [],\nuserRemoteConfigs: [[credentialsId: \'${git_auth}\', url:\n\'git@192.168.66.100:itheima_group/tensquare_back_cluster.git\']]])\n}\nstage(\'代码审查\') {\ndef scannerHome = tool \'sonarqube-scanner\'\nwithSonarQubeEnv(\'sonarqube6.7.4\') {\nfor(int i=0;i<selectedProjects.size();i++){\n//取出每个项目的名称和端口\ndef currentProject = selectedProjects[i];\n//项目名称\ndef currentProjectName = currentProject.split(\'@\')[0]\n//项目启动端口\ndef currentProjectPort = currentProject.split(\'@\')[1]\nsh """\ncd ${currentProjectName}\n${scannerHome}/bin/sonar-scanner\n"""\necho "${currentProjectName}完成代码审查"\n}\n}\n} s\ntage(\'编译，构建镜像，部署服务\') {\n//编译并安装公共工程\nsh "mvn -f tensquare_common clean install"\nfor(int i=0;i<selectedProjects.size();i++){\n//取出每个项目的名称和端口\ndef currentProject = selectedProjects[i];\n//项目名称\ndef currentProjectName = currentProject.split(\'@\')[0]\n//项目启动端口\ndef currentProjectPort = currentProject.split(\'@\')[1]\n//定义镜像名称\ndef imageName = "${currentProjectName}:${tag}"\n//编译，构建本地镜像\nsh "mvn -f ${currentProjectName} clean package\ndockerfile:build"\n//给镜像打标签\nsh "docker tag ${imageName}\n${harbor_url}/${harbor_project_name}/${imageName}"\n//登录Harbor，并上传镜像\nwithCredentials([usernamePassword(credentialsId:\n"${harbor_auth}", passwordVariable: \'password\', usernameVariable: \'username\')])\n{\n//登录\nsh "docker login -u ${username} -p ${password}\n${harbor_url}"\n//上传镜像\nsh "docker push\n${harbor_url}/${harbor_project_name}/${imageName}"\n}\n//删除本地镜像\nsh "docker rmi -f ${imageName}"\nsh "docker rmi -f\n${harbor_url}/${harbor_project_name}/${imageName}"\n//=====以下为远程调用进行项目部署========\nfor(int j=0;j<selectedServers.size();j++){\n//每个服务名称\ndef currentServer = selectedServers[j]\n//添加微服务运行时的参数：spring.profiles.active\ndef activeProfile = "--spring.profiles.active="\nif(currentServer=="master_server"){\nactiveProfile = activeProfile+"eureka-server1"\n}else if(currentServer=="slave_server1"){\nactiveProfile = activeProfile+"eureka-server2"\n} s\nshPublisher(publishers: [sshPublisherDesc(configName:\n"${currentServer}", transfers: [sshTransfer(cleanRemote: false, excludes: \'\',\nexecCommand: "/opt/jenkins_shell/deployCluster.sh $harbor_url\n$harbor_project_name $currentProjectName $tag $currentProjectPort\n$activeProfile", execTimeout: 120000, flatten: false, makeEmptyDirs: false,\nnoDefaultExcludes: false, patternSeparator: \'[, ]+\', remoteDirectory: \'\',\nremoteDirectorySDF: false, removePrefix: \'\', sourceFiles: \'\')],\nusePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])\n} e\ncho "${currentProjectName}完成编译，构建镜像"\n}\n}\n}\n\n\n5）编写deployCluster.sh部署脚本\n\n#\n! /bin/sh\n#接收外部参数\nharbor_url=$1\nharbor_project_name=$2\nproject_name=$3\ntag=$4\nport=$5\nprofile=$6\nimageName=$harbor_url/$harbor_project_name/$project_name:$tag\necho "$imageName"\n#查询容器是否存在，存在则删除\ncontainerId=`docker ps -a | grep -w ${project_name}:${tag} | awk \'{print $1}\'`\nif [ "$containerId" != "" ] ; then\n#停掉容器\ndocker stop $containerId\n#删除容器\ndocker rm $containerId\necho "成功删除容器"\nfi\n#查询镜像是否存在，存在则删除\nimageId=`docker images | grep -w $project_name | awk \'{print $3}\'`\nif [ "$imageId" != "" ] ; then\n#删除镜像\ndocker rmi -f $imageId\necho "成功删除镜像"\nfi\n# 登录Harbor私服\ndocker login -u itcast -p Itcast123 $harbor_url\n# 下载镜像\ndocker pull $imageName\n# 启动容器\ndocker run -di -p $port:$port $imageName $profile\necho "容器启动成功"\n\n\n6）集群效果\n\n\n# Nginx+Zuul集群实现高可用网关\n\n\n\n1）安装Nginx（已完成）\n\n2）修改Nginx配置\n\nvi /etc/nginx/nginx.conf\n\n\n内容如下：\n\nupstream zuulServer{\nserver 192.168.66.103:10020 weight=1;\nserver 192.168.66.104:10020 weight=1;\n}\nserver {\nlisten 85 default_server;\nlisten [::]:85 default_server;\nserver_name _;\nroot /usr/share/nginx/html;\n# Load configuration files for the default server block.\ninclude /etc/nginx/default.d/*.conf;\nlocation / {\n### 指定服务器负载均衡服务器\nproxy_pass http://zuulServer/;\n}\n}\n\n\n\n3）重启Nginx： systemctl restart nginx\n\n4）修改前端Nginx的访问地址\n\n',normalizedContent:'# 微服务持续集成(下)\n\n\n# jenkins+docker+springcloud部署方案优化\n\n上面部署方案存在的问题：\n\n1）一次只能选择一个微服务部署\n\n2）只有一台生产者部署服务器\n\n3）每个微服务只有一个实例，容错率低\n\n优化方案：\n\n1）在一个jenkins工程中可以选择多个微服务同时发布\n\n2）在一个jenkins工程中可以选择多台生产服务器同时部署\n\n3）每个微服务都是以集群高可用形式部署\n\n\n# jenkins+docker+springcloud集群部署流程说明\n\n\n\n\n# 修改所有微服务配置\n\n注册中心配置(*)\n\n\n# 设计jenkins集群项目的构建参数\n\n1）安装extended choice parameter插件\n\n\n\n支持多选框\n\n2）创建流水线项目\n\n\n\n3）添加参数\n\n字符串参数：分支名称\n\n\n\n多选框：项目名称\n\n\n\n\n\ntensquare_eureka_server@10086,tensquare_zuul@10020,tensquare_admin_service@9001,\ntensquare_gathering@9002\n\n\n\n\n最后效果：\n\n\n\n完成微服务构建镜像，上传私服\n\n//gitlab的凭证\ndef git_auth = "68f2087f-a034-4d39-a9ff-1f776dd3dfa8"\n//构建版本的名称\ndef tag = "latest"\n//harbor私服地址\ndef harbor_url = "192.168.66.102:85"\n//harbor的项目名称\ndef harbor_project_name = "tensquare"\n//harbor的凭证\ndef harbor_auth = "ef499f29-f138-44dd-975e-ff1ca1d8c933"\nnode {\n//把选择的项目信息转为数组\ndef selectedprojects = "${project_name}".split(\',\')\nstage(\'checkout\') {\ncheckout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']],\ndogeneratesubmoduleconfigurations: false, extensions: [], submodulecfg: [],\nuserremoteconfigs: [[credentialsid: \'${git_auth}\', url:\n\'git@192.168.66.100:itheima_group/tensquare_back_cluster.git\']]])\n} s\ntage(\'代码审查\') {\ndef scannerhome = tool \'sonarqube-scanner\'\nwithsonarqubeenv(\'sonarqube6.7.4\') {\nfor(int i=0;i<selectedprojects.size();i++){\n//取出每个项目的名称和端口\ndef currentproject = selectedprojects[i];\n//项目名称\ndef currentprojectname = currentproject.split(\'@\')[0]\n//项目启动端口\ndef currentprojectport = currentproject.split(\'@\')[1]\nsh """\ncd ${currentprojectname}\n${scannerhome}/bin/sonar-scanner\n"""\necho "${currentprojectname}完成代码审查"\n}\n}\n} s\ntage(\'编译，构建镜像，部署服务\') {\n//编译并安装公共工程\nsh "mvn -f tensquare_common clean install"\nfor(int i=0;i<selectedprojects.size();i++){\n//取出每个项目的名称和端口\ndef currentproject = selectedprojects[i];\n//项目名称\ndef currentprojectname = currentproject.split(\'@\')[0]\n//项目启动端口\ndef currentprojectport = currentproject.split(\'@\')[1]\n//定义镜像名称\ndef imagename = "${currentprojectname}:${tag}"\n//编译，构建本地镜像\nsh "mvn -f ${currentprojectname} clean package\ndockerfile:build"\n//给镜像打标签\nsh "docker tag ${imagename}\n${harbor_url}/${harbor_project_name}/${imagename}"\n//登录harbor，并上传镜像\nwithcredentials([usernamepassword(credentialsid:\n"${harbor_auth}", passwordvariable: \'password\', usernamevariable: \'username\')])\n{\n//登录\nsh "docker login -u ${username} -p ${password}\n${harbor_url}"\n//上传镜像\nsh "docker push\n${harbor_url}/${harbor_project_name}/${imagename}"\n} /\n/删除本地镜像\nsh "docker rmi -f ${imagename}"\nsh "docker rmi -f\n${harbor_url}/${harbor_project_name}/${imagename}"\n//=====以下为远程调用进行项目部署========\n//sshpublisher(publishers: [sshpublisherdesc(configname:\n\'master_server\', transfers: [sshtransfer(cleanremote: false, excludes: \'\',\nexeccommand: "/opt/jenkins_shell/deploycluster.sh $harbor_url\n$harbor_project_name $currentprojectname $tag $currentprojectport", exectimeout:\n120000, flatten: false, makeemptydirs: false, nodefaultexcludes: false,\npatternseparator: \'[, ]+\', remotedirectory: \'\', remotedirectorysdf: false,\nremoveprefix: \'\', sourcefiles: \'\')], usepromotiontimestamp: false,\nuseworkspaceinpromotion: false, verbose: false)])\necho "${currentprojectname}完成编译，构建镜像"\n}\n}\n}\n\n\n\n# 完成微服务多服务器远程发布\n\n1）配置远程部署服务器\n\n拷贝公钥到远程服务器\n\nssh-copy-id 192.168.66.104\n\n系统配置->添加远程服务器\n\n\n\n2）修改docker配置信任harbor私服地址\n\n{ "\nregistry-mirrors": ["https://zydiol88.mirror.aliyuncs.com"],\n"insecure-registries": ["192.168.66.102:85"]\n}\n\n\n重启docker\n\n3）添加参数\n\n多选框：部署服务器\n\n\n\n\n\n最终效果：\n\n\n\n4）修改jenkinsfile构建脚本\n\n//gitlab的凭证\ndef git_auth = "68f2087f-a034-4d39-a9ff-1f776dd3dfa8"\n//构建版本的名称\ndef tag = "latest"\n//harbor私服地址\ndef harbor_url = "192.168.66.102:85"\n//harbor的项目名称\ndef harbor_project_name = "tensquare"\n//harbor的凭证\ndef harbor_auth = "ef499f29-f138-44dd-975e-ff1ca1d8c933"\nnode {\n//把选择的项目信息转为数组\ndef selectedprojects = "${project_name}".split(\',\')\n//把选择的服务区信息转为数组\ndef selectedservers = "${publish_server}".split(\',\')\nstage(\'checkout\') {\ncheckout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']],\ndogeneratesubmoduleconfigurations: false, extensions: [], submodulecfg: [],\nuserremoteconfigs: [[credentialsid: \'${git_auth}\', url:\n\'git@192.168.66.100:itheima_group/tensquare_back_cluster.git\']]])\n}\nstage(\'代码审查\') {\ndef scannerhome = tool \'sonarqube-scanner\'\nwithsonarqubeenv(\'sonarqube6.7.4\') {\nfor(int i=0;i<selectedprojects.size();i++){\n//取出每个项目的名称和端口\ndef currentproject = selectedprojects[i];\n//项目名称\ndef currentprojectname = currentproject.split(\'@\')[0]\n//项目启动端口\ndef currentprojectport = currentproject.split(\'@\')[1]\nsh """\ncd ${currentprojectname}\n${scannerhome}/bin/sonar-scanner\n"""\necho "${currentprojectname}完成代码审查"\n}\n}\n} s\ntage(\'编译，构建镜像，部署服务\') {\n//编译并安装公共工程\nsh "mvn -f tensquare_common clean install"\nfor(int i=0;i<selectedprojects.size();i++){\n//取出每个项目的名称和端口\ndef currentproject = selectedprojects[i];\n//项目名称\ndef currentprojectname = currentproject.split(\'@\')[0]\n//项目启动端口\ndef currentprojectport = currentproject.split(\'@\')[1]\n//定义镜像名称\ndef imagename = "${currentprojectname}:${tag}"\n//编译，构建本地镜像\nsh "mvn -f ${currentprojectname} clean package\ndockerfile:build"\n//给镜像打标签\nsh "docker tag ${imagename}\n${harbor_url}/${harbor_project_name}/${imagename}"\n//登录harbor，并上传镜像\nwithcredentials([usernamepassword(credentialsid:\n"${harbor_auth}", passwordvariable: \'password\', usernamevariable: \'username\')])\n{\n//登录\nsh "docker login -u ${username} -p ${password}\n${harbor_url}"\n//上传镜像\nsh "docker push\n${harbor_url}/${harbor_project_name}/${imagename}"\n}\n//删除本地镜像\nsh "docker rmi -f ${imagename}"\nsh "docker rmi -f\n${harbor_url}/${harbor_project_name}/${imagename}"\n//=====以下为远程调用进行项目部署========\nfor(int j=0;j<selectedservers.size();j++){\n//每个服务名称\ndef currentserver = selectedservers[j]\n//添加微服务运行时的参数：spring.profiles.active\ndef activeprofile = "--spring.profiles.active="\nif(currentserver=="master_server"){\nactiveprofile = activeprofile+"eureka-server1"\n}else if(currentserver=="slave_server1"){\nactiveprofile = activeprofile+"eureka-server2"\n} s\nshpublisher(publishers: [sshpublisherdesc(configname:\n"${currentserver}", transfers: [sshtransfer(cleanremote: false, excludes: \'\',\nexeccommand: "/opt/jenkins_shell/deploycluster.sh $harbor_url\n$harbor_project_name $currentprojectname $tag $currentprojectport\n$activeprofile", exectimeout: 120000, flatten: false, makeemptydirs: false,\nnodefaultexcludes: false, patternseparator: \'[, ]+\', remotedirectory: \'\',\nremotedirectorysdf: false, removeprefix: \'\', sourcefiles: \'\')],\nusepromotiontimestamp: false, useworkspaceinpromotion: false, verbose: false)])\n} e\ncho "${currentprojectname}完成编译，构建镜像"\n}\n}\n}\n\n\n5）编写deploycluster.sh部署脚本\n\n#\n! /bin/sh\n#接收外部参数\nharbor_url=$1\nharbor_project_name=$2\nproject_name=$3\ntag=$4\nport=$5\nprofile=$6\nimagename=$harbor_url/$harbor_project_name/$project_name:$tag\necho "$imagename"\n#查询容器是否存在，存在则删除\ncontainerid=`docker ps -a | grep -w ${project_name}:${tag} | awk \'{print $1}\'`\nif [ "$containerid" != "" ] ; then\n#停掉容器\ndocker stop $containerid\n#删除容器\ndocker rm $containerid\necho "成功删除容器"\nfi\n#查询镜像是否存在，存在则删除\nimageid=`docker images | grep -w $project_name | awk \'{print $3}\'`\nif [ "$imageid" != "" ] ; then\n#删除镜像\ndocker rmi -f $imageid\necho "成功删除镜像"\nfi\n# 登录harbor私服\ndocker login -u itcast -p itcast123 $harbor_url\n# 下载镜像\ndocker pull $imagename\n# 启动容器\ndocker run -di -p $port:$port $imagename $profile\necho "容器启动成功"\n\n\n6）集群效果\n\n\n# nginx+zuul集群实现高可用网关\n\n\n\n1）安装nginx（已完成）\n\n2）修改nginx配置\n\nvi /etc/nginx/nginx.conf\n\n\n内容如下：\n\nupstream zuulserver{\nserver 192.168.66.103:10020 weight=1;\nserver 192.168.66.104:10020 weight=1;\n}\nserver {\nlisten 85 default_server;\nlisten [::]:85 default_server;\nserver_name _;\nroot /usr/share/nginx/html;\n# load configuration files for the default server block.\ninclude /etc/nginx/default.d/*.conf;\nlocation / {\n### 指定服务器负载均衡服务器\nproxy_pass http://zuulserver/;\n}\n}\n\n\n\n3）重启nginx： systemctl restart nginx\n\n4）修改前端nginx的访问地址\n\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Lombok使用指南",frontmatter:{title:"Lombok使用指南",date:"2022-02-09T13:15:12.000Z",permalink:"/pages/d49c54/"},regularPath:"/21.Simplify/01.Simplify%20Docs/01.Lombok%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97.html",relativePath:"21.Simplify/01.Simplify Docs/01.Lombok使用指南.md",key:"v-f97f4056",path:"/pages/d49c54/",headers:[{level:2,title:"Lombok 简介",slug:"lombok-简介",normalizedTitle:"lombok 简介",charIndex:19},{level:2,title:"Lombok 安装",slug:"lombok-安装",normalizedTitle:"lombok 安装",charIndex:32},{level:3,title:"构建工具",slug:"构建工具",normalizedTitle:"构建工具",charIndex:47},{level:3,title:"IDE",slug:"ide",normalizedTitle:"ide",charIndex:57},{level:2,title:"Lombok 详解",slug:"lombok-详解",normalizedTitle:"lombok 详解",charIndex:64},{level:3,title:"@Getter and @Setter",slug:"getter-and-setter",normalizedTitle:"@getter and @setter",charIndex:79},{level:3,title:"Constructor Annotations",slug:"constructor-annotations",normalizedTitle:"constructor annotations",charIndex:104},{level:3,title:"@UtilityClass",slug:"utilityclass",normalizedTitle:"@utilityclass",charIndex:133},{level:3,title:"@EqualsAndHashCode",slug:"equalsandhashcode",normalizedTitle:"@equalsandhashcode",charIndex:152},{level:3,title:"@ToString",slug:"tostring",normalizedTitle:"@tostring",charIndex:176},{level:3,title:"@Data",slug:"data",normalizedTitle:"@data",charIndex:191},{level:3,title:"@Log",slug:"log",normalizedTitle:"@log",charIndex:202},{level:3,title:"@Synchronized",slug:"synchronized",normalizedTitle:"@synchronized",charIndex:212},{level:3,title:"@Builder",slug:"builder",normalizedTitle:"@builder",charIndex:231},{level:3,title:"@SneakyThrows",slug:"sneakythrows",normalizedTitle:"@sneakythrows",charIndex:245},{level:3,title:"@NonNull",slug:"nonnull",normalizedTitle:"@nonnull",charIndex:264},{level:3,title:"@Clean",slug:"clean",normalizedTitle:"@clean",charIndex:278},{level:3,title:"@With",slug:"with",normalizedTitle:"@with",charIndex:290},{level:3,title:"@Accessors",slug:"accessors",normalizedTitle:"@accessors",charIndex:301},{level:3,title:"其它特性",slug:"其它特性",normalizedTitle:"其它特性",charIndex:317},{level:2,title:"Lombok注解原理",slug:"lombok注解原理",normalizedTitle:"lombok注解原理",charIndex:325}],headersStr:"Lombok 简介 Lombok 安装 构建工具 IDE Lombok 详解 @Getter and @Setter Constructor Annotations @UtilityClass @EqualsAndHashCode @ToString @Data @Log @Synchronized @Builder @SneakyThrows @NonNull @Clean @With @Accessors 其它特性 Lombok注解原理",content:'# Lombok使用指南\n\n\n\n * Lombok 简介\n * Lombok 安装\n   * 构建工具\n   * IDE\n * Lombok 详解\n   * @Getter and @Setter\n   * Constructor Annotations\n   * @UtilityClass\n   * @EqualsAndHashCode\n   * @ToString\n   * @Data\n   * @Log\n   * @Synchronized\n   * @Builder\n   * @SneakyThrows\n   * @NonNull\n   * @Clean\n   * @With\n   * @Accessors\n   * 其它特性\n * Lombok注解原理\n\n\n\n\n# Lombok 简介\n\nLombok 是一款 Java 开发插件，使得 Java 开发者可以通过其定义的一些注解来消除业务工程中冗长和繁琐的代码，尤其对于简单的 Java 模型对象（POJO）。在开发环境中使用 Lombok 插件后，Java 开发人员可以节省出重复构建，诸如 hashCode 和 equals 这样的方法以及各种业务对象模型的 accessor 和 toString 等方法的大量时间。对于这些方法，Lombok 能够在编译源代码期间自动帮我们生成这些方法，但并不会像反射那样降低程序的性能。\n\n\n# Lombok 安装\n\n\n# 构建工具\n\n * Maven\n\n在 Maven 项目的 pom.xml 文件中添加 Lombok 依赖：\n\n<dependency>\n    <groupId>org.projectlombok</groupId>\n    <artifactId>lombok</artifactId>\n    <version>1.18.10</version>\n    <scope>provided</scope>\n</dependency>\n\n\n\n# IDE\n\n由于 Lombok 仅在编译阶段生成代码，所以使用 Lombok 注解的源代码，在 IDE 中会被高亮显示错误，针对这个问题可以通过安装 IDE 对应的插件来解决。具体的安装方式可以参考:https://www.baeldung.com/lombok-ide。\n\nLombok in IntelliJ IDEA\n\n\n\n\n# Lombok 详解\n\n注解说明\n\n * val：用在局部变量前面，相当于将变量声明为final\n\n * @NonNull：给方法参数增加这个注解会自动在方法内对该参数进行是否为空的校验，如果为空，则抛出NPE（NullPointerException）\n\n * @Cleanup：自动管理资源，用在局部变量之前，在当前变量范围内即将执行完毕退出之前会自动清理资源，自动生成try-finally这样的代码来关闭流\n\n * @Getter/@Setter：用在属性上，再也不用自己手写setter和getter方法了，还可以指定访问范围\n\n * @ToString：用在类上，可以自动覆写toString方法，当然还可以加其他参数，例如@ToString(exclude=”id”)排除id属性，或者@ToString(callSuper=true, includeFieldNames=true)调用父类的toString方法，包含所有属性\n\n * @EqualsAndHashCode：用在类上，自动生成equals方法和hashCode方法\n\n * @NoArgsConstructor, @RequiredArgsConstructor and @AllArgsConstructor：用在类上，自动生成无参构造和使用所有参数的构造函数以及把所有@NonNull属性作为参数的构造函数，如果指定staticName = “of”参数，同时还会生成一个返回类对象的静态工厂方法，比使用构造函数方便很多\n\n * @UtilityClass:注解在类上，添加了static同时创建了一个私有的构造方法\n\n * @Data：注解在类上，相当于同时使用了@ToString、@EqualsAndHashCode、@Getter、@Setter和@RequiredArgsConstrutor这些注解，对于POJO类十分有用\n\n * @Value：用在类上，是@Data的不可变形式，相当于为属性添加final声明，只提供getter方法，而不提供setter方法\n\n * @Builder：用在类、构造器、方法上，为你提供复杂的builder APIs，让你可以像如下方式一样调用Person.builder().name("Adam Savage").city("San Francisco").job("Mythbusters").job("Unchained Reaction").build();更多说明参考Builder\n\n * @SneakyThrows：自动抛受检异常，而无需显式在方法上使用throws语句\n\n * @Synchronized：用在方法上，将方法声明为同步的，并自动加锁，而锁对象是一个私有的属性$lock或$LOCK，而java中的synchronized关键字锁对象是this，锁在this或者自己的类对象上存在副作用，就是你不能阻止非受控代码去锁this或者类对象，这可能会导致竞争条件或者其它线程错误\n\n * @Getter(lazy=true)：可以替代经典的Double Check Lock样板代码\n\n * @Log：根据不同的注解生成不同类型的log对象，但是实例名称都是log，有六种可选实现类\n\n * * @CommonsLog Creates log = org.apache.commons.logging.LogFactory.getLog(LogExample.class);\n   * @Log Creates log = java.util.logging.Logger.getLogger(LogExample.class.getName());\n   * @Log4j Creates log = org.apache.log4j.Logger.getLogger(LogExample.class);\n   * @Log4j2 Creates log = org.apache.logging.log4j.LogManager.getLogger(LogExample.class);\n   * @Slf4j Creates log = org.slf4j.LoggerFactory.getLogger(LogExample.class);\n   * @XSlf4j Creates log = org.slf4j.ext.XLoggerFactory.getXLogger(LogExample.class);\n\n\n# @Getter and @Setter\n\n你可以使用 @Getter 或 @Setter 注释任何类或字段，Lombok 会自动生成默认的 getter/setter 方法。\n\n@Getter\n\n@Target({ElementType.FIELD, ElementType.TYPE})\n@Retention(RetentionPolicy.SOURCE)\npublic @interface Getter {\n  // 若getter方法非public的话，可以设置可访问级别\n    lombok.AccessLevel value() default lombok.AccessLevel.PUBLIC;\n    AnyAnnotation[] onMethod() default {};\n  // 是否启用延迟初始化\n    boolean lazy() default false;\n}\n\n\n@Setter\n\n@Target({ElementType.FIELD, ElementType.TYPE})\n@Retention(RetentionPolicy.SOURCE)\npublic @interface Setter {\n  // 若setter方法非public的话，可以设置可访问级别\n    lombok.AccessLevel value() default lombok.AccessLevel.PUBLIC;\n    AnyAnnotation[] onMethod() default {};\n    AnyAnnotation[] onParam() default {};\n}\n\n\n使用示例\n\n@Getter\n@Setter\npublic class GetterAndSetterDemo {\n    String firstName;\n    String lastName;\n    LocalDate dateOfBirth;\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class GetterAndSetterDemo {\n    String firstName;\n    String lastName;\n    LocalDate dateOfBirth;\n\n    public GetterAndSetterDemo() {\n    }\n\n    // 省略其它setter和getter方法\n    public String getFirstName() {\n        return this.firstName;\n    }\n\n    public void setFirstName(String firstName) {\n        this.firstName = firstName;\n    }\n}\n\n\nLazy Getter\n\n@Getter 注解支持一个 lazy 属性，该属性默认为 false。当设置为 true 时，会启用延迟初始化，即当首次调用 getter 方法时才进行初始化。\n\n示例\n\npublic class LazyGetterDemo {\n    public static void main(String[] args) {\n        LazyGetterDemo m = new LazyGetterDemo();\n        System.out.println("Main instance is created");\n        m.getLazy();\n    }\n\n    @Getter\n    private final String notLazy = createValue("not lazy");\n\n    @Getter(lazy = true)\n    private final String lazy = createValue("lazy");\n\n    private String createValue(String name) {\n        System.out.println("createValue(" + name + ")");\n        return null;\n    }\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class LazyGetterDemo {\n    private final String notLazy = this.createValue("not lazy");\n    private final AtomicReference<Object> lazy = new AtomicReference();\n\n    // 已省略部分代码\n    public String getNotLazy() {\n        return this.notLazy;\n    }\n\n    public String getLazy() {\n        Object value = this.lazy.get();\n        if (value == null) {\n            synchronized(this.lazy) {\n                value = this.lazy.get();\n                if (value == null) {\n                    String actualValue = this.createValue("lazy");\n                    value = actualValue == null ? this.lazy : actualValue;\n                    this.lazy.set(value);\n                }\n            }\n        }\n\n        return (String)((String)(value == this.lazy ? null : value));\n    }\n}\n\n\n通过以上代码可知，调用 getLazy 方法时，若发现 value 为 null，则会在同步代码块中执行初始化操作。\n\n\n# Constructor Annotations\n\n@NoArgsConstructor\n\n使用 @NoArgsConstructor 注解可以为指定类，生成默认的构造函数，@NoArgsConstructor 注解的定义如下：\n\n@Target(ElementType.TYPE)\n@Retention(RetentionPolicy.SOURCE)\npublic @interface NoArgsConstructor {\n  // 若设置该属性，将会生成一个私有的构造函数且生成一个staticName指定的静态方法\n    String staticName() default "";\n    AnyAnnotation[] onConstructor() default {};\n  // 设置生成构造函数的访问级别，默认是public\n    AccessLevel access() default lombok.AccessLevel.PUBLIC;\n  // 若设置为true，则初始化所有final的字段为0/null/false\n    boolean force() default false;\n}\n\n\n示例\n\n@NoArgsConstructor(staticName = "getInstance")\npublic class NoArgsConstructorDemo {\n    private long id;\n    private String name;\n    private int age;\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class NoArgsConstructorDemo {\n    private long id;\n    private String name;\n    private int age;\n\n    private NoArgsConstructorDemo() {\n    }\n\n    public static NoArgsConstructorDemo getInstance() {\n        return new NoArgsConstructorDemo();\n    }\n}\n\n\n@AllArgsConstructor\n\n使用 @AllArgsConstructor 注解可以为指定类，生成包含所有成员的构造函数，@AllArgsConstructor 注解的定义如下：\n\n@Target(ElementType.TYPE)\n@Retention(RetentionPolicy.SOURCE)\npublic @interface AllArgsConstructor {\n  // 若设置该属性，将会生成一个私有的构造函数且生成一个staticName指定的静态方法\n    String staticName() default "";\n    AnyAnnotation[] onConstructor() default {};\n  // 设置生成构造函数的访问级别，默认是public\n    AccessLevel access() default lombok.AccessLevel.PUBLIC;\n}\n\n\n示例\n\n@AllArgsConstructor\npublic class AllArgsConstructorDemo {\n    private long id;\n    private String name;\n    private int age;\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class AllArgsConstructorDemo {\n    private long id;\n    private String name;\n    private int age;\n\n    public AllArgsConstructorDemo(long id, String name, int age) {\n        this.id = id;\n        this.name = name;\n        this.age = age;\n    }\n}\n\n\n@RequiredArgsConstructor\n\n使用 @RequiredArgsConstructor 注解可以为指定类必需初始化的成员变量，如 final 成员变量，生成对应的构造函数，@RequiredArgsConstructor 注解的定义如下：\n\n@Target(ElementType.TYPE)\n@Retention(RetentionPolicy.SOURCE)\npublic @interface RequiredArgsConstructor {\n  // 若设置该属性，将会生成一个私有的构造函数且生成一个staticName指定的静态方法\n    String staticName() default "";\n    AnyAnnotation[] onConstructor() default {};\n  // 设置生成构造函数的访问级别，默认是public\n    AccessLevel access() default lombok.AccessLevel.PUBLIC;\n}\n\n\n示例\n\n@RequiredArgsConstructor\npublic class RequiredArgsConstructorDemo {\n    private final long id;\n    private String name;\n    private int age;\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class RequiredArgsConstructorDemo {\n    private final long id;\n    private String name;\n    private int age;\n\n    public RequiredArgsConstructorDemo(long id) {\n        this.id = id;\n    }\n}\n\n\n\n# @UtilityClass\n\n示例\n\n@UtilityClass\npublic class SecurityUtils {\n\n    public Authentication getAuthentication() {\n        return SecurityContextHolder.getContext().getAuthentication();\n    }\n\n    public SimplifyUser getUser(Authentication authentication) {\n        Object principal = authentication.getPrincipal();\n        if (principal instanceof SimplifyUser) {\n            return (SimplifyUser) principal;\n        }\n        return null;\n    }\n\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic final class SecurityUtils {\n    public static Authentication getAuthentication() {\n        return SecurityContextHolder.getContext().getAuthentication();\n    }\n\n    private SecurityUtils() {\n        throw new UnsupportedOperationException("This is a utility class and cannot be instantiated");\n    }\n}\n\n\n\n# @EqualsAndHashCode\n\n使用 @EqualsAndHashCode 注解可以为指定类生成 equals 和 hashCode 方法， @EqualsAndHashCode 注解的定义如下：\n\n@Target(ElementType.TYPE)\n@Retention(RetentionPolicy.SOURCE)\npublic @interface EqualsAndHashCode {\n  // 指定在生成的equals和hashCode方法中需要排除的字段列表\n    String[] exclude() default {};\n    \n  // 显式列出用于identity的字段，一般情况下non-static,non-transient字段会被用于identity\n    String[] of() default {};\n    \n  // 标识在执行字段计算前，是否调用父类的equals和hashCode方法\n    boolean callSuper() default false;\n    \n    boolean doNotUseGetters() default false;\n    \n    AnyAnnotation[] onParam() default {};\n    \n    @Deprecated\n    @Retention(RetentionPolicy.SOURCE)\n    @Target({})\n    @interface AnyAnnotation {}\n    \n    @Target(ElementType.FIELD)\n    @Retention(RetentionPolicy.SOURCE)\n    public @interface Exclude {}\n    \n    @Target({ElementType.FIELD, ElementType.METHOD})\n    @Retention(RetentionPolicy.SOURCE)\n    public @interface Include {\n        String replaces() default "";\n    }\n}\n\n\n示例\n\npublic class EqualsAndHashCodeDemo {\n    String firstName;\n    String lastName;\n    LocalDate dateOfBirth;\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class EqualsAndHashCodeDemo {\n    String firstName;\n    String lastName;\n    LocalDate dateOfBirth;\n\n    public EqualsAndHashCodeDemo() {\n    }\n\n    public boolean equals(Object o) {\n        if (o == this) {\n            return true;\n        } else if (!(o instanceof EqualsAndHashCodeDemo)) {\n            return false;\n        } else {\n            EqualsAndHashCodeDemo other = (EqualsAndHashCodeDemo)o;\n            if (!other.canEqual(this)) {\n                return false;\n            } else {\n              // 已省略大量代码\n        }\n    }\n\n    public int hashCode() {\n        int PRIME = true;\n        int result = 1;\n        Object $firstName = this.firstName;\n        int result = result * 59 + ($firstName == null ? 43 : $firstName.hashCode());\n        Object $lastName = this.lastName;\n        result = result * 59 + ($lastName == null ? 43 : $lastName.hashCode());\n        Object $dateOfBirth = this.dateOfBirth;\n        result = result * 59 + ($dateOfBirth == null ? 43 : $dateOfBirth.hashCode());\n        return result;\n    }\n}\n\n\n\n# @ToString\n\n使用 @ToString 注解可以为指定类生成 toString 方法， @ToString 注解的定义如下：\n\n@Target(ElementType.TYPE)\n@Retention(RetentionPolicy.SOURCE)\npublic @interface ToString {\n  // 打印输出时是否包含字段的名称\n    boolean includeFieldNames() default true;\n    \n  // 列出打印输出时，需要排除的字段列表\n    String[] exclude() default {};\n    \n  // 显式的列出需要打印输出的字段列表\n    String[] of() default {};\n    \n  // 打印输出的结果中是否包含父类的toString方法的返回结果\n    boolean callSuper() default false;\n    \n    boolean doNotUseGetters() default false;\n    \n    boolean onlyExplicitlyIncluded() default false;\n    \n    @Target(ElementType.FIELD)\n    @Retention(RetentionPolicy.SOURCE)\n    public @interface Exclude {}\n    \n    @Target({ElementType.FIELD, ElementType.METHOD})\n    @Retention(RetentionPolicy.SOURCE)\n    public @interface Include {\n        int rank() default 0;\n        String name() default "";\n    }\n}\n\n\n示例\n\n@ToString(exclude = {"dateOfBirth"})\npublic class ToStringDemo {\n    String firstName;\n    String lastName;\n    LocalDate dateOfBirth;\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class ToStringDemo {\n    String firstName;\n    String lastName;\n    LocalDate dateOfBirth;\n\n    public ToStringDemo() {\n    }\n\n    public String toString() {\n        return "ToStringDemo(firstName=" + this.firstName + ", lastName=" + this.lastName + ")";\n    }\n}\n\n\n\n# @Data\n\n@Data 注解与同时使用以下的注解的效果是一样的：\n\n@ToString\n@Getter\n@Setter\n@RequiredArgsConstructor\n@EqualsAndHashCode\n\n\n@Data 注解的定义如下：\n\n@Target(ElementType.TYPE)\n@Retention(RetentionPolicy.SOURCE)\npublic @interface Data {\n    String staticConstructor() default "";\n}\n\n\n示例\n\n@Data\npublic class DataDemo {\n    private Long id;\n    private String summary;\n    private String description;\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class DataDemo {\n    private Long id;\n    private String summary;\n    private String description;\n\n    public DataDemo() {\n    }\n\n    // 省略summary和description成员属性的setter和getter方法\n    public Long getId() {\n        return this.id;\n    }\n  \n    public void setId(Long id) {\n        this.id = id;\n    }\n\n    public boolean equals(Object o) {\n        if (o == this) {\n            return true;\n        } else if (!(o instanceof DataDemo)) {\n            return false;\n        } else {\n            DataDemo other = (DataDemo)o;\n            if (!other.canEqual(this)) {\n                return false;\n            } else {\n               // 已省略大量代码\n            }\n        }\n    }\n\n    protected boolean canEqual(Object other) {\n        return other instanceof DataDemo;\n    }\n\n    public int hashCode() {\n        int PRIME = true;\n        int result = 1;\n        Object $id = this.getId();\n        int result = result * 59 + ($id == null ? 43 : $id.hashCode());\n        Object $summary = this.getSummary();\n        result = result * 59 + ($summary == null ? 43 : $summary.hashCode());\n        Object $description = this.getDescription();\n        result = result * 59 + ($description == null ? 43 : $description.hashCode());\n        return result;\n    }\n\n    public String toString() {\n        return "DataDemo(id=" + this.getId() + ", summary=" + this.getSummary() + ", description=" + this.getDescription() + ")";\n    }\n}\n\n\n\n# @Log\n\n若你将 @Log 的变体放在类上（适用于你所使用的日志记录系统的任何一种）；之后，你将拥有一个静态的 final log 字段，然后你就可以使用该字段来输出日志。\n\n@Log\nprivate static final java.util.logging.Logger log = java.util.logging.Logger.getLogger(LogExample.class.getName());\n@Log4j\nprivate static final org.apache.log4j.Logger log = org.apache.log4j.Logger.getLogger(LogExample.class);\n@Log4j2\nprivate static final org.apache.logging.log4j.Logger log = org.apache.logging.log4j.LogManager.getLogger(LogExample.class);\n@Slf4j\nprivate static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(LogExample.class);\n@XSlf4j\nprivate static final org.slf4j.ext.XLogger log = org.slf4j.ext.XLoggerFactory.getXLogger(LogExample.class);\n@CommonsLog\nprivate static final org.apache.commons.logging.Log log = org.apache.commons.logging.LogFactory.getLog(LogExample.class);\n\n\n\n# @Synchronized\n\n@Synchronized 是同步方法修饰符的更安全的变体。与 synchronized 一样，该注解只能应用在静态和实例方法上。它的操作类似于 synchronized 关键字，但是它锁定在不同的对象上。synchronized 关键字应用在实例方法时，锁定的是 this 对象，而应用在静态方法上锁定的是类对象。对于 @Synchronized 注解声明的方法来说，它锁定的是\n\n或lock。@Synchronized 注解的定义如下：\n\n@Target(ElementType.METHOD)\n@Retention(RetentionPolicy.SOURCE)\npublic @interface Synchronized {\n  // 指定锁定的字段名称\n    String value() default "";\n}\n\n\n示例\n\npublic class SynchronizedDemo {\n    private final Object readLock = new Object();\n\n    @Synchronized\n    public static void hello() {\n        System.out.println("world");\n    }\n\n    @Synchronized\n    public int answerToLife() {\n        return 42;\n    }\n\n    @Synchronized("readLock")\n    public void foo() {\n        System.out.println("bar");\n    }\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class SynchronizedDemo {\n    private static final Object $LOCK = new Object[0];\n    private final Object $lock = new Object[0];\n    private final Object readLock = new Object();\n\n    public SynchronizedDemo() {\n    }\n\n    public static void hello() {\n        synchronized($LOCK) {\n            System.out.println("world");\n        }\n    }\n\n    public int answerToLife() {\n        synchronized(this.$lock) {\n            return 42;\n        }\n    }\n\n    public void foo() {\n        synchronized(this.readLock) {\n            System.out.println("bar");\n        }\n    }\n}\n\n\n\n# @Builder\n\n使用 @Builder 注解可以为指定类实现建造者模式，该注解可以放在类、构造函数或方法上。@Builder 注解的定义如下：\n\n@Target({TYPE, METHOD, CONSTRUCTOR})\n@Retention(SOURCE)\npublic @interface Builder {\n    @Target(FIELD)\n    @Retention(SOURCE)\n    public @interface Default {}\n\n  // 创建新的builder实例的方法名称\n    String builderMethodName() default "builder";\n    // 创建Builder注解类对应实例的方法名称\n    String buildMethodName() default "build";\n    // builder类的名称\n    String builderClassName() default "";\n    \n    boolean toBuilder() default false;\n    \n    AccessLevel access() default lombok.AccessLevel.PUBLIC;\n    \n    @Target({FIELD, PARAMETER})\n    @Retention(SOURCE)\n    public @interface ObtainVia {\n        String field() default "";\n        String method() default "";\n        boolean isStatic() default false;\n    }\n}\n\n\n示例\n\n@Builder\npublic class BuilderDemo {\n    private final String firstname;\n    private final String lastname;\n    private final String email;\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class BuilderDemo {\n    private final String firstname;\n    private final String lastname;\n    private final String email;\n\n    BuilderDemo(String firstname, String lastname, String email) {\n        this.firstname = firstname;\n        this.lastname = lastname;\n        this.email = email;\n    }\n\n    public static BuilderDemo.BuilderDemoBuilder builder() {\n        return new BuilderDemo.BuilderDemoBuilder();\n    }\n\n    public static class BuilderDemoBuilder {\n        private String firstname;\n        private String lastname;\n        private String email;\n\n        BuilderDemoBuilder() {\n        }\n\n        public BuilderDemo.BuilderDemoBuilder firstname(String firstname) {\n            this.firstname = firstname;\n            return this;\n        }\n\n        public BuilderDemo.BuilderDemoBuilder lastname(String lastname) {\n            this.lastname = lastname;\n            return this;\n        }\n\n        public BuilderDemo.BuilderDemoBuilder email(String email) {\n            this.email = email;\n            return this;\n        }\n\n        public BuilderDemo build() {\n            return new BuilderDemo(this.firstname, this.lastname, this.email);\n        }\n\n        public String toString() {\n            return "BuilderDemo.BuilderDemoBuilder(firstname=" + this.firstname + ", lastname=" + this.lastname + ", email=" + this.email + ")";\n        }\n    }\n}\n\n\n\n# @SneakyThrows\n\n@SneakyThrows 注解用于自动抛出已检查的异常，而无需在方法中使用 throw 语句显式抛出。@SneakyThrows 注解的定义如下：\n\n@Target({ElementType.METHOD, ElementType.CONSTRUCTOR})\n@Retention(RetentionPolicy.SOURCE)\npublic @interface SneakyThrows {\n    // 设置你希望向上抛的异常类\n    Class<? extends Throwable>[] value() default java.lang.Throwable.class;\n}\n\n\n示例\n\npublic class SneakyThrowsDemo {\n    @SneakyThrows\n    @Override\n    protected Object clone() {\n        return super.clone();\n    }\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class SneakyThrowsDemo {\n    public SneakyThrowsDemo() {\n    }\n\n    protected Object clone() {\n        try {\n            return super.clone();\n        } catch (Throwable var2) {\n            throw var2;\n        }\n    }\n}\n\n\n\n# @NonNull\n\n你可以在方法或构造函数的参数上使用 @NonNull 注解，它将会为你自动生成非空校验语句。@NonNull 注解的定义如下：\n\n@Target({ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER, ElementType.LOCAL_VARIABLE, ElementType.TYPE_USE})\n@Retention(RetentionPolicy.CLASS)\n@Documented\npublic @interface NonNull {\n\n}\n\n\n示例\n\npublic class NonNullDemo {\n    @Getter\n    @Setter\n    @NonNull\n    private String name;\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class NonNullDemo {\n    @NonNull\n    private String name;\n\n    public NonNullDemo() {\n    }\n\n    @NonNull\n    public String getName() {\n        return this.name;\n    }\n\n    public void setName(@NonNull String name) {\n        if (name == null) {\n            throw new NullPointerException("name is marked non-null but is null");\n        } else {\n            this.name = name;\n        }\n    }\n}\n\n\n\n# @Clean\n\n@Clean 注解用于自动管理资源，用在局部变量之前，在当前变量范围内即将执行完毕退出之前会自动清理资源，自动生成 try-finally 这样的代码来关闭流。\n\n@Target(ElementType.LOCAL_VARIABLE)\n@Retention(RetentionPolicy.SOURCE)\npublic @interface Cleanup {\n  // 设置用于执行资源清理/回收的方法名称，对应方法不能包含任何参数，默认名称为close。\n    String value() default "close";\n}\n\n\n示例\n\npublic class CleanupDemo {\n    public static void main(String[] args) throws IOException {\n        @Cleanup InputStream in = new FileInputStream(args[0]);\n        @Cleanup OutputStream out = new FileOutputStream(args[1]);\n        byte[] b = new byte[10000];\n        while (true) {\n            int r = in.read(b);\n            if (r == -1) break;\n            out.write(b, 0, r);\n        }\n    }\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class CleanupDemo {\n    public CleanupDemo() {\n    }\n\n    public static void main(String[] args) throws IOException {\n        FileInputStream in = new FileInputStream(args[0]);\n\n        try {\n            FileOutputStream out = new FileOutputStream(args[1]);\n\n            try {\n                byte[] b = new byte[10000];\n\n                while(true) {\n                    int r = in.read(b);\n                    if (r == -1) {\n                        return;\n                    }\n\n                    out.write(b, 0, r);\n                }\n            } finally {\n                if (Collections.singletonList(out).get(0) != null) {\n                    out.close();\n                }\n\n            }\n        } finally {\n            if (Collections.singletonList(in).get(0) != null) {\n                in.close();\n            }\n        }\n    }\n}\n\n\n\n# @With\n\n在类的字段上应用 @With 注解之后，将会自动生成一个 withFieldName(newValue) 的方法，该方法会基于 newValue 调用相应构造函数，创建一个当前类对应的实例。@With 注解的定义如下：\n\n@Target({ElementType.FIELD, ElementType.TYPE})\n@Retention(RetentionPolicy.SOURCE)\npublic @interface With {\n    AccessLevel value() default AccessLevel.PUBLIC;\n\n    With.AnyAnnotation[] onMethod() default {};\n\n    With.AnyAnnotation[] onParam() default {};\n\n    @Deprecated\n    @Retention(RetentionPolicy.SOURCE)\n    @Target({})\n    public @interface AnyAnnotation {\n    }\n}\n\n\n示例\n\npublic class WithDemo {\n    @With(AccessLevel.PROTECTED)\n    @NonNull\n    private final String name;\n    @With\n    private final int age;\n\n    public WithDemo(String name, int age) {\n        if (name == null) throw new NullPointerException();\n        this.name = name;\n        this.age = age;\n    }\n}\n\n\n以上代码经过 Lombok 编译后，会生成如下代码：\n\npublic class WithDemo {\n    @NonNull\n    private final String name;\n    private final int age;\n\n    public WithDemo(String name, int age) {\n        if (name == null) {\n            throw new NullPointerException();\n        } else {\n            this.name = name;\n            this.age = age;\n        }\n    }\n\n    protected WithDemo withName(@NonNull String name) {\n        if (name == null) {\n            throw new NullPointerException("name is marked non-null but is null");\n        } else {\n            return this.name == name ? this : new WithDemo(name, this.age);\n        }\n    }\n\n    public WithDemo withAge(int age) {\n        return this.age == age ? this : new WithDemo(this.name, age);\n    }\n}\n\n\n\n# @Accessors\n\n@Accessors 注解用来配置lombok如何产生和显示getters和setters的方法。@Accessors注解既可以注解在类上也可以注解在属性上 @Accessors有三个属性\n\n# fluent\n\nfluent为一个布尔值，如果为true生成的get/set方法则没有set/get前缀，默认为false 例如：\n\n@Accessors(flunet = true)\npublic class MyClass {\n    @Getter\n    private int num;\n}\n\n\n生成的get方法为num()，而不是getNum()\n\n# chain\n\nchain为一个布尔值，如果为true生成的set方法返回this，为false生成的set方法是void类型。默认为false，除非当fluent为true时，chain默认则为true\n\n# prefix\n\nprefix为一系列string类型，可以指定前缀，生成get/set方法时会去掉指定的前缀 例如：\n\n@Accessors(prefix = "m")\npublic class MyClass {\n    @Getter\n    private int mNum;\n}\n\n\n生成的get方法为getNum()，而不是getMNum()\n\n\n# 其它特性\n\n# val\n\nval 用在局部变量前面，相当于将变量声明为 final，此外 Lombok 在编译时还会自动进行类型推断。val 的使用示例：\n\npublic class ValExample {\n  public String example() {\n    val example = new ArrayList<String>();\n    example.add("Hello, World!");\n    val foo = example.get(0);\n    return foo.toLowerCase();\n  }\n  \n  public void example2() {\n    val map = new HashMap<Integer, String>();\n    map.put(0, "zero");\n    map.put(5, "five");\n    for (val entry : map.entrySet()) {\n      System.out.printf("%d: %s\\n", entry.getKey(), entry.getValue());\n    }\n  }\n}\n\n\n以上代码等价于：\n\npublic class ValExample {\n  public String example() {\n    final ArrayList<String> example = new ArrayList<String>();\n    example.add("Hello, World!");\n    final String foo = example.get(0);\n    return foo.toLowerCase();\n  }\n  \n  public void example2() {\n    final HashMap<Integer, String> map = new HashMap<Integer, String>();\n    map.put(0, "zero");\n    map.put(5, "five");\n    for (final Map.Entry<Integer, String> entry : map.entrySet()) {\n      System.out.printf("%d: %s\\n", entry.getKey(), entry.getValue());\n    }\n  }\n}\n\n\n\n# Lombok注解原理\n\n说到 Lombok，我们就得去提到 JSR 269: Pluggable Annotation Processing API 。JSR 269 之前我们也有注解这样的神器，可是我们比如想要做什么必须使用反射，反射的方法局限性较大。首先，它必须定义@Retention为RetentionPolicy.RUNTIME，只能在运行时通过反射来获取注解值，使得运行时代码效率降低。其次，如果想在编译阶段利用注解来进行一些检查，对用户的某些不合理代码给出错误报告，反射的使用方法就无能为力了。而 JSR 269 之后我们可以在 Javac的编译期利用注解做这些事情。所以我们发现核心的区分是在 运行期 还是 编译期。\n\n\n\n从上图可知，Annotation Processing 是在解析和生成之间的一个步骤。具体详细步骤如下：\n\n\n\n上图是 Lombok 处理流程，在Javac 解析成抽象语法树之后(AST), Lombok 根据自己的注解处理器，动态的修改 AST，增加新的节点(所谓代码)，最终通过分析和生成字节码。\n\n自从Java 6起，javac就支持“JSR 269 Pluggable Annotation Processing API”规范，只要程序实现了该API，就能在javac运行的时候得到调用。\n\n提示\n\n 1. 常用的项目管理工具Maven所使用的java编译工具来源于配置的第三方工具，如果我们配置这个第三方工具为Oracle javac的话，那么Maven也就直接支持lombok了;\n 2. Intellij Idea配置的编译工具为Oracle javac的话，也就直接支持lombok了。\n\nIDE工具问题解决：\n\n提示\n\n现在有一个A类，其中有一些字段，没有创建它们的setter和getter方法，使用了lombok的@Data注解，另外有一个B类，它调用了A类实例的相应字段的setter和getter方法\n\n编译A类和B类所在的项目，并不会报错，因为最终生成的A类字节码文件中存在相应字段的setter和getter方法\n\n但是，IDE发现B类源代码中所使用的A类实例的setter和getter方法在A类源代码中找不到定义，IDE会认为这是错误\n\n要解决以上这个不是真正错误的错误，可以下载安装Intellij Idea中的"Lombok plugin"。',normalizedContent:'# lombok使用指南\n\n\n\n * lombok 简介\n * lombok 安装\n   * 构建工具\n   * ide\n * lombok 详解\n   * @getter and @setter\n   * constructor annotations\n   * @utilityclass\n   * @equalsandhashcode\n   * @tostring\n   * @data\n   * @log\n   * @synchronized\n   * @builder\n   * @sneakythrows\n   * @nonnull\n   * @clean\n   * @with\n   * @accessors\n   * 其它特性\n * lombok注解原理\n\n\n\n\n# lombok 简介\n\nlombok 是一款 java 开发插件，使得 java 开发者可以通过其定义的一些注解来消除业务工程中冗长和繁琐的代码，尤其对于简单的 java 模型对象（pojo）。在开发环境中使用 lombok 插件后，java 开发人员可以节省出重复构建，诸如 hashcode 和 equals 这样的方法以及各种业务对象模型的 accessor 和 tostring 等方法的大量时间。对于这些方法，lombok 能够在编译源代码期间自动帮我们生成这些方法，但并不会像反射那样降低程序的性能。\n\n\n# lombok 安装\n\n\n# 构建工具\n\n * maven\n\n在 maven 项目的 pom.xml 文件中添加 lombok 依赖：\n\n<dependency>\n    <groupid>org.projectlombok</groupid>\n    <artifactid>lombok</artifactid>\n    <version>1.18.10</version>\n    <scope>provided</scope>\n</dependency>\n\n\n\n# ide\n\n由于 lombok 仅在编译阶段生成代码，所以使用 lombok 注解的源代码，在 ide 中会被高亮显示错误，针对这个问题可以通过安装 ide 对应的插件来解决。具体的安装方式可以参考:https://www.baeldung.com/lombok-ide。\n\nlombok in intellij idea\n\n\n\n\n# lombok 详解\n\n注解说明\n\n * val：用在局部变量前面，相当于将变量声明为final\n\n * @nonnull：给方法参数增加这个注解会自动在方法内对该参数进行是否为空的校验，如果为空，则抛出npe（nullpointerexception）\n\n * @cleanup：自动管理资源，用在局部变量之前，在当前变量范围内即将执行完毕退出之前会自动清理资源，自动生成try-finally这样的代码来关闭流\n\n * @getter/@setter：用在属性上，再也不用自己手写setter和getter方法了，还可以指定访问范围\n\n * @tostring：用在类上，可以自动覆写tostring方法，当然还可以加其他参数，例如@tostring(exclude=”id”)排除id属性，或者@tostring(callsuper=true, includefieldnames=true)调用父类的tostring方法，包含所有属性\n\n * @equalsandhashcode：用在类上，自动生成equals方法和hashcode方法\n\n * @noargsconstructor, @requiredargsconstructor and @allargsconstructor：用在类上，自动生成无参构造和使用所有参数的构造函数以及把所有@nonnull属性作为参数的构造函数，如果指定staticname = “of”参数，同时还会生成一个返回类对象的静态工厂方法，比使用构造函数方便很多\n\n * @utilityclass:注解在类上，添加了static同时创建了一个私有的构造方法\n\n * @data：注解在类上，相当于同时使用了@tostring、@equalsandhashcode、@getter、@setter和@requiredargsconstrutor这些注解，对于pojo类十分有用\n\n * @value：用在类上，是@data的不可变形式，相当于为属性添加final声明，只提供getter方法，而不提供setter方法\n\n * @builder：用在类、构造器、方法上，为你提供复杂的builder apis，让你可以像如下方式一样调用person.builder().name("adam savage").city("san francisco").job("mythbusters").job("unchained reaction").build();更多说明参考builder\n\n * @sneakythrows：自动抛受检异常，而无需显式在方法上使用throws语句\n\n * @synchronized：用在方法上，将方法声明为同步的，并自动加锁，而锁对象是一个私有的属性$lock或$lock，而java中的synchronized关键字锁对象是this，锁在this或者自己的类对象上存在副作用，就是你不能阻止非受控代码去锁this或者类对象，这可能会导致竞争条件或者其它线程错误\n\n * @getter(lazy=true)：可以替代经典的double check lock样板代码\n\n * @log：根据不同的注解生成不同类型的log对象，但是实例名称都是log，有六种可选实现类\n\n * * @commonslog creates log = org.apache.commons.logging.logfactory.getlog(logexample.class);\n   * @log creates log = java.util.logging.logger.getlogger(logexample.class.getname());\n   * @log4j creates log = org.apache.log4j.logger.getlogger(logexample.class);\n   * @log4j2 creates log = org.apache.logging.log4j.logmanager.getlogger(logexample.class);\n   * @slf4j creates log = org.slf4j.loggerfactory.getlogger(logexample.class);\n   * @xslf4j creates log = org.slf4j.ext.xloggerfactory.getxlogger(logexample.class);\n\n\n# @getter and @setter\n\n你可以使用 @getter 或 @setter 注释任何类或字段，lombok 会自动生成默认的 getter/setter 方法。\n\n@getter\n\n@target({elementtype.field, elementtype.type})\n@retention(retentionpolicy.source)\npublic @interface getter {\n  // 若getter方法非public的话，可以设置可访问级别\n    lombok.accesslevel value() default lombok.accesslevel.public;\n    anyannotation[] onmethod() default {};\n  // 是否启用延迟初始化\n    boolean lazy() default false;\n}\n\n\n@setter\n\n@target({elementtype.field, elementtype.type})\n@retention(retentionpolicy.source)\npublic @interface setter {\n  // 若setter方法非public的话，可以设置可访问级别\n    lombok.accesslevel value() default lombok.accesslevel.public;\n    anyannotation[] onmethod() default {};\n    anyannotation[] onparam() default {};\n}\n\n\n使用示例\n\n@getter\n@setter\npublic class getterandsetterdemo {\n    string firstname;\n    string lastname;\n    localdate dateofbirth;\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class getterandsetterdemo {\n    string firstname;\n    string lastname;\n    localdate dateofbirth;\n\n    public getterandsetterdemo() {\n    }\n\n    // 省略其它setter和getter方法\n    public string getfirstname() {\n        return this.firstname;\n    }\n\n    public void setfirstname(string firstname) {\n        this.firstname = firstname;\n    }\n}\n\n\nlazy getter\n\n@getter 注解支持一个 lazy 属性，该属性默认为 false。当设置为 true 时，会启用延迟初始化，即当首次调用 getter 方法时才进行初始化。\n\n示例\n\npublic class lazygetterdemo {\n    public static void main(string[] args) {\n        lazygetterdemo m = new lazygetterdemo();\n        system.out.println("main instance is created");\n        m.getlazy();\n    }\n\n    @getter\n    private final string notlazy = createvalue("not lazy");\n\n    @getter(lazy = true)\n    private final string lazy = createvalue("lazy");\n\n    private string createvalue(string name) {\n        system.out.println("createvalue(" + name + ")");\n        return null;\n    }\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class lazygetterdemo {\n    private final string notlazy = this.createvalue("not lazy");\n    private final atomicreference<object> lazy = new atomicreference();\n\n    // 已省略部分代码\n    public string getnotlazy() {\n        return this.notlazy;\n    }\n\n    public string getlazy() {\n        object value = this.lazy.get();\n        if (value == null) {\n            synchronized(this.lazy) {\n                value = this.lazy.get();\n                if (value == null) {\n                    string actualvalue = this.createvalue("lazy");\n                    value = actualvalue == null ? this.lazy : actualvalue;\n                    this.lazy.set(value);\n                }\n            }\n        }\n\n        return (string)((string)(value == this.lazy ? null : value));\n    }\n}\n\n\n通过以上代码可知，调用 getlazy 方法时，若发现 value 为 null，则会在同步代码块中执行初始化操作。\n\n\n# constructor annotations\n\n@noargsconstructor\n\n使用 @noargsconstructor 注解可以为指定类，生成默认的构造函数，@noargsconstructor 注解的定义如下：\n\n@target(elementtype.type)\n@retention(retentionpolicy.source)\npublic @interface noargsconstructor {\n  // 若设置该属性，将会生成一个私有的构造函数且生成一个staticname指定的静态方法\n    string staticname() default "";\n    anyannotation[] onconstructor() default {};\n  // 设置生成构造函数的访问级别，默认是public\n    accesslevel access() default lombok.accesslevel.public;\n  // 若设置为true，则初始化所有final的字段为0/null/false\n    boolean force() default false;\n}\n\n\n示例\n\n@noargsconstructor(staticname = "getinstance")\npublic class noargsconstructordemo {\n    private long id;\n    private string name;\n    private int age;\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class noargsconstructordemo {\n    private long id;\n    private string name;\n    private int age;\n\n    private noargsconstructordemo() {\n    }\n\n    public static noargsconstructordemo getinstance() {\n        return new noargsconstructordemo();\n    }\n}\n\n\n@allargsconstructor\n\n使用 @allargsconstructor 注解可以为指定类，生成包含所有成员的构造函数，@allargsconstructor 注解的定义如下：\n\n@target(elementtype.type)\n@retention(retentionpolicy.source)\npublic @interface allargsconstructor {\n  // 若设置该属性，将会生成一个私有的构造函数且生成一个staticname指定的静态方法\n    string staticname() default "";\n    anyannotation[] onconstructor() default {};\n  // 设置生成构造函数的访问级别，默认是public\n    accesslevel access() default lombok.accesslevel.public;\n}\n\n\n示例\n\n@allargsconstructor\npublic class allargsconstructordemo {\n    private long id;\n    private string name;\n    private int age;\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class allargsconstructordemo {\n    private long id;\n    private string name;\n    private int age;\n\n    public allargsconstructordemo(long id, string name, int age) {\n        this.id = id;\n        this.name = name;\n        this.age = age;\n    }\n}\n\n\n@requiredargsconstructor\n\n使用 @requiredargsconstructor 注解可以为指定类必需初始化的成员变量，如 final 成员变量，生成对应的构造函数，@requiredargsconstructor 注解的定义如下：\n\n@target(elementtype.type)\n@retention(retentionpolicy.source)\npublic @interface requiredargsconstructor {\n  // 若设置该属性，将会生成一个私有的构造函数且生成一个staticname指定的静态方法\n    string staticname() default "";\n    anyannotation[] onconstructor() default {};\n  // 设置生成构造函数的访问级别，默认是public\n    accesslevel access() default lombok.accesslevel.public;\n}\n\n\n示例\n\n@requiredargsconstructor\npublic class requiredargsconstructordemo {\n    private final long id;\n    private string name;\n    private int age;\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class requiredargsconstructordemo {\n    private final long id;\n    private string name;\n    private int age;\n\n    public requiredargsconstructordemo(long id) {\n        this.id = id;\n    }\n}\n\n\n\n# @utilityclass\n\n示例\n\n@utilityclass\npublic class securityutils {\n\n    public authentication getauthentication() {\n        return securitycontextholder.getcontext().getauthentication();\n    }\n\n    public simplifyuser getuser(authentication authentication) {\n        object principal = authentication.getprincipal();\n        if (principal instanceof simplifyuser) {\n            return (simplifyuser) principal;\n        }\n        return null;\n    }\n\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic final class securityutils {\n    public static authentication getauthentication() {\n        return securitycontextholder.getcontext().getauthentication();\n    }\n\n    private securityutils() {\n        throw new unsupportedoperationexception("this is a utility class and cannot be instantiated");\n    }\n}\n\n\n\n# @equalsandhashcode\n\n使用 @equalsandhashcode 注解可以为指定类生成 equals 和 hashcode 方法， @equalsandhashcode 注解的定义如下：\n\n@target(elementtype.type)\n@retention(retentionpolicy.source)\npublic @interface equalsandhashcode {\n  // 指定在生成的equals和hashcode方法中需要排除的字段列表\n    string[] exclude() default {};\n    \n  // 显式列出用于identity的字段，一般情况下non-static,non-transient字段会被用于identity\n    string[] of() default {};\n    \n  // 标识在执行字段计算前，是否调用父类的equals和hashcode方法\n    boolean callsuper() default false;\n    \n    boolean donotusegetters() default false;\n    \n    anyannotation[] onparam() default {};\n    \n    @deprecated\n    @retention(retentionpolicy.source)\n    @target({})\n    @interface anyannotation {}\n    \n    @target(elementtype.field)\n    @retention(retentionpolicy.source)\n    public @interface exclude {}\n    \n    @target({elementtype.field, elementtype.method})\n    @retention(retentionpolicy.source)\n    public @interface include {\n        string replaces() default "";\n    }\n}\n\n\n示例\n\npublic class equalsandhashcodedemo {\n    string firstname;\n    string lastname;\n    localdate dateofbirth;\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class equalsandhashcodedemo {\n    string firstname;\n    string lastname;\n    localdate dateofbirth;\n\n    public equalsandhashcodedemo() {\n    }\n\n    public boolean equals(object o) {\n        if (o == this) {\n            return true;\n        } else if (!(o instanceof equalsandhashcodedemo)) {\n            return false;\n        } else {\n            equalsandhashcodedemo other = (equalsandhashcodedemo)o;\n            if (!other.canequal(this)) {\n                return false;\n            } else {\n              // 已省略大量代码\n        }\n    }\n\n    public int hashcode() {\n        int prime = true;\n        int result = 1;\n        object $firstname = this.firstname;\n        int result = result * 59 + ($firstname == null ? 43 : $firstname.hashcode());\n        object $lastname = this.lastname;\n        result = result * 59 + ($lastname == null ? 43 : $lastname.hashcode());\n        object $dateofbirth = this.dateofbirth;\n        result = result * 59 + ($dateofbirth == null ? 43 : $dateofbirth.hashcode());\n        return result;\n    }\n}\n\n\n\n# @tostring\n\n使用 @tostring 注解可以为指定类生成 tostring 方法， @tostring 注解的定义如下：\n\n@target(elementtype.type)\n@retention(retentionpolicy.source)\npublic @interface tostring {\n  // 打印输出时是否包含字段的名称\n    boolean includefieldnames() default true;\n    \n  // 列出打印输出时，需要排除的字段列表\n    string[] exclude() default {};\n    \n  // 显式的列出需要打印输出的字段列表\n    string[] of() default {};\n    \n  // 打印输出的结果中是否包含父类的tostring方法的返回结果\n    boolean callsuper() default false;\n    \n    boolean donotusegetters() default false;\n    \n    boolean onlyexplicitlyincluded() default false;\n    \n    @target(elementtype.field)\n    @retention(retentionpolicy.source)\n    public @interface exclude {}\n    \n    @target({elementtype.field, elementtype.method})\n    @retention(retentionpolicy.source)\n    public @interface include {\n        int rank() default 0;\n        string name() default "";\n    }\n}\n\n\n示例\n\n@tostring(exclude = {"dateofbirth"})\npublic class tostringdemo {\n    string firstname;\n    string lastname;\n    localdate dateofbirth;\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class tostringdemo {\n    string firstname;\n    string lastname;\n    localdate dateofbirth;\n\n    public tostringdemo() {\n    }\n\n    public string tostring() {\n        return "tostringdemo(firstname=" + this.firstname + ", lastname=" + this.lastname + ")";\n    }\n}\n\n\n\n# @data\n\n@data 注解与同时使用以下的注解的效果是一样的：\n\n@tostring\n@getter\n@setter\n@requiredargsconstructor\n@equalsandhashcode\n\n\n@data 注解的定义如下：\n\n@target(elementtype.type)\n@retention(retentionpolicy.source)\npublic @interface data {\n    string staticconstructor() default "";\n}\n\n\n示例\n\n@data\npublic class datademo {\n    private long id;\n    private string summary;\n    private string description;\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class datademo {\n    private long id;\n    private string summary;\n    private string description;\n\n    public datademo() {\n    }\n\n    // 省略summary和description成员属性的setter和getter方法\n    public long getid() {\n        return this.id;\n    }\n  \n    public void setid(long id) {\n        this.id = id;\n    }\n\n    public boolean equals(object o) {\n        if (o == this) {\n            return true;\n        } else if (!(o instanceof datademo)) {\n            return false;\n        } else {\n            datademo other = (datademo)o;\n            if (!other.canequal(this)) {\n                return false;\n            } else {\n               // 已省略大量代码\n            }\n        }\n    }\n\n    protected boolean canequal(object other) {\n        return other instanceof datademo;\n    }\n\n    public int hashcode() {\n        int prime = true;\n        int result = 1;\n        object $id = this.getid();\n        int result = result * 59 + ($id == null ? 43 : $id.hashcode());\n        object $summary = this.getsummary();\n        result = result * 59 + ($summary == null ? 43 : $summary.hashcode());\n        object $description = this.getdescription();\n        result = result * 59 + ($description == null ? 43 : $description.hashcode());\n        return result;\n    }\n\n    public string tostring() {\n        return "datademo(id=" + this.getid() + ", summary=" + this.getsummary() + ", description=" + this.getdescription() + ")";\n    }\n}\n\n\n\n# @log\n\n若你将 @log 的变体放在类上（适用于你所使用的日志记录系统的任何一种）；之后，你将拥有一个静态的 final log 字段，然后你就可以使用该字段来输出日志。\n\n@log\nprivate static final java.util.logging.logger log = java.util.logging.logger.getlogger(logexample.class.getname());\n@log4j\nprivate static final org.apache.log4j.logger log = org.apache.log4j.logger.getlogger(logexample.class);\n@log4j2\nprivate static final org.apache.logging.log4j.logger log = org.apache.logging.log4j.logmanager.getlogger(logexample.class);\n@slf4j\nprivate static final org.slf4j.logger log = org.slf4j.loggerfactory.getlogger(logexample.class);\n@xslf4j\nprivate static final org.slf4j.ext.xlogger log = org.slf4j.ext.xloggerfactory.getxlogger(logexample.class);\n@commonslog\nprivate static final org.apache.commons.logging.log log = org.apache.commons.logging.logfactory.getlog(logexample.class);\n\n\n\n# @synchronized\n\n@synchronized 是同步方法修饰符的更安全的变体。与 synchronized 一样，该注解只能应用在静态和实例方法上。它的操作类似于 synchronized 关键字，但是它锁定在不同的对象上。synchronized 关键字应用在实例方法时，锁定的是 this 对象，而应用在静态方法上锁定的是类对象。对于 @synchronized 注解声明的方法来说，它锁定的是\n\n或lock。@synchronized 注解的定义如下：\n\n@target(elementtype.method)\n@retention(retentionpolicy.source)\npublic @interface synchronized {\n  // 指定锁定的字段名称\n    string value() default "";\n}\n\n\n示例\n\npublic class synchronizeddemo {\n    private final object readlock = new object();\n\n    @synchronized\n    public static void hello() {\n        system.out.println("world");\n    }\n\n    @synchronized\n    public int answertolife() {\n        return 42;\n    }\n\n    @synchronized("readlock")\n    public void foo() {\n        system.out.println("bar");\n    }\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class synchronizeddemo {\n    private static final object $lock = new object[0];\n    private final object $lock = new object[0];\n    private final object readlock = new object();\n\n    public synchronizeddemo() {\n    }\n\n    public static void hello() {\n        synchronized($lock) {\n            system.out.println("world");\n        }\n    }\n\n    public int answertolife() {\n        synchronized(this.$lock) {\n            return 42;\n        }\n    }\n\n    public void foo() {\n        synchronized(this.readlock) {\n            system.out.println("bar");\n        }\n    }\n}\n\n\n\n# @builder\n\n使用 @builder 注解可以为指定类实现建造者模式，该注解可以放在类、构造函数或方法上。@builder 注解的定义如下：\n\n@target({type, method, constructor})\n@retention(source)\npublic @interface builder {\n    @target(field)\n    @retention(source)\n    public @interface default {}\n\n  // 创建新的builder实例的方法名称\n    string buildermethodname() default "builder";\n    // 创建builder注解类对应实例的方法名称\n    string buildmethodname() default "build";\n    // builder类的名称\n    string builderclassname() default "";\n    \n    boolean tobuilder() default false;\n    \n    accesslevel access() default lombok.accesslevel.public;\n    \n    @target({field, parameter})\n    @retention(source)\n    public @interface obtainvia {\n        string field() default "";\n        string method() default "";\n        boolean isstatic() default false;\n    }\n}\n\n\n示例\n\n@builder\npublic class builderdemo {\n    private final string firstname;\n    private final string lastname;\n    private final string email;\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class builderdemo {\n    private final string firstname;\n    private final string lastname;\n    private final string email;\n\n    builderdemo(string firstname, string lastname, string email) {\n        this.firstname = firstname;\n        this.lastname = lastname;\n        this.email = email;\n    }\n\n    public static builderdemo.builderdemobuilder builder() {\n        return new builderdemo.builderdemobuilder();\n    }\n\n    public static class builderdemobuilder {\n        private string firstname;\n        private string lastname;\n        private string email;\n\n        builderdemobuilder() {\n        }\n\n        public builderdemo.builderdemobuilder firstname(string firstname) {\n            this.firstname = firstname;\n            return this;\n        }\n\n        public builderdemo.builderdemobuilder lastname(string lastname) {\n            this.lastname = lastname;\n            return this;\n        }\n\n        public builderdemo.builderdemobuilder email(string email) {\n            this.email = email;\n            return this;\n        }\n\n        public builderdemo build() {\n            return new builderdemo(this.firstname, this.lastname, this.email);\n        }\n\n        public string tostring() {\n            return "builderdemo.builderdemobuilder(firstname=" + this.firstname + ", lastname=" + this.lastname + ", email=" + this.email + ")";\n        }\n    }\n}\n\n\n\n# @sneakythrows\n\n@sneakythrows 注解用于自动抛出已检查的异常，而无需在方法中使用 throw 语句显式抛出。@sneakythrows 注解的定义如下：\n\n@target({elementtype.method, elementtype.constructor})\n@retention(retentionpolicy.source)\npublic @interface sneakythrows {\n    // 设置你希望向上抛的异常类\n    class<? extends throwable>[] value() default java.lang.throwable.class;\n}\n\n\n示例\n\npublic class sneakythrowsdemo {\n    @sneakythrows\n    @override\n    protected object clone() {\n        return super.clone();\n    }\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class sneakythrowsdemo {\n    public sneakythrowsdemo() {\n    }\n\n    protected object clone() {\n        try {\n            return super.clone();\n        } catch (throwable var2) {\n            throw var2;\n        }\n    }\n}\n\n\n\n# @nonnull\n\n你可以在方法或构造函数的参数上使用 @nonnull 注解，它将会为你自动生成非空校验语句。@nonnull 注解的定义如下：\n\n@target({elementtype.field, elementtype.method, elementtype.parameter, elementtype.local_variable, elementtype.type_use})\n@retention(retentionpolicy.class)\n@documented\npublic @interface nonnull {\n\n}\n\n\n示例\n\npublic class nonnulldemo {\n    @getter\n    @setter\n    @nonnull\n    private string name;\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class nonnulldemo {\n    @nonnull\n    private string name;\n\n    public nonnulldemo() {\n    }\n\n    @nonnull\n    public string getname() {\n        return this.name;\n    }\n\n    public void setname(@nonnull string name) {\n        if (name == null) {\n            throw new nullpointerexception("name is marked non-null but is null");\n        } else {\n            this.name = name;\n        }\n    }\n}\n\n\n\n# @clean\n\n@clean 注解用于自动管理资源，用在局部变量之前，在当前变量范围内即将执行完毕退出之前会自动清理资源，自动生成 try-finally 这样的代码来关闭流。\n\n@target(elementtype.local_variable)\n@retention(retentionpolicy.source)\npublic @interface cleanup {\n  // 设置用于执行资源清理/回收的方法名称，对应方法不能包含任何参数，默认名称为close。\n    string value() default "close";\n}\n\n\n示例\n\npublic class cleanupdemo {\n    public static void main(string[] args) throws ioexception {\n        @cleanup inputstream in = new fileinputstream(args[0]);\n        @cleanup outputstream out = new fileoutputstream(args[1]);\n        byte[] b = new byte[10000];\n        while (true) {\n            int r = in.read(b);\n            if (r == -1) break;\n            out.write(b, 0, r);\n        }\n    }\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class cleanupdemo {\n    public cleanupdemo() {\n    }\n\n    public static void main(string[] args) throws ioexception {\n        fileinputstream in = new fileinputstream(args[0]);\n\n        try {\n            fileoutputstream out = new fileoutputstream(args[1]);\n\n            try {\n                byte[] b = new byte[10000];\n\n                while(true) {\n                    int r = in.read(b);\n                    if (r == -1) {\n                        return;\n                    }\n\n                    out.write(b, 0, r);\n                }\n            } finally {\n                if (collections.singletonlist(out).get(0) != null) {\n                    out.close();\n                }\n\n            }\n        } finally {\n            if (collections.singletonlist(in).get(0) != null) {\n                in.close();\n            }\n        }\n    }\n}\n\n\n\n# @with\n\n在类的字段上应用 @with 注解之后，将会自动生成一个 withfieldname(newvalue) 的方法，该方法会基于 newvalue 调用相应构造函数，创建一个当前类对应的实例。@with 注解的定义如下：\n\n@target({elementtype.field, elementtype.type})\n@retention(retentionpolicy.source)\npublic @interface with {\n    accesslevel value() default accesslevel.public;\n\n    with.anyannotation[] onmethod() default {};\n\n    with.anyannotation[] onparam() default {};\n\n    @deprecated\n    @retention(retentionpolicy.source)\n    @target({})\n    public @interface anyannotation {\n    }\n}\n\n\n示例\n\npublic class withdemo {\n    @with(accesslevel.protected)\n    @nonnull\n    private final string name;\n    @with\n    private final int age;\n\n    public withdemo(string name, int age) {\n        if (name == null) throw new nullpointerexception();\n        this.name = name;\n        this.age = age;\n    }\n}\n\n\n以上代码经过 lombok 编译后，会生成如下代码：\n\npublic class withdemo {\n    @nonnull\n    private final string name;\n    private final int age;\n\n    public withdemo(string name, int age) {\n        if (name == null) {\n            throw new nullpointerexception();\n        } else {\n            this.name = name;\n            this.age = age;\n        }\n    }\n\n    protected withdemo withname(@nonnull string name) {\n        if (name == null) {\n            throw new nullpointerexception("name is marked non-null but is null");\n        } else {\n            return this.name == name ? this : new withdemo(name, this.age);\n        }\n    }\n\n    public withdemo withage(int age) {\n        return this.age == age ? this : new withdemo(this.name, age);\n    }\n}\n\n\n\n# @accessors\n\n@accessors 注解用来配置lombok如何产生和显示getters和setters的方法。@accessors注解既可以注解在类上也可以注解在属性上 @accessors有三个属性\n\n# fluent\n\nfluent为一个布尔值，如果为true生成的get/set方法则没有set/get前缀，默认为false 例如：\n\n@accessors(flunet = true)\npublic class myclass {\n    @getter\n    private int num;\n}\n\n\n生成的get方法为num()，而不是getnum()\n\n# chain\n\nchain为一个布尔值，如果为true生成的set方法返回this，为false生成的set方法是void类型。默认为false，除非当fluent为true时，chain默认则为true\n\n# prefix\n\nprefix为一系列string类型，可以指定前缀，生成get/set方法时会去掉指定的前缀 例如：\n\n@accessors(prefix = "m")\npublic class myclass {\n    @getter\n    private int mnum;\n}\n\n\n生成的get方法为getnum()，而不是getmnum()\n\n\n# 其它特性\n\n# val\n\nval 用在局部变量前面，相当于将变量声明为 final，此外 lombok 在编译时还会自动进行类型推断。val 的使用示例：\n\npublic class valexample {\n  public string example() {\n    val example = new arraylist<string>();\n    example.add("hello, world!");\n    val foo = example.get(0);\n    return foo.tolowercase();\n  }\n  \n  public void example2() {\n    val map = new hashmap<integer, string>();\n    map.put(0, "zero");\n    map.put(5, "five");\n    for (val entry : map.entryset()) {\n      system.out.printf("%d: %s\\n", entry.getkey(), entry.getvalue());\n    }\n  }\n}\n\n\n以上代码等价于：\n\npublic class valexample {\n  public string example() {\n    final arraylist<string> example = new arraylist<string>();\n    example.add("hello, world!");\n    final string foo = example.get(0);\n    return foo.tolowercase();\n  }\n  \n  public void example2() {\n    final hashmap<integer, string> map = new hashmap<integer, string>();\n    map.put(0, "zero");\n    map.put(5, "five");\n    for (final map.entry<integer, string> entry : map.entryset()) {\n      system.out.printf("%d: %s\\n", entry.getkey(), entry.getvalue());\n    }\n  }\n}\n\n\n\n# lombok注解原理\n\n说到 lombok，我们就得去提到 jsr 269: pluggable annotation processing api 。jsr 269 之前我们也有注解这样的神器，可是我们比如想要做什么必须使用反射，反射的方法局限性较大。首先，它必须定义@retention为retentionpolicy.runtime，只能在运行时通过反射来获取注解值，使得运行时代码效率降低。其次，如果想在编译阶段利用注解来进行一些检查，对用户的某些不合理代码给出错误报告，反射的使用方法就无能为力了。而 jsr 269 之后我们可以在 javac的编译期利用注解做这些事情。所以我们发现核心的区分是在 运行期 还是 编译期。\n\n\n\n从上图可知，annotation processing 是在解析和生成之间的一个步骤。具体详细步骤如下：\n\n\n\n上图是 lombok 处理流程，在javac 解析成抽象语法树之后(ast), lombok 根据自己的注解处理器，动态的修改 ast，增加新的节点(所谓代码)，最终通过分析和生成字节码。\n\n自从java 6起，javac就支持“jsr 269 pluggable annotation processing api”规范，只要程序实现了该api，就能在javac运行的时候得到调用。\n\n提示\n\n 1. 常用的项目管理工具maven所使用的java编译工具来源于配置的第三方工具，如果我们配置这个第三方工具为oracle javac的话，那么maven也就直接支持lombok了;\n 2. intellij idea配置的编译工具为oracle javac的话，也就直接支持lombok了。\n\nide工具问题解决：\n\n提示\n\n现在有一个a类，其中有一些字段，没有创建它们的setter和getter方法，使用了lombok的@data注解，另外有一个b类，它调用了a类实例的相应字段的setter和getter方法\n\n编译a类和b类所在的项目，并不会报错，因为最终生成的a类字节码文件中存在相应字段的setter和getter方法\n\n但是，ide发现b类源代码中所使用的a类实例的setter和getter方法在a类源代码中找不到定义，ide会认为这是错误\n\n要解决以上这个不是真正错误的错误，可以下载安装intellij idea中的"lombok plugin"。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Spring Boot整合Swagger",frontmatter:{title:"Spring Boot整合Swagger",date:"2022-02-09T13:15:12.000Z",permalink:"/pages/ff62ec/"},regularPath:"/21.Simplify/01.Simplify%20Docs/02.Spring%20Boot%E6%95%B4%E5%90%88Swagger.html",relativePath:"21.Simplify/01.Simplify Docs/02.Spring Boot整合Swagger.md",key:"v-03f86a9e",path:"/pages/ff62ec/",headers:[{level:2,title:"版本说明",slug:"版本说明",normalizedTitle:"版本说明",charIndex:27},{level:2,title:"创建工程",slug:"创建工程",normalizedTitle:"创建工程",charIndex:120},{level:2,title:"创建 entity",slug:"创建-entity",normalizedTitle:"创建 entity",charIndex:2637},{level:2,title:"创建 controller",slug:"创建-controller",normalizedTitle:"创建 controller",charIndex:3452},{level:2,title:"Swagger 核心配置文件",slug:"swagger-核心配置文件",normalizedTitle:"swagger 核心配置文件",charIndex:5080},{level:2,title:"启动项目",slug:"启动项目",normalizedTitle:"启动项目",charIndex:8493},{level:2,title:"查看效果",slug:"查看效果",normalizedTitle:"查看效果",charIndex:10867},{level:2,title:"常用注解说明",slug:"常用注解说明",normalizedTitle:"常用注解说明",charIndex:10960}],headersStr:"版本说明 创建工程 创建 entity 创建 controller Swagger 核心配置文件 启动项目 查看效果 常用注解说明",content:'# Spring Boot整合Swagger\n\n\n# 版本说明\n\n软件                  版本\nspring boot         2.3.5.RELEASE\nspringfox-swagger   3.0.0\n\n\n# 创建工程\n\n使用 Spring Initializr 创建工程 pom.xml 文件内容如下 ：\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.3.5.RELEASE</version>\n        <relativePath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n    <groupId>com.ieooc.demo</groupId>\n    <artifactId>springfox-swagger-demo</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <name>springfox-swagger-demo</name>\n    <description>springfox-swagger 单模块整合demo</description>\n\n    <properties>\n        <java.version>1.8</java.version>\n        <swagger.fox.version>3.0.0</swagger.fox.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n            <exclusions>\n                <exclusion>\n                    <groupId>org.junit.vintage</groupId>\n                    <artifactId>junit-vintage-engine</artifactId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n\n        \x3c!--lombok敏捷开发工具包--\x3e\n        <dependency>\n            <groupId>org.projectlombok</groupId>\n            <artifactId>lombok</artifactId>\n            <scope>provided</scope>\n        </dependency>\n\n        \x3c!--swagger 依赖--\x3e\n        <dependency>\n            <groupId>io.springfox</groupId>\n            <artifactId>springfox-boot-starter</artifactId>\n            <version>${swagger.fox.version}</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n\n\n\n> 引入lombok为了简化代码，可以不引入，不影响swagger集成。\n\n主要依赖 springfox-boot-starter ，这是用官方提供的包含自动配置。\n\n\x3c!--swagger 依赖--\x3e\n<dependency>\n    <groupId>io.springfox</groupId>\n    <artifactId>springfox-boot-starter</artifactId>\n    <version>${swagger.fox.version}</version>\n</dependency>\n\n\n\n# 创建 entity\n\n新建包名 com.ieooc.demo.swagger.entity,创建SysUser实体类，内容如下：\n\npackage com.ieooc.demo.swagger.entity;\n\nimport io.swagger.annotations.ApiModel;\nimport io.swagger.annotations.ApiModelProperty;\nimport lombok.Data;\nimport lombok.EqualsAndHashCode;\n\nimport java.io.Serializable;\n\n/**\n * <p>\n * 用户表\n * </p>\n *\n * @author heyuqiang\n * @since 2020-08-30\n */\n@Data\n@EqualsAndHashCode(callSuper = false)\n@ApiModel(value = "SysUser", description = "用户表")\npublic class SysUser implements Serializable {\n\n    private static final long serialVersionUID = 1L;\n\n    @ApiModelProperty(value = "主键ID")\n    private Integer userId;\n\n    @ApiModelProperty(value = "用户名")\n    private String username;\n\n    @ApiModelProperty(value = "密码")\n    private String password;\n\n}\n\n\n\n> @Data注解由lombok提供，如果没用引入lombok依赖，需要手动生成getter,setter方法。\n\n\n# 创建 controller\n\n新建包名 com.ieooc.demo.swagger.controller,创建SysUserController控制器，内容如下：\n\npackage com.ieooc.demo.swagger.controller;\n\n\nimport com.ieooc.demo.swagger.entity.SysUser;\nimport io.swagger.annotations.Api;\nimport io.swagger.annotations.ApiOperation;\nimport io.swagger.annotations.ApiParam;\nimport lombok.AllArgsConstructor;\nimport org.springframework.web.bind.annotation.*;\n\n/**\n * <p>\n * 用户管理模块\n * </p>\n *\n * @author heyuqiang\n * @since 2020-08-30\n */\n@RestController\n@RequestMapping("/users")\n@AllArgsConstructor\n@Api(value = "user", tags = "用户管理")\npublic class SysUserController {\n\n    /**\n     * 新增用户\n     */\n    @PostMapping()\n    @ApiOperation(value = "saveUser", notes = "新增用户")\n    public void saveUser(@ApiParam(value = "用户信息") @RequestBody SysUser sysUser) {\n\n    }\n\n    /**\n     * 根据用户id删除用户信息\n     */\n    @DeleteMapping("/{userId}")\n    @ApiOperation(value = "deleteById", notes = "根据用户id删除用户信息")\n    public void deleteById(@ApiParam(value = "用户id") @PathVariable("userId") Integer userId) {\n\n    }\n\n    /**\n     * 根根据用户名查询用户角色，权限信息\n     */\n    @PutMapping("/{userId}")\n    @ApiOperation(value = "info", notes = "根据用户名查询用户角色，权限信息")\n    public void info(@ApiParam(value = "用户id") @PathVariable("userId") Integer userId,\n                     @ApiParam(value = "用户信息") @RequestBody SysUser sysUser) {\n\n    }\n\n    /**\n     * 根据用户id查询用户信息\n     *\n     * @param userId 用户id\n     * @return 用户信息\n     */\n    @GetMapping("/{userId}")\n    @ApiOperation(value = "getUserVoById", notes = "根据用户id查询用户信息")\n    public void getUserVoById(@ApiParam(value = "用户id") @PathVariable Integer userId) {\n    }\n\n\n}\n\n\n> 包含了常用的增、删、改、查。\n\n\n# Swagger 核心配置文件\n\n新建包名 com.ieooc.demo.swagger.config,创建SwaggerAutoConfiguration配置类，内容如下：\n\npackage com.ieooc.demo.swagger.config;\n\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport springfox.documentation.builders.ApiInfoBuilder;\nimport springfox.documentation.builders.PathSelectors;\nimport springfox.documentation.builders.RequestHandlerSelectors;\nimport springfox.documentation.service.ApiInfo;\nimport springfox.documentation.service.Contact;\nimport springfox.documentation.spi.DocumentationType;\nimport springfox.documentation.spring.web.plugins.Docket;\nimport springfox.documentation.swagger.web.*;\nimport springfox.documentation.swagger2.annotations.EnableSwagger2;\n\nimport java.util.Arrays;\nimport java.util.LinkedHashSet;\n\n/**\n * @author heyuqiang\n * @date 2020/10/03\n * Swagger 核心配置文件\n * 官方文档：http://springfox.github.io/springfox/docs/current/\n */\n@Configuration\n@EnableSwagger2\npublic class SwaggerAutoConfiguration {\n\n    /**\n     * 组织Docket对象，初始化swagger主要配置\n     *\n     * @return Docket\n     */\n    @Bean\n    public Docket api() {\n        //自定义配置，指定规范，这里是SWAGGER_2\n        return new Docket(DocumentationType.SWAGGER_2)\n                //定义是否开启swagger，false为关闭，可以通过变量控制\n                .enable(true)\n                //设定Api文档头信息，这个信息会展示在文档UI的头部位置\n                .apiInfo(apiInfo())\n                //指定需要发布到Swagger的接口目录，不支持通配符\n                .select()\n                .apis(RequestHandlerSelectors.basePackage("com.ieooc.demo.swagger.controller"))\n                .paths(PathSelectors.any())\n                .build()\n                //将servlet路径映射（如果有）添加到apis基本路径\n                .pathMapping("/")\n                //支持的通讯协议集合\n                .protocols(new LinkedHashSet<>(Arrays.asList("https", "http")));\n    }\n\n    /**\n     * 自定义API文档基本信息实体\n     *\n     * @return ApiInfo\n     */\n    public ApiInfo apiInfo() {\n        return new ApiInfoBuilder()\n                //构建联系实体，在UI界面会显示\n                .contact(new Contact("simplify", "https://www.ieooc.com", "info@ieooc.com"))\n                //标题\n                .title("simplify title")\n                //描述\n                .description("simplify description")\n                //许可证URL\n                .license("simplify license")\n                //许可证URL\n                .licenseUrl("simplify licenseUrl")\n                //服务条款URL\n                .termsOfServiceUrl("simplify termsOfServiceUrl")\n                //版本\n                .version("v1.0")\n                .build();\n    }\n\n    /**\n     * UI配置\n     */\n    @Bean\n    UiConfiguration uiConfig() {\n        return UiConfigurationBuilder.builder()\n                .deepLinking(true)\n                .displayOperationId(false)\n                .defaultModelsExpandDepth(1)\n                .defaultModelExpandDepth(1)\n                .defaultModelRendering(ModelRendering.EXAMPLE)\n                .displayRequestDuration(false)\n                .docExpansion(DocExpansion.NONE)\n                .filter(false)\n                .maxDisplayedTags(null)\n                .operationsSorter(OperationsSorter.ALPHA)\n                .showExtensions(false)\n                .showCommonExtensions(false)\n                .tagsSorter(TagsSorter.ALPHA)\n                .supportedSubmitMethods(UiConfiguration.Constants.DEFAULT_SUBMIT_METHODS)\n                .validatorUrl(null)\n                .build();\n    }\n\n}\n\n\n\n> uiConfig 可选配置，默认可以创建。\n\n\n# 启动项目\n\n  .   ____          _            __ _ _\n /\\\\ / ___\'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | \'_ | \'_| | \'_ \\/ _` | \\ \\ \\ \\\n \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n  \'  |____| .__|_| |_|_| |_\\__, | / / / /\n =========|_|==============|___/=/_/_/_/\n :: Spring Boot ::        (v2.3.5.RELEASE)\n\n2020-11-09 17:33:54.619  INFO 132 --- [           main] com.ieooc.demo.swagger.DemoApplication   : Starting DemoApplication on DESKTOP-7V8Q19U with PID 132 (D:\\IdeaProjects\\springfox-swagger-demo\\target\\classes started by heyuq in D:\\IdeaProjects\\springfox-swagger-demo)\n2020-11-09 17:33:54.622  INFO 132 --- [           main] com.ieooc.demo.swagger.DemoApplication   : No active profile set, falling back to default profiles: default\n2020-11-09 17:33:56.474  INFO 132 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)\n2020-11-09 17:33:56.482  INFO 132 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]\n2020-11-09 17:33:56.483  INFO 132 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.39]\n2020-11-09 17:33:56.657  INFO 132 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext\n2020-11-09 17:33:56.657  INFO 132 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1815 ms\n2020-11-09 17:33:57.170  INFO 132 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService \'applicationTaskExecutor\'\n2020-11-09 17:33:57.390  INFO 132 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path \'\'\n2020-11-09 17:33:57.672  INFO 132 --- [           main] com.ieooc.demo.swagger.DemoApplication   : Started DemoApplication in 3.672 seconds (JVM running for 7.904)\n2020-11-09 17:34:17.480  INFO 132 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet \'dispatcherServlet\'\n2020-11-09 17:34:17.481  INFO 132 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet \'dispatcherServlet\'\n2020-11-09 17:34:17.486  INFO 132 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 5 ms\n\n\n\n# 查看效果\n\n浏览器访问：http://localhost:8080/swagger-ui/index.html\n\n\n\n至此，springboot整合swagger3.0 完成。\n\n\n# 常用注解说明\n\n@Api()：注解类，描述Controller的作用\n\ttags = ”描述Controller的作用“\n@ApiOperation()：注解方法，描述具体接口的作用\n\tvalue=“具体说明接口的作用”\n    notes="接口方法备注“\n@ApiImplicitParams()：注解接口方法，描述一组请求参数，可以包含多个@ApiImplicitParam()\n@ApiImplicitParam()：注解接口方法，描述一个具体的请求参数\n\tname：请求参数名\n\tvalue：请求参数描述\n\trequired：是否必传\n\tdataType：参数类型\n\tdefaultValue：默认值\n@ApiResponses()：注解接口方法，描述一组HTTP返回值，可以包含多个@ApiResponse()\n@ApiResponse()：注解接口方法，描述一个HTTP响应信息\n\tcode：HTTP返回值\n    message：返回信息\n    response：抛出异常的类\n@ApiModel()：注解Model，描述响应数据Model类\n@ApiModelProperty()：注解属性，描述响应Model类的属性\n@ApiIgnore()：注解类，表示忽略这个Api\n',normalizedContent:'# spring boot整合swagger\n\n\n# 版本说明\n\n软件                  版本\nspring boot         2.3.5.release\nspringfox-swagger   3.0.0\n\n\n# 创建工程\n\n使用 spring initializr 创建工程 pom.xml 文件内容如下 ：\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n    <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>2.3.5.release</version>\n        <relativepath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n    <groupid>com.ieooc.demo</groupid>\n    <artifactid>springfox-swagger-demo</artifactid>\n    <version>0.0.1-snapshot</version>\n    <name>springfox-swagger-demo</name>\n    <description>springfox-swagger 单模块整合demo</description>\n\n    <properties>\n        <java.version>1.8</java.version>\n        <swagger.fox.version>3.0.0</swagger.fox.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-web</artifactid>\n        </dependency>\n\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-test</artifactid>\n            <scope>test</scope>\n            <exclusions>\n                <exclusion>\n                    <groupid>org.junit.vintage</groupid>\n                    <artifactid>junit-vintage-engine</artifactid>\n                </exclusion>\n            </exclusions>\n        </dependency>\n\n        \x3c!--lombok敏捷开发工具包--\x3e\n        <dependency>\n            <groupid>org.projectlombok</groupid>\n            <artifactid>lombok</artifactid>\n            <scope>provided</scope>\n        </dependency>\n\n        \x3c!--swagger 依赖--\x3e\n        <dependency>\n            <groupid>io.springfox</groupid>\n            <artifactid>springfox-boot-starter</artifactid>\n            <version>${swagger.fox.version}</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-maven-plugin</artifactid>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n\n\n\n> 引入lombok为了简化代码，可以不引入，不影响swagger集成。\n\n主要依赖 springfox-boot-starter ，这是用官方提供的包含自动配置。\n\n\x3c!--swagger 依赖--\x3e\n<dependency>\n    <groupid>io.springfox</groupid>\n    <artifactid>springfox-boot-starter</artifactid>\n    <version>${swagger.fox.version}</version>\n</dependency>\n\n\n\n# 创建 entity\n\n新建包名 com.ieooc.demo.swagger.entity,创建sysuser实体类，内容如下：\n\npackage com.ieooc.demo.swagger.entity;\n\nimport io.swagger.annotations.apimodel;\nimport io.swagger.annotations.apimodelproperty;\nimport lombok.data;\nimport lombok.equalsandhashcode;\n\nimport java.io.serializable;\n\n/**\n * <p>\n * 用户表\n * </p>\n *\n * @author heyuqiang\n * @since 2020-08-30\n */\n@data\n@equalsandhashcode(callsuper = false)\n@apimodel(value = "sysuser", description = "用户表")\npublic class sysuser implements serializable {\n\n    private static final long serialversionuid = 1l;\n\n    @apimodelproperty(value = "主键id")\n    private integer userid;\n\n    @apimodelproperty(value = "用户名")\n    private string username;\n\n    @apimodelproperty(value = "密码")\n    private string password;\n\n}\n\n\n\n> @data注解由lombok提供，如果没用引入lombok依赖，需要手动生成getter,setter方法。\n\n\n# 创建 controller\n\n新建包名 com.ieooc.demo.swagger.controller,创建sysusercontroller控制器，内容如下：\n\npackage com.ieooc.demo.swagger.controller;\n\n\nimport com.ieooc.demo.swagger.entity.sysuser;\nimport io.swagger.annotations.api;\nimport io.swagger.annotations.apioperation;\nimport io.swagger.annotations.apiparam;\nimport lombok.allargsconstructor;\nimport org.springframework.web.bind.annotation.*;\n\n/**\n * <p>\n * 用户管理模块\n * </p>\n *\n * @author heyuqiang\n * @since 2020-08-30\n */\n@restcontroller\n@requestmapping("/users")\n@allargsconstructor\n@api(value = "user", tags = "用户管理")\npublic class sysusercontroller {\n\n    /**\n     * 新增用户\n     */\n    @postmapping()\n    @apioperation(value = "saveuser", notes = "新增用户")\n    public void saveuser(@apiparam(value = "用户信息") @requestbody sysuser sysuser) {\n\n    }\n\n    /**\n     * 根据用户id删除用户信息\n     */\n    @deletemapping("/{userid}")\n    @apioperation(value = "deletebyid", notes = "根据用户id删除用户信息")\n    public void deletebyid(@apiparam(value = "用户id") @pathvariable("userid") integer userid) {\n\n    }\n\n    /**\n     * 根根据用户名查询用户角色，权限信息\n     */\n    @putmapping("/{userid}")\n    @apioperation(value = "info", notes = "根据用户名查询用户角色，权限信息")\n    public void info(@apiparam(value = "用户id") @pathvariable("userid") integer userid,\n                     @apiparam(value = "用户信息") @requestbody sysuser sysuser) {\n\n    }\n\n    /**\n     * 根据用户id查询用户信息\n     *\n     * @param userid 用户id\n     * @return 用户信息\n     */\n    @getmapping("/{userid}")\n    @apioperation(value = "getuservobyid", notes = "根据用户id查询用户信息")\n    public void getuservobyid(@apiparam(value = "用户id") @pathvariable integer userid) {\n    }\n\n\n}\n\n\n> 包含了常用的增、删、改、查。\n\n\n# swagger 核心配置文件\n\n新建包名 com.ieooc.demo.swagger.config,创建swaggerautoconfiguration配置类，内容如下：\n\npackage com.ieooc.demo.swagger.config;\n\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\nimport springfox.documentation.builders.apiinfobuilder;\nimport springfox.documentation.builders.pathselectors;\nimport springfox.documentation.builders.requesthandlerselectors;\nimport springfox.documentation.service.apiinfo;\nimport springfox.documentation.service.contact;\nimport springfox.documentation.spi.documentationtype;\nimport springfox.documentation.spring.web.plugins.docket;\nimport springfox.documentation.swagger.web.*;\nimport springfox.documentation.swagger2.annotations.enableswagger2;\n\nimport java.util.arrays;\nimport java.util.linkedhashset;\n\n/**\n * @author heyuqiang\n * @date 2020/10/03\n * swagger 核心配置文件\n * 官方文档：http://springfox.github.io/springfox/docs/current/\n */\n@configuration\n@enableswagger2\npublic class swaggerautoconfiguration {\n\n    /**\n     * 组织docket对象，初始化swagger主要配置\n     *\n     * @return docket\n     */\n    @bean\n    public docket api() {\n        //自定义配置，指定规范，这里是swagger_2\n        return new docket(documentationtype.swagger_2)\n                //定义是否开启swagger，false为关闭，可以通过变量控制\n                .enable(true)\n                //设定api文档头信息，这个信息会展示在文档ui的头部位置\n                .apiinfo(apiinfo())\n                //指定需要发布到swagger的接口目录，不支持通配符\n                .select()\n                .apis(requesthandlerselectors.basepackage("com.ieooc.demo.swagger.controller"))\n                .paths(pathselectors.any())\n                .build()\n                //将servlet路径映射（如果有）添加到apis基本路径\n                .pathmapping("/")\n                //支持的通讯协议集合\n                .protocols(new linkedhashset<>(arrays.aslist("https", "http")));\n    }\n\n    /**\n     * 自定义api文档基本信息实体\n     *\n     * @return apiinfo\n     */\n    public apiinfo apiinfo() {\n        return new apiinfobuilder()\n                //构建联系实体，在ui界面会显示\n                .contact(new contact("simplify", "https://www.ieooc.com", "info@ieooc.com"))\n                //标题\n                .title("simplify title")\n                //描述\n                .description("simplify description")\n                //许可证url\n                .license("simplify license")\n                //许可证url\n                .licenseurl("simplify licenseurl")\n                //服务条款url\n                .termsofserviceurl("simplify termsofserviceurl")\n                //版本\n                .version("v1.0")\n                .build();\n    }\n\n    /**\n     * ui配置\n     */\n    @bean\n    uiconfiguration uiconfig() {\n        return uiconfigurationbuilder.builder()\n                .deeplinking(true)\n                .displayoperationid(false)\n                .defaultmodelsexpanddepth(1)\n                .defaultmodelexpanddepth(1)\n                .defaultmodelrendering(modelrendering.example)\n                .displayrequestduration(false)\n                .docexpansion(docexpansion.none)\n                .filter(false)\n                .maxdisplayedtags(null)\n                .operationssorter(operationssorter.alpha)\n                .showextensions(false)\n                .showcommonextensions(false)\n                .tagssorter(tagssorter.alpha)\n                .supportedsubmitmethods(uiconfiguration.constants.default_submit_methods)\n                .validatorurl(null)\n                .build();\n    }\n\n}\n\n\n\n> uiconfig 可选配置，默认可以创建。\n\n\n# 启动项目\n\n  .   ____          _            __ _ _\n /\\\\ / ___\'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | \'_ | \'_| | \'_ \\/ _` | \\ \\ \\ \\\n \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n  \'  |____| .__|_| |_|_| |_\\__, | / / / /\n =========|_|==============|___/=/_/_/_/\n :: spring boot ::        (v2.3.5.release)\n\n2020-11-09 17:33:54.619  info 132 --- [           main] com.ieooc.demo.swagger.demoapplication   : starting demoapplication on desktop-7v8q19u with pid 132 (d:\\ideaprojects\\springfox-swagger-demo\\target\\classes started by heyuq in d:\\ideaprojects\\springfox-swagger-demo)\n2020-11-09 17:33:54.622  info 132 --- [           main] com.ieooc.demo.swagger.demoapplication   : no active profile set, falling back to default profiles: default\n2020-11-09 17:33:56.474  info 132 --- [           main] o.s.b.w.embedded.tomcat.tomcatwebserver  : tomcat initialized with port(s): 8080 (http)\n2020-11-09 17:33:56.482  info 132 --- [           main] o.apache.catalina.core.standardservice   : starting service [tomcat]\n2020-11-09 17:33:56.483  info 132 --- [           main] org.apache.catalina.core.standardengine  : starting servlet engine: [apache tomcat/9.0.39]\n2020-11-09 17:33:56.657  info 132 --- [           main] o.a.c.c.c.[tomcat].[localhost].[/]       : initializing spring embedded webapplicationcontext\n2020-11-09 17:33:56.657  info 132 --- [           main] w.s.c.servletwebserverapplicationcontext : root webapplicationcontext: initialization completed in 1815 ms\n2020-11-09 17:33:57.170  info 132 --- [           main] o.s.s.concurrent.threadpooltaskexecutor  : initializing executorservice \'applicationtaskexecutor\'\n2020-11-09 17:33:57.390  info 132 --- [           main] o.s.b.w.embedded.tomcat.tomcatwebserver  : tomcat started on port(s): 8080 (http) with context path \'\'\n2020-11-09 17:33:57.672  info 132 --- [           main] com.ieooc.demo.swagger.demoapplication   : started demoapplication in 3.672 seconds (jvm running for 7.904)\n2020-11-09 17:34:17.480  info 132 --- [nio-8080-exec-1] o.a.c.c.c.[tomcat].[localhost].[/]       : initializing spring dispatcherservlet \'dispatcherservlet\'\n2020-11-09 17:34:17.481  info 132 --- [nio-8080-exec-1] o.s.web.servlet.dispatcherservlet        : initializing servlet \'dispatcherservlet\'\n2020-11-09 17:34:17.486  info 132 --- [nio-8080-exec-1] o.s.web.servlet.dispatcherservlet        : completed initialization in 5 ms\n\n\n\n# 查看效果\n\n浏览器访问：http://localhost:8080/swagger-ui/index.html\n\n\n\n至此，springboot整合swagger3.0 完成。\n\n\n# 常用注解说明\n\n@api()：注解类，描述controller的作用\n\ttags = ”描述controller的作用“\n@apioperation()：注解方法，描述具体接口的作用\n\tvalue=“具体说明接口的作用”\n    notes="接口方法备注“\n@apiimplicitparams()：注解接口方法，描述一组请求参数，可以包含多个@apiimplicitparam()\n@apiimplicitparam()：注解接口方法，描述一个具体的请求参数\n\tname：请求参数名\n\tvalue：请求参数描述\n\trequired：是否必传\n\tdatatype：参数类型\n\tdefaultvalue：默认值\n@apiresponses()：注解接口方法，描述一组http返回值，可以包含多个@apiresponse()\n@apiresponse()：注解接口方法，描述一个http响应信息\n\tcode：http返回值\n    message：返回信息\n    response：抛出异常的类\n@apimodel()：注解model，描述响应数据model类\n@apimodelproperty()：注解属性，描述响应model类的属性\n@apiignore()：注解类，表示忽略这个api\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"axios中文文档",frontmatter:{title:"axios中文文档",date:"2022-02-09T13:15:12.000Z",permalink:"/pages/92c82f/"},regularPath:"/21.Simplify/01.Simplify%20Docs/03.axios%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3.html",relativePath:"21.Simplify/01.Simplify Docs/03.axios中文文档.md",key:"v-31abbd86",path:"/pages/92c82f/",headers:[{level:2,title:"什么是 axios？",slug:"什么是-axios",normalizedTitle:"什么是 axios？",charIndex:69},{level:2,title:"特性",slug:"特性",normalizedTitle:"特性",charIndex:134},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:277},{level:2,title:"案例",slug:"案例",normalizedTitle:"案例",charIndex:427},{level:2,title:"axios API",slug:"axios-api",normalizedTitle:"axios api",charIndex:1310},{level:3,title:"axios(config)",slug:"axios-config",normalizedTitle:"axios(config)",charIndex:1349},{level:3,title:"axios(url[, config])",slug:"axios-url-config",normalizedTitle:"axios(url[, config])",charIndex:1687},{level:2,title:"请求方法的别名",slug:"请求方法的别名",normalizedTitle:"请求方法的别名",charIndex:1756},{level:2,title:"并发",slug:"并发",normalizedTitle:"并发",charIndex:1027},{level:2,title:"创建实例",slug:"创建实例",normalizedTitle:"创建实例",charIndex:2164},{level:2,title:"实例方法",slug:"实例方法",normalizedTitle:"实例方法",charIndex:2363},{level:2,title:"请求配置",slug:"请求配置",normalizedTitle:"请求配置",charIndex:2657},{level:2,title:"响应结构",slug:"响应结构",normalizedTitle:"响应结构",charIndex:6704},{level:2,title:"配置默认值",slug:"配置默认值",normalizedTitle:"配置默认值",charIndex:7505},{level:2,title:"全局的 axios 默认值",slug:"全局的-axios-默认值",normalizedTitle:"全局的 axios 默认值",charIndex:7536},{level:2,title:"自定义实例默认值",slug:"自定义实例默认值",normalizedTitle:"自定义实例默认值",charIndex:7752},{level:2,title:"配置的优先顺序",slug:"配置的优先顺序",normalizedTitle:"配置的优先顺序",charIndex:8005},{level:2,title:"拦截器",slug:"拦截器",normalizedTitle:"拦截器",charIndex:8344},{level:2,title:"错误处理",slug:"错误处理",normalizedTitle:"错误处理",charIndex:7490},{level:2,title:"取消",slug:"取消",normalizedTitle:"取消",charIndex:236},{level:2,title:"使用 application/x-www-form-urlencoded format",slug:"使用-application-x-www-form-urlencoded-format",normalizedTitle:"使用 application/x-www-form-urlencoded format",charIndex:10904},{level:3,title:"浏览器",slug:"浏览器",normalizedTitle:"浏览器",charIndex:114},{level:3,title:"Node.js",slug:"node-js",normalizedTitle:"node.js",charIndex:11623},{level:2,title:"Semver",slug:"semver",normalizedTitle:"semver",charIndex:11805},{level:2,title:"Promises",slug:"promises",normalizedTitle:"promises",charIndex:11888},{level:2,title:"TypeScript",slug:"typescript",normalizedTitle:"typescript",charIndex:11972},{level:2,title:"协议",slug:"协议",normalizedTitle:"协议",charIndex:12067}],headersStr:"什么是 axios？ 特性 安装 案例 axios API axios(config) axios(url[, config]) 请求方法的别名 并发 创建实例 实例方法 请求配置 响应结构 配置默认值 全局的 axios 默认值 自定义实例默认值 配置的优先顺序 拦截器 错误处理 取消 使用 application/x-www-form-urlencoded format 浏览器 Node.js Semver Promises TypeScript 协议",content:"# axios中文文档\n\n欢迎使用 axios，本文档将帮助您快速上手。(troubleshooting.html) 中的解答，\n\n\n# 什么是 axios？\n\nAxios 是一个基于 promise 的 HTTP 库，可以用在浏览器和 node.js 中。\n\n\n# 特性\n\n * 从浏览器中创建 XMLHttpRequests\n * 从 node.js 创建 http 请求\n * 支持 Promise API\n * 拦截请求和响应\n * 转换请求数据和响应数据\n * 取消请求\n * 自动转换 JSON 数据\n * 客户端支持防御 XSRF\n\n\n# 安装\n\n使用 npm:\n\n$ npm install axios\n\n\n使用 bower:\n\n$ bower install axios\n\n\n使用 cdn:\n\n<script src=\"https://unpkg.com/axios/dist/axios.min.js\"><\/script>\n\n\n\n# 案例\n\n执行 GET 请求\n\n// 为给定 ID 的 user 创建请求\naxios.get('/user?ID=12345')\n  .then(function (response) {\n    console.log(response);\n  })\n  .catch(function (error) {\n    console.log(error);\n  });\n\n// 上面的请求也可以这样做\naxios.get('/user', {\n    params: {\n      ID: 12345\n    }\n  })\n  .then(function (response) {\n    console.log(response);\n  })\n  .catch(function (error) {\n    console.log(error);\n  });\n\n\n执行 POST 请求\n\naxios.post('/user', {\n    firstName: 'Fred',\n    lastName: 'Flintstone'\n  })\n  .then(function (response) {\n    console.log(response);\n  })\n  .catch(function (error) {\n    console.log(error);\n  });\n\n\n执行多个并发请求\n\nfunction getUserAccount() {\n  return axios.get('/user/12345');\n}\n\nfunction getUserPermissions() {\n  return axios.get('/user/12345/permissions');\n}\n\naxios.all([getUserAccount(), getUserPermissions()])\n  .then(axios.spread(function (acct, perms) {\n    // 两个请求现在都执行完成\n  }));\n\n\n\n# axios API\n\n可以通过向 axios 传递相关配置来创建请求\n\n\n# axios(config)\n\n// 发送 POST 请求\naxios({\n  method: 'post',\n  url: '/user/12345',\n  data: {\n    firstName: 'Fred',\n    lastName: 'Flintstone'\n  }\n});\n// 获取远端图片\naxios({\n  method:'get',\n  url:'http://bit.ly/2mTM3nY',\n  responseType:'stream'\n})\n  .then(function(response) {\n  response.data.pipe(fs.createWriteStream('ada_lovelace.jpg'))\n});\n\n\n\n# axios(url[, config])\n\n// 发送 GET 请求（默认的方法）\naxios('/user/12345');\n\n\n\n# 请求方法的别名\n\n为方便起见，为所有支持的请求方法提供了别名\n\n * axios.request(config)\n * axios.get(url[, config])\n * axios.delete(url[, config])\n * axios.head(url[, config])\n * axios.options(url[, config])\n * axios.post(url[, data[, config]])\n * axios.put(url[, data[, config]])\n * axios.patch(url[, data[, config]])\n\n提示\n\n在使用别名方法时， url、method、data 这些属性都不必在配置中指定。\n\n\n# 并发\n\n处理并发请求的助手函数\n\n * axios.all(iterable)\n * axios.spread(callback)\n\n\n# 创建实例\n\n可以使用自定义配置新建一个 axios 实例\n\n * axios.create([config])\n\nconst instance = axios.create({\n  baseURL: 'https://some-domain.com/api/',\n  timeout: 1000,\n  headers: {'X-Custom-Header': 'foobar'}\n});\n\n\n\n# 实例方法\n\n以下是可用的实例方法。指定的配置将与实例的配置合并。\n\n * axios#request(config)\n * axios#get(url[, config])\n * axios#delete(url[, config])\n * axios#head(url[, config])\n * axios#options(url[, config])\n * axios#post(url[, data[, config]])\n * axios#put(url[, data[, config]])\n * axios#patch(url[, data[, config]])\n\n\n# 请求配置\n\n这些是创建请求时可以用的配置选项。只有 url 是必需的。如果没有指定 method，请求将默认使用 get 方法。\n\n{\n   // `url` 是用于请求的服务器 URL\n  url: '/user',\n\n  // `method` 是创建请求时使用的方法\n  method: 'get', // default\n\n  // `baseURL` 将自动加在 `url` 前面，除非 `url` 是一个绝对 URL。\n  // 它可以通过设置一个 `baseURL` 便于为 axios 实例的方法传递相对 URL\n  baseURL: 'https://some-domain.com/api/',\n\n  // `transformRequest` 允许在向服务器发送前，修改请求数据\n  // 只能用在 'PUT', 'POST' 和 'PATCH' 这几个请求方法\n  // 后面数组中的函数必须返回一个字符串，或 ArrayBuffer，或 Stream\n  transformRequest: [function (data, headers) {\n    // 对 data 进行任意转换处理\n    return data;\n  }],\n\n  // `transformResponse` 在传递给 then/catch 前，允许修改响应数据\n  transformResponse: [function (data) {\n    // 对 data 进行任意转换处理\n    return data;\n  }],\n\n  // `headers` 是即将被发送的自定义请求头\n  headers: {'X-Requested-With': 'XMLHttpRequest'},\n\n  // `params` 是即将与请求一起发送的 URL 参数\n  // 必须是一个无格式对象(plain object)或 URLSearchParams 对象\n  params: {\n    ID: 12345\n  },\n\n   // `paramsSerializer` 是一个负责 `params` 序列化的函数\n  // (e.g. https://www.npmjs.com/package/qs, http://api.jquery.com/jquery.param/)\n  paramsSerializer: function(params) {\n    return Qs.stringify(params, {arrayFormat: 'brackets'})\n  },\n\n  // `data` 是作为请求主体被发送的数据\n  // 只适用于这些请求方法 'PUT', 'POST', 和 'PATCH'\n  // 在没有设置 `transformRequest` 时，必须是以下类型之一：\n  // - string, plain object, ArrayBuffer, ArrayBufferView, URLSearchParams\n  // - 浏览器专属：FormData, File, Blob\n  // - Node 专属： Stream\n  data: {\n    firstName: 'Fred'\n  },\n\n  // `timeout` 指定请求超时的毫秒数(0 表示无超时时间)\n  // 如果请求话费了超过 `timeout` 的时间，请求将被中断\n  timeout: 1000,\n\n   // `withCredentials` 表示跨域请求时是否需要使用凭证\n  withCredentials: false, // default\n\n  // `adapter` 允许自定义处理请求，以使测试更轻松\n  // 返回一个 promise 并应用一个有效的响应 (查阅 [response docs](#response-api)).\n  adapter: function (config) {\n    /* ... */\n  },\n\n // `auth` 表示应该使用 HTTP 基础验证，并提供凭据\n  // 这将设置一个 `Authorization` 头，覆写掉现有的任意使用 `headers` 设置的自定义 `Authorization`头\n  auth: {\n    username: 'janedoe',\n    password: 's00pers3cret'\n  },\n\n   // `responseType` 表示服务器响应的数据类型，可以是 'arraybuffer', 'blob', 'document', 'json', 'text', 'stream'\n  responseType: 'json', // default\n\n  // `responseEncoding` indicates encoding to use for decoding responses\n  // Note: Ignored for `responseType` of 'stream' or client-side requests\n  responseEncoding: 'utf8', // default\n\n   // `xsrfCookieName` 是用作 xsrf token 的值的cookie的名称\n  xsrfCookieName: 'XSRF-TOKEN', // default\n\n  // `xsrfHeaderName` is the name of the http header that carries the xsrf token value\n  xsrfHeaderName: 'X-XSRF-TOKEN', // default\n\n   // `onUploadProgress` 允许为上传处理进度事件\n  onUploadProgress: function (progressEvent) {\n    // Do whatever you want with the native progress event\n  },\n\n  // `onDownloadProgress` 允许为下载处理进度事件\n  onDownloadProgress: function (progressEvent) {\n    // 对原生进度事件的处理\n  },\n\n   // `maxContentLength` 定义允许的响应内容的最大尺寸\n  maxContentLength: 2000,\n\n  // `validateStatus` 定义对于给定的HTTP 响应状态码是 resolve 或 reject  promise 。如果 `validateStatus` 返回 `true` (或者设置为 `null` 或 `undefined`)，promise 将被 resolve; 否则，promise 将被 rejecte\n  validateStatus: function (status) {\n    return status >= 200 && status < 300; // default\n  },\n\n  // `maxRedirects` 定义在 node.js 中 follow 的最大重定向数目\n  // 如果设置为0，将不会 follow 任何重定向\n  maxRedirects: 5, // default\n\n  // `socketPath` defines a UNIX Socket to be used in node.js.\n  // e.g. '/var/run/docker.sock' to send requests to the docker daemon.\n  // Only either `socketPath` or `proxy` can be specified.\n  // If both are specified, `socketPath` is used.\n  socketPath: null, // default\n\n  // `httpAgent` 和 `httpsAgent` 分别在 node.js 中用于定义在执行 http 和 https 时使用的自定义代理。允许像这样配置选项：\n  // `keepAlive` 默认没有启用\n  httpAgent: new http.Agent({ keepAlive: true }),\n  httpsAgent: new https.Agent({ keepAlive: true }),\n\n  // 'proxy' 定义代理服务器的主机名称和端口\n  // `auth` 表示 HTTP 基础验证应当用于连接代理，并提供凭据\n  // 这将会设置一个 `Proxy-Authorization` 头，覆写掉已有的通过使用 `header` 设置的自定义 `Proxy-Authorization` 头。\n  proxy: {\n    host: '127.0.0.1',\n    port: 9000,\n    auth: {\n      username: 'mikeymike',\n      password: 'rapunz3l'\n    }\n  },\n\n  // `cancelToken` 指定用于取消请求的 cancel token\n  // （查看后面的 Cancellation 这节了解更多）\n  cancelToken: new CancelToken(function (cancel) {\n  })\n}\n\n\n\n# 响应结构\n\n某个请求的响应包含以下信息\n\n{\n  // `data` 由服务器提供的响应\n  data: {},\n\n  // `status` 来自服务器响应的 HTTP 状态码\n  status: 200,\n\n  // `statusText` 来自服务器响应的 HTTP 状态信息\n  statusText: 'OK',\n\n  // `headers` 服务器响应的头\n  headers: {},\n\n   // `config` 是为请求提供的配置信息\n  config: {},\n // 'request'\n  // `request` is the request that generated this response\n  // It is the last ClientRequest instance in node.js (in redirects)\n  // and an XMLHttpRequest instance the browser\n  request: {}\n}\n\n\n使用 then 时，你将接收下面这样的响应 :\n\naxios.get('/user/12345')\n  .then(function(response) {\n    console.log(response.data);\n    console.log(response.status);\n    console.log(response.statusText);\n    console.log(response.headers);\n    console.log(response.config);\n  });\n\n\n在使用 catch 时，或传递 rejection callback 作为 then 的第二个参数时，响应可以通过 error 对象可被使用，正如在错误处理这一节所讲。\n\n\n# 配置默认值\n\n你可以指定将被用在各个请求的配置默认值\n\n\n# 全局的 axios 默认值\n\naxios.defaults.baseURL = 'https://api.example.com';\naxios.defaults.headers.common['Authorization'] = AUTH_TOKEN;\naxios.defaults.headers.post['Content-Type'] = 'application/x-www-form-urlencoded';\n\n\n\n# 自定义实例默认值\n\n// Set config defaults when creating the instance\nconst instance = axios.create({\n  baseURL: 'https://api.example.com'\n});\n\n// Alter defaults after instance has been created\ninstance.defaults.headers.common['Authorization'] = AUTH_TOKEN;\n\n\n\n# 配置的优先顺序\n\n配置会以一个优先顺序进行合并。这个顺序是：在 lib/defaults.js 找到的库的默认值，然后是实例的 defaults 属性，最后是请求的 config 参数。后者将优先于前者。这里是一个例子：\n\n// 使用由库提供的配置的默认值来创建实例\n// 此时超时配置的默认值是 `0`\nvar instance = axios.create();\n\n// 覆写库的超时默认值\n// 现在，在超时前，所有请求都会等待 2.5 秒\ninstance.defaults.timeout = 2500;\n\n// 为已知需要花费很长时间的请求覆写超时设置\ninstance.get('/longRequest', {\n  timeout: 5000\n});\n\n\n\n# 拦截器\n\n在请求或响应被 then 或 catch 处理前拦截它们。\n\n// 添加请求拦截器\naxios.interceptors.request.use(function (config) {\n    // 在发送请求之前做些什么\n    return config;\n  }, function (error) {\n    // 对请求错误做些什么\n    return Promise.reject(error);\n  });\n\n// 添加响应拦截器\naxios.interceptors.response.use(function (response) {\n    // 对响应数据做点什么\n    return response;\n  }, function (error) {\n    // 对响应错误做点什么\n    return Promise.reject(error);\n  });\n\n\n如果你想在稍后移除拦截器，可以这样：\n\nconst myInterceptor = axios.interceptors.request.use(function () {/*...*/});\naxios.interceptors.request.eject(myInterceptor);\n\n\n可以为自定义 axios 实例添加拦截器\n\nconst instance = axios.create();\ninstance.interceptors.request.use(function () {/*...*/});\n\n\n\n# 错误处理\n\naxios.get('/user/12345')\n  .catch(function (error) {\n    if (error.response) {\n      // The request was made and the server responded with a status code\n      // that falls out of the range of 2xx\n      console.log(error.response.data);\n      console.log(error.response.status);\n      console.log(error.response.headers);\n    } else if (error.request) {\n      // The request was made but no response was received\n      // `error.request` is an instance of XMLHttpRequest in the browser and an instance of\n      // http.ClientRequest in node.js\n      console.log(error.request);\n    } else {\n      // Something happened in setting up the request that triggered an Error\n      console.log('Error', error.message);\n    }\n    console.log(error.config);\n  });\n\n\nY可以使用 validateStatus 配置选项定义一个自定义 HTTP 状态码的错误范围。\n\naxios.get('/user/12345', {\n  validateStatus: function (status) {\n    return status < 500; // Reject only if the status code is greater than or equal to 500\n  }\n})\n\n\n\n# 取消\n\n使用 cancel token 取消请求\n\n> Axios 的 cancel token API 基于cancelable promises proposal，它还处于第一阶段。\n\n可以使用 CancelToken.source 工厂方法创建 cancel token，像这样：\n\nconst CancelToken = axios.CancelToken;\nconst source = CancelToken.source();\n\naxios.get('/user/12345', {\n  cancelToken: source.token\n}).catch(function(thrown) {\n  if (axios.isCancel(thrown)) {\n    console.log('Request canceled', thrown.message);\n  } else {\n     // 处理错误\n  }\n});\n\naxios.post('/user/12345', {\n  name: 'new name'\n}, {\n  cancelToken: source.token\n})\n\n// 取消请求（message 参数是可选的）\nsource.cancel('Operation canceled by the user.');\n\n\n还可以通过传递一个 executor 函数到 CancelToken 的构造函数来创建 cancel token：\n\nconst CancelToken = axios.CancelToken;\nlet cancel;\n\naxios.get('/user/12345', {\n  cancelToken: new CancelToken(function executor(c) {\n    // executor 函数接收一个 cancel 函数作为参数\n    cancel = c;\n  })\n});\n\n// cancel the request\ncancel();\n\n\n> 注意: 可以使用同一个 cancel token 取消多个请求\n\n\n# 使用 application/x-www-form-urlencoded format\n\n默认情况下，axios将JavaScript对象序列化为JSON。 要以application / x-www-form-urlencoded格式发送数据，您可以使用以下选项之一。\n\n\n# 浏览器\n\n在浏览器中，您可以使用URLSearchParams API，如下所示：\n\nconst params = new URLSearchParams();\nparams.append('param1', 'value1');\nparams.append('param2', 'value2');\naxios.post('/foo', params);\n\n\n> 请注意，所有浏览器都不支持URLSearchParams（请参阅caniuse.com），但可以使用polyfill（确保填充全局环境）。\n\n或者，您可以使用qs库编码数据：\n\nconst qs = require('qs');\naxios.post('/foo', qs.stringify({ 'bar': 123 }));\n\n\n或者以另一种方式（ES6），\n\nimport qs from 'qs';\nconst data = { 'bar': 123 };\nconst options = {\n  method: 'POST',\n  headers: { 'content-type': 'application/x-www-form-urlencoded' },\n  data: qs.stringify(data),\n  url,\n};\naxios(options);\n\n\n\n# Node.js\n\n在node.js中，您可以使用querystring模块，如下所示：\n\nconst querystring = require('querystring');\naxios.post('http://something.com/', querystring.stringify({ foo: 'bar' }));\n\n\n您也可以使用qs库。\n\n\n# Semver\n\n在axios达到1.0版本之前，破坏性更改将以新的次要版本发布。 例如0.5.1和0.5.4将具有相同的API，但0.6.0将具有重大变化。\n\n\n# Promises\n\naxios 依赖原生的 ES6 Promise 实现而被支持. 如果你的环境不支持 ES6 Promise，你可以使用 polyfill.\n\n\n# TypeScript\n\naxios包括TypeScript定义。\n\nimport axios from 'axios';\naxios.get('/user?ID=12345');\n\n\n\n# 协议\n\nMIT",normalizedContent:"# axios中文文档\n\n欢迎使用 axios，本文档将帮助您快速上手。(troubleshooting.html) 中的解答，\n\n\n# 什么是 axios？\n\naxios 是一个基于 promise 的 http 库，可以用在浏览器和 node.js 中。\n\n\n# 特性\n\n * 从浏览器中创建 xmlhttprequests\n * 从 node.js 创建 http 请求\n * 支持 promise api\n * 拦截请求和响应\n * 转换请求数据和响应数据\n * 取消请求\n * 自动转换 json 数据\n * 客户端支持防御 xsrf\n\n\n# 安装\n\n使用 npm:\n\n$ npm install axios\n\n\n使用 bower:\n\n$ bower install axios\n\n\n使用 cdn:\n\n<script src=\"https://unpkg.com/axios/dist/axios.min.js\"><\/script>\n\n\n\n# 案例\n\n执行 get 请求\n\n// 为给定 id 的 user 创建请求\naxios.get('/user?id=12345')\n  .then(function (response) {\n    console.log(response);\n  })\n  .catch(function (error) {\n    console.log(error);\n  });\n\n// 上面的请求也可以这样做\naxios.get('/user', {\n    params: {\n      id: 12345\n    }\n  })\n  .then(function (response) {\n    console.log(response);\n  })\n  .catch(function (error) {\n    console.log(error);\n  });\n\n\n执行 post 请求\n\naxios.post('/user', {\n    firstname: 'fred',\n    lastname: 'flintstone'\n  })\n  .then(function (response) {\n    console.log(response);\n  })\n  .catch(function (error) {\n    console.log(error);\n  });\n\n\n执行多个并发请求\n\nfunction getuseraccount() {\n  return axios.get('/user/12345');\n}\n\nfunction getuserpermissions() {\n  return axios.get('/user/12345/permissions');\n}\n\naxios.all([getuseraccount(), getuserpermissions()])\n  .then(axios.spread(function (acct, perms) {\n    // 两个请求现在都执行完成\n  }));\n\n\n\n# axios api\n\n可以通过向 axios 传递相关配置来创建请求\n\n\n# axios(config)\n\n// 发送 post 请求\naxios({\n  method: 'post',\n  url: '/user/12345',\n  data: {\n    firstname: 'fred',\n    lastname: 'flintstone'\n  }\n});\n// 获取远端图片\naxios({\n  method:'get',\n  url:'http://bit.ly/2mtm3ny',\n  responsetype:'stream'\n})\n  .then(function(response) {\n  response.data.pipe(fs.createwritestream('ada_lovelace.jpg'))\n});\n\n\n\n# axios(url[, config])\n\n// 发送 get 请求（默认的方法）\naxios('/user/12345');\n\n\n\n# 请求方法的别名\n\n为方便起见，为所有支持的请求方法提供了别名\n\n * axios.request(config)\n * axios.get(url[, config])\n * axios.delete(url[, config])\n * axios.head(url[, config])\n * axios.options(url[, config])\n * axios.post(url[, data[, config]])\n * axios.put(url[, data[, config]])\n * axios.patch(url[, data[, config]])\n\n提示\n\n在使用别名方法时， url、method、data 这些属性都不必在配置中指定。\n\n\n# 并发\n\n处理并发请求的助手函数\n\n * axios.all(iterable)\n * axios.spread(callback)\n\n\n# 创建实例\n\n可以使用自定义配置新建一个 axios 实例\n\n * axios.create([config])\n\nconst instance = axios.create({\n  baseurl: 'https://some-domain.com/api/',\n  timeout: 1000,\n  headers: {'x-custom-header': 'foobar'}\n});\n\n\n\n# 实例方法\n\n以下是可用的实例方法。指定的配置将与实例的配置合并。\n\n * axios#request(config)\n * axios#get(url[, config])\n * axios#delete(url[, config])\n * axios#head(url[, config])\n * axios#options(url[, config])\n * axios#post(url[, data[, config]])\n * axios#put(url[, data[, config]])\n * axios#patch(url[, data[, config]])\n\n\n# 请求配置\n\n这些是创建请求时可以用的配置选项。只有 url 是必需的。如果没有指定 method，请求将默认使用 get 方法。\n\n{\n   // `url` 是用于请求的服务器 url\n  url: '/user',\n\n  // `method` 是创建请求时使用的方法\n  method: 'get', // default\n\n  // `baseurl` 将自动加在 `url` 前面，除非 `url` 是一个绝对 url。\n  // 它可以通过设置一个 `baseurl` 便于为 axios 实例的方法传递相对 url\n  baseurl: 'https://some-domain.com/api/',\n\n  // `transformrequest` 允许在向服务器发送前，修改请求数据\n  // 只能用在 'put', 'post' 和 'patch' 这几个请求方法\n  // 后面数组中的函数必须返回一个字符串，或 arraybuffer，或 stream\n  transformrequest: [function (data, headers) {\n    // 对 data 进行任意转换处理\n    return data;\n  }],\n\n  // `transformresponse` 在传递给 then/catch 前，允许修改响应数据\n  transformresponse: [function (data) {\n    // 对 data 进行任意转换处理\n    return data;\n  }],\n\n  // `headers` 是即将被发送的自定义请求头\n  headers: {'x-requested-with': 'xmlhttprequest'},\n\n  // `params` 是即将与请求一起发送的 url 参数\n  // 必须是一个无格式对象(plain object)或 urlsearchparams 对象\n  params: {\n    id: 12345\n  },\n\n   // `paramsserializer` 是一个负责 `params` 序列化的函数\n  // (e.g. https://www.npmjs.com/package/qs, http://api.jquery.com/jquery.param/)\n  paramsserializer: function(params) {\n    return qs.stringify(params, {arrayformat: 'brackets'})\n  },\n\n  // `data` 是作为请求主体被发送的数据\n  // 只适用于这些请求方法 'put', 'post', 和 'patch'\n  // 在没有设置 `transformrequest` 时，必须是以下类型之一：\n  // - string, plain object, arraybuffer, arraybufferview, urlsearchparams\n  // - 浏览器专属：formdata, file, blob\n  // - node 专属： stream\n  data: {\n    firstname: 'fred'\n  },\n\n  // `timeout` 指定请求超时的毫秒数(0 表示无超时时间)\n  // 如果请求话费了超过 `timeout` 的时间，请求将被中断\n  timeout: 1000,\n\n   // `withcredentials` 表示跨域请求时是否需要使用凭证\n  withcredentials: false, // default\n\n  // `adapter` 允许自定义处理请求，以使测试更轻松\n  // 返回一个 promise 并应用一个有效的响应 (查阅 [response docs](#response-api)).\n  adapter: function (config) {\n    /* ... */\n  },\n\n // `auth` 表示应该使用 http 基础验证，并提供凭据\n  // 这将设置一个 `authorization` 头，覆写掉现有的任意使用 `headers` 设置的自定义 `authorization`头\n  auth: {\n    username: 'janedoe',\n    password: 's00pers3cret'\n  },\n\n   // `responsetype` 表示服务器响应的数据类型，可以是 'arraybuffer', 'blob', 'document', 'json', 'text', 'stream'\n  responsetype: 'json', // default\n\n  // `responseencoding` indicates encoding to use for decoding responses\n  // note: ignored for `responsetype` of 'stream' or client-side requests\n  responseencoding: 'utf8', // default\n\n   // `xsrfcookiename` 是用作 xsrf token 的值的cookie的名称\n  xsrfcookiename: 'xsrf-token', // default\n\n  // `xsrfheadername` is the name of the http header that carries the xsrf token value\n  xsrfheadername: 'x-xsrf-token', // default\n\n   // `onuploadprogress` 允许为上传处理进度事件\n  onuploadprogress: function (progressevent) {\n    // do whatever you want with the native progress event\n  },\n\n  // `ondownloadprogress` 允许为下载处理进度事件\n  ondownloadprogress: function (progressevent) {\n    // 对原生进度事件的处理\n  },\n\n   // `maxcontentlength` 定义允许的响应内容的最大尺寸\n  maxcontentlength: 2000,\n\n  // `validatestatus` 定义对于给定的http 响应状态码是 resolve 或 reject  promise 。如果 `validatestatus` 返回 `true` (或者设置为 `null` 或 `undefined`)，promise 将被 resolve; 否则，promise 将被 rejecte\n  validatestatus: function (status) {\n    return status >= 200 && status < 300; // default\n  },\n\n  // `maxredirects` 定义在 node.js 中 follow 的最大重定向数目\n  // 如果设置为0，将不会 follow 任何重定向\n  maxredirects: 5, // default\n\n  // `socketpath` defines a unix socket to be used in node.js.\n  // e.g. '/var/run/docker.sock' to send requests to the docker daemon.\n  // only either `socketpath` or `proxy` can be specified.\n  // if both are specified, `socketpath` is used.\n  socketpath: null, // default\n\n  // `httpagent` 和 `httpsagent` 分别在 node.js 中用于定义在执行 http 和 https 时使用的自定义代理。允许像这样配置选项：\n  // `keepalive` 默认没有启用\n  httpagent: new http.agent({ keepalive: true }),\n  httpsagent: new https.agent({ keepalive: true }),\n\n  // 'proxy' 定义代理服务器的主机名称和端口\n  // `auth` 表示 http 基础验证应当用于连接代理，并提供凭据\n  // 这将会设置一个 `proxy-authorization` 头，覆写掉已有的通过使用 `header` 设置的自定义 `proxy-authorization` 头。\n  proxy: {\n    host: '127.0.0.1',\n    port: 9000,\n    auth: {\n      username: 'mikeymike',\n      password: 'rapunz3l'\n    }\n  },\n\n  // `canceltoken` 指定用于取消请求的 cancel token\n  // （查看后面的 cancellation 这节了解更多）\n  canceltoken: new canceltoken(function (cancel) {\n  })\n}\n\n\n\n# 响应结构\n\n某个请求的响应包含以下信息\n\n{\n  // `data` 由服务器提供的响应\n  data: {},\n\n  // `status` 来自服务器响应的 http 状态码\n  status: 200,\n\n  // `statustext` 来自服务器响应的 http 状态信息\n  statustext: 'ok',\n\n  // `headers` 服务器响应的头\n  headers: {},\n\n   // `config` 是为请求提供的配置信息\n  config: {},\n // 'request'\n  // `request` is the request that generated this response\n  // it is the last clientrequest instance in node.js (in redirects)\n  // and an xmlhttprequest instance the browser\n  request: {}\n}\n\n\n使用 then 时，你将接收下面这样的响应 :\n\naxios.get('/user/12345')\n  .then(function(response) {\n    console.log(response.data);\n    console.log(response.status);\n    console.log(response.statustext);\n    console.log(response.headers);\n    console.log(response.config);\n  });\n\n\n在使用 catch 时，或传递 rejection callback 作为 then 的第二个参数时，响应可以通过 error 对象可被使用，正如在错误处理这一节所讲。\n\n\n# 配置默认值\n\n你可以指定将被用在各个请求的配置默认值\n\n\n# 全局的 axios 默认值\n\naxios.defaults.baseurl = 'https://api.example.com';\naxios.defaults.headers.common['authorization'] = auth_token;\naxios.defaults.headers.post['content-type'] = 'application/x-www-form-urlencoded';\n\n\n\n# 自定义实例默认值\n\n// set config defaults when creating the instance\nconst instance = axios.create({\n  baseurl: 'https://api.example.com'\n});\n\n// alter defaults after instance has been created\ninstance.defaults.headers.common['authorization'] = auth_token;\n\n\n\n# 配置的优先顺序\n\n配置会以一个优先顺序进行合并。这个顺序是：在 lib/defaults.js 找到的库的默认值，然后是实例的 defaults 属性，最后是请求的 config 参数。后者将优先于前者。这里是一个例子：\n\n// 使用由库提供的配置的默认值来创建实例\n// 此时超时配置的默认值是 `0`\nvar instance = axios.create();\n\n// 覆写库的超时默认值\n// 现在，在超时前，所有请求都会等待 2.5 秒\ninstance.defaults.timeout = 2500;\n\n// 为已知需要花费很长时间的请求覆写超时设置\ninstance.get('/longrequest', {\n  timeout: 5000\n});\n\n\n\n# 拦截器\n\n在请求或响应被 then 或 catch 处理前拦截它们。\n\n// 添加请求拦截器\naxios.interceptors.request.use(function (config) {\n    // 在发送请求之前做些什么\n    return config;\n  }, function (error) {\n    // 对请求错误做些什么\n    return promise.reject(error);\n  });\n\n// 添加响应拦截器\naxios.interceptors.response.use(function (response) {\n    // 对响应数据做点什么\n    return response;\n  }, function (error) {\n    // 对响应错误做点什么\n    return promise.reject(error);\n  });\n\n\n如果你想在稍后移除拦截器，可以这样：\n\nconst myinterceptor = axios.interceptors.request.use(function () {/*...*/});\naxios.interceptors.request.eject(myinterceptor);\n\n\n可以为自定义 axios 实例添加拦截器\n\nconst instance = axios.create();\ninstance.interceptors.request.use(function () {/*...*/});\n\n\n\n# 错误处理\n\naxios.get('/user/12345')\n  .catch(function (error) {\n    if (error.response) {\n      // the request was made and the server responded with a status code\n      // that falls out of the range of 2xx\n      console.log(error.response.data);\n      console.log(error.response.status);\n      console.log(error.response.headers);\n    } else if (error.request) {\n      // the request was made but no response was received\n      // `error.request` is an instance of xmlhttprequest in the browser and an instance of\n      // http.clientrequest in node.js\n      console.log(error.request);\n    } else {\n      // something happened in setting up the request that triggered an error\n      console.log('error', error.message);\n    }\n    console.log(error.config);\n  });\n\n\ny可以使用 validatestatus 配置选项定义一个自定义 http 状态码的错误范围。\n\naxios.get('/user/12345', {\n  validatestatus: function (status) {\n    return status < 500; // reject only if the status code is greater than or equal to 500\n  }\n})\n\n\n\n# 取消\n\n使用 cancel token 取消请求\n\n> axios 的 cancel token api 基于cancelable promises proposal，它还处于第一阶段。\n\n可以使用 canceltoken.source 工厂方法创建 cancel token，像这样：\n\nconst canceltoken = axios.canceltoken;\nconst source = canceltoken.source();\n\naxios.get('/user/12345', {\n  canceltoken: source.token\n}).catch(function(thrown) {\n  if (axios.iscancel(thrown)) {\n    console.log('request canceled', thrown.message);\n  } else {\n     // 处理错误\n  }\n});\n\naxios.post('/user/12345', {\n  name: 'new name'\n}, {\n  canceltoken: source.token\n})\n\n// 取消请求（message 参数是可选的）\nsource.cancel('operation canceled by the user.');\n\n\n还可以通过传递一个 executor 函数到 canceltoken 的构造函数来创建 cancel token：\n\nconst canceltoken = axios.canceltoken;\nlet cancel;\n\naxios.get('/user/12345', {\n  canceltoken: new canceltoken(function executor(c) {\n    // executor 函数接收一个 cancel 函数作为参数\n    cancel = c;\n  })\n});\n\n// cancel the request\ncancel();\n\n\n> 注意: 可以使用同一个 cancel token 取消多个请求\n\n\n# 使用 application/x-www-form-urlencoded format\n\n默认情况下，axios将javascript对象序列化为json。 要以application / x-www-form-urlencoded格式发送数据，您可以使用以下选项之一。\n\n\n# 浏览器\n\n在浏览器中，您可以使用urlsearchparams api，如下所示：\n\nconst params = new urlsearchparams();\nparams.append('param1', 'value1');\nparams.append('param2', 'value2');\naxios.post('/foo', params);\n\n\n> 请注意，所有浏览器都不支持urlsearchparams（请参阅caniuse.com），但可以使用polyfill（确保填充全局环境）。\n\n或者，您可以使用qs库编码数据：\n\nconst qs = require('qs');\naxios.post('/foo', qs.stringify({ 'bar': 123 }));\n\n\n或者以另一种方式（es6），\n\nimport qs from 'qs';\nconst data = { 'bar': 123 };\nconst options = {\n  method: 'post',\n  headers: { 'content-type': 'application/x-www-form-urlencoded' },\n  data: qs.stringify(data),\n  url,\n};\naxios(options);\n\n\n\n# node.js\n\n在node.js中，您可以使用querystring模块，如下所示：\n\nconst querystring = require('querystring');\naxios.post('http://something.com/', querystring.stringify({ foo: 'bar' }));\n\n\n您也可以使用qs库。\n\n\n# semver\n\n在axios达到1.0版本之前，破坏性更改将以新的次要版本发布。 例如0.5.1和0.5.4将具有相同的api，但0.6.0将具有重大变化。\n\n\n# promises\n\naxios 依赖原生的 es6 promise 实现而被支持. 如果你的环境不支持 es6 promise，你可以使用 polyfill.\n\n\n# typescript\n\naxios包括typescript定义。\n\nimport axios from 'axios';\naxios.get('/user?id=12345');\n\n\n\n# 协议\n\nmit",charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"vue-axios",frontmatter:{title:"vue-axios",date:"2022-02-09T13:15:12.000Z",permalink:"/pages/305234/"},regularPath:"/21.Simplify/01.Simplify%20Docs/04.vue-axios.html",relativePath:"21.Simplify/01.Simplify Docs/04.vue-axios.md",key:"v-77f35f9e",path:"/pages/305234/",headers:[{level:2,title:"安装:",slug:"安装",normalizedTitle:"安装:",charIndex:31},{level:3,title:"CommonJS:",slug:"commonjs",normalizedTitle:"commonjs:",charIndex:39},{level:3,title:"Script:",slug:"script",normalizedTitle:"script:",charIndex:225},{level:2,title:"Usage:",slug:"usage",normalizedTitle:"usage:",charIndex:280}],headersStr:"安装: CommonJS: Script: Usage:",content:"# vue-axios\n\n基于vuejs 的轻度封装\n\n\n# 安装:\n\n\n# CommonJS:\n\nnpm install --save axios vue-axios\n\n\n将下面代码加入入口文件:\n\nimport Vue from 'vue'\nimport axios from 'axios'\nimport VueAxios from 'docs/zh/simplify/1140'\n\nVue.use(VueAxios, axios)\n\n\n\n# Script:\n\n按照这个顺序分别引入这三个文件： vue, axios and vue-axios\n\n\n# Usage:\n\nThis wrapper bind axios to Vue or this if you’re using single file component.\n\n你可以按照以下方式使用:\n\nVue.axios.get(api).then((response) => {\n  console.log(response.data)\n})\n\nthis.axios.get(api).then((response) => {\n  console.log(response.data)\n})\n\nthis.$http.get(api).then((response) => {\n  console.log(response.data)\n})\n",normalizedContent:"# vue-axios\n\n基于vuejs 的轻度封装\n\n\n# 安装:\n\n\n# commonjs:\n\nnpm install --save axios vue-axios\n\n\n将下面代码加入入口文件:\n\nimport vue from 'vue'\nimport axios from 'axios'\nimport vueaxios from 'docs/zh/simplify/1140'\n\nvue.use(vueaxios, axios)\n\n\n\n# script:\n\n按照这个顺序分别引入这三个文件： vue, axios and vue-axios\n\n\n# usage:\n\nthis wrapper bind axios to vue or this if you’re using single file component.\n\n你可以按照以下方式使用:\n\nvue.axios.get(api).then((response) => {\n  console.log(response.data)\n})\n\nthis.axios.get(api).then((response) => {\n  console.log(response.data)\n})\n\nthis.$http.get(api).then((response) => {\n  console.log(response.data)\n})\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"自己制作一个java11的docker镜像",frontmatter:{title:"自己制作一个java11的docker镜像",date:"2022-02-09T13:15:12.000Z",permalink:"/pages/46a509/"},regularPath:"/21.Simplify/01.Simplify%20Docs/06.%E8%87%AA%E5%B7%B1%E5%88%B6%E4%BD%9C%E4%B8%80%E4%B8%AAjava11%E7%9A%84docker%E9%95%9C%E5%83%8F.html",relativePath:"21.Simplify/01.Simplify Docs/06.自己制作一个java11的docker镜像.md",key:"v-6de74afa",path:"/pages/46a509/",headers:[{level:2,title:"资源准备",slug:"资源准备",normalizedTitle:"资源准备",charIndex:80},{level:3,title:"下载java11的jdk",slug:"下载java11的jdk",normalizedTitle:"下载java11的jdk",charIndex:89},{level:2,title:"创建Dockerfile文件",slug:"创建dockerfile文件",normalizedTitle:"创建dockerfile文件",charIndex:229},{level:2,title:"构建镜像(docker build)",slug:"构建镜像-docker-build",normalizedTitle:"构建镜像(docker build)",charIndex:832},{level:2,title:"推送到远程仓库",slug:"推送到远程仓库",normalizedTitle:"推送到远程仓库",charIndex:2457},{level:2,title:"推送至本地私服harbor",slug:"推送至本地私服harbor",normalizedTitle:"推送至本地私服harbor",charIndex:2553},{level:2,title:"测试",slug:"测试",normalizedTitle:"测试",charIndex:70}],headersStr:"资源准备 下载java11的jdk 创建Dockerfile文件 构建镜像(docker build) 推送到远程仓库 推送至本地私服harbor 测试",content:"# 自己制作一个java11的docker镜像\n\n提示\n\n本教程基于ubuntu官方镜像，制作java11的docker镜像，仅供个人学习及测试使用。\n\n\n# 资源准备\n\n\n# 下载java11的jdk\n\n下载地址：https://www.oracle.com/java/technologies/javase-jdk11-downloads.html\n\n选择linux-64的版本：jdk-11.0.11_linux-x64_bin.tar.gz\n\n\n# 创建Dockerfile文件\n\n文件内容如下：\n\nFROM ubuntu:focal\n\nLABEL maintainer=\"info@ieooc.com\"\n\nADD jdk-11.0.11_linux-x64_bin.tar.gz /usr/local\n\n## 设置时区\nRUN sed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.list \\\n          && apt-get update \\\n          && ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone \\\n          && apt-get install tzdata \\ \n          && apt-get clean \\\n          && apt-get autoclean \\\n          && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n\n## 设置JAVA环境变量\nENV JAVA_HOME=/usr/local/jdk-11.0.11\nENV PATH=.:$JAVA_HOME/bin:$PATH\n\nCMD [\"/bin/bash\"]\n\n\n\n# 构建镜像(docker build)\n\n我这里jdk-11.0.11_linux-x64_bin.tar.gz 和Dockerfile放在同一个目录下面的，直接用docker build指定当前路径即可\n\ndocker build -t 仓库路径xxx/java:11 .\n\n\n例如：\n\n$ docker build -t ieooc/java:11-jdk .\n[+] Building 31.3s (8/8) FINISHED\n => [internal] load build definition from Dockerfile                       0.1s\n => => transferring dockerfile: 684B                                       0.0s\n => [internal] load .dockerignore                                          0.0s\n => => transferring context: 2B                                            0.0s\n => [internal] load metadata for docker.io/library/ubuntu:focal            0.0s\n => [internal] load build context                                          6.7s\n => => transferring context: 181.76MB                                      6.7s\n => [1/3] FROM docker.io/library/ubuntu:focal                              0.1s\n => => resolve docker.io/library/ubuntu:focal                              0.0s\n => [2/3] ADD jdk-11.0.11_linux-x64_bin.tar.gz /usr/local                  5.3s\n => [3/3] RUN sed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/ap  17.1s\n => exporting to image                                                     2.0s\n => => exporting layers                                                    2.0s\n => => writing image sha256:a1acc660560e92203cca1ecdcc4ff1fd88098e8f1133a  0.0s\n => => naming to docker.io/ieooc/java:11-jdk                               0.0s\n\n\n查看构建结果：\n\n$ docker images\nREPOSITORY          TAG        IMAGE ID       CREATED              SIZE\nieooc/java          11-jdk     a1acc660560e   About a minute ago   383MB\n\n\n注意\n\n后面的.表示当前路径，如果不是当前路径需要用-f指定\n\n\n# 推送到远程仓库\n\n前面如果-t指定了仓库的路径，则直接使用docker push即可 ，否则用docker tag\n\n$ docker push ieooc/java:11-jdk\n\n\n\n# 推送至本地私服harbor\n\n重新打tag\n\n$ docker tag ieooc/java:11-jdk harbor.ieooc.com/library/java:11-jdk\n\n\n开始推送\n\n$ docker push harbor.ieooc.com/library/java:11-jdk\n\n\n\n# 测试\n\n（1）测试ubuntu时区：进入容器输入date\n\n（2）测试java环境：进入容器输入java -version\n\n（3）测试jdk时区：编写一个.java文件，里面打印LocalDateTime打印时间",normalizedContent:"# 自己制作一个java11的docker镜像\n\n提示\n\n本教程基于ubuntu官方镜像，制作java11的docker镜像，仅供个人学习及测试使用。\n\n\n# 资源准备\n\n\n# 下载java11的jdk\n\n下载地址：https://www.oracle.com/java/technologies/javase-jdk11-downloads.html\n\n选择linux-64的版本：jdk-11.0.11_linux-x64_bin.tar.gz\n\n\n# 创建dockerfile文件\n\n文件内容如下：\n\nfrom ubuntu:focal\n\nlabel maintainer=\"info@ieooc.com\"\n\nadd jdk-11.0.11_linux-x64_bin.tar.gz /usr/local\n\n## 设置时区\nrun sed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.list \\\n          && apt-get update \\\n          && ln -snf /usr/share/zoneinfo/$tz /etc/localtime && echo $tz > /etc/timezone \\\n          && apt-get install tzdata \\ \n          && apt-get clean \\\n          && apt-get autoclean \\\n          && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n\n## 设置java环境变量\nenv java_home=/usr/local/jdk-11.0.11\nenv path=.:$java_home/bin:$path\n\ncmd [\"/bin/bash\"]\n\n\n\n# 构建镜像(docker build)\n\n我这里jdk-11.0.11_linux-x64_bin.tar.gz 和dockerfile放在同一个目录下面的，直接用docker build指定当前路径即可\n\ndocker build -t 仓库路径xxx/java:11 .\n\n\n例如：\n\n$ docker build -t ieooc/java:11-jdk .\n[+] building 31.3s (8/8) finished\n => [internal] load build definition from dockerfile                       0.1s\n => => transferring dockerfile: 684b                                       0.0s\n => [internal] load .dockerignore                                          0.0s\n => => transferring context: 2b                                            0.0s\n => [internal] load metadata for docker.io/library/ubuntu:focal            0.0s\n => [internal] load build context                                          6.7s\n => => transferring context: 181.76mb                                      6.7s\n => [1/3] from docker.io/library/ubuntu:focal                              0.1s\n => => resolve docker.io/library/ubuntu:focal                              0.0s\n => [2/3] add jdk-11.0.11_linux-x64_bin.tar.gz /usr/local                  5.3s\n => [3/3] run sed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/ap  17.1s\n => exporting to image                                                     2.0s\n => => exporting layers                                                    2.0s\n => => writing image sha256:a1acc660560e92203cca1ecdcc4ff1fd88098e8f1133a  0.0s\n => => naming to docker.io/ieooc/java:11-jdk                               0.0s\n\n\n查看构建结果：\n\n$ docker images\nrepository          tag        image id       created              size\nieooc/java          11-jdk     a1acc660560e   about a minute ago   383mb\n\n\n注意\n\n后面的.表示当前路径，如果不是当前路径需要用-f指定\n\n\n# 推送到远程仓库\n\n前面如果-t指定了仓库的路径，则直接使用docker push即可 ，否则用docker tag\n\n$ docker push ieooc/java:11-jdk\n\n\n\n# 推送至本地私服harbor\n\n重新打tag\n\n$ docker tag ieooc/java:11-jdk harbor.ieooc.com/library/java:11-jdk\n\n\n开始推送\n\n$ docker push harbor.ieooc.com/library/java:11-jdk\n\n\n\n# 测试\n\n（1）测试ubuntu时区：进入容器输入date\n\n（2）测试java环境：进入容器输入java -version\n\n（3）测试jdk时区：编写一个.java文件，里面打印localdatetime打印时间",charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"优化Docker中的Spring Boot应用",frontmatter:{title:"优化Docker中的Spring Boot应用",date:"2022-02-09T13:15:12.000Z",permalink:"/pages/59f664/"},regularPath:"/21.Simplify/01.Simplify%20Docs/05.%E4%BC%98%E5%8C%96Docker%E4%B8%AD%E7%9A%84Spring%20Boot%E5%BA%94%E7%94%A8.html",relativePath:"21.Simplify/01.Simplify Docs/05.优化Docker中的Spring Boot应用.md",key:"v-4c1cfbd2",path:"/pages/59f664/",headers:[{level:2,title:"Docker关键概念",slug:"docker关键概念",normalizedTitle:"docker关键概念",charIndex:335},{level:2,title:"镜像层内容很重要",slug:"镜像层内容很重要",normalizedTitle:"镜像层内容很重要",charIndex:599},{level:2,title:"镜像层影响部署",slug:"镜像层影响部署",normalizedTitle:"镜像层影响部署",charIndex:904},{level:2,title:"Docker中的Spring Boot应用",slug:"docker中的spring-boot应用",normalizedTitle:"docker中的spring boot应用",charIndex:4},{level:2,title:"单层方法",slug:"单层方法",normalizedTitle:"单层方法",charIndex:1502},{level:2,title:"更深入地研究单层方法",slug:"更深入地研究单层方法",normalizedTitle:"更深入地研究单层方法",charIndex:1943},{level:2,title:"镜像层生命周期",slug:"镜像层生命周期",normalizedTitle:"镜像层生命周期",charIndex:3964},{level:2,title:"拥抱Docker，进入双层",slug:"拥抱docker-进入双层",normalizedTitle:"拥抱docker，进入双层",charIndex:4312}],headersStr:"Docker关键概念 镜像层内容很重要 镜像层影响部署 Docker中的Spring Boot应用 单层方法 更深入地研究单层方法 镜像层生命周期 拥抱Docker，进入双层",content:'# 优化Docker中的Spring Boot应用\n\n提示\n\n文介绍了如何使用Docker中的一些关键概念和结构组件来优化Spring Boot应用程序。\n\nDocker功能强大且易于使用。Docker允许开发人员研制的软件创建可移植的镜像。这些镜像可重复地部署。你可以很容易地从Docker中获得很多价值，但是要从Docker中获得最大的价值，需要理解一些概念。在进行持续集成和持续交付时，如何构建Docker镜像具有重要的作用。在本文中，我将重点介绍在进行迭代开发和部署时，如何采用更有效的方法为Spring Boot应用程序构建Docker镜像。为Spring Boot应用程序构建Docker镜像的标准方法有一些缺点，因此在这里我们要介绍如何做得更好。\n\n\n# Docker关键概念\n\nDocker有四个关键概念： images, layers, Dockerfile 和 Docker cache。简而言之，Dockerfile描述了如何构建Docker镜像。镜像由许多层组成。Dockerfile从基础镜像开始，并添加了其他层。当新内容添加到镜像时，将生成一个新层。所构建的每个层都被缓存，因此可以在后续构建中重复使用。当Docker构建运行时，它可以从缓存中获取重复使用任何已有层。这就减少了每次构建所需的时间和空间。任何已更改或以前尚未构建的内容都将根据需要进行构建。\n\n\n\n\n# 镜像层内容很重要\n\n镜像各层的重要性。Docker缓存中的现有层，只有当改镜像层内容没有变更时，才能被使用。在Docker构建期间更改的层越多，Docker需要执行更多的工作来重建镜像。镜像层顺序也很重要。如果某个图层的所有父图层均未更改，则该图层就能被重用。因此，最好把比较频繁更改的图层放在上面，以便对其更改会影响较少的子图层。\n\n镜像层的顺序和内容很重要。当你把应用程序打包为Docker镜像时，最简单的方法是将整个应用程序放置到一个单独的镜像层中。但是，如果该应用程序包含大量静态库依赖，那么即使更改很少的代码，也需要重新构建整个镜像层。这就需要在Docker缓存中，花费大量构建时间和空间。\n\n\n# 镜像层影响部署\n\n部署Docker镜像时，镜像层也很重要。在部署Docker镜像之前，它们会被推送到Docker远程仓库。该仓库是所有部署镜像的源头，并且经常包含同一镜像的许多版本。Docker非常高效，每个层仅存储一次。但是，对于频繁部署且具有不断重建的大体积层的镜像，这就不行了。大体积层的镜像，即使内部只有很少的更改，也必须单独存储在仓库中并在网络中推送。因为需要移动并存储不变的内容，这就会增加部署时间，\n\n\n# Docker中的Spring Boot应用\n\n使用uber-jar方法的Spring Boot应用程序本身就是独立的部署单元。该模型非常适合在虚拟机或构建包上进行部署，因为该应用程序可带来所需的一切。但是，这对Docker部署是一个缺点：Docker已经提供了打包依赖项的方法。将整个Spring Boot JAR放入Docker镜像是很常见的，但是，这会导致Docker镜像的应用程序层中的不变内容太多。\n\n\n\nSpring社区中正在进行有关减少运行Spring Boot应用程序时的部署大小和时间的讨论，尤其是在Docker中。在我看来，这最终是在简单性与效率之间进行权衡。为Spring Boot应用程序构建Docker镜像的最常见方法是我所说的“单层”方法。从技术上讲，这不是正确的，因为Dockerfile实际上创建了多个层，但是对于讨论来说已经足够了。\n\n\n# 单层方法\n\n让我们看一下单层方法。单层方法快速，简单，易于理解和使用。Docker的Spring Boot指南 列出了单层Dockerfile来构建你的Docker镜像：\n\nFROM openjdk:8-jdk-alpine\nVOLUME /tmp\nARG JAR_FILE\nCOPY ${JAR_FILE} app.jar\nENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]\n\n\n它的最终结果是一个正常运行的Docker镜像，其运行方式与你期望Spring Boot应用程序运行的方式完全相同。但是，由于它基于整个应用程序JAR，因此存在分层效率问题。随着应用程序源的更改，整个Spring Boot JAR都会被重建。下次构建Docker镜像时，将重新构建整个应用程序层，包括所有不变的依赖库。\n\n让我们看一个具体的例子， Spring Pet Clinic。\n\n\n# 更深入地研究单层方法\n\n单层方法使用Open Boot JDK基础镜像之上的Spring Boot JAR作为Docker层构建Docker镜像：\n\n$ docker images\nREPOSITORY                    TAG         IMAGE ID            CREATED             SIZE\nspringio/spring-petclinic     latest      94b0366d5ba2        16 seconds ago      140MB\n\n\n生成的Docker镜像为140 MB。你可以使用docker history 命令检查图层 。你可以看到Spring Boot应用程序JAR已复制到镜像中，大小为38.3 MB。\n\n$ docker history springio/spring-petclinic\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n94b0366d5ba2        52 seconds ago      /bin/sh -c #(nop)  ENTRYPOINT ["java" "-Djav…   0B\n213dff56a4bd        53 seconds ago      /bin/sh -c #(nop) COPY file:d3551559c2aa35af…   38.3MB\nbc453a32748e        6 minutes ago       /bin/sh -c #(nop)  ARG JAR_FILE                 0B\n7fe0bb0d8026        6 minutes ago       /bin/sh -c #(nop)  VOLUME [/tmp]                0B\ncc2179b8f042        8 days ago          /bin/sh -c set -x  && apk add --no-cache   o…   97.4MB\n<missing>           8 days ago          /bin/sh -c #(nop)  ENV JAVA_ALPINE_VERSION=8…   0B\n<missing>           8 days ago          /bin/sh -c #(nop)  ENV JAVA_VERSION=8u151       0B\n<missing>           8 days ago          /bin/sh -c #(nop)  ENV PATH=/usr/local/sbin:…   0B\n<missing>           8 days ago          /bin/sh -c #(nop)  ENV JAVA_HOME=/usr/lib/jv…   0B\n<missing>           8 days ago          /bin/sh -c {   echo \'#!/bin/sh\';   echo \'set…   87B\n<missing>           8 days ago          /bin/sh -c #(nop)  ENV LANG=C.UTF-8             0B\n<missing>           5 months ago        /bin/sh -c #(nop)  CMD ["/bin/sh"]              0B\n<missing>           5 months ago        /bin/sh -c #(nop) ADD file:093f0723fa46f6cdb…   4.15MB\n\n\n下次构建Docker镜像时，将重新创建整个38 MB的层，因为重新打包了JAR文件。\n\n在此示例中，应用程序的大小相对较小（因为仅基于spring-boot-starter-web和其他依赖项，例如spring-actuator）。在实际开发中，这些大小通常要大得多，因为它们不仅包括Spring Boot库，还包括其他第三方库。根据我的经验，实际的Spring Boot应用程序的大小范围可能在50 MB到250 MB之间（如果不是更大的话）。\n\n仔细观察该应用程序，应用程序JAR中只有372 KB是应用程序代码。其余38 MB是依赖库。这意味着实际上只有0.1％的层在变化。其余99.9％不变。\n\n\n# 镜像层生命周期\n\n这是基于镜像层的基本考虑：内容的生命周期。镜像层的内容应具有相同的生命周期。Spring Boot应用程序的内容有两个不同的生命周期：不经常更改的依赖库和经常更改的应用程序类。\n\n每次由于应用程序代码更改而重建该层时，也会包含不变的二进制文件。在快速的应用程序开发环境中，不断更改和重新部署应用程序代码，这种附加成本可能变得非常昂贵。\n\n想象一个应用团队在Pet Clinic上进行迭代。团队每天更改和重新部署应用程序10次。这10个新层的成本为每天383 MB。如果使用更多实际大小，则每天最多可以达到2.5 GB或更多。最终将浪费大量的构建时间，部署时间和Docker仓库空间。\n\n快速迭代的开发和交付是决定我们是继续使用简单的单层方法，还是采用更有效的替代方法。\n\n\n# 拥抱Docker，进入双层\n\n在简单性和效率之间进行权衡时，我认为正确的选择是“双层”方法。（可以有更多的层，但是太多的层可能有害，并且违反了 Docker最佳实践）。在双层方法中，我们构建Docker镜像，以使Spring Boot应用程序的依赖库，存在于应用程序代码下方的一层中。这样，各层将遵循内容的不同生命周期。通过将不经常更改的依赖库推入一个单独的层，并仅将应用程序类保留在顶层，那么迭代重建和重新部署就会更快。\n\n\n\n双层方法加快了迭代开发的速度，并最大程度地缩短了部署时间。当然实际效率因应用程序而异，但是平均而言，这将使应用程序部署大小减少90％，同时相应地缩短了部署周期。',normalizedContent:'# 优化docker中的spring boot应用\n\n提示\n\n文介绍了如何使用docker中的一些关键概念和结构组件来优化spring boot应用程序。\n\ndocker功能强大且易于使用。docker允许开发人员研制的软件创建可移植的镜像。这些镜像可重复地部署。你可以很容易地从docker中获得很多价值，但是要从docker中获得最大的价值，需要理解一些概念。在进行持续集成和持续交付时，如何构建docker镜像具有重要的作用。在本文中，我将重点介绍在进行迭代开发和部署时，如何采用更有效的方法为spring boot应用程序构建docker镜像。为spring boot应用程序构建docker镜像的标准方法有一些缺点，因此在这里我们要介绍如何做得更好。\n\n\n# docker关键概念\n\ndocker有四个关键概念： images, layers, dockerfile 和 docker cache。简而言之，dockerfile描述了如何构建docker镜像。镜像由许多层组成。dockerfile从基础镜像开始，并添加了其他层。当新内容添加到镜像时，将生成一个新层。所构建的每个层都被缓存，因此可以在后续构建中重复使用。当docker构建运行时，它可以从缓存中获取重复使用任何已有层。这就减少了每次构建所需的时间和空间。任何已更改或以前尚未构建的内容都将根据需要进行构建。\n\n\n\n\n# 镜像层内容很重要\n\n镜像各层的重要性。docker缓存中的现有层，只有当改镜像层内容没有变更时，才能被使用。在docker构建期间更改的层越多，docker需要执行更多的工作来重建镜像。镜像层顺序也很重要。如果某个图层的所有父图层均未更改，则该图层就能被重用。因此，最好把比较频繁更改的图层放在上面，以便对其更改会影响较少的子图层。\n\n镜像层的顺序和内容很重要。当你把应用程序打包为docker镜像时，最简单的方法是将整个应用程序放置到一个单独的镜像层中。但是，如果该应用程序包含大量静态库依赖，那么即使更改很少的代码，也需要重新构建整个镜像层。这就需要在docker缓存中，花费大量构建时间和空间。\n\n\n# 镜像层影响部署\n\n部署docker镜像时，镜像层也很重要。在部署docker镜像之前，它们会被推送到docker远程仓库。该仓库是所有部署镜像的源头，并且经常包含同一镜像的许多版本。docker非常高效，每个层仅存储一次。但是，对于频繁部署且具有不断重建的大体积层的镜像，这就不行了。大体积层的镜像，即使内部只有很少的更改，也必须单独存储在仓库中并在网络中推送。因为需要移动并存储不变的内容，这就会增加部署时间，\n\n\n# docker中的spring boot应用\n\n使用uber-jar方法的spring boot应用程序本身就是独立的部署单元。该模型非常适合在虚拟机或构建包上进行部署，因为该应用程序可带来所需的一切。但是，这对docker部署是一个缺点：docker已经提供了打包依赖项的方法。将整个spring boot jar放入docker镜像是很常见的，但是，这会导致docker镜像的应用程序层中的不变内容太多。\n\n\n\nspring社区中正在进行有关减少运行spring boot应用程序时的部署大小和时间的讨论，尤其是在docker中。在我看来，这最终是在简单性与效率之间进行权衡。为spring boot应用程序构建docker镜像的最常见方法是我所说的“单层”方法。从技术上讲，这不是正确的，因为dockerfile实际上创建了多个层，但是对于讨论来说已经足够了。\n\n\n# 单层方法\n\n让我们看一下单层方法。单层方法快速，简单，易于理解和使用。docker的spring boot指南 列出了单层dockerfile来构建你的docker镜像：\n\nfrom openjdk:8-jdk-alpine\nvolume /tmp\narg jar_file\ncopy ${jar_file} app.jar\nentrypoint ["java","-djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]\n\n\n它的最终结果是一个正常运行的docker镜像，其运行方式与你期望spring boot应用程序运行的方式完全相同。但是，由于它基于整个应用程序jar，因此存在分层效率问题。随着应用程序源的更改，整个spring boot jar都会被重建。下次构建docker镜像时，将重新构建整个应用程序层，包括所有不变的依赖库。\n\n让我们看一个具体的例子， spring pet clinic。\n\n\n# 更深入地研究单层方法\n\n单层方法使用open boot jdk基础镜像之上的spring boot jar作为docker层构建docker镜像：\n\n$ docker images\nrepository                    tag         image id            created             size\nspringio/spring-petclinic     latest      94b0366d5ba2        16 seconds ago      140mb\n\n\n生成的docker镜像为140 mb。你可以使用docker history 命令检查图层 。你可以看到spring boot应用程序jar已复制到镜像中，大小为38.3 mb。\n\n$ docker history springio/spring-petclinic\nimage               created             created by                                      size                comment\n94b0366d5ba2        52 seconds ago      /bin/sh -c #(nop)  entrypoint ["java" "-djav…   0b\n213dff56a4bd        53 seconds ago      /bin/sh -c #(nop) copy file:d3551559c2aa35af…   38.3mb\nbc453a32748e        6 minutes ago       /bin/sh -c #(nop)  arg jar_file                 0b\n7fe0bb0d8026        6 minutes ago       /bin/sh -c #(nop)  volume [/tmp]                0b\ncc2179b8f042        8 days ago          /bin/sh -c set -x  && apk add --no-cache   o…   97.4mb\n<missing>           8 days ago          /bin/sh -c #(nop)  env java_alpine_version=8…   0b\n<missing>           8 days ago          /bin/sh -c #(nop)  env java_version=8u151       0b\n<missing>           8 days ago          /bin/sh -c #(nop)  env path=/usr/local/sbin:…   0b\n<missing>           8 days ago          /bin/sh -c #(nop)  env java_home=/usr/lib/jv…   0b\n<missing>           8 days ago          /bin/sh -c {   echo \'#!/bin/sh\';   echo \'set…   87b\n<missing>           8 days ago          /bin/sh -c #(nop)  env lang=c.utf-8             0b\n<missing>           5 months ago        /bin/sh -c #(nop)  cmd ["/bin/sh"]              0b\n<missing>           5 months ago        /bin/sh -c #(nop) add file:093f0723fa46f6cdb…   4.15mb\n\n\n下次构建docker镜像时，将重新创建整个38 mb的层，因为重新打包了jar文件。\n\n在此示例中，应用程序的大小相对较小（因为仅基于spring-boot-starter-web和其他依赖项，例如spring-actuator）。在实际开发中，这些大小通常要大得多，因为它们不仅包括spring boot库，还包括其他第三方库。根据我的经验，实际的spring boot应用程序的大小范围可能在50 mb到250 mb之间（如果不是更大的话）。\n\n仔细观察该应用程序，应用程序jar中只有372 kb是应用程序代码。其余38 mb是依赖库。这意味着实际上只有0.1％的层在变化。其余99.9％不变。\n\n\n# 镜像层生命周期\n\n这是基于镜像层的基本考虑：内容的生命周期。镜像层的内容应具有相同的生命周期。spring boot应用程序的内容有两个不同的生命周期：不经常更改的依赖库和经常更改的应用程序类。\n\n每次由于应用程序代码更改而重建该层时，也会包含不变的二进制文件。在快速的应用程序开发环境中，不断更改和重新部署应用程序代码，这种附加成本可能变得非常昂贵。\n\n想象一个应用团队在pet clinic上进行迭代。团队每天更改和重新部署应用程序10次。这10个新层的成本为每天383 mb。如果使用更多实际大小，则每天最多可以达到2.5 gb或更多。最终将浪费大量的构建时间，部署时间和docker仓库空间。\n\n快速迭代的开发和交付是决定我们是继续使用简单的单层方法，还是采用更有效的替代方法。\n\n\n# 拥抱docker，进入双层\n\n在简单性和效率之间进行权衡时，我认为正确的选择是“双层”方法。（可以有更多的层，但是太多的层可能有害，并且违反了 docker最佳实践）。在双层方法中，我们构建docker镜像，以使spring boot应用程序的依赖库，存在于应用程序代码下方的一层中。这样，各层将遵循内容的不同生命周期。通过将不经常更改的依赖库推入一个单独的层，并仅将应用程序类保留在顶层，那么迭代重建和重新部署就会更快。\n\n\n\n双层方法加快了迭代开发的速度，并最大程度地缩短了部署时间。当然实际效率因应用程序而异，但是平均而言，这将使应用程序部署大小减少90％，同时相应地缩短了部署周期。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Spring Boot 2.3 新特性 Layered Jar",frontmatter:{title:"Spring Boot 2.3 新特性 Layered Jar",date:"2022-02-09T13:15:12.000Z",permalink:"/pages/97d41d/"},regularPath:"/21.Simplify/01.Simplify%20Docs/07.Spring%20Boot%20%E6%96%B0%E7%89%B9%E6%80%A7%20Layered%20Jar.html",relativePath:"21.Simplify/01.Simplify Docs/07.Spring Boot 新特性 Layered Jar.md",key:"v-a146c5d8",path:"/pages/97d41d/",headers:[{level:2,title:"背景",slug:"背景",normalizedTitle:"背景",charIndex:38},{level:2,title:"Layered Jar",slug:"layered-jar",normalizedTitle:"layered jar",charIndex:22},{level:2,title:"查看镜像分层信息",slug:"查看镜像分层信息",normalizedTitle:"查看镜像分层信息",charIndex:2911},{level:2,title:"模块升级重新构建",slug:"模块升级重新构建",normalizedTitle:"模块升级重新构建",charIndex:3758},{level:2,title:"比较 v1.0、v1.1 镜像",slug:"比较-v1-0、v1-1-镜像",normalizedTitle:"比较 v1.0、v1.1 镜像",charIndex:4747},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:11940}],headersStr:"背景 Layered Jar 查看镜像分层信息 模块升级重新构建 比较 v1.0、v1.1 镜像 总结",content:'# Spring Boot 2.3 新特性 Layered Jar\n\n\n# 背景\n\nSpring Boot 2.3 为容器化部署提供了一个新特性 Layered Jar。通常 Spring Boot 程序都是以 fat jar 的方式构建的，文件大小动辄 50M、100M 这样子，对 docker image 其实很不友好。Docker image 本身是分层结构，如果某一层没有变化在 pull 时就不必上传，一旦有变化就要上传整层。一个程序中，程序自身代码、资源的变更频率要远大于依赖库的变更频率，大多数时候因为几行代码变化导致上传整个 jar 文件，无论是存储占用还是时间效率上都是很大的浪费，后者在国内网速下更是一个严重问题。\n\n\n# Layered Jar\n\n新特性 layered jar 为不同变更频率内容分离提供了支持工具，在此基础上分层构建 docker image 就变得很容易了。本质上这个特性是 org.springframework.boot:spring-boot-maven-plugin 提供的一个新的 layers 配置项，当启用 layers 模式打包时，一个 spring-boot-jarmode-layertools jar 会打包到 fat jar 中，新特性是由这个 jar 提供的。这里不深入解析实现细节，而是重点关注如何模式化使用这个特性获得收益。\n\n要使用此功能，必须启用分层功能：\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n\n\n\n<project>\n\t<build>\n\t\t<plugins>\n\t\t\t<plugin>\n\t\t\t\t<groupId>org.springframework.boot</groupId>\n\t\t\t\t<artifactId>spring-boot-maven-plugin</artifactId>\n\t\t\t\t<configuration>\n\t\t\t\t\t<layers>\n\t\t\t\t\t\t<enabled>true</enabled>\n\t\t\t\t\t</layers>\n\t\t\t\t</configuration>\n\t\t\t</plugin>\n\t\t</plugins>\n\t</build>\n</project>\n\n\n默认情况下，定义了以下层：\n\n * dependencies 对于版本不包含的任何SNAPSHOT依赖项。\n\n * spring-boot-loader 用于jar加载程序类。\n\n * snapshot-dependencies 对于其版本包含SNAPSHOT的任何依赖项。\n\n * application 应用程序类和资源。\n\n层顺序很重要，因为它确定了部分应用程序更改时可以缓存先前的层的可能性。默认顺序为dependencies，spring-boot-loader，snapshot-dependencies，application。应该首先添加最不可能更改的内容，然后添加更可能更改的层。\n\n增加配置之后 package 打包，然后可以执行以下命令验证配置正确与否\n\n$ java -Djarmode=layertools -jar simplify-upms-biz.jar\n\n\n一个示例输出：\n\n\n\n\n\n\n\n \n\n\n\nUsage:\n  java -Djarmode=layertools -jar simplify-upms-biz.jar\n\nAvailable commands:\n  list     List layers from the jar that can be extracted\n  extract  Extracts layers from the jar for image creation\n  help     Help about any command\n\n\n该extract命令可用于轻松地将应用程序拆分为多个层，以添加到Dockerfile中。这是使用jarmode的Dockerfile的示例。\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n \n\n\nFROM adoptopenjdk:11-jre-hotspot as builder\nWORKDIR application\nARG JAR_FILE=target/*.jar\nCOPY ${JAR_FILE} application.jar\nRUN java -Djarmode=layertools -jar application.jar extract\n\nFROM adoptopenjdk:11-jre-hotspot\n\nLABEL maintainer="info@ieooc.com"\nLABEL version="1.0"\nLABEL description="simplify-upms-biz"\n\nENV TZ=Asia/Shanghai\nWORKDIR application\n\nCOPY --from=builder application/dependencies/ ./\nCOPY --from=builder application/spring-boot-loader/ ./\nCOPY --from=builder application/snapshot-dependencies/ ./\nCOPY --from=builder application/application/ ./\n\nEXPOSE 4000\n\nENTRYPOINT ["java", "org.springframework.boot.loader.JarLauncher"]\n\n\n注意\n\n这是一个多阶段的Dockerfile。构建器阶段提取以后需要的目录。每个COPY命令都与jarmode提取的层有关。\n\n对上面的 Dockerfile 逐行解释一下：\n\n * 2 个 FROM 保持统一为运行时使用的基础镜像即可；\n * 目的文件保持 application.jar 即可，它要跟 java -Djarmode=layertools -jar后面的文件名一致；\n * LABEL指定作者及版本信息，非必须项。但是算是 Spring Boot 容器化的良好实践\n * 4 个 COPY 1 个 ENTRYPOINT 照抄即可，这是本文新特性的关键。\n\n然后，用这个 Dockerfile 构建出来的镜像内容已经是拆解分成多层的镜像了，当变更源码、资源、快照版依赖、正式版依赖时会依次影响更多层镜像，从而实现每次构建上传仓库时存储和传输耗时最小化。\n\n提示\n\n假设以上Dockerfile内容位于当前目录中，则可以使用构建docker映像docker build .，或者可以选择指定应用程序jar的路径，如以下示例所示：\n\n$ docker build --build-arg JAR_FILE=./target/simplify-upms-biz.jar . -t simplify-upms-biz:v1.0\n\n\n\n# 查看镜像分层信息\n\n我们通过docker inspect simplify-upms-biz:v1.0查看镜像的每层的散列值\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n// simplify-upms-biz:v1.0 版本镜像分层信息摘要\n"Layers": [\n    "sha256:50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a",\n    "sha256:2104e5b926f69c4ad7a73be2c50541b8c2b8bd45ab882c9e5c4e388ccb556bef",\n    "sha256:f9a0984f718aa572e28dc4f776e4aa258f67e4f44ed5c63b6174962cc5617e78",\n    "sha256:d3f8c53ca74aa0fcc2e52c1ecd37f7507ccd6f8e63219ca60d05b94e68f524d5",\n    "sha256:2254b66d09be2cc6ae6feed4ea9f49dc4b14edaedbd7573d18d1f7d965389586",\n    "sha256:18bfcba3d17a9b06705a78c06cbc84e127f04a76fff70790af496b490b4a9e2e",\n    "sha256:28f08c714a6e10df22b986048e237dba87d850d25f4e03bea68cbf7de91b8841",\n    "sha256:e662f81e0c921b2a18dbe472d842185f2d767938b1118d83d4d1d2d1ce032e74",\n    "sha256:519d5530049b599f94a6ccbb15b4e659d01aa7b3f3bc18a772414a6c5aed0035"\n]\n\n\n\n# 模块升级重新构建\n\n我们对 simplify-upms-biz 程序进行部分修改（模拟开发过程），然后重新构建镜像\n\n$ docker build --build-arg JAR_FILE=./target/simplify-upms-biz.jar . -t simplify-upms-biz:v1.1\n\n\n此时镜像分层信息如下 docker inspect simplify-upms-biz:v1.1\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n// simplify-upms-biz:v1.1 版本镜像分层信息摘要\n"Layers": [\n    "sha256:50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a",\n    "sha256:2104e5b926f69c4ad7a73be2c50541b8c2b8bd45ab882c9e5c4e388ccb556bef",\n    "sha256:f9a0984f718aa572e28dc4f776e4aa258f67e4f44ed5c63b6174962cc5617e78",\n    "sha256:d3f8c53ca74aa0fcc2e52c1ecd37f7507ccd6f8e63219ca60d05b94e68f524d5",\n    "sha256:2254b66d09be2cc6ae6feed4ea9f49dc4b14edaedbd7573d18d1f7d965389586",\n    "sha256:18bfcba3d17a9b06705a78c06cbc84e127f04a76fff70790af496b490b4a9e2e",\n    "sha256:28f08c714a6e10df22b986048e237dba87d850d25f4e03bea68cbf7de91b8841",\n    "sha256:a1dc1f8cdf9ea2744fe64ecb20a9094ba8f094728c51b21ac6bf5e652916bdb0",\n    "sha256:f379fb50808d8e5483b276747d93b729308e7fe52de1d52c02a4912e6316c9fb"\n]\n\n\n\n# 比较 v1.0、v1.1 镜像\n\n通过比较 v1.0 和 v1.1 版本的镜像摘要信息，我们会发现只有最后的一层发生了变化，我们通过 Dive 是一个用 Go 语言编写的 Docker 镜像分析工具 来确定一下 最后一层是做了哪些事情\n\n使用镜像分析工具进行分析\n\n docker run --rm -it \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    wagoodman/dive:latest simplify-upms-biz\n\n\n分析内容输出如下：\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n│ Layers ├────────────────────────────────────────────────────────────────────────────────────────── ┃ ● Current Layer Contents ┣━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nCmp   Size  Command                                                                                  Permission     UID:GID       Size  Filetree\n    5.6 MB  FROM 31609b718dd2bed                                                                     -rw-r--r--         0:0      30 kB  │   │       ├── LatencyUtils-2.0.3.jar\n     12 MB  ALPINE_GLIBC_BASE_URL="https://github.com/sgerrand/alpine-pkg-glibc/releases/download" & -rw-r--r--         0:0     140 kB  │   │       ├── archaius-core-0.7.6.jar\n       0 B  mkdir -p /usr/lib/java/                                                                  -rw-r--r--         0:0     122 kB  │   │       ├── aspectjrt-1.9.6.jar\n    402 MB  #(nop) ADD file:65d2a21c343d4f40c5b76862829bee1b24ef58f2bd077129ce6e61765786a638 in .    -rw-r--r--         0:0     2.1 MB  │   │       ├── aspectjweaver-1.9.6.jar\n       0 B  WORKDIR /application                                                                     -rw-r--r--         0:0     878 kB  │   │       ├── bcpkix-jdk15on-1.64.jar\n     76 MB  COPY application/dependencies/ ./ # buildkit                                             -rw-r--r--         0:0     4.8 MB  │   │       ├── bcprov-jdk15on-1.64.jar\n    240 kB  COPY application/spring-boot-loader/ ./ # buildkit                                       -rw-r--r--         0:0     3.5 MB  │   │       ├── byte-buddy-1.10.18.jar\n     82 kB  COPY application/snapshot-dependencies/ ./ # buildkit                                    -rw-r--r--         0:0     201 kB  │   │       ├── checker-qual-2.11.1.jar\n    108 kB  COPY application/application/ ./ # buildkit                                              -rw-r--r--         0:0     505 kB  │   │       ├── classgraph-4.8.83.jar\n                                                                                                     -rw-r--r--         0:0      68 kB  │   │       ├── classmate-1.5.1.jar\n│ Layer Details ├─────────────────────────────────────────────────────────────────────────────────── -rw-r--r--         0:0     348 kB  │   │       ├── commons-codec-1.14.jar\n                                                                                                     -rw-r--r--         0:0     588 kB  │   │       ├── commons-collections-3.2.2.jar\nTags:   (unavailable)                                                                                -rw-r--r--         0:0     354 kB  │   │       ├── commons-configuration-1.8.jar\nId:     bc864cc37f3f9cc88d08849efd54796b8f69644cfa4c3cc45cb49e66c7af381f                             -rw-r--r--         0:0      72 kB  │   │       ├── commons-fileupload-1.4.jar\nDigest: sha256:519d5530049b599f94a6ccbb15b4e659d01aa7b3f3bc18a772414a6c5aed0035                      -rw-r--r--         0:0     174 kB  │   │       ├── commons-io-2.2.jar\nCommand:                                                                                             -rw-r--r--         0:0     284 kB  │   │       ├── commons-lang-2.6.jar\nCOPY application/application/ ./ # buildkit                                                          -rw-r--r--         0:0     117 kB  │   │       ├── concurrentlinkedhashmap-lru-1.4.2.jar\n                                                                                                     -rw-r--r--         0:0     3.5 MB  │   │       ├── druid-1.2.3.jar\n│ Image Details ├─────────────────────────────────────────────────────────────────────────────────── -rw-r--r--         0:0      20 kB  │   │       ├── druid-spring-boot-starter-1.2.3.jar\n                                                                                                     -rw-r--r--         0:0      14 kB  │   │       ├── error_prone_annotations-2.3.4.jar\n                                                                                                     -rw-r--r--         0:0     4.6 kB  │   │       ├── failureaccess-1.0.1.jar\nTotal Image size: 497 MB                                                                             -rw-r--r--         0:0     652 kB  │   │       ├── fastjson-1.2.71.jar\nPotential wasted space: 483 kB                                                                       -rw-r--r--         0:0     189 kB  │   │       ├── feign-core-10.10.1.jar\nImage efficiency score: 99 %                                                                         -rw-r--r--         0:0      30 kB  │   │       ├── feign-form-3.8.0.jar\n                                                                                                     -rw-r--r--         0:0      14 kB  │   │       ├── feign-form-spring-3.8.0.jar\nCount   Total Space  Path                                                                            -rw-r--r--         0:0      18 kB  │   │       ├── feign-hystrix-10.10.1.jar\n    2        430 kB  /etc/ssl/certs/ca-certificates.crt                                              -rw-r--r--         0:0     3.7 kB  │   │       ├── feign-slf4j-10.10.1.jar\n    2         30 kB  /lib/apk/db/installed                                                           -rw-r--r--         0:0     1.7 MB  │   │       ├── freemarker-2.3.30.jar\n    2         22 kB  /lib/apk/db/scripts.tar                                                         -rw-r--r--         0:0     2.8 MB  │   │       ├── guava-29.0-jre.jar\n    2         219 B  /lib/apk/db/triggers                                                            -rw-r--r--         0:0     1.3 MB  │   │       ├── hibernate-validator-6.1.6.Final.jar\n    2         198 B  /etc/apk/world                                                                  -rw-r--r--         0:0     180 kB  │   │       ├── httpasyncclient-4.1.4.jar\n    2           0 B  /var/cache/misc                                                                 -rw-r--r--         0:0     780 kB  │   │       ├── httpclient-4.5.13.jar\n    2           0 B  /lib/apk/db/lock                                                                -rw-r--r--         0:0     328 kB  │   │       ├── httpcore-4.4.14.jar\n    2           0 B  /usr/local/share                                                                -rw-r--r--         0:0     369 kB  │   │       ├── httpcore-nio-4.4.14.jar\n    2           0 B  /usr/bin/wget                                                                   -rw-r--r--         0:0     1.8 MB  │   │       ├── hutool-all-5.4.5.jar\n    2           0 B  /root                                                                           -rw-r--r--         0:0     468 kB  │   │       ├── hystrix-core-1.5.18.jar\n    2           0 B  /etc/apk/protected_paths.d                                                      -rw-r--r--         0:0     8.8 kB  │   │       ├── j2objc-annotations-1.3.jar\n\n\n\n# 总结\n\n无论从易用程度还是达到的效果上讲，layered jar 都非常值得使用。要使用该特性只需要开启 pom 中一个配置项并使用一个几乎不需要任何修改的 Dockerfile 模板，几乎零负担获得可观的收益。',normalizedContent:'# spring boot 2.3 新特性 layered jar\n\n\n# 背景\n\nspring boot 2.3 为容器化部署提供了一个新特性 layered jar。通常 spring boot 程序都是以 fat jar 的方式构建的，文件大小动辄 50m、100m 这样子，对 docker image 其实很不友好。docker image 本身是分层结构，如果某一层没有变化在 pull 时就不必上传，一旦有变化就要上传整层。一个程序中，程序自身代码、资源的变更频率要远大于依赖库的变更频率，大多数时候因为几行代码变化导致上传整个 jar 文件，无论是存储占用还是时间效率上都是很大的浪费，后者在国内网速下更是一个严重问题。\n\n\n# layered jar\n\n新特性 layered jar 为不同变更频率内容分离提供了支持工具，在此基础上分层构建 docker image 就变得很容易了。本质上这个特性是 org.springframework.boot:spring-boot-maven-plugin 提供的一个新的 layers 配置项，当启用 layers 模式打包时，一个 spring-boot-jarmode-layertools jar 会打包到 fat jar 中，新特性是由这个 jar 提供的。这里不深入解析实现细节，而是重点关注如何模式化使用这个特性获得收益。\n\n要使用此功能，必须启用分层功能：\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n\n\n\n<project>\n\t<build>\n\t\t<plugins>\n\t\t\t<plugin>\n\t\t\t\t<groupid>org.springframework.boot</groupid>\n\t\t\t\t<artifactid>spring-boot-maven-plugin</artifactid>\n\t\t\t\t<configuration>\n\t\t\t\t\t<layers>\n\t\t\t\t\t\t<enabled>true</enabled>\n\t\t\t\t\t</layers>\n\t\t\t\t</configuration>\n\t\t\t</plugin>\n\t\t</plugins>\n\t</build>\n</project>\n\n\n默认情况下，定义了以下层：\n\n * dependencies 对于版本不包含的任何snapshot依赖项。\n\n * spring-boot-loader 用于jar加载程序类。\n\n * snapshot-dependencies 对于其版本包含snapshot的任何依赖项。\n\n * application 应用程序类和资源。\n\n层顺序很重要，因为它确定了部分应用程序更改时可以缓存先前的层的可能性。默认顺序为dependencies，spring-boot-loader，snapshot-dependencies，application。应该首先添加最不可能更改的内容，然后添加更可能更改的层。\n\n增加配置之后 package 打包，然后可以执行以下命令验证配置正确与否\n\n$ java -djarmode=layertools -jar simplify-upms-biz.jar\n\n\n一个示例输出：\n\n\n\n\n\n\n\n \n\n\n\nusage:\n  java -djarmode=layertools -jar simplify-upms-biz.jar\n\navailable commands:\n  list     list layers from the jar that can be extracted\n  extract  extracts layers from the jar for image creation\n  help     help about any command\n\n\n该extract命令可用于轻松地将应用程序拆分为多个层，以添加到dockerfile中。这是使用jarmode的dockerfile的示例。\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n \n\n\nfrom adoptopenjdk:11-jre-hotspot as builder\nworkdir application\narg jar_file=target/*.jar\ncopy ${jar_file} application.jar\nrun java -djarmode=layertools -jar application.jar extract\n\nfrom adoptopenjdk:11-jre-hotspot\n\nlabel maintainer="info@ieooc.com"\nlabel version="1.0"\nlabel description="simplify-upms-biz"\n\nenv tz=asia/shanghai\nworkdir application\n\ncopy --from=builder application/dependencies/ ./\ncopy --from=builder application/spring-boot-loader/ ./\ncopy --from=builder application/snapshot-dependencies/ ./\ncopy --from=builder application/application/ ./\n\nexpose 4000\n\nentrypoint ["java", "org.springframework.boot.loader.jarlauncher"]\n\n\n注意\n\n这是一个多阶段的dockerfile。构建器阶段提取以后需要的目录。每个copy命令都与jarmode提取的层有关。\n\n对上面的 dockerfile 逐行解释一下：\n\n * 2 个 from 保持统一为运行时使用的基础镜像即可；\n * 目的文件保持 application.jar 即可，它要跟 java -djarmode=layertools -jar后面的文件名一致；\n * label指定作者及版本信息，非必须项。但是算是 spring boot 容器化的良好实践\n * 4 个 copy 1 个 entrypoint 照抄即可，这是本文新特性的关键。\n\n然后，用这个 dockerfile 构建出来的镜像内容已经是拆解分成多层的镜像了，当变更源码、资源、快照版依赖、正式版依赖时会依次影响更多层镜像，从而实现每次构建上传仓库时存储和传输耗时最小化。\n\n提示\n\n假设以上dockerfile内容位于当前目录中，则可以使用构建docker映像docker build .，或者可以选择指定应用程序jar的路径，如以下示例所示：\n\n$ docker build --build-arg jar_file=./target/simplify-upms-biz.jar . -t simplify-upms-biz:v1.0\n\n\n\n# 查看镜像分层信息\n\n我们通过docker inspect simplify-upms-biz:v1.0查看镜像的每层的散列值\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n// simplify-upms-biz:v1.0 版本镜像分层信息摘要\n"layers": [\n    "sha256:50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a",\n    "sha256:2104e5b926f69c4ad7a73be2c50541b8c2b8bd45ab882c9e5c4e388ccb556bef",\n    "sha256:f9a0984f718aa572e28dc4f776e4aa258f67e4f44ed5c63b6174962cc5617e78",\n    "sha256:d3f8c53ca74aa0fcc2e52c1ecd37f7507ccd6f8e63219ca60d05b94e68f524d5",\n    "sha256:2254b66d09be2cc6ae6feed4ea9f49dc4b14edaedbd7573d18d1f7d965389586",\n    "sha256:18bfcba3d17a9b06705a78c06cbc84e127f04a76fff70790af496b490b4a9e2e",\n    "sha256:28f08c714a6e10df22b986048e237dba87d850d25f4e03bea68cbf7de91b8841",\n    "sha256:e662f81e0c921b2a18dbe472d842185f2d767938b1118d83d4d1d2d1ce032e74",\n    "sha256:519d5530049b599f94a6ccbb15b4e659d01aa7b3f3bc18a772414a6c5aed0035"\n]\n\n\n\n# 模块升级重新构建\n\n我们对 simplify-upms-biz 程序进行部分修改（模拟开发过程），然后重新构建镜像\n\n$ docker build --build-arg jar_file=./target/simplify-upms-biz.jar . -t simplify-upms-biz:v1.1\n\n\n此时镜像分层信息如下 docker inspect simplify-upms-biz:v1.1\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n// simplify-upms-biz:v1.1 版本镜像分层信息摘要\n"layers": [\n    "sha256:50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a",\n    "sha256:2104e5b926f69c4ad7a73be2c50541b8c2b8bd45ab882c9e5c4e388ccb556bef",\n    "sha256:f9a0984f718aa572e28dc4f776e4aa258f67e4f44ed5c63b6174962cc5617e78",\n    "sha256:d3f8c53ca74aa0fcc2e52c1ecd37f7507ccd6f8e63219ca60d05b94e68f524d5",\n    "sha256:2254b66d09be2cc6ae6feed4ea9f49dc4b14edaedbd7573d18d1f7d965389586",\n    "sha256:18bfcba3d17a9b06705a78c06cbc84e127f04a76fff70790af496b490b4a9e2e",\n    "sha256:28f08c714a6e10df22b986048e237dba87d850d25f4e03bea68cbf7de91b8841",\n    "sha256:a1dc1f8cdf9ea2744fe64ecb20a9094ba8f094728c51b21ac6bf5e652916bdb0",\n    "sha256:f379fb50808d8e5483b276747d93b729308e7fe52de1d52c02a4912e6316c9fb"\n]\n\n\n\n# 比较 v1.0、v1.1 镜像\n\n通过比较 v1.0 和 v1.1 版本的镜像摘要信息，我们会发现只有最后的一层发生了变化，我们通过 dive 是一个用 go 语言编写的 docker 镜像分析工具 来确定一下 最后一层是做了哪些事情\n\n使用镜像分析工具进行分析\n\n docker run --rm -it \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    wagoodman/dive:latest simplify-upms-biz\n\n\n分析内容输出如下：\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n│ layers ├────────────────────────────────────────────────────────────────────────────────────────── ┃ ● current layer contents ┣━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\ncmp   size  command                                                                                  permission     uid:gid       size  filetree\n    5.6 mb  from 31609b718dd2bed                                                                     -rw-r--r--         0:0      30 kb  │   │       ├── latencyutils-2.0.3.jar\n     12 mb  alpine_glibc_base_url="https://github.com/sgerrand/alpine-pkg-glibc/releases/download" & -rw-r--r--         0:0     140 kb  │   │       ├── archaius-core-0.7.6.jar\n       0 b  mkdir -p /usr/lib/java/                                                                  -rw-r--r--         0:0     122 kb  │   │       ├── aspectjrt-1.9.6.jar\n    402 mb  #(nop) add file:65d2a21c343d4f40c5b76862829bee1b24ef58f2bd077129ce6e61765786a638 in .    -rw-r--r--         0:0     2.1 mb  │   │       ├── aspectjweaver-1.9.6.jar\n       0 b  workdir /application                                                                     -rw-r--r--         0:0     878 kb  │   │       ├── bcpkix-jdk15on-1.64.jar\n     76 mb  copy application/dependencies/ ./ # buildkit                                             -rw-r--r--         0:0     4.8 mb  │   │       ├── bcprov-jdk15on-1.64.jar\n    240 kb  copy application/spring-boot-loader/ ./ # buildkit                                       -rw-r--r--         0:0     3.5 mb  │   │       ├── byte-buddy-1.10.18.jar\n     82 kb  copy application/snapshot-dependencies/ ./ # buildkit                                    -rw-r--r--         0:0     201 kb  │   │       ├── checker-qual-2.11.1.jar\n    108 kb  copy application/application/ ./ # buildkit                                              -rw-r--r--         0:0     505 kb  │   │       ├── classgraph-4.8.83.jar\n                                                                                                     -rw-r--r--         0:0      68 kb  │   │       ├── classmate-1.5.1.jar\n│ layer details ├─────────────────────────────────────────────────────────────────────────────────── -rw-r--r--         0:0     348 kb  │   │       ├── commons-codec-1.14.jar\n                                                                                                     -rw-r--r--         0:0     588 kb  │   │       ├── commons-collections-3.2.2.jar\ntags:   (unavailable)                                                                                -rw-r--r--         0:0     354 kb  │   │       ├── commons-configuration-1.8.jar\nid:     bc864cc37f3f9cc88d08849efd54796b8f69644cfa4c3cc45cb49e66c7af381f                             -rw-r--r--         0:0      72 kb  │   │       ├── commons-fileupload-1.4.jar\ndigest: sha256:519d5530049b599f94a6ccbb15b4e659d01aa7b3f3bc18a772414a6c5aed0035                      -rw-r--r--         0:0     174 kb  │   │       ├── commons-io-2.2.jar\ncommand:                                                                                             -rw-r--r--         0:0     284 kb  │   │       ├── commons-lang-2.6.jar\ncopy application/application/ ./ # buildkit                                                          -rw-r--r--         0:0     117 kb  │   │       ├── concurrentlinkedhashmap-lru-1.4.2.jar\n                                                                                                     -rw-r--r--         0:0     3.5 mb  │   │       ├── druid-1.2.3.jar\n│ image details ├─────────────────────────────────────────────────────────────────────────────────── -rw-r--r--         0:0      20 kb  │   │       ├── druid-spring-boot-starter-1.2.3.jar\n                                                                                                     -rw-r--r--         0:0      14 kb  │   │       ├── error_prone_annotations-2.3.4.jar\n                                                                                                     -rw-r--r--         0:0     4.6 kb  │   │       ├── failureaccess-1.0.1.jar\ntotal image size: 497 mb                                                                             -rw-r--r--         0:0     652 kb  │   │       ├── fastjson-1.2.71.jar\npotential wasted space: 483 kb                                                                       -rw-r--r--         0:0     189 kb  │   │       ├── feign-core-10.10.1.jar\nimage efficiency score: 99 %                                                                         -rw-r--r--         0:0      30 kb  │   │       ├── feign-form-3.8.0.jar\n                                                                                                     -rw-r--r--         0:0      14 kb  │   │       ├── feign-form-spring-3.8.0.jar\ncount   total space  path                                                                            -rw-r--r--         0:0      18 kb  │   │       ├── feign-hystrix-10.10.1.jar\n    2        430 kb  /etc/ssl/certs/ca-certificates.crt                                              -rw-r--r--         0:0     3.7 kb  │   │       ├── feign-slf4j-10.10.1.jar\n    2         30 kb  /lib/apk/db/installed                                                           -rw-r--r--         0:0     1.7 mb  │   │       ├── freemarker-2.3.30.jar\n    2         22 kb  /lib/apk/db/scripts.tar                                                         -rw-r--r--         0:0     2.8 mb  │   │       ├── guava-29.0-jre.jar\n    2         219 b  /lib/apk/db/triggers                                                            -rw-r--r--         0:0     1.3 mb  │   │       ├── hibernate-validator-6.1.6.final.jar\n    2         198 b  /etc/apk/world                                                                  -rw-r--r--         0:0     180 kb  │   │       ├── httpasyncclient-4.1.4.jar\n    2           0 b  /var/cache/misc                                                                 -rw-r--r--         0:0     780 kb  │   │       ├── httpclient-4.5.13.jar\n    2           0 b  /lib/apk/db/lock                                                                -rw-r--r--         0:0     328 kb  │   │       ├── httpcore-4.4.14.jar\n    2           0 b  /usr/local/share                                                                -rw-r--r--         0:0     369 kb  │   │       ├── httpcore-nio-4.4.14.jar\n    2           0 b  /usr/bin/wget                                                                   -rw-r--r--         0:0     1.8 mb  │   │       ├── hutool-all-5.4.5.jar\n    2           0 b  /root                                                                           -rw-r--r--         0:0     468 kb  │   │       ├── hystrix-core-1.5.18.jar\n    2           0 b  /etc/apk/protected_paths.d                                                      -rw-r--r--         0:0     8.8 kb  │   │       ├── j2objc-annotations-1.3.jar\n\n\n\n# 总结\n\n无论从易用程度还是达到的效果上讲，layered jar 都非常值得使用。要使用该特性只需要开启 pom 中一个配置项并使用一个几乎不需要任何修改的 dockerfile 模板，几乎零负担获得可观的收益。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"Docker客户端使用TLS保护连接远程Docker服务",frontmatter:{title:"Docker客户端使用TLS保护连接远程Docker服务",date:"2022-02-09T13:15:12.000Z",permalink:"/pages/a60efa/"},regularPath:"/21.Simplify/01.Simplify%20Docs/08.Docker%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8TLS%E4%BF%9D%E6%8A%A4%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8BDocker%E6%9C%8D%E5%8A%A1.html",relativePath:"21.Simplify/01.Simplify Docs/08.Docker客户端使用TLS保护连接远程Docker服务.md",key:"v-9d6dab12",path:"/pages/a60efa/",headers:[{level:2,title:"服务器环境",slug:"服务器环境",normalizedTitle:"服务器环境",charIndex:78},{level:2,title:"Docker服务端,客户端和CA证书",slug:"docker服务端-客户端和ca证书",normalizedTitle:"docker服务端,客户端和ca证书",charIndex:190},{level:2,title:"使用OpenSSL创建CA和服务端密钥key",slug:"使用openssl创建ca和服务端密钥key",normalizedTitle:"使用openssl创建ca和服务端密钥key",charIndex:1076},{level:3,title:"步骤1:生成CA私钥ca-key.pem",slug:"步骤1-生成ca私钥ca-key-pem",normalizedTitle:"步骤1:生成ca私钥ca-key.pem",charIndex:1167},{level:3,title:"步骤2:使用CA私钥生成自签名CA证书ca.pem",slug:"步骤2-使用ca私钥生成自签名ca证书ca-pem",normalizedTitle:"步骤2:使用ca私钥生成自签名ca证书ca.pem",charIndex:1631},{level:3,title:"生成服务器私钥server-key.pem和证书签名请求server-csr",slug:"生成服务器私钥server-key-pem和证书签名请求server-csr",normalizedTitle:"生成服务器私钥server-key.pem和证书签名请求server-csr",charIndex:2738},{level:3,title:"编写extfile.cnf",slug:"编写extfile-cnf",normalizedTitle:"编写extfile.cnf",charIndex:3131},{level:3,title:"使用CA证书生成服务器签名证书server-cert.pem",slug:"使用ca证书生成服务器签名证书server-cert-pem",normalizedTitle:"使用ca证书生成服务器签名证书server-cert.pem",charIndex:3920},{level:2,title:"利用CA创建客户端密钥key",slug:"利用ca创建客户端密钥key",normalizedTitle:"利用ca创建客户端密钥key",charIndex:4359},{level:3,title:"步骤1:生成客户端私钥key.pem和证书签名请求client-csr",slug:"步骤1-生成客户端私钥key-pem和证书签名请求client-csr",normalizedTitle:"步骤1:生成客户端私钥key.pem和证书签名请求client-csr",charIndex:4500},{level:3,title:"步骤2:编写扩展配置文件extfile.cnf",slug:"步骤2-编写扩展配置文件extfile-cnf",normalizedTitle:"步骤2:编写扩展配置文件extfile.cnf",charIndex:4960},{level:3,title:"步骤3:生成签名文件cert.pem",slug:"步骤3-生成签名文件cert-pem",normalizedTitle:"步骤3:生成签名文件cert.pem",charIndex:5167},{level:3,title:"修改文件权限",slug:"修改文件权限",normalizedTitle:"修改文件权限",charIndex:5448},{level:2,title:"启动Docker守护进程",slug:"启动docker守护进程",normalizedTitle:"启动docker守护进程",charIndex:5650},{level:3,title:"重新加载配置",slug:"重新加载配置",normalizedTitle:"重新加载配置",charIndex:8050},{level:3,title:"重新启动",slug:"重新启动",normalizedTitle:"重新启动",charIndex:8094},{level:3,title:"重启后查看服务状态：",slug:"重启后查看服务状态",normalizedTitle:"重启后查看服务状态：",charIndex:8137},{level:2,title:"使用证书连接",slug:"使用证书连接",normalizedTitle:"使用证书连接",charIndex:479},{level:2,title:"docker-java 启用TLS",slug:"docker-java-启用tls",normalizedTitle:"docker-java 启用tls",charIndex:10853}],headersStr:"服务器环境 Docker服务端,客户端和CA证书 使用OpenSSL创建CA和服务端密钥key 步骤1:生成CA私钥ca-key.pem 步骤2:使用CA私钥生成自签名CA证书ca.pem 生成服务器私钥server-key.pem和证书签名请求server-csr 编写extfile.cnf 使用CA证书生成服务器签名证书server-cert.pem 利用CA创建客户端密钥key 步骤1:生成客户端私钥key.pem和证书签名请求client-csr 步骤2:编写扩展配置文件extfile.cnf 步骤3:生成签名文件cert.pem 修改文件权限 启动Docker守护进程 重新加载配置 重新启动 重启后查看服务状态： 使用证书连接 docker-java 启用TLS",content:'# Docker客户端使用TLS保护连接远程Docker服务\n\n提示\n\n参考官方教程实现通过自定义证书对远程Docker主机的安全访问和控制。\n\n\n\n\n# 服务器环境\n\n * Ubuntu 20.04 LTS focal\n * Docker Version Server: Docker Engine - Community Engine:Version：20.10.3\n\n\n# Docker服务端,客户端和CA证书\n\n默认情况下，Docker通过非联网的Unix套接字运行。它还可以选择使用HTTP套接字进行通信。\n\n这里有三个角色,Docker服务端和Docker客户端和CA签名的证书\n\n * Docker服务端：对应运行Docker守护程序的主机\n   * 通过开放端口(一般为2376)支持远程连接\n   * 通过指定tlsverify标志并将Docker的tlscacert标志指向可信的CA证书来启用TLS以实现安全访问(仅允许由该CA签名的证书进行身份验证的客户端连接)\n * Docker客户端：即默认情况下的Docker主机\n   * 当使用证书连接时,仅能连接到具有该CA签名的证书的服务器\n * CA签名的证书\n   * 作用：服务端和客户端证书都只对应一份信任列表，信任列表里是服务端的信息(如ip或域名等等)，服务端持有服务端证书，仅接受持有客户端证书的主机访问(这里可以再加上其他的限制,详情见下文)\n\n注意\n\n * Docker服务端也可以不开启TLS验证，不过这样子很不安全，生产环境下应当尽量避免。如果只是试验性的，可以指定关闭TLS验证,但只对特定主机开放，方法见后文/etc/docker/daemon.json配置相关\n * 如果Docker服务端没有开启TLS验证，则Docker客户端不需要使用证书连接。但如果客户端不使用证书连接开启了TLS验证的服务端，则会报错,如下:\n\nGet http://远程主机ip:2376/v1.38/version: net/http: HTTP/1.x transport connection broken: malformed HTTP response "\\x15\\x03\\x01\\x00\\x02\\x02".\n* Are you trying to connect to a TLS-enabled daemon without TLS?\n\n\n * 如果Docker客户端连接时使用的证书内不含目的主机的信息,则会提示对方主机不在证书信任列表内,访问失败\n\n\n# 使用OpenSSL创建CA和服务端密钥key\n\n注意\n\n将以下示例中的所有$HOST实例替换为Docker守护程序主机的域名或IP地址\n\n以下步骤在Docker服务端进行:\n\n\n# 步骤1:生成CA私钥ca-key.pem\n\n提示\n\nca-key.pem是一个临时文件，最后可以删除。\n\n$ openssl genrsa -aes256 -out ca-key.pem 4096\n\n\n例子如下，需要设置密码并验证（自定义输入密码，记住后面需要用）\n\n$ openssl genrsa -aes256 -out ca-key.pem 4096\nGenerating RSA private key, 4096 bit long modulus (2 primes)\n......++++\n....................................................................................................++++\ne is 65537 (0x010001)\nEnter pass phrase for ca-key.pem:\nVerifying - Enter pass phrase for ca-key.pem:\n\n\n\n# 步骤2:使用CA私钥生成自签名CA证书ca.pem\n\n提示\n\n生成证书时，通过-days 365设置证书的有效期。单位为天，默认情况下为30天。有了CA证书后,就可以创建服务器密钥和证书签名请求（CSR）了\n\n$ openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem\n\n\n例子如下，需要验证密码并输入信息，(其他属性直接回车也没关系,反正也只是自己用的证书)\n\n注意\n\n将以下示例中的所有$HOST实例替换为Docker守护程序主机的域名或IP地址\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n$ openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem\nEnter pass phrase for ca-key.pem: # 输入上一步设置的密码 回车\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter \'.\', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:CN\nState or Province Name (full name) [Some-State]:BeiJing\nLocality Name (eg, city) []:BeiJing\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Docker Inc\nOrganizational Unit Name (eg, section) []:IT Dept\nCommon Name (e.g. server FQDN or YOUR name) []:$HOST # 这里如果没有域名可以替换成主机IP\nEmail Address []:info@ieooc.com\n\n\n\n# 生成服务器私钥server-key.pem和证书签名请求server-csr\n\n提示\n\nCSR:Certificate Signing Request,证书签名请求,server-csr是一个临时文件，生成server-cert.pem以后，可以删除。\n\n注意\n\n注意这里的CN信息对应的是服务器所在主机域名,如果没有的话也没关系,可以通过下一步的extfile.cnf配置IP地址连接\n\n第一步，生成服务器私钥server-key.pem\n\n$ openssl genrsa -out server-key.pem 4096\n\n\n第二步，使用服务器私钥另加CN信息生成证书签名请求server-csr.pem\n\n$ openssl req -subj "/CN=$HOST" -sha256 -new -key server-key.pem -out server.csr\n\n\n\n# 编写extfile.cnf\n\n提示\n\n这个文件用于指定下一步生成签名证书的一些属性配置,这里我们主要用到两个属性,如果要其他要求(如限制指定ip范围的客户端才能连接)的可以看OpenSSL x509v3_config文档\n\n * subjectAltName： 主题备选名称,是有点像上一步生成server.csr时所用的选项-subj "/CN=10.17.1.101"的东西,这个更像一个说明补充,这里可以填信任的DNS域名和主机IP等等,因为没有域名,所以我只填了Docker服务器的主机IP，另外,需要特别注意,这里对应生成的是一份信任列表,这里所说的信任是对服务端的信任,所以填的是服务端的信息(如域名,IP),我之前看到有文章说这里的列表是客户端的列表,只有在列表中的客户端才能访问服务器,这种说法是错误,在使用证书连接到服务器时,会报错说服务器IP不在信任列表中( 如远程主机ip为ip3,证书的信任列表为ip1和ip2时,若使用该证书访问远程主机,则会报错x509: certificate is valid for ip1, ip2, not ip3)\n * extendedKeyUsage： 扩展密钥用法,此扩展包含一个用法列表，用于指示证书公钥可用于的目的\n\n由于可以通过IP地址和DNS名称建立TLS连接，因此在创建证书时需要指定IP地址。例如，允许使用10.10.10.20和127.0.0.1进行连接：\n\n$ echo subjectAltName = DNS:$HOST,IP:10.10.10.20,IP:127.0.0.1 >> extfile.cnf\n\n\n将Docker守护程序密钥的扩展使用属性设置为仅用于服务器身份验证\n\n$ echo extendedKeyUsage = serverAuth >> extfile.cnf\n\n\n\n# 使用CA证书生成服务器签名证书server-cert.pem\n\n提示\n\n上面总共生成了两大模块的文件和一个extfile.cnf配置文件,这三部分之间是彼此独立的,到了这一步,也是最后一步才真正做了整合\n\n这里要用到的有两个文件：\n\n * (1)CA私钥ca-key.pem和CA签名文件ca.pem\n * (2)证书签名请求server-csr(3)配置文件extfile.cnf\n\n指令如下:\n\n$ openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile.cnf\n\n\n控制台输出：\n\nSignature ok\nsubject=CN = your.host.com\nGetting CA Private Key\n\n\n至此,Docker服务端密钥创建完毕。\n\n\n# 利用CA创建客户端密钥key\n\n提示\n\n生成密钥需要使用CA私钥和签名文件,为简化流程,避免CA文件在服务端和客户端之间的传输,以下步骤仍在Docker服务端进行\n\n创建客户端密钥的过程和服务端类似,CA相关已经创建好了,extfile.cnf配置文件也简单很多,具体如下\n\n\n# 步骤1:生成客户端私钥key.pem和证书签名请求client-csr\n\n生成客户端私钥key.pem\n\nopenssl genrsa -out key.pem 4096\n\n\n控制台输出：\n\nGenerating RSA private key, 4096 bit long modulus (2 primes)\n.............................++++\n......................................................................................................................................................++++\ne is 65537 (0x010001)\n\n\n证书签名请求client-csr\n\n$ openssl req -subj \'/CN=client\' -new -key key.pem -out client.csr\n\n\n\n# 步骤2:编写扩展配置文件extfile.cnf\n\n注意\n\n如果是在刚才创建服务器私钥的文件夹下,应该还有原来的extfile.cnf文件,为避免覆写,可以先执行重命名\n\n$ mv extfile.cnf extfile.cnf.old\n\n\n创建扩展配置文件并使密钥适用于客户端身份验证的指令如下:\n\n$ echo extendedKeyUsage = clientAuth >> extfile.cnf\n\n\n\n# 步骤3:生成签名文件cert.pem\n\n$ openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \\\n  -CAcreateserial -out cert.pem -extfile extfile.cnf\n\n\n控制台输出：\n\n\n\n\n\n \n\n\nSignature ok\nsubject=/CN=client\nGetting CA Private Key\nEnter pass phrase for ca-key.pem: # 输入验证密码 回车\n\n\n\n# 修改文件权限\n\n步骤1:删除两个证书签名请求文件\n\n$ rm -v client.csr server.csr\n\n\n步骤2:修改密钥文件权限为只由所有者读取\n\n$ chmod -v 0400 ca-key.pem key.pem server-key.pem\n\n\n步骤3:修改证书文件权限为只读\n\n$ chmod -v 0444 ca.pem server-cert.pem cert.pem\n\n\n\n# 启动Docker守护进程\n\n启动Docker守护进程有两种方法,直接用带参指令或者修改daemon.json配置文件,另外,还有一种方式使用systemctl修改docker.service文件,这种不推荐,这里不作介绍.需要注意的是,不管是哪一种方法,只要对同一属性做了配置,都会导致冲突而启动失败.所以建议只使用一种.\n\n注意\n\n监听unix:///var/run/docker.sock是为了实现本机docker直接控制,监听tcp://0.0.0.0:2376表示监听2376端口所有连接,又这里开启了TLS验证,则会根据我们给定的TLS文件去做验证\n\n服务端需要的TLS文件有CA证书ca.pem,服务端证书server-cert.pem,服务端密钥server-key.pem\n\n将Docker服务停止，然后修改docker服务文件\n\n$ vim /usr/lib/systemd/system/docker.service\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network-online.target firewalld.service containerd.service\nWants=network-online.target\nRequires=docker.socket containerd.service\n\n[Service]\nType=notify\n# the default is not to use systemd for cgroups because the delegate issues still\n# exists and systemd currently does not support the cgroup feature set required\n# for containers run by docker\n#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --tlsverify --tlscacert=/opt/docker-ssl/ca.pem --tlscert=/opt/docker-ssl/server-cert.pem --tlskey=/opt/docker-ssl/server-key.pem -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2376\nTimeoutSec=0\nRestartSec=2\nRestart=always\n\n# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.\n# Both the old, and new location are accepted by systemd 229 and up, so using the old location\n# to make them work for either version of systemd.\nStartLimitBurst=3\n\n# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.\n# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make\n# this option work for either version of systemd.\nStartLimitInterval=60s\n\n# Having non-zero Limit*s causes performance problems due to accounting overhead\n# in the kernel. We recommend using cgroups to do container-local accounting.\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\n\n# Comment TasksMax if your systemd version does not support it.\n# Only systemd 226 and above support this option.\nTasksMax=infinity\n\n# set delegate yes so that systemd does not reset the cgroups of docker containers\nDelegate=yes\n\n# kill only the docker process, not all processes in the cgroup\nKillMode=process\nOOMScoreAdjust=-500\n\n[Install]\nWantedBy=multi-user.target\n\n\n\n# 重新加载配置\n\n$ sudo systemctl daemon-reload\n\n\n\n# 重新启动\n\n$ sudo systemctl restart docker\n\n\n\n# 重启后查看服务状态：\n\n$ systemctl status docker \n\n\n控制台输出：\n\n● docker.service - Docker Application Container Engine\n     Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)\n     Active: active (running) since Mon 2021-03-01 17:41:53 CST; 5min ago\nTriggeredBy: ● docker.socket\n       Docs: https://docs.docker.com\n   Main PID: 3604191 (dockerd)\n      Tasks: 30\n     Memory: 56.0M\n     CGroup: /system.slice/docker.service\n             ├─3604191 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --tlsverify --tlscacert=/opt/docker-ssl/ca.pem --tlscert=/opt/docker-ssl/server-cert.pem --tlskey=/opt/docker-ssl/server-key.pem -H unix:/>\n             └─3604407 /usr/bin/docker-proxy -proto tcp -host-ip 127.0.0.1 -host-port 1514 -container-ip 172.18.0.2 -container-port 10514\n\nMar 01 17:41:52 harbor dockerd[3604191]: time="2021-03-01T17:41:52.112268091+08:00" level=error msg="failed to start container" container=070eb39f7a00c182e7247b744d66289db8c91d5e762bc5127f68aa8481fc97d9 error="failed to initialize l>\nMar 01 17:41:52 harbor dockerd[3604191]: time="2021-03-01T17:41:52.148018184+08:00" level=error msg="753ed974e6c469f1fb0a40405563aaa9ea39596462fee72bec4f8edc4bcd90f2 cleanup: failed to delete container from containerd: no such conta>\nMar 01 17:41:52 harbor dockerd[3604191]: time="2021-03-01T17:41:52.148052892+08:00" level=error msg="failed to start container" container=753ed974e6c469f1fb0a40405563aaa9ea39596462fee72bec4f8edc4bcd90f2 error="failed to initialize l>\nMar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01T17:41:53.111270963+08:00" level=info msg="Loading containers: done."\nMar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01T17:41:53.138717581+08:00" level=info msg="Docker daemon" commit=46229ca graphdriver(s)=overlay2 version=20.10.3\nMar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01T17:41:53.138839158+08:00" level=info msg="Daemon has completed initialization"\nMar 01 17:41:53 harbor systemd[1]: Started Docker Application Container Engine.\nMar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01T17:41:53.157523613+08:00" level=info msg="API listen on /run/docker.sock"\nMar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01T17:41:53.168302054+08:00" level=info msg="API listen on /var/run/docker.sock"\nMar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01T17:41:53.173756154+08:00" level=info msg="API listen on [::]:2376"\n\n\n已经生效。\n\n\n# 使用证书连接\n\n复制 ca.pem , cert.pem , key.pem 三个文件到客户端\n\n客户端需要的TLS文件有CA证书ca.pem,客户端证书cert.pem,客户端密钥key.pem\n\n注意\n\n将以下示例中的所有$HOST实例替换为Docker守护程序主机的域名或IP地址，连接的指令格式如下,以docker --version为例\n\n$ docker --tlsverify --tlscacert=ca.pem --tlscert=cert.pem --tlskey=key.pem -H=$HOST:2376 --version\n\n\n\n# docker-java 启用TLS\n\n项目里使用docker的java客户端docker-java调用docker，为了支持TLS，在创建客户端时，需要增加TLS设置。\n\n首先将ca.pem cert.pem key.pem这三个文件拷贝到本地，例如E:\\\\docker\\\\",\n\n然后DefaultDockerClientConfig里withDockerTlsVerify设为true，并设置certpath为刚拷贝的目录。\n\nCopyDefaultDockerClientConfig.Builder builder =\n                DefaultDockerClientConfig.createDefaultConfigBuilder()\n                    .withDockerHost("tcp://" + server + ":2375")\n                    .withApiVersion("1.30");\n            if (containerConfiguration.getDockerTlsVerify()) {\n                builder = builder.withDockerTlsVerify(true)\n                    .withDockerCertPath("E:\\\\docker\\\\");\n            }\n\treturn  DockerClientBuilder.getInstance(builder.build()).build()\n\t\t\t\n\n\n大工搞定。',normalizedContent:'# docker客户端使用tls保护连接远程docker服务\n\n提示\n\n参考官方教程实现通过自定义证书对远程docker主机的安全访问和控制。\n\n\n\n\n# 服务器环境\n\n * ubuntu 20.04 lts focal\n * docker version server: docker engine - community engine:version：20.10.3\n\n\n# docker服务端,客户端和ca证书\n\n默认情况下，docker通过非联网的unix套接字运行。它还可以选择使用http套接字进行通信。\n\n这里有三个角色,docker服务端和docker客户端和ca签名的证书\n\n * docker服务端：对应运行docker守护程序的主机\n   * 通过开放端口(一般为2376)支持远程连接\n   * 通过指定tlsverify标志并将docker的tlscacert标志指向可信的ca证书来启用tls以实现安全访问(仅允许由该ca签名的证书进行身份验证的客户端连接)\n * docker客户端：即默认情况下的docker主机\n   * 当使用证书连接时,仅能连接到具有该ca签名的证书的服务器\n * ca签名的证书\n   * 作用：服务端和客户端证书都只对应一份信任列表，信任列表里是服务端的信息(如ip或域名等等)，服务端持有服务端证书，仅接受持有客户端证书的主机访问(这里可以再加上其他的限制,详情见下文)\n\n注意\n\n * docker服务端也可以不开启tls验证，不过这样子很不安全，生产环境下应当尽量避免。如果只是试验性的，可以指定关闭tls验证,但只对特定主机开放，方法见后文/etc/docker/daemon.json配置相关\n * 如果docker服务端没有开启tls验证，则docker客户端不需要使用证书连接。但如果客户端不使用证书连接开启了tls验证的服务端，则会报错,如下:\n\nget http://远程主机ip:2376/v1.38/version: net/http: http/1.x transport connection broken: malformed http response "\\x15\\x03\\x01\\x00\\x02\\x02".\n* are you trying to connect to a tls-enabled daemon without tls?\n\n\n * 如果docker客户端连接时使用的证书内不含目的主机的信息,则会提示对方主机不在证书信任列表内,访问失败\n\n\n# 使用openssl创建ca和服务端密钥key\n\n注意\n\n将以下示例中的所有$host实例替换为docker守护程序主机的域名或ip地址\n\n以下步骤在docker服务端进行:\n\n\n# 步骤1:生成ca私钥ca-key.pem\n\n提示\n\nca-key.pem是一个临时文件，最后可以删除。\n\n$ openssl genrsa -aes256 -out ca-key.pem 4096\n\n\n例子如下，需要设置密码并验证（自定义输入密码，记住后面需要用）\n\n$ openssl genrsa -aes256 -out ca-key.pem 4096\ngenerating rsa private key, 4096 bit long modulus (2 primes)\n......++++\n....................................................................................................++++\ne is 65537 (0x010001)\nenter pass phrase for ca-key.pem:\nverifying - enter pass phrase for ca-key.pem:\n\n\n\n# 步骤2:使用ca私钥生成自签名ca证书ca.pem\n\n提示\n\n生成证书时，通过-days 365设置证书的有效期。单位为天，默认情况下为30天。有了ca证书后,就可以创建服务器密钥和证书签名请求（csr）了\n\n$ openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem\n\n\n例子如下，需要验证密码并输入信息，(其他属性直接回车也没关系,反正也只是自己用的证书)\n\n注意\n\n将以下示例中的所有$host实例替换为docker守护程序主机的域名或ip地址\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n$ openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem\nenter pass phrase for ca-key.pem: # 输入上一步设置的密码 回车\nyou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nwhat you are about to enter is what is called a distinguished name or a dn.\nthere are quite a few fields but you can leave some blank\nfor some fields there will be a default value,\nif you enter \'.\', the field will be left blank.\n-----\ncountry name (2 letter code) [au]:cn\nstate or province name (full name) [some-state]:beijing\nlocality name (eg, city) []:beijing\norganization name (eg, company) [internet widgits pty ltd]:docker inc\norganizational unit name (eg, section) []:it dept\ncommon name (e.g. server fqdn or your name) []:$host # 这里如果没有域名可以替换成主机ip\nemail address []:info@ieooc.com\n\n\n\n# 生成服务器私钥server-key.pem和证书签名请求server-csr\n\n提示\n\ncsr:certificate signing request,证书签名请求,server-csr是一个临时文件，生成server-cert.pem以后，可以删除。\n\n注意\n\n注意这里的cn信息对应的是服务器所在主机域名,如果没有的话也没关系,可以通过下一步的extfile.cnf配置ip地址连接\n\n第一步，生成服务器私钥server-key.pem\n\n$ openssl genrsa -out server-key.pem 4096\n\n\n第二步，使用服务器私钥另加cn信息生成证书签名请求server-csr.pem\n\n$ openssl req -subj "/cn=$host" -sha256 -new -key server-key.pem -out server.csr\n\n\n\n# 编写extfile.cnf\n\n提示\n\n这个文件用于指定下一步生成签名证书的一些属性配置,这里我们主要用到两个属性,如果要其他要求(如限制指定ip范围的客户端才能连接)的可以看openssl x509v3_config文档\n\n * subjectaltname： 主题备选名称,是有点像上一步生成server.csr时所用的选项-subj "/cn=10.17.1.101"的东西,这个更像一个说明补充,这里可以填信任的dns域名和主机ip等等,因为没有域名,所以我只填了docker服务器的主机ip，另外,需要特别注意,这里对应生成的是一份信任列表,这里所说的信任是对服务端的信任,所以填的是服务端的信息(如域名,ip),我之前看到有文章说这里的列表是客户端的列表,只有在列表中的客户端才能访问服务器,这种说法是错误,在使用证书连接到服务器时,会报错说服务器ip不在信任列表中( 如远程主机ip为ip3,证书的信任列表为ip1和ip2时,若使用该证书访问远程主机,则会报错x509: certificate is valid for ip1, ip2, not ip3)\n * extendedkeyusage： 扩展密钥用法,此扩展包含一个用法列表，用于指示证书公钥可用于的目的\n\n由于可以通过ip地址和dns名称建立tls连接，因此在创建证书时需要指定ip地址。例如，允许使用10.10.10.20和127.0.0.1进行连接：\n\n$ echo subjectaltname = dns:$host,ip:10.10.10.20,ip:127.0.0.1 >> extfile.cnf\n\n\n将docker守护程序密钥的扩展使用属性设置为仅用于服务器身份验证\n\n$ echo extendedkeyusage = serverauth >> extfile.cnf\n\n\n\n# 使用ca证书生成服务器签名证书server-cert.pem\n\n提示\n\n上面总共生成了两大模块的文件和一个extfile.cnf配置文件,这三部分之间是彼此独立的,到了这一步,也是最后一步才真正做了整合\n\n这里要用到的有两个文件：\n\n * (1)ca私钥ca-key.pem和ca签名文件ca.pem\n * (2)证书签名请求server-csr(3)配置文件extfile.cnf\n\n指令如下:\n\n$ openssl x509 -req -days 365 -sha256 -in server.csr -ca ca.pem -cakey ca-key.pem -cacreateserial -out server-cert.pem -extfile extfile.cnf\n\n\n控制台输出：\n\nsignature ok\nsubject=cn = your.host.com\ngetting ca private key\n\n\n至此,docker服务端密钥创建完毕。\n\n\n# 利用ca创建客户端密钥key\n\n提示\n\n生成密钥需要使用ca私钥和签名文件,为简化流程,避免ca文件在服务端和客户端之间的传输,以下步骤仍在docker服务端进行\n\n创建客户端密钥的过程和服务端类似,ca相关已经创建好了,extfile.cnf配置文件也简单很多,具体如下\n\n\n# 步骤1:生成客户端私钥key.pem和证书签名请求client-csr\n\n生成客户端私钥key.pem\n\nopenssl genrsa -out key.pem 4096\n\n\n控制台输出：\n\ngenerating rsa private key, 4096 bit long modulus (2 primes)\n.............................++++\n......................................................................................................................................................++++\ne is 65537 (0x010001)\n\n\n证书签名请求client-csr\n\n$ openssl req -subj \'/cn=client\' -new -key key.pem -out client.csr\n\n\n\n# 步骤2:编写扩展配置文件extfile.cnf\n\n注意\n\n如果是在刚才创建服务器私钥的文件夹下,应该还有原来的extfile.cnf文件,为避免覆写,可以先执行重命名\n\n$ mv extfile.cnf extfile.cnf.old\n\n\n创建扩展配置文件并使密钥适用于客户端身份验证的指令如下:\n\n$ echo extendedkeyusage = clientauth >> extfile.cnf\n\n\n\n# 步骤3:生成签名文件cert.pem\n\n$ openssl x509 -req -days 365 -sha256 -in client.csr -ca ca.pem -cakey ca-key.pem \\\n  -cacreateserial -out cert.pem -extfile extfile.cnf\n\n\n控制台输出：\n\n\n\n\n\n \n\n\nsignature ok\nsubject=/cn=client\ngetting ca private key\nenter pass phrase for ca-key.pem: # 输入验证密码 回车\n\n\n\n# 修改文件权限\n\n步骤1:删除两个证书签名请求文件\n\n$ rm -v client.csr server.csr\n\n\n步骤2:修改密钥文件权限为只由所有者读取\n\n$ chmod -v 0400 ca-key.pem key.pem server-key.pem\n\n\n步骤3:修改证书文件权限为只读\n\n$ chmod -v 0444 ca.pem server-cert.pem cert.pem\n\n\n\n# 启动docker守护进程\n\n启动docker守护进程有两种方法,直接用带参指令或者修改daemon.json配置文件,另外,还有一种方式使用systemctl修改docker.service文件,这种不推荐,这里不作介绍.需要注意的是,不管是哪一种方法,只要对同一属性做了配置,都会导致冲突而启动失败.所以建议只使用一种.\n\n注意\n\n监听unix:///var/run/docker.sock是为了实现本机docker直接控制,监听tcp://0.0.0.0:2376表示监听2376端口所有连接,又这里开启了tls验证,则会根据我们给定的tls文件去做验证\n\n服务端需要的tls文件有ca证书ca.pem,服务端证书server-cert.pem,服务端密钥server-key.pem\n\n将docker服务停止，然后修改docker服务文件\n\n$ vim /usr/lib/systemd/system/docker.service\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[unit]\ndescription=docker application container engine\ndocumentation=https://docs.docker.com\nafter=network-online.target firewalld.service containerd.service\nwants=network-online.target\nrequires=docker.socket containerd.service\n\n[service]\ntype=notify\n# the default is not to use systemd for cgroups because the delegate issues still\n# exists and systemd currently does not support the cgroup feature set required\n# for containers run by docker\n#execstart=/usr/bin/dockerd -h fd:// --containerd=/run/containerd/containerd.sock\nexecstart=/usr/bin/dockerd -h fd:// --containerd=/run/containerd/containerd.sock --tlsverify --tlscacert=/opt/docker-ssl/ca.pem --tlscert=/opt/docker-ssl/server-cert.pem --tlskey=/opt/docker-ssl/server-key.pem -h unix:///var/run/docker.sock -h tcp://0.0.0.0:2376\ntimeoutsec=0\nrestartsec=2\nrestart=always\n\n# note that startlimit* options were moved from "service" to "unit" in systemd 229.\n# both the old, and new location are accepted by systemd 229 and up, so using the old location\n# to make them work for either version of systemd.\nstartlimitburst=3\n\n# note that startlimitinterval was renamed to startlimitintervalsec in systemd 230.\n# both the old, and new name are accepted by systemd 230 and up, so using the old name to make\n# this option work for either version of systemd.\nstartlimitinterval=60s\n\n# having non-zero limit*s causes performance problems due to accounting overhead\n# in the kernel. we recommend using cgroups to do container-local accounting.\nlimitnofile=infinity\nlimitnproc=infinity\nlimitcore=infinity\n\n# comment tasksmax if your systemd version does not support it.\n# only systemd 226 and above support this option.\ntasksmax=infinity\n\n# set delegate yes so that systemd does not reset the cgroups of docker containers\ndelegate=yes\n\n# kill only the docker process, not all processes in the cgroup\nkillmode=process\noomscoreadjust=-500\n\n[install]\nwantedby=multi-user.target\n\n\n\n# 重新加载配置\n\n$ sudo systemctl daemon-reload\n\n\n\n# 重新启动\n\n$ sudo systemctl restart docker\n\n\n\n# 重启后查看服务状态：\n\n$ systemctl status docker \n\n\n控制台输出：\n\n● docker.service - docker application container engine\n     loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)\n     active: active (running) since mon 2021-03-01 17:41:53 cst; 5min ago\ntriggeredby: ● docker.socket\n       docs: https://docs.docker.com\n   main pid: 3604191 (dockerd)\n      tasks: 30\n     memory: 56.0m\n     cgroup: /system.slice/docker.service\n             ├─3604191 /usr/bin/dockerd -h fd:// --containerd=/run/containerd/containerd.sock --tlsverify --tlscacert=/opt/docker-ssl/ca.pem --tlscert=/opt/docker-ssl/server-cert.pem --tlskey=/opt/docker-ssl/server-key.pem -h unix:/>\n             └─3604407 /usr/bin/docker-proxy -proto tcp -host-ip 127.0.0.1 -host-port 1514 -container-ip 172.18.0.2 -container-port 10514\n\nmar 01 17:41:52 harbor dockerd[3604191]: time="2021-03-01t17:41:52.112268091+08:00" level=error msg="failed to start container" container=070eb39f7a00c182e7247b744d66289db8c91d5e762bc5127f68aa8481fc97d9 error="failed to initialize l>\nmar 01 17:41:52 harbor dockerd[3604191]: time="2021-03-01t17:41:52.148018184+08:00" level=error msg="753ed974e6c469f1fb0a40405563aaa9ea39596462fee72bec4f8edc4bcd90f2 cleanup: failed to delete container from containerd: no such conta>\nmar 01 17:41:52 harbor dockerd[3604191]: time="2021-03-01t17:41:52.148052892+08:00" level=error msg="failed to start container" container=753ed974e6c469f1fb0a40405563aaa9ea39596462fee72bec4f8edc4bcd90f2 error="failed to initialize l>\nmar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01t17:41:53.111270963+08:00" level=info msg="loading containers: done."\nmar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01t17:41:53.138717581+08:00" level=info msg="docker daemon" commit=46229ca graphdriver(s)=overlay2 version=20.10.3\nmar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01t17:41:53.138839158+08:00" level=info msg="daemon has completed initialization"\nmar 01 17:41:53 harbor systemd[1]: started docker application container engine.\nmar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01t17:41:53.157523613+08:00" level=info msg="api listen on /run/docker.sock"\nmar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01t17:41:53.168302054+08:00" level=info msg="api listen on /var/run/docker.sock"\nmar 01 17:41:53 harbor dockerd[3604191]: time="2021-03-01t17:41:53.173756154+08:00" level=info msg="api listen on [::]:2376"\n\n\n已经生效。\n\n\n# 使用证书连接\n\n复制 ca.pem , cert.pem , key.pem 三个文件到客户端\n\n客户端需要的tls文件有ca证书ca.pem,客户端证书cert.pem,客户端密钥key.pem\n\n注意\n\n将以下示例中的所有$host实例替换为docker守护程序主机的域名或ip地址，连接的指令格式如下,以docker --version为例\n\n$ docker --tlsverify --tlscacert=ca.pem --tlscert=cert.pem --tlskey=key.pem -h=$host:2376 --version\n\n\n\n# docker-java 启用tls\n\n项目里使用docker的java客户端docker-java调用docker，为了支持tls，在创建客户端时，需要增加tls设置。\n\n首先将ca.pem cert.pem key.pem这三个文件拷贝到本地，例如e:\\\\docker\\\\",\n\n然后defaultdockerclientconfig里withdockertlsverify设为true，并设置certpath为刚拷贝的目录。\n\ncopydefaultdockerclientconfig.builder builder =\n                defaultdockerclientconfig.createdefaultconfigbuilder()\n                    .withdockerhost("tcp://" + server + ":2375")\n                    .withapiversion("1.30");\n            if (containerconfiguration.getdockertlsverify()) {\n                builder = builder.withdockertlsverify(true)\n                    .withdockercertpath("e:\\\\docker\\\\");\n            }\n\treturn  dockerclientbuilder.getinstance(builder.build()).build()\n\t\t\t\n\n\n大工搞定。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"docker-maven-plugin",frontmatter:{title:"docker-maven-plugin",date:"2022-02-09T13:15:12.000Z",permalink:"/pages/779e01/"},regularPath:"/21.Simplify/01.Simplify%20Docs/09.docker-maven-plugin.html",relativePath:"21.Simplify/01.Simplify Docs/09.docker-maven-plugin.md",key:"v-1572d344",path:"/pages/779e01/",headers:[{level:2,title:"介绍",slug:"介绍",normalizedTitle:"介绍",charIndex:69},{level:2,title:"fabric8io和spotify的对比",slug:"fabric8io和spotify的对比",normalizedTitle:"fabric8io和spotify的对比",charIndex:434},{level:3,title:"1 Stars",slug:"_1-stars",normalizedTitle:"1 stars",charIndex:752},{level:3,title:"2 文档易用性",slug:"_2-文档易用性",normalizedTitle:"2 文档易用性",charIndex:897},{level:3,title:"3 更新频率",slug:"_3-更新频率",normalizedTitle:"3 更新频率",charIndex:1308},{level:3,title:"4 功能",slug:"_4-功能",normalizedTitle:"4 功能",charIndex:1380},{level:2,title:"dockerHost 配置",slug:"dockerhost-配置",normalizedTitle:"dockerhost 配置",charIndex:2091},{level:3,title:"1 docker官方的监听配置",slug:"_1-docker官方的监听配置",normalizedTitle:"1 docker官方的监听配置",charIndex:2146},{level:3,title:"2 fabric8io 的主机配置",slug:"_2-fabric8io-的主机配置",normalizedTitle:"2 fabric8io 的主机配置",charIndex:2302},{level:2,title:"fabric8io的docker-maven插件的使用",slug:"fabric8io的docker-maven插件的使用",normalizedTitle:"fabric8io的docker-maven插件的使用",charIndex:2759},{level:3,title:"1 参考配置",slug:"_1-参考配置",normalizedTitle:"1 参考配置",charIndex:2791},{level:3,title:"2 mvn指令说明",slug:"_2-mvn指令说明",normalizedTitle:"2 mvn指令说明",charIndex:5092},{level:3,title:"3 指令实践",slug:"_3-指令实践",normalizedTitle:"3 指令实践",charIndex:5671},{level:3,title:"4 注意事项",slug:"_4-注意事项",normalizedTitle:"4 注意事项",charIndex:5852},{level:2,title:"参考文档:",slug:"参考文档",normalizedTitle:"参考文档:",charIndex:6230}],headersStr:"介绍 fabric8io和spotify的对比 1 Stars 2 文档易用性 3 更新频率 4 功能 dockerHost 配置 1 docker官方的监听配置 2 fabric8io 的主机配置 fabric8io的docker-maven插件的使用 1 参考配置 2 mvn指令说明 3 指令实践 4 注意事项 参考文档:",content:'# docker-maven-plugin\n\n提示\n\n这是一个Maven插件，用于构建Docker映像和管理用于集成测试的容器。\n\n\n# 介绍\n\nfabric8io的插件不仅支持通过dockerfile和docker-compose文件进行docker镜像构造(build),镜像推送(push),还支持对容器操作(如run,stop,remove等),也就是说它可以实现对Docker从构建镜像到运行和推送完整的控制流程,这是它独树一帜的地方.\n\n在众多docker-maven插件中,fabric8io的docker-maven-plugin毫无疑问是最强大的.\n\n现在网上相关的教程大多是使用spotify的,本人一开始也是用spotify,但发现使用起来很有局限性,然后就找其他的docker-maven插件,最终挖掘到了fabric8io的插件.简单来说,spotify能做的事,fabric8io都能做,而且fabric8io选择更多,功能更多.\n\n\n# fabric8io和spotify的对比\n\n下面用fabric8io和spotify代指其插件\n\n首先,在对比之前,要先认识一个观点,不管使用哪一种docker插件来构造image,都比不上直接用Dockerfile编写简单,而且可复用,不用学习不同插件不同的构造规则.这一点也在spotify的github项目的README文档里有特别强调,为此他们还特地开发了一个dockerfile-maven插件,这个插件强制要求使用Dockerfile,他们认为这才是正途,事实也是如此.\n\n很多人选择用spotify是因为所谓"从Stars、文档易用性以及更新频率三个纬度考虑",那下面我们就先从这三者来做简单的对比.再对比功能.\n\n\n# 1 Stars\n\n截至目前(2021年3月),各插件star数如下：\n\n * fabric8io/dockerfile-maven 1.4K+\n * spotify/dockerfile-maven 2.4+\n * spotify/docker-maven-plugin 2.5+\n\n\n# 2 文档易用性\n\n这里毫无疑问是spotify的文档更容易用,因为不管是docker-maven-plugin还是dockerfile-maven,spotify的所谓文档都只是放在对应github项目的首页,就是一份README的MarkDown文档而已,可读性很强,可以简单上手.相信这也是spotify插件拥有大量Stars的原因.\n\n相比之下,fabric8io的github的README相当于什么都没写,就只是简单粗暴地给了用户手册的访问链接,虽然也有一个简单的"introlmd",但是对于构建项目没有太大的帮助,最后还是得来看用户手册\n\nfabric8io的用户手册内容相当丰富,这也是因为它功能比较强大.所以这里见仁见智.另外,如果使用Dockerfile和docker-compose文件的话,其实可以避开大部分的配置,剩下的配置都是因为它额外的功能,所以其文档虽然复杂,但配置却可以很简单.\n\n\n# 3 更新频率\n\n截至目前,fabric8io最近一次更新推送是在2020年9月28日,spotify的项目则是2019年10月15日。\n\n\n# 4 功能\n\n这才是要比较的重点，也是 fabric8io全面碾压spotify的地方\n\n# 1 对dockerfile和docker-compose文件配置的支持\n\n截至到2019年1月。 spotify才刚刚支持自定义dockerfile文件的位置，在这之前dockerfile必须安装一定的规则放置才行。对于docker-compose.yml文件的支持为零。\n\nfabric8io很早就支持自定义dockerfile文件和docker-compose.yml文件的导入了。不过目前对docker-compose.yml版本不支持3.0及更新。对于我目前来说是够用了，以后希望会更新支持。\n\n对于我这种喜欢把启动参数放在docker-compose文件的人来说，对docker-compose的支持很重要，而且这样就完全剥离了插件对docker构建和运行参数的繁琐配置了。\n\n# 2 对docker容器的控制\n\nfabric8io的有两大功能:\n\n 1. 构造并推送Docker镜像\n 2. 启动和停止Docker容器\n\n也就是说通过fabric8io的docker插件,我们可以实现对Docker从构建镜像到运行和推送完整的控制流程\n\n而包括spotify在内的其他插件则仅有第一部分功能,即构造并推送镜像,而没有实现对容器的控制.这也是我使用fabric8io而不使用spotify的最主要原因.\n\n# 2 对远程docker主机的连接支持\n\nspotify只支持在本机docker的镜像build、tag、push，而fabric8io可以用于远程主机，这个时候要注意 dockerHost 项的配置。\n\n\n# dockerHost 配置\n\n注意\n\ndockerHost的配置和docker官方的配置规则不太一样\n\n\n# 1 docker官方的监听配置\n\ndocker可以配置三种不同类型的Socket请求：unix，tcp，和 fd，其中 fd 和 unix 通常用于本机连接，tcp 用于 ip 连接。三者在 /etc/docker/daemon.json 的配置项为 hosts，需要注意不支持 http(https)\n\n\n# 2 fabric8io 的主机配置\n\nfabric8io的 的dockerHost支持 tcp 和http（https）两种格式\n\n# 1 tcp\n\n如果使用tcp则只能使用 2375 和 2376端口，分别对应不加密验证和加密验证 这个时候在url里配置其他端口是无效的。 （也就是说，服务器设置了监听 tcp:///0.0.0.0:9999，fabric8io的插件设置 tcp:公网ip:9999，也是无效的，这个时候如果启用了ssl加密的话就只能一起用 2376）\n\n# 2 http/https\n\n这个时候端口是可以自定义的 即是说直接用 http://公网ip（域名）:9999 即可。\n\n# 3 端口报错的提示\n\n端口问题连接时会报错 ： Cannot create docker access object org.apache.http.ProtocolException: The server failed to respond with a valid HTTP response\n\n\n# fabric8io的docker-maven插件的使用\n\n\n# 1 参考配置\n\n如果是想要快速启动,可以参考下面的配置\n\n远程Docker的控制和配置可以看这一篇文章:Docker客户端使用TLS保护连接远程Docker服务\n\n下面使用Dockerfile完成docker镜像构建,使用docker-compose.yml完成docker容器启动参数控制\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <properties>\n        <docker.plugin.version>0.34.1</docker.plugin.version>\n        <docker.host>https://10.17.1.101:2376</docker.host>\n        <docker.registry>10.17.1.101</docker.registry>\n        <docker.registry.namespace>library</docker.registry.namespace>\n        <docker.registry.username>admin</docker.registry.username>\n        <docker.registry.password>Harbor12345</docker.registry.password>\n    </properties>\n\n    <plugin>\n        <groupId>io.fabric8</groupId>\n        <artifactId>docker-maven-plugin</artifactId>\n        <version>${docker.plugin.version}</version>\n        \x3c!--全局配置--\x3e\n        <configuration>\n            \x3c!--远程docker主机地址,用于完成docker各项功能--\x3e\n            <dockerHost>${docker.host}</dockerHost>\n            \x3c!--docker远程访问所需证书地址,如果docker远程主机没有启用TLS验证则不需要配证书--\x3e\n            <certPath>${project.basedir}/docker/ssh</certPath>\n            \x3c!--registry地址,用于推送,拉取镜像--\x3e\n            <registry>${docker.registry}</registry>\n            <authConfig>\n                <push>\n                    <username>${docker.registry.username}</username>\n                    <password>${docker.registry.password}</password>\n                </push>\n            </authConfig>\n            <images>\n                <image>\n                    \x3c!--镜像名(含版本号)--\x3e\n                    <name>${docker.registry}/${docker.registry.namespace}/${project.name}:${project.version}</name>\n                    <build>\n                        \x3c!--使用dockerFile文件--\x3e\n                        <dockerFile>${project.basedir}/Dockerfile</dockerFile>\n                    </build>\n                </image>\n            </images>\n        </configuration>\n    </plugin>\n</project>\n\n\n可以看到,其实配置是相当简单清晰的,只要你确保Dockerfile和docker-compose.yml符合规范,能够使用,这个插件就可以代替你使用了,而你仅需要给它配置一些最基础的授权信息。\n\n提示\n\n如果配置启用TLS验证，Windows 10 系统需要将ca.pem,cert.pem,key.pem这三个文件复制到C:\\Users\\${USER}\\.docker目录下。\n\n\n# 2 mvn指令说明\n\nMAVEN指令                功能\ndocker:start           创建和启动容器\ndocker:stop            停止并销毁容器\ndocker:build           构建镜像\ndocker:watch           自动进行重建和重启\ndocker:push            将镜像推送到registry\ndocker:remove          从本地docker主机删除镜像\ndocker:logs            显示容器日志\ndocker:source          将docker build archive附加到Maven项目\ndocker:save            将镜像保存到文件\ndocker:volume-create   创建卷以在容器之间共享数据\ndocker:volume-remove   删除创建的卷\n\n注意,start和run是同义的,可以互相代替,另外,这里的stop和remove和在docker的不一样,docker的stop会保留容器,这里默认是停止并销毁(当然也可以在pom配置或mvn参数里指定keepContainer使其不销毁),但是我认为一般是不会用到暂停的,stop就是为了remove\n\n\n# 3 指令实践\n\n这里我贴一下我常用的maven指令 这样就可以完成从打包到容器运行的完整流程\n\n$ mvn clean package docker:stop docker:remove docker:build docker:run \n\n\n不是每一次运行都要推送的,如果要的话可以单独调用docker:push\n\n$ mvn docker:push\n\n\n\n# 4 注意事项\n\ndocker:run 这里有个坑,docker:run是在前台执行,也就是说,它并不能像使用docker一下直接docker start -d来实现后台运行,所以如果是在命令行下启动的话,按Ctrl+C就直接退出了,方法是命令行下启动的时候使用如下指令包装成后台运行\n\n$ nohup mvn docker:run & \n\n\n如果是用IDE控制的话就不用担心,因为IDE只是监控运行日志而已,IDE这里退出不影响其在远程服务器上的运行.\n\n另外,docker:run之后的指令不会运行,因为run之后它会监控Java程序的控制台输出直到运行结束,而一般Java Web程序是作为服务器一直运行的,所以就导致docker:run后面的指令不执行 解决方法有两个,第一,让docker:run放最后,第二,另外单独执行第二条mvn指令\n\n\n# 参考文档:\n\n * github项目地址:https://github.com/fabric8io/docker-maven-plugin\n * 用户手册:http://dmp.fabric8.io/',normalizedContent:'# docker-maven-plugin\n\n提示\n\n这是一个maven插件，用于构建docker映像和管理用于集成测试的容器。\n\n\n# 介绍\n\nfabric8io的插件不仅支持通过dockerfile和docker-compose文件进行docker镜像构造(build),镜像推送(push),还支持对容器操作(如run,stop,remove等),也就是说它可以实现对docker从构建镜像到运行和推送完整的控制流程,这是它独树一帜的地方.\n\n在众多docker-maven插件中,fabric8io的docker-maven-plugin毫无疑问是最强大的.\n\n现在网上相关的教程大多是使用spotify的,本人一开始也是用spotify,但发现使用起来很有局限性,然后就找其他的docker-maven插件,最终挖掘到了fabric8io的插件.简单来说,spotify能做的事,fabric8io都能做,而且fabric8io选择更多,功能更多.\n\n\n# fabric8io和spotify的对比\n\n下面用fabric8io和spotify代指其插件\n\n首先,在对比之前,要先认识一个观点,不管使用哪一种docker插件来构造image,都比不上直接用dockerfile编写简单,而且可复用,不用学习不同插件不同的构造规则.这一点也在spotify的github项目的readme文档里有特别强调,为此他们还特地开发了一个dockerfile-maven插件,这个插件强制要求使用dockerfile,他们认为这才是正途,事实也是如此.\n\n很多人选择用spotify是因为所谓"从stars、文档易用性以及更新频率三个纬度考虑",那下面我们就先从这三者来做简单的对比.再对比功能.\n\n\n# 1 stars\n\n截至目前(2021年3月),各插件star数如下：\n\n * fabric8io/dockerfile-maven 1.4k+\n * spotify/dockerfile-maven 2.4+\n * spotify/docker-maven-plugin 2.5+\n\n\n# 2 文档易用性\n\n这里毫无疑问是spotify的文档更容易用,因为不管是docker-maven-plugin还是dockerfile-maven,spotify的所谓文档都只是放在对应github项目的首页,就是一份readme的markdown文档而已,可读性很强,可以简单上手.相信这也是spotify插件拥有大量stars的原因.\n\n相比之下,fabric8io的github的readme相当于什么都没写,就只是简单粗暴地给了用户手册的访问链接,虽然也有一个简单的"introlmd",但是对于构建项目没有太大的帮助,最后还是得来看用户手册\n\nfabric8io的用户手册内容相当丰富,这也是因为它功能比较强大.所以这里见仁见智.另外,如果使用dockerfile和docker-compose文件的话,其实可以避开大部分的配置,剩下的配置都是因为它额外的功能,所以其文档虽然复杂,但配置却可以很简单.\n\n\n# 3 更新频率\n\n截至目前,fabric8io最近一次更新推送是在2020年9月28日,spotify的项目则是2019年10月15日。\n\n\n# 4 功能\n\n这才是要比较的重点，也是 fabric8io全面碾压spotify的地方\n\n# 1 对dockerfile和docker-compose文件配置的支持\n\n截至到2019年1月。 spotify才刚刚支持自定义dockerfile文件的位置，在这之前dockerfile必须安装一定的规则放置才行。对于docker-compose.yml文件的支持为零。\n\nfabric8io很早就支持自定义dockerfile文件和docker-compose.yml文件的导入了。不过目前对docker-compose.yml版本不支持3.0及更新。对于我目前来说是够用了，以后希望会更新支持。\n\n对于我这种喜欢把启动参数放在docker-compose文件的人来说，对docker-compose的支持很重要，而且这样就完全剥离了插件对docker构建和运行参数的繁琐配置了。\n\n# 2 对docker容器的控制\n\nfabric8io的有两大功能:\n\n 1. 构造并推送docker镜像\n 2. 启动和停止docker容器\n\n也就是说通过fabric8io的docker插件,我们可以实现对docker从构建镜像到运行和推送完整的控制流程\n\n而包括spotify在内的其他插件则仅有第一部分功能,即构造并推送镜像,而没有实现对容器的控制.这也是我使用fabric8io而不使用spotify的最主要原因.\n\n# 2 对远程docker主机的连接支持\n\nspotify只支持在本机docker的镜像build、tag、push，而fabric8io可以用于远程主机，这个时候要注意 dockerhost 项的配置。\n\n\n# dockerhost 配置\n\n注意\n\ndockerhost的配置和docker官方的配置规则不太一样\n\n\n# 1 docker官方的监听配置\n\ndocker可以配置三种不同类型的socket请求：unix，tcp，和 fd，其中 fd 和 unix 通常用于本机连接，tcp 用于 ip 连接。三者在 /etc/docker/daemon.json 的配置项为 hosts，需要注意不支持 http(https)\n\n\n# 2 fabric8io 的主机配置\n\nfabric8io的 的dockerhost支持 tcp 和http（https）两种格式\n\n# 1 tcp\n\n如果使用tcp则只能使用 2375 和 2376端口，分别对应不加密验证和加密验证 这个时候在url里配置其他端口是无效的。 （也就是说，服务器设置了监听 tcp:///0.0.0.0:9999，fabric8io的插件设置 tcp:公网ip:9999，也是无效的，这个时候如果启用了ssl加密的话就只能一起用 2376）\n\n# 2 http/https\n\n这个时候端口是可以自定义的 即是说直接用 http://公网ip（域名）:9999 即可。\n\n# 3 端口报错的提示\n\n端口问题连接时会报错 ： cannot create docker access object org.apache.http.protocolexception: the server failed to respond with a valid http response\n\n\n# fabric8io的docker-maven插件的使用\n\n\n# 1 参考配置\n\n如果是想要快速启动,可以参考下面的配置\n\n远程docker的控制和配置可以看这一篇文章:docker客户端使用tls保护连接远程docker服务\n\n下面使用dockerfile完成docker镜像构建,使用docker-compose.yml完成docker容器启动参数控制\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <properties>\n        <docker.plugin.version>0.34.1</docker.plugin.version>\n        <docker.host>https://10.17.1.101:2376</docker.host>\n        <docker.registry>10.17.1.101</docker.registry>\n        <docker.registry.namespace>library</docker.registry.namespace>\n        <docker.registry.username>admin</docker.registry.username>\n        <docker.registry.password>harbor12345</docker.registry.password>\n    </properties>\n\n    <plugin>\n        <groupid>io.fabric8</groupid>\n        <artifactid>docker-maven-plugin</artifactid>\n        <version>${docker.plugin.version}</version>\n        \x3c!--全局配置--\x3e\n        <configuration>\n            \x3c!--远程docker主机地址,用于完成docker各项功能--\x3e\n            <dockerhost>${docker.host}</dockerhost>\n            \x3c!--docker远程访问所需证书地址,如果docker远程主机没有启用tls验证则不需要配证书--\x3e\n            <certpath>${project.basedir}/docker/ssh</certpath>\n            \x3c!--registry地址,用于推送,拉取镜像--\x3e\n            <registry>${docker.registry}</registry>\n            <authconfig>\n                <push>\n                    <username>${docker.registry.username}</username>\n                    <password>${docker.registry.password}</password>\n                </push>\n            </authconfig>\n            <images>\n                <image>\n                    \x3c!--镜像名(含版本号)--\x3e\n                    <name>${docker.registry}/${docker.registry.namespace}/${project.name}:${project.version}</name>\n                    <build>\n                        \x3c!--使用dockerfile文件--\x3e\n                        <dockerfile>${project.basedir}/dockerfile</dockerfile>\n                    </build>\n                </image>\n            </images>\n        </configuration>\n    </plugin>\n</project>\n\n\n可以看到,其实配置是相当简单清晰的,只要你确保dockerfile和docker-compose.yml符合规范,能够使用,这个插件就可以代替你使用了,而你仅需要给它配置一些最基础的授权信息。\n\n提示\n\n如果配置启用tls验证，windows 10 系统需要将ca.pem,cert.pem,key.pem这三个文件复制到c:\\users\\${user}\\.docker目录下。\n\n\n# 2 mvn指令说明\n\nmaven指令                功能\ndocker:start           创建和启动容器\ndocker:stop            停止并销毁容器\ndocker:build           构建镜像\ndocker:watch           自动进行重建和重启\ndocker:push            将镜像推送到registry\ndocker:remove          从本地docker主机删除镜像\ndocker:logs            显示容器日志\ndocker:source          将docker build archive附加到maven项目\ndocker:save            将镜像保存到文件\ndocker:volume-create   创建卷以在容器之间共享数据\ndocker:volume-remove   删除创建的卷\n\n注意,start和run是同义的,可以互相代替,另外,这里的stop和remove和在docker的不一样,docker的stop会保留容器,这里默认是停止并销毁(当然也可以在pom配置或mvn参数里指定keepcontainer使其不销毁),但是我认为一般是不会用到暂停的,stop就是为了remove\n\n\n# 3 指令实践\n\n这里我贴一下我常用的maven指令 这样就可以完成从打包到容器运行的完整流程\n\n$ mvn clean package docker:stop docker:remove docker:build docker:run \n\n\n不是每一次运行都要推送的,如果要的话可以单独调用docker:push\n\n$ mvn docker:push\n\n\n\n# 4 注意事项\n\ndocker:run 这里有个坑,docker:run是在前台执行,也就是说,它并不能像使用docker一下直接docker start -d来实现后台运行,所以如果是在命令行下启动的话,按ctrl+c就直接退出了,方法是命令行下启动的时候使用如下指令包装成后台运行\n\n$ nohup mvn docker:run & \n\n\n如果是用ide控制的话就不用担心,因为ide只是监控运行日志而已,ide这里退出不影响其在远程服务器上的运行.\n\n另外,docker:run之后的指令不会运行,因为run之后它会监控java程序的控制台输出直到运行结束,而一般java web程序是作为服务器一直运行的,所以就导致docker:run后面的指令不执行 解决方法有两个,第一,让docker:run放最后,第二,另外单独执行第二条mvn指令\n\n\n# 参考文档:\n\n * github项目地址:https://github.com/fabric8io/docker-maven-plugin\n * 用户手册:http://dmp.fabric8.io/',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"Inner注解使用说明",frontmatter:{title:"Inner注解使用说明",date:"2022-02-09T13:15:12.000Z",permalink:"/pages/9dfd5c/"},regularPath:"/21.Simplify/01.Simplify%20Docs/10.Inner%E6%B3%A8%E8%A7%A3%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.html",relativePath:"21.Simplify/01.Simplify Docs/10.Inner注解使用说明.md",key:"v-613443b0",path:"/pages/9dfd5c/",headers:[{level:2,title:"Inner的诞生",slug:"inner的诞生",normalizedTitle:"inner的诞生",charIndex:18},{level:2,title:"Inner的处理流程",slug:"inner的处理流程",normalizedTitle:"inner的处理流程",charIndex:746},{level:3,title:"统一的ignore-url处理",slug:"统一的ignore-url处理",normalizedTitle:"统一的ignore-url处理",charIndex:761},{level:2,title:"统一的安全性处理",slug:"统一的安全性处理",normalizedTitle:"统一的安全性处理",charIndex:2847},{level:2,title:"合理的使用@Inner注解",slug:"合理的使用-inner注解",normalizedTitle:"合理的使用@inner注解",charIndex:3882},{level:2,title:"服务网关header清理",slug:"服务网关header清理",normalizedTitle:"服务网关header清理",charIndex:5503}],headersStr:"Inner的诞生 Inner的处理流程 统一的ignore-url处理 统一的安全性处理 合理的使用@Inner注解 服务网关header清理",content:'# Inner注解使用说明\n\n\n# Inner的诞生\n\nPig中Url的访问大致分为两种，一种是由Gateway网关从外网路由进来，一种是内网通过Feign进行内部服务调用。那我们结合Security就存在以下几种应用场景：\n\n 1. 外部从Gateway访问，需要鉴权（eg.CURD操作）。这种是最常使用的，用户登录后正常访问接口，不需要我们做什么处理（可能有的接口需要加权限字段）。\n 2. 外部从Gateway访问，不需要鉴权（eg.短信验证码）。需要我们将uri加入到security.oauth2.client.ignore-urls配置中，可以不需要鉴权访问\n 3. 内部服务间用Feign访问，不需要鉴权（eg.Auth查询用户信息）。也是需要我们将uri加入到security.oauth2.client.ignore-urls配置中，那与第二种的区别就是这种情况下大多数都是服务可以请求另一个服务的所有数据，不受约束，那我们如果仅仅只配置ignore-url的话，外部所有人都可以通过url请求到我们内部的链接，安全达不到保障。\n\n鉴于上述第三种情况，我们配置了ignore-url和Feign，此时该接口不需要鉴权，服务内部通过Feign访问，服务外部通过url也可以访问，所以Pig中，加入了一种@RequestHeader(SecurityConstants.FROM)的处理方式。即在接口方法中，对头部进行判断，只有请求带上相应的Header参数时，才允许通过访问，否则抛出异常。那这时候其实我们在外网通过Gateway访问的时候，也可以手动带上这个Header参数，来达到这个目的。所以我们便在Gateway中设置了一个GlobalFilter过滤器：\n\n\n# Inner的处理流程\n\n\n# 统一的ignore-url处理\n\n首先我们来看看这个注解的代码\n\n@Target({ElementType.METHOD, ElementType.TYPE})\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\npublic @interface Inner {\n   /**\n    * 是否AOP统一处理(可以理解为是否仅允许Feign之间调用)\n    *\n    * @return false, true\n    */\n   boolean value() default true;\n\n   /**\n    * 需要特殊判空的字段(预留)\n    *\n    * @return {}\n    */\n   String[] field() default {};\n}\n\n\n首先，在我们项目加载阶段，我们获取有Inner注解的类和方法，然后获取我们配置的uri，经过正则替换后面的可变参数为*，然后将此uri加入到ignore-url中。此时我们就能达到所有Inner配置的方法/类上的接口地址，都统一在项目加载阶段自动帮我们加到ignore-url中，不需要我们手动配置，免去了很多开发工作，同时也能避免我们忘记配置，而浪费开发时间。核心代码如下：\n\n@Slf4j\n@Configuration\n@ConditionalOnExpression("!\'${security.oauth2.client.ignore-urls}\'.isEmpty()")\n@ConfigurationProperties(prefix = "security.oauth2.client")\npublic class PermitAllUrlProperties implements InitializingBean {\n   private static final Pattern PATTERN = Pattern.compile("\\\\{(.*?)\\\\}");\n   @Autowired\n   private WebApplicationContext applicationContext;\n   @Getter\n   @Setter\n   private List<String> ignoreUrls = new ArrayList<>();\n   @Override\n   public void afterPropertiesSet() {\n      RequestMappingHandlerMapping mapping = applicationContext.getBean(RequestMappingHandlerMapping.class);\n      Map<RequestMappingInfo, HandlerMethod> map = mapping.getHandlerMethods();\n      map.keySet().forEach(info -> {\n         HandlerMethod handlerMethod = map.get(info);\n         // 获取方法上边的注解 替代path variable 为 *\n         Inner method = AnnotationUtils.findAnnotation(handlerMethod.getMethod(), Inner.class);\n         Optional.ofNullable(method)\n               .ifPresent(inner -> info.getPatternsCondition().getPatterns()\n                     .forEach(url -> ignoreUrls.add(ReUtil.replaceAll(url, PATTERN, StringPool.ASTERISK))));\n         // 获取类上边的注解, 替代path variable 为 *\n         Inner controller = AnnotationUtils.findAnnotation(handlerMethod.getBeanType(), Inner.class);\n         Optional.ofNullable(controller)\n               .ifPresent(inner -> info.getPatternsCondition().getPatterns()\n                     .forEach(url -> ignoreUrls.add(ReUtil.replaceAll(url, PATTERN, StringPool.ASTERISK))));\n      });\n   }\n}\n\n\n\n# 统一的安全性处理\n\n那上面讲到的，如果我们不希望这个url可以直接被外网调用，仅能在Feign服务中调用，改如何统一处理呢？\n\n我们使用一个Spring-AOP，在对所有Inner注解的方法做一个环绕增强的切点，进行统一的处理。在上面我们提到的Inner的value参数，当该参数为true时，我们对方法的入参进行判断，仅当符合我们定制的入参规则时（Pigx这里是用的@RequestHeader(SecurityConstants.FROM) 与SecurityConstants.FROM_IN做比较）,我们对它进行放行，不符合时，抛出异常；当value为false时，咱不做任何处理，此时Inner仅起到了一个ignore-url的作用。\n\n@Slf4j\n@Aspect\n@AllArgsConstructor\npublic class PigSecurityInnerAspect {\n   private final HttpServletRequest request;\n\n   @SneakyThrows\n   @Around("@annotation(inner)")\n   public Object around(ProceedingJoinPoint point, Inner inner) {\n      String header = request.getHeader(SecurityConstants.FROM);\n      if (inner.value() && !StrUtil.equals(SecurityConstants.FROM_IN, header)) {\n         log.warn("访问接口 {} 没有权限", point.getSignature().getName());\n         throw new AccessDeniedException("Access is denied");\n      }\n      return point.proceed();\n   }\n}\n\n\n通过这两步呢，我们首先是在加载时通过找到Inner注解，将相应的uri加入到ignore-url中，达到自动化配置的目的；之后我们又使用切面对Inner的方法进行环绕处理，达到安全控制。对比之前的处理方式，现在我们使用一个@Inner注解，就能很快的满足上面说的两种场景，大大节省了我们的开发时间。\n\n\n# 合理的使用@Inner注解\n\n上面提到的两种应用场景，在我们的代码中，其实都是可以使用Inner注解的，下面结合Feign做一个简单的示例，示例场景就是我们的用户密码登录中的一环：\n\n 1. 在接口上使用@Inner注解，使得url无需鉴权\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n/**\n * 获取指定用户全部信息\n *\n * @return 用户信息\n */\n@Inner\n@GetMapping("/info/{username}")\npublic R info(@PathVariable String username) {\n   SysUser user = userService.getOne(Wrappers.<SysUser>query()\n         .lambda().eq(SysUser::getUsername, username));\n   if (user == null) {\n      return R.failed(null, String.format("用户信息为空 %s", username));\n   }\n   return R.ok(userService.findUserInfo(user));\n}\n\n\n 1. 编写Feign接口\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n@FeignClient(contextId = "remoteUserService", value = ServiceNameConstants.UMPS_SERVICE)\npublic interface RemoteUserService {\n    /**\n     * 通过用户名查询用户、角色信息\n     *\n     * @param username 用户名\n     * @param from     调用标志\n     * @return R\n     */\n    @GetMapping("/user/info/{username}")\n    R<UserInfo> info(@PathVariable("username") String username\n          , @RequestHeader(SecurityConstants.FROM) String from);\n}\n\n\n 1. Feign-Client中调用接口，带上SecurityConstants.FROM_IN参数为内部识别\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n/**\n * 用户密码登录\n *\n * @param username 用户名\n * @return\n * @throws UsernameNotFoundException\n */\n@Override\n@SneakyThrows\npublic UserDetails loadUserByUsername(String username) {\n   Cache cache = cacheManager.getCache(CacheConstants.USER_DETAILS);\n   if (cache != null && cache.get(username) != null) {\n      return (PigxUser) cache.get(username).get();\n   }\n   R<UserInfo> result = remoteUserService.info(username, SecurityConstants.FROM_IN);\n   UserDetails userDetails = getUserDetails(result);\n   cache.put(username, userDetails);\n   return userDetails;\n}\n\n\n\n# 服务网关header清理\n\n@Component\npublic class HttpRequestGlobalFilter implements GlobalFilter, Ordered {\n\n    @Override\n    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {\n\n        // 1、清理请求头中from参数，保证接口安全。\n        ServerHttpRequest request = exchange.getRequest().mutate()\n                .headers(httpHeaders -> httpHeaders.remove(SecurityConstants.FROM)).build();\n        // 2、重写StripPrefix\n        addOriginalRequestUrl(exchange, request.getURI());\n        String path = request.getURI().getRawPath();\n        String newPath = "/" + Arrays.stream(StringUtils.tokenizeToStringArray(path, "/")).skip(1)\n                .collect(Collectors.joining("/"));\n        ServerHttpRequest newRequest = request.mutate().path(newPath).build();\n        exchange.getAttributes().put(GATEWAY_REQUEST_URL_ATTR, newRequest.getURI());\n\n        return chain.filter(exchange.mutate().request(newRequest).build());\n    }\n\n    @Override\n    public int getOrder() {\n        return -1000;\n    }\n}\n\n\n现在/info/{username} 这个uri从网关外部我们访问是报错的（一般来说服务都是走网关暴露接口），而Feign内部带上参数是可以正常访问的。大功告成喝杯啤酒。',normalizedContent:'# inner注解使用说明\n\n\n# inner的诞生\n\npig中url的访问大致分为两种，一种是由gateway网关从外网路由进来，一种是内网通过feign进行内部服务调用。那我们结合security就存在以下几种应用场景：\n\n 1. 外部从gateway访问，需要鉴权（eg.curd操作）。这种是最常使用的，用户登录后正常访问接口，不需要我们做什么处理（可能有的接口需要加权限字段）。\n 2. 外部从gateway访问，不需要鉴权（eg.短信验证码）。需要我们将uri加入到security.oauth2.client.ignore-urls配置中，可以不需要鉴权访问\n 3. 内部服务间用feign访问，不需要鉴权（eg.auth查询用户信息）。也是需要我们将uri加入到security.oauth2.client.ignore-urls配置中，那与第二种的区别就是这种情况下大多数都是服务可以请求另一个服务的所有数据，不受约束，那我们如果仅仅只配置ignore-url的话，外部所有人都可以通过url请求到我们内部的链接，安全达不到保障。\n\n鉴于上述第三种情况，我们配置了ignore-url和feign，此时该接口不需要鉴权，服务内部通过feign访问，服务外部通过url也可以访问，所以pig中，加入了一种@requestheader(securityconstants.from)的处理方式。即在接口方法中，对头部进行判断，只有请求带上相应的header参数时，才允许通过访问，否则抛出异常。那这时候其实我们在外网通过gateway访问的时候，也可以手动带上这个header参数，来达到这个目的。所以我们便在gateway中设置了一个globalfilter过滤器：\n\n\n# inner的处理流程\n\n\n# 统一的ignore-url处理\n\n首先我们来看看这个注解的代码\n\n@target({elementtype.method, elementtype.type})\n@retention(retentionpolicy.runtime)\n@documented\npublic @interface inner {\n   /**\n    * 是否aop统一处理(可以理解为是否仅允许feign之间调用)\n    *\n    * @return false, true\n    */\n   boolean value() default true;\n\n   /**\n    * 需要特殊判空的字段(预留)\n    *\n    * @return {}\n    */\n   string[] field() default {};\n}\n\n\n首先，在我们项目加载阶段，我们获取有inner注解的类和方法，然后获取我们配置的uri，经过正则替换后面的可变参数为*，然后将此uri加入到ignore-url中。此时我们就能达到所有inner配置的方法/类上的接口地址，都统一在项目加载阶段自动帮我们加到ignore-url中，不需要我们手动配置，免去了很多开发工作，同时也能避免我们忘记配置，而浪费开发时间。核心代码如下：\n\n@slf4j\n@configuration\n@conditionalonexpression("!\'${security.oauth2.client.ignore-urls}\'.isempty()")\n@configurationproperties(prefix = "security.oauth2.client")\npublic class permitallurlproperties implements initializingbean {\n   private static final pattern pattern = pattern.compile("\\\\{(.*?)\\\\}");\n   @autowired\n   private webapplicationcontext applicationcontext;\n   @getter\n   @setter\n   private list<string> ignoreurls = new arraylist<>();\n   @override\n   public void afterpropertiesset() {\n      requestmappinghandlermapping mapping = applicationcontext.getbean(requestmappinghandlermapping.class);\n      map<requestmappinginfo, handlermethod> map = mapping.gethandlermethods();\n      map.keyset().foreach(info -> {\n         handlermethod handlermethod = map.get(info);\n         // 获取方法上边的注解 替代path variable 为 *\n         inner method = annotationutils.findannotation(handlermethod.getmethod(), inner.class);\n         optional.ofnullable(method)\n               .ifpresent(inner -> info.getpatternscondition().getpatterns()\n                     .foreach(url -> ignoreurls.add(reutil.replaceall(url, pattern, stringpool.asterisk))));\n         // 获取类上边的注解, 替代path variable 为 *\n         inner controller = annotationutils.findannotation(handlermethod.getbeantype(), inner.class);\n         optional.ofnullable(controller)\n               .ifpresent(inner -> info.getpatternscondition().getpatterns()\n                     .foreach(url -> ignoreurls.add(reutil.replaceall(url, pattern, stringpool.asterisk))));\n      });\n   }\n}\n\n\n\n# 统一的安全性处理\n\n那上面讲到的，如果我们不希望这个url可以直接被外网调用，仅能在feign服务中调用，改如何统一处理呢？\n\n我们使用一个spring-aop，在对所有inner注解的方法做一个环绕增强的切点，进行统一的处理。在上面我们提到的inner的value参数，当该参数为true时，我们对方法的入参进行判断，仅当符合我们定制的入参规则时（pigx这里是用的@requestheader(securityconstants.from) 与securityconstants.from_in做比较）,我们对它进行放行，不符合时，抛出异常；当value为false时，咱不做任何处理，此时inner仅起到了一个ignore-url的作用。\n\n@slf4j\n@aspect\n@allargsconstructor\npublic class pigsecurityinneraspect {\n   private final httpservletrequest request;\n\n   @sneakythrows\n   @around("@annotation(inner)")\n   public object around(proceedingjoinpoint point, inner inner) {\n      string header = request.getheader(securityconstants.from);\n      if (inner.value() && !strutil.equals(securityconstants.from_in, header)) {\n         log.warn("访问接口 {} 没有权限", point.getsignature().getname());\n         throw new accessdeniedexception("access is denied");\n      }\n      return point.proceed();\n   }\n}\n\n\n通过这两步呢，我们首先是在加载时通过找到inner注解，将相应的uri加入到ignore-url中，达到自动化配置的目的；之后我们又使用切面对inner的方法进行环绕处理，达到安全控制。对比之前的处理方式，现在我们使用一个@inner注解，就能很快的满足上面说的两种场景，大大节省了我们的开发时间。\n\n\n# 合理的使用@inner注解\n\n上面提到的两种应用场景，在我们的代码中，其实都是可以使用inner注解的，下面结合feign做一个简单的示例，示例场景就是我们的用户密码登录中的一环：\n\n 1. 在接口上使用@inner注解，使得url无需鉴权\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n/**\n * 获取指定用户全部信息\n *\n * @return 用户信息\n */\n@inner\n@getmapping("/info/{username}")\npublic r info(@pathvariable string username) {\n   sysuser user = userservice.getone(wrappers.<sysuser>query()\n         .lambda().eq(sysuser::getusername, username));\n   if (user == null) {\n      return r.failed(null, string.format("用户信息为空 %s", username));\n   }\n   return r.ok(userservice.finduserinfo(user));\n}\n\n\n 1. 编写feign接口\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n@feignclient(contextid = "remoteuserservice", value = servicenameconstants.umps_service)\npublic interface remoteuserservice {\n    /**\n     * 通过用户名查询用户、角色信息\n     *\n     * @param username 用户名\n     * @param from     调用标志\n     * @return r\n     */\n    @getmapping("/user/info/{username}")\n    r<userinfo> info(@pathvariable("username") string username\n          , @requestheader(securityconstants.from) string from);\n}\n\n\n 1. feign-client中调用接口，带上securityconstants.from_in参数为内部识别\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n/**\n * 用户密码登录\n *\n * @param username 用户名\n * @return\n * @throws usernamenotfoundexception\n */\n@override\n@sneakythrows\npublic userdetails loaduserbyusername(string username) {\n   cache cache = cachemanager.getcache(cacheconstants.user_details);\n   if (cache != null && cache.get(username) != null) {\n      return (pigxuser) cache.get(username).get();\n   }\n   r<userinfo> result = remoteuserservice.info(username, securityconstants.from_in);\n   userdetails userdetails = getuserdetails(result);\n   cache.put(username, userdetails);\n   return userdetails;\n}\n\n\n\n# 服务网关header清理\n\n@component\npublic class httprequestglobalfilter implements globalfilter, ordered {\n\n    @override\n    public mono<void> filter(serverwebexchange exchange, gatewayfilterchain chain) {\n\n        // 1、清理请求头中from参数，保证接口安全。\n        serverhttprequest request = exchange.getrequest().mutate()\n                .headers(httpheaders -> httpheaders.remove(securityconstants.from)).build();\n        // 2、重写stripprefix\n        addoriginalrequesturl(exchange, request.geturi());\n        string path = request.geturi().getrawpath();\n        string newpath = "/" + arrays.stream(stringutils.tokenizetostringarray(path, "/")).skip(1)\n                .collect(collectors.joining("/"));\n        serverhttprequest newrequest = request.mutate().path(newpath).build();\n        exchange.getattributes().put(gateway_request_url_attr, newrequest.geturi());\n\n        return chain.filter(exchange.mutate().request(newrequest).build());\n    }\n\n    @override\n    public int getorder() {\n        return -1000;\n    }\n}\n\n\n现在/info/{username} 这个uri从网关外部我们访问是报错的（一般来说服务都是走网关暴露接口），而feign内部带上参数是可以正常访问的。大功告成喝杯啤酒。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"Java8 Stream",frontmatter:{title:"Java8 Stream",date:"2022-02-09T13:15:12.000Z",permalink:"/pages/5f71ee/"},regularPath:"/21.Simplify/01.Simplify%20Docs/11.Java8%20Stream.html",relativePath:"21.Simplify/01.Simplify Docs/11.Java8 Stream.md",key:"v-2da1a437",path:"/pages/5f71ee/",headers:[{level:2,title:"1 Stream概述",slug:"_1-stream概述",normalizedTitle:"1 stream概述",charIndex:21},{level:2,title:"2 Stream的创建",slug:"_2-stream的创建",normalizedTitle:"2 stream的创建",charIndex:35},{level:2,title:"3 Stream的使用",slug:"_3-stream的使用",normalizedTitle:"3 stream的使用",charIndex:50},{level:3,title:"3.1 遍历/匹配(foreach/find/match)",slug:"_3-1-遍历-匹配-foreach-find-match",normalizedTitle:"3.1 遍历/匹配(foreach/find/match)",charIndex:67},{level:3,title:"3.2 筛选(filter)",slug:"_3-2-筛选-filter",normalizedTitle:"3.2 筛选(filter)",charIndex:102},{level:3,title:"3.3 聚合(max/min/count)",slug:"_3-3-聚合-max-min-count",normalizedTitle:"3.3 聚合(max/min/count)",charIndex:122},{level:3,title:"3.4 映射(map/flatMap)",slug:"_3-4-映射-map-flatmap",normalizedTitle:"3.4 映射(map/flatmap)",charIndex:149},{level:3,title:"3.5 归约(reduce)",slug:"_3-5-归约-reduce",normalizedTitle:"3.5 归约(reduce)",charIndex:174},{level:3,title:"3.6 收集(collect)",slug:"_3-6-收集-collect",normalizedTitle:"3.6 收集(collect)",charIndex:194},{level:3,title:"3.7 排序(sorted)",slug:"_3-7-排序-sorted",normalizedTitle:"3.7 排序(sorted)",charIndex:215},{level:3,title:"3.8 提取/组合",slug:"_3-8-提取-组合",normalizedTitle:"3.8 提取/组合",charIndex:235}],headersStr:"1 Stream概述 2 Stream的创建 3 Stream的使用 3.1 遍历/匹配(foreach/find/match) 3.2 筛选(filter) 3.3 聚合(max/min/count) 3.4 映射(map/flatMap) 3.5 归约(reduce) 3.6 收集(collect) 3.7 排序(sorted) 3.8 提取/组合",content:'# Java8 Stream\n\n\n\n * 1 Stream概述\n * 2 Stream的创建\n * 3 Stream的使用\n   * 3.1 遍历/匹配(foreach/find/match)\n   * 3.2 筛选(filter)\n   * 3.3 聚合(max/min/count)\n   * 3.4 映射(map/flatMap)\n   * 3.5 归约(reduce)\n   * 3.6 收集(collect)\n   * 3.7 排序(sorted)\n   * 3.8 提取/组合\n\n\n\n先贴上几个案例，水平高超的同学可以挑战一下：\n\n 1. 从员工集合中筛选出salary大于8000的员工，并放置到新的集合里。\n 2. 统计员工的最高薪资、平均薪资、薪资之和。\n 3. 将员工按薪资从高到低排序，同样薪资者年龄小者在前。\n 4. 将员工按性别分类，将员工按性别和地区分类，将员工按薪资是否高于8000分为两部分。\n\n用传统的迭代处理也不是很难，但代码就显得冗余了，跟Stream相比高下立判。\n\n\n# 1 Stream概述\n\nJava 8 是一个非常成功的版本，这个版本新增的Stream，配合同版本出现的 Lambda ，给我们操作集合（Collection）提供了极大的便利。\n\n那么什么是Stream？\n\n提示\n\nStream将要处理的元素集合看作一种流，在流的过程中，借助Stream API对流中的元素进行操作，比如：筛选、排序、聚合等。\n\nStream可以由数组或集合创建，对流的操作分为两种：\n\n 1. 中间操作，每次返回一个新的流，可以有多个。\n 2. 终端操作，每个流只能进行一次终端操作，终端操作结束后流无法再次使用。终端操作会产生一个新的集合或值。\n\n另外，Stream有几个特性：\n\n 1. stream不存储数据，而是按照特定的规则对数据进行计算，一般会输出结果。\n 2. stream不会改变数据源，通常情况下会产生一个新的集合或一个值。\n 3. stream具有延迟执行特性，只有调用终端操作时，中间操作才会执行。\n\n\n# 2 Stream的创建\n\nStream可以通过集合数组创建。\n\n1、通过 java.util.Collection.stream() 方法用集合创建流\n\nList<String> list = Arrays.asList("a", "b", "c");\n// 创建一个顺序流\nStream<String> stream = list.stream();\n// 创建一个并行流\nStream<String> parallelStream = list.parallelStream();\n\n\n2、使用java.util.Arrays.stream(T[] array)方法用数组创建流\n\nint[] array={1,3,5,6,8};\nIntStream stream = Arrays.stream(array);\n\n\n3、使用Stream的静态方法：of()、iterate()、generate()\n\nStream<Integer> stream = Stream.of(1, 2, 3, 4, 5, 6);\n\nStream<Integer> stream2 = Stream.iterate(0, (x) -> x + 3).limit(4);\nstream2.forEach(System.out::println);\n\nStream<Double> stream3 = Stream.generate(Math::random).limit(3);\nstream3.forEach(System.out::println);\n\n\n输出结果：\n\n> 0 3 6 9\n> \n> 0.6796156909271994\n> \n> 0.1914314208854283\n> \n> 0.8116932592396652\n\nstream和parallelStream的简单区分： stream是顺序流，由主线程按顺序对流执行操作，而parallelStream是并行流，内部以多线程并行执行的方式对流进行操作，但前提是流中的数据处理没有顺序要求。例如筛选集合中的奇数，两者的处理不同之处：\n\n\n\n如果流中的数据量足够大，并行流可以加快处速度。\n\n除了直接创建并行流，还可以通过parallel()把顺序流转换成并行流：\n\nOptional<Integer> findFirst = list.stream().parallel().filter(x->x>6).findFirst();\n\n\n\n# 3 Stream的使用\n\n在使用stream之前，先理解一个概念：Optional 。\n\n提示\n\nOptional类是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。 更详细说明请见：菜鸟教程Java 8 Optional类\n\n接下来，大批代码向你袭来！我将用20个案例将Stream的使用整得明明白白，只要跟着敲一遍代码，就能很好地掌握。\n\n\n\n案例使用的员工类\n\n这是后面案例中使用的员工类：\n\nList<Person> personList = new ArrayList<Person>();\npersonList.add(new Person("Tom", 8900, "male", "New York"));\npersonList.add(new Person("Jack", 7000, "male", "Washington"));\npersonList.add(new Person("Lily", 7800, "female", "Washington"));\npersonList.add(new Person("Anni", 8200, "female", "New York"));\npersonList.add(new Person("Owen", 9500, "male", "New York"));\npersonList.add(new Person("Alisa", 7900, "female", "New York"));\n\nclass Person {\n\tprivate String name;  // 姓名\n\tprivate int salary; // 薪资\n\tprivate int age; // 年龄\n\tprivate String sex; //性别\n\tprivate String area;  // 地区\n\n\t// 构造方法\n\tpublic Person(String name, int salary, int age,String sex,String area) {\n\t\tthis.name = name;\n\t\tthis.salary = salary;\n\t\tthis.age = age;\n\t\tthis.sex = sex;\n\t\tthis.area = area;\n\t}\n\t// 省略了get和set，请自行添加\n\n}\n\n\n\n# 3.1 遍历/匹配(foreach/find/match)\n\nStream也是支持类似集合的遍历和匹配元素的，只是Stream中的元素是以Optional类型存在的。Stream的遍历、匹配非常简单。\n\n\n\n// import已省略，请自行添加，后面代码亦是\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n        List<Integer> list = Arrays.asList(7, 6, 9, 3, 8, 2, 1);\n\n        // 遍历输出符合条件的元素\n        list.stream().filter(x -> x > 6).forEach(System.out::println);\n        // 匹配第一个\n        Optional<Integer> findFirst = list.stream().filter(x -> x > 6).findFirst();\n        // 匹配任意（适用于并行流）\n        Optional<Integer> findAny = list.parallelStream().filter(x -> x > 6).findAny();\n        // 是否包含符合特定条件的元素\n        boolean anyMatch = list.stream().anyMatch(x -> x > 6);\n        System.out.println("匹配第一个值：" + findFirst.get());\n        System.out.println("匹配任意一个值：" + findAny.get());\n        System.out.println("是否存在大于6的值：" + anyMatch);\n    }\n}\n\n\n\n# 3.2 筛选(filter)\n\n筛选，是按照一定的规则校验流中的元素，将符合条件的元素提取到新的流中的操作。\n\n\n\n案例一：筛选出Integer集合中大于7的元素，并打印出来\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Integer> list = Arrays.asList(6, 7, 3, 8, 1, 2, 9);\n\t\tStream<Integer> stream = list.stream();\n\t\tstream.filter(x -> x > 7).forEach(System.out::println);\n\t}\n}\n\n\n预期结果：\n\n> 8\n> \n> 9\n\n案例二： 筛选员工中工资高于8000的人，并形成新的集合。 形成新集合依赖collect（收集），后文有详细介绍。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Person> personList = new ArrayList<Person>();\n\t\tpersonList.add(new Person("Tom", 8900, 23, "male", "New York"));\n\t\tpersonList.add(new Person("Jack", 7000, 25, "male", "Washington"));\n\t\tpersonList.add(new Person("Lily", 7800, 21, "female", "Washington"));\n\t\tpersonList.add(new Person("Anni", 8200, 24, "female", "New York"));\n\t\tpersonList.add(new Person("Owen", 9500, 25, "male", "New York"));\n\t\tpersonList.add(new Person("Alisa", 7900, 26, "female", "New York"));\n\n\t\tList<String> fiterList = personList.stream().filter(x -> x.getSalary() > 8000).map(Person::getName)\n\t\t\t\t.collect(Collectors.toList());\n\t\tSystem.out.print("高于8000的员工姓名：" + fiterList);\n\t}\n}\n\n\n运行结果：\n\n> 高于8000的员工姓名：[Tom, Anni, Owen]\n\n\n# 3.3 聚合(max/min/count)\n\nmax、min、count这些字眼你一定不陌生，没错，在mysql中我们常用它们进行数据统计。Java stream中也引入了这些概念和用法，极大地方便了我们对集合、数组的数据统计工作。\n\n\n\n案例一：获取String集合中最长的元素。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<String> list = Arrays.asList("adnm", "admmt", "pot", "xbangd", "weoujgsd");\n\n\t\tOptional<String> max = list.stream().max(Comparator.comparing(String::length));\n\t\tSystem.out.println("最长的字符串：" + max.get());\n\t}\n}\n\n\n输出结果：\n\n> 最长的字符串：weoujgsd\n\n案例二：获取Integer集合中的最大值。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Integer> list = Arrays.asList(7, 6, 9, 4, 11, 6);\n\n\t\t// 自然排序\n\t\tOptional<Integer> max = list.stream().max(Integer::compareTo);\n\t\t// 自定义排序\n\t\tOptional<Integer> max2 = list.stream().max(new Comparator<Integer>() {\n\t\t\t@Override\n\t\t\tpublic int compare(Integer o1, Integer o2) {\n\t\t\t\treturn o1.compareTo(o2);\n\t\t\t}\n\t\t});\n\t\tSystem.out.println("自然排序的最大值：" + max.get());\n\t\tSystem.out.println("自定义排序的最大值：" + max2.get());\n\t}\n}\n\n\n输出结果：\n\n> 自然排序的最大值：11\n> \n> 自定义排序的最大值：11\n\n案例三：获取员工工资最高的人。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Person> personList = new ArrayList<Person>();\n\t\tpersonList.add(new Person("Tom", 8900, 23, "male", "New York"));\n\t\tpersonList.add(new Person("Jack", 7000, 25, "male", "Washington"));\n\t\tpersonList.add(new Person("Lily", 7800, 21, "female", "Washington"));\n\t\tpersonList.add(new Person("Anni", 8200, 24, "female", "New York"));\n\t\tpersonList.add(new Person("Owen", 9500, 25, "male", "New York"));\n\t\tpersonList.add(new Person("Alisa", 7900, 26, "female", "New York"));\n\n\t\tOptional<Person> max = personList.stream().max(Comparator.comparingInt(Person::getSalary));\n\t\tSystem.out.println("员工工资最大值：" + max.get().getSalary());\n\t}\n}\n\n\n输出结果：\n\n> 员工工资最大值：9500\n\n案例四：计算Integer集合中大于6的元素的个数。\n\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Integer> list = Arrays.asList(7, 6, 4, 8, 2, 11, 9);\n\n\t\tlong count = list.stream().filter(x -> x > 6).count();\n\t\tSystem.out.println("list中大于6的元素个数：" + count);\n\t}\n}\n\n\n输出结果：\n\n> list中大于6的元素个数：4\n\n\n# 3.4 映射(map/flatMap)\n\n映射，可以将一个流的元素按照一定的映射规则映射到另一个流中。分为map和flatMap：\n\n * map：接收一个函数作为参数，该函数会被应用到每个元素上，并将其映射成一个新的元素。\n * flatMap：接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流。\n\n\n\n\n\n案例一：英文字符串数组的元素全部改为大写。整数数组每个元素+3。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tString[] strArr = { "abcd", "bcdd", "defde", "fTr" };\n\t\tList<String> strList = Arrays.stream(strArr).map(String::toUpperCase).collect(Collectors.toList());\n\n\t\tList<Integer> intList = Arrays.asList(1, 3, 5, 7, 9, 11);\n\t\tList<Integer> intListNew = intList.stream().map(x -> x + 3).collect(Collectors.toList());\n\n\t\tSystem.out.println("每个元素大写：" + strList);\n\t\tSystem.out.println("每个元素+3：" + intListNew);\n\t}\n}\n\n\n输出结果：\n\n> 每个元素大写：[ABCD, BCDD, DEFDE, FTR]\n> \n> 每个元素+3：[4, 6, 8, 10, 12, 14]\n\n案例二：将员工的薪资全部增加1000。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Person> personList = new ArrayList<Person>();\n\t\tpersonList.add(new Person("Tom", 8900, 23, "male", "New York"));\n\t\tpersonList.add(new Person("Jack", 7000, 25, "male", "Washington"));\n\t\tpersonList.add(new Person("Lily", 7800, 21, "female", "Washington"));\n\t\tpersonList.add(new Person("Anni", 8200, 24, "female", "New York"));\n\t\tpersonList.add(new Person("Owen", 9500, 25, "male", "New York"));\n\t\tpersonList.add(new Person("Alisa", 7900, 26, "female", "New York"));\n\n\t\t// 不改变原来员工集合的方式\n\t\tList<Person> personListNew = personList.stream().map(person -> {\n\t\t\tPerson personNew = new Person(person.getName(), 0, 0, null, null);\n\t\t\tpersonNew.setSalary(person.getSalary() + 10000);\n\t\t\treturn personNew;\n\t\t}).collect(Collectors.toList());\n\t\tSystem.out.println("一次改动前：" + personList.get(0).getName() + "--\x3e" + personList.get(0).getSalary());\n\t\tSystem.out.println("一次改动后：" + personListNew.get(0).getName() + "--\x3e" + personListNew.get(0).getSalary());\n\n\t\t// 改变原来员工集合的方式\n\t\tList<Person> personListNew2 = personList.stream().map(person -> {\n\t\t\tperson.setSalary(person.getSalary() + 10000);\n\t\t\treturn person;\n\t\t}).collect(Collectors.toList());\n\t\tSystem.out.println("二次改动前：" + personList.get(0).getName() + "--\x3e" + personListNew.get(0).getSalary());\n\t\tSystem.out.println("二次改动后：" + personListNew2.get(0).getName() + "--\x3e" + personListNew.get(0).getSalary());\n\t}\n}\n\n\n输出结果：\n\n> 一次改动前：Tom–>8900\n> \n> 一次改动后：Tom–>18900\n> \n> 二次改动前：Tom–>18900\n> \n> 二次改动后：Tom–>18900\n\n案例三：将两个字符数组合并成一个新的字符数组。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<String> list = Arrays.asList("m,k,l,a", "1,3,5,7");\n\t\tList<String> listNew = list.stream().flatMap(s -> {\n\t\t\t// 将每个元素转换成一个stream\n\t\t\tString[] split = s.split(",");\n\t\t\tStream<String> s2 = Arrays.stream(split);\n\t\t\treturn s2;\n\t\t}).collect(Collectors.toList());\n\n\t\tSystem.out.println("处理前的集合：" + list);\n\t\tSystem.out.println("处理后的集合：" + listNew);\n\t}\n}\n\n\n输出结果：\n\n> 处理前的集合：[m-k-l-a, 1-3-5]\n> \n> 处理后的集合：[m, k, l, a, 1, 3, 5]\n\n\n# 3.5 归约(reduce)\n\n归约，也称缩减，顾名思义，是把一个流缩减成一个值，能实现对集合求和、求乘积和求最值操作。\n\n案例一：求Integer集合的元素之和、乘积和最大值。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Integer> list = Arrays.asList(1, 3, 2, 8, 11, 4);\n\t\t// 求和方式1\n\t\tOptional<Integer> sum = list.stream().reduce((x, y) -> x + y);\n\t\t// 求和方式2\n\t\tOptional<Integer> sum2 = list.stream().reduce(Integer::sum);\n\t\t// 求和方式3\n\t\tInteger sum3 = list.stream().reduce(0, Integer::sum);\n\t\t\n\t\t// 求乘积\n\t\tOptional<Integer> product = list.stream().reduce((x, y) -> x * y);\n\n\t\t// 求最大值方式1\n\t\tOptional<Integer> max = list.stream().reduce((x, y) -> x > y ? x : y);\n\t\t// 求最大值写法2\n\t\tInteger max2 = list.stream().reduce(1, Integer::max);\n\n\t\tSystem.out.println("list求和：" + sum.get() + "," + sum2.get() + "," + sum3);\n\t\tSystem.out.println("list求积：" + product.get());\n\t\tSystem.out.println("list求和：" + max.get() + "," + max2);\n\t}\n}\n\n\n输出结果：\n\n> list求和：29,29,29\n> \n> list求积：2112\n> \n> list求和：11,11\n\n案例二：求所有员工的工资之和和最高工资。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Person> personList = new ArrayList<Person>();\n\t\tpersonList.add(new Person("Tom", 8900, 23, "male", "New York"));\n\t\tpersonList.add(new Person("Jack", 7000, 25, "male", "Washington"));\n\t\tpersonList.add(new Person("Lily", 7800, 21, "female", "Washington"));\n\t\tpersonList.add(new Person("Anni", 8200, 24, "female", "New York"));\n\t\tpersonList.add(new Person("Owen", 9500, 25, "male", "New York"));\n\t\tpersonList.add(new Person("Alisa", 7900, 26, "female", "New York"));\n\n\t\t// 求工资之和方式1：\n\t\tOptional<Integer> sumSalary = personList.stream().map(Person::getSalary).reduce(Integer::sum);\n\t\t// 求工资之和方式2：\n\t\tInteger sumSalary2 = personList.stream().reduce(0, (sum, p) -> sum += p.getSalary(),\n\t\t\t\t(sum1, sum2) -> sum1 + sum2);\n\t\t// 求工资之和方式3：\n\t\tInteger sumSalary3 = personList.stream().reduce(0, (sum, p) -> sum += p.getSalary(), Integer::sum);\n\n\t\t// 求最高工资方式1：\n\t\tInteger maxSalary = personList.stream().reduce(0, (max, p) -> max > p.getSalary() ? max : p.getSalary(),\n\t\t\t\tInteger::max);\n\t\t// 求最高工资方式2：\n\t\tInteger maxSalary2 = personList.stream().reduce(0, (max, p) -> max > p.getSalary() ? max : p.getSalary(),\n\t\t\t\t(max1, max2) -> max1 > max2 ? max1 : max2);\n\n\t\tSystem.out.println("工资之和：" + sumSalary.get() + "," + sumSalary2 + "," + sumSalary3);\n\t\tSystem.out.println("最高工资：" + maxSalary + "," + maxSalary2);\n\t}\n}\n\n\n输出结果：\n\n> 工资之和：49300,49300,49300\n> \n> 最高工资：9500,9500\n\n\n# 3.6 收集(collect)\n\ncollect，收集，可以说是内容最繁多、功能最丰富的部分了。从字面上去理解，就是把一个流收集起来，最终可以是收集成一个值也可以收集成一个新的集合。\n\n提示\n\ncollect主要依赖java.util.stream.Collectors类内置的静态方法。\n\n# 3.6.1 归集(toList/toSet/toMap)\n\n因为流不存储数据，那么在流中的数据完成处理后，需要将流中的数据重新归集到新的集合里。toList、toSet和toMap比较常用，另外还有toCollection、toConcurrentMap等复杂一些的用法。\n\n下面用一个案例演示toList、toSet和toMap：\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Integer> list = Arrays.asList(1, 6, 3, 4, 6, 7, 9, 6, 20);\n\t\tList<Integer> listNew = list.stream().filter(x -> x % 2 == 0).collect(Collectors.toList());\n\t\tSet<Integer> set = list.stream().filter(x -> x % 2 == 0).collect(Collectors.toSet());\n\n\t\tList<Person> personList = new ArrayList<Person>();\n\t\tpersonList.add(new Person("Tom", 8900, 23, "male", "New York"));\n\t\tpersonList.add(new Person("Jack", 7000, 25, "male", "Washington"));\n\t\tpersonList.add(new Person("Lily", 7800, 21, "female", "Washington"));\n\t\tpersonList.add(new Person("Anni", 8200, 24, "female", "New York"));\n\t\t\n\t\tMap<?, Person> map = personList.stream().filter(p -> p.getSalary() > 8000)\n\t\t\t\t.collect(Collectors.toMap(Person::getName, p -> p));\n\t\tSystem.out.println("toList:" + listNew);\n\t\tSystem.out.println("toSet:" + set);\n\t\tSystem.out.println("toMap:" + map);\n\t}\n}\n\n\n运行结果：\n\n> toList：[6, 4, 6, 6, 20]\n> \n> toSet：[4, 20, 6]\n> \n> toMap：{Tom=mutest.Person@5fd0d5ae, Anni=mutest.Person@2d98a335}\n\n# 3.6.2 统计(count/averaging)\n\nCollectors提供了一系列用于数据统计的静态方法：\n\n * 计数：count\n\n * 平均值：averagingInt、averagingLong、averagingDouble\n\n * 最值：maxBy、minBy\n\n * 求和：summingInt、summingLong、summingDouble\n\n * 统计以上所有：summarizingInt、summarizingLong、summarizingDouble\n\n案例：统计员工人数、平均工资、工资总额、最高工资。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Person> personList = new ArrayList<Person>();\n\t\tpersonList.add(new Person("Tom", 8900, 23, "male", "New York"));\n\t\tpersonList.add(new Person("Jack", 7000, 25, "male", "Washington"));\n\t\tpersonList.add(new Person("Lily", 7800, 21, "female", "Washington"));\n\n\t\t// 求总数\n\t\tLong count = personList.stream().collect(Collectors.counting());\n\t\t// 求平均工资\n\t\tDouble average = personList.stream().collect(Collectors.averagingDouble(Person::getSalary));\n\t\t// 求最高工资\n\t\tOptional<Integer> max = personList.stream().map(Person::getSalary).collect(Collectors.maxBy(Integer::compare));\n\t\t// 求工资之和\n\t\tInteger sum = personList.stream().collect(Collectors.summingInt(Person::getSalary));\n\t\t// 一次性统计所有信息\n\t\tDoubleSummaryStatistics collect = personList.stream().collect(Collectors.summarizingDouble(Person::getSalary));\n\n\t\tSystem.out.println("员工总数：" + count);\n\t\tSystem.out.println("员工平均工资：" + average);\n\t\tSystem.out.println("员工工资总和：" + sum);\n\t\tSystem.out.println("员工工资所有统计：" + collect);\n\t}\n}\n\n\n运行结果：\n\n> 员工总数：3\n> \n> 员工平均工资：7900.0\n> \n> 员工工资总和：23700\n> \n> 员工工资所有统计：DoubleSummaryStatistics{count=3, sum=23700.000000,min=7000.000000, average=7900.000000, max=8900.000000}\n\n# 3.6.3 分组(partitioningBy/groupingBy)\n\n * 分区：将stream按条件分为两个Map，比如员工按薪资是否高于8000分为两部分。\n * 分组：将集合分为多个Map，比如员工按性别分组。有单级分组和多级分组。\n\n\n\n案例：将员工按薪资是否高于8000分为两部分；将员工按性别和地区分组\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Person> personList = new ArrayList<Person>();\n\t\tpersonList.add(new Person("Tom", 8900, "male", "New York"));\n\t\tpersonList.add(new Person("Jack", 7000, "male", "Washington"));\n\t\tpersonList.add(new Person("Lily", 7800, "female", "Washington"));\n\t\tpersonList.add(new Person("Anni", 8200, "female", "New York"));\n\t\tpersonList.add(new Person("Owen", 9500, "male", "New York"));\n\t\tpersonList.add(new Person("Alisa", 7900, "female", "New York"));\n\n\t\t// 将员工按薪资是否高于8000分组\n        Map<Boolean, List<Person>> part = personList.stream().collect(Collectors.partitioningBy(x -> x.getSalary() > 8000));\n        // 将员工按性别分组\n        Map<String, List<Person>> group = personList.stream().collect(Collectors.groupingBy(Person::getSex));\n        // 将员工先按性别分组，再按地区分组\n        Map<String, Map<String, List<Person>>> group2 = personList.stream().collect(Collectors.groupingBy(Person::getSex, Collectors.groupingBy(Person::getArea)));\n        System.out.println("员工按薪资是否大于8000分组情况：" + part);\n        System.out.println("员工按性别分组情况：" + group);\n        System.out.println("员工按性别、地区：" + group2);\n\t}\n}\n\n\n输出结果：\n\n员工按薪资是否大于8000分组情况：{false=[mutest.Person@2d98a335, mutest.Person@16b98e56, mutest.Person@7ef20235], true=[mutest.Person@27d6c5e0, mutest.Person@4f3f5b24, mutest.Person@15aeb7ab]}\n员工按性别分组情况：{female=[mutest.Person@16b98e56, mutest.Person@4f3f5b24, mutest.Person@7ef20235], male=[mutest.Person@27d6c5e0, mutest.Person@2d98a335, mutest.Person@15aeb7ab]}\n员工按性别、地区：{female={New York=[mutest.Person@4f3f5b24, mutest.Person@7ef20235], Washington=[mutest.Person@16b98e56]}, male={New York=[mutest.Person@27d6c5e0, mutest.Person@15aeb7ab], Washington=[mutest.Person@2d98a335]}}\n\n\n# 3.6.4 接合(joining)\n\njoining可以将stream中的元素用特定的连接符（没有的话，则直接连接）连接成一个字符串。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Person> personList = new ArrayList<Person>();\n\t\tpersonList.add(new Person("Tom", 8900, 23, "male", "New York"));\n\t\tpersonList.add(new Person("Jack", 7000, 25, "male", "Washington"));\n\t\tpersonList.add(new Person("Lily", 7800, 21, "female", "Washington"));\n\n\t\tString names = personList.stream().map(p -> p.getName()).collect(Collectors.joining(","));\n\t\tSystem.out.println("所有员工的姓名：" + names);\n\t\tList<String> list = Arrays.asList("A", "B", "C");\n\t\tString string = list.stream().collect(Collectors.joining("-"));\n\t\tSystem.out.println("拼接后的字符串：" + string);\n\t}\n}\n\n\n运行结果：\n\n> 所有员工的姓名：Tom,Jack,Lily\n> \n> 拼接后的字符串：A-B-C\n\n# 3.6.5 归约(reducing)\n\nCollectors类提供的reducing方法，相比于stream本身的reduce方法，增加了对自定义归约的支持。\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Person> personList = new ArrayList<Person>();\n\t\tpersonList.add(new Person("Tom", 8900, 23, "male", "New York"));\n\t\tpersonList.add(new Person("Jack", 7000, 25, "male", "Washington"));\n\t\tpersonList.add(new Person("Lily", 7800, 21, "female", "Washington"));\n\n\t\t// 每个员工减去起征点后的薪资之和（这个例子并不严谨，但一时没想到好的例子）\n\t\tInteger sum = personList.stream().collect(Collectors.reducing(0, Person::getSalary, (i, j) -> (i + j - 5000)));\n\t\tSystem.out.println("员工扣税薪资总和：" + sum);\n\n\t\t// stream的reduce\n\t\tOptional<Integer> sum2 = personList.stream().map(Person::getSalary).reduce(Integer::sum);\n\t\tSystem.out.println("员工薪资总和：" + sum2.get());\n\t}\n}\n\n\n运行结果：\n\n> 员工扣税薪资总和：8700\n> \n> 员工薪资总和：23700\n\n\n# 3.7 排序(sorted)\n\nsorted，中间操作。有两种排序：\n\n * sorted()：自然排序，流中元素需实现Comparable接口\n * sorted(Comparator com)：Comparator排序器自定义排序\n\n案例：将员工按工资由高到低（工资一样则按年龄由大到小）排序\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tList<Person> personList = new ArrayList<Person>();\n\n\t\tpersonList.add(new Person("Sherry", 9000, 24, "female", "New York"));\n\t\tpersonList.add(new Person("Tom", 8900, 22, "male", "Washington"));\n\t\tpersonList.add(new Person("Jack", 9000, 25, "male", "Washington"));\n\t\tpersonList.add(new Person("Lily", 8800, 26, "male", "New York"));\n\t\tpersonList.add(new Person("Alisa", 9000, 26, "female", "New York"));\n\n\t\t// 按工资升序排序（自然排序）\n\t\tList<String> newList = personList.stream().sorted(Comparator.comparing(Person::getSalary)).map(Person::getName)\n\t\t\t\t.collect(Collectors.toList());\n\t\t// 按工资倒序排序\n\t\tList<String> newList2 = personList.stream().sorted(Comparator.comparing(Person::getSalary).reversed())\n\t\t\t\t.map(Person::getName).collect(Collectors.toList());\n\t\t// 先按工资再按年龄升序排序\n\t\tList<String> newList3 = personList.stream()\n\t\t\t\t.sorted(Comparator.comparing(Person::getSalary).thenComparing(Person::getAge)).map(Person::getName)\n\t\t\t\t.collect(Collectors.toList());\n\t\t// 先按工资再按年龄自定义排序（降序）\n\t\tList<String> newList4 = personList.stream().sorted((p1, p2) -> {\n\t\t\tif (p1.getSalary() == p2.getSalary()) {\n\t\t\t\treturn p2.getAge() - p1.getAge();\n\t\t\t} else {\n\t\t\t\treturn p2.getSalary() - p1.getSalary();\n\t\t\t}\n\t\t}).map(Person::getName).collect(Collectors.toList());\n\n\t\tSystem.out.println("按工资升序排序：" + newList);\n\t\tSystem.out.println("按工资降序排序：" + newList2);\n\t\tSystem.out.println("先按工资再按年龄升序排序：" + newList3);\n\t\tSystem.out.println("先按工资再按年龄自定义降序排序：" + newList4);\n\t}\n}\n\n\n运行结果：\n\n> 按工资升序排序：[Lily, Tom, Sherry, Jack, Alisa]\n> \n> 按工资降序排序：[Sherry, Jack, Alisa, Tom, Lily]\n> \n> 先按工资再按年龄升序排序：[Lily, Tom, Sherry, Jack, Alisa]\n> \n> 先按工资再按年龄自定义降序排序：[Alisa, Jack, Sherry, Tom, Lily]\n\n\n# 3.8 提取/组合\n\n流也可以进行合并、去重、限制、跳过等操作。\n\n\n\n\n\n\n\npublic class StreamTest {\n\tpublic static void main(String[] args) {\n\t\tString[] arr1 = { "a", "b", "c", "d" };\n\t\tString[] arr2 = { "d", "e", "f", "g" };\n\n\t\tStream<String> stream1 = Stream.of(arr1);\n\t\tStream<String> stream2 = Stream.of(arr2);\n\t\t// concat:合并两个流 distinct：去重\n\t\tList<String> newList = Stream.concat(stream1, stream2).distinct().collect(Collectors.toList());\n\t\t// limit：限制从流中获得前n个数据\n\t\tList<Integer> collect = Stream.iterate(1, x -> x + 2).limit(10).collect(Collectors.toList());\n\t\t// skip：跳过前n个数据\n\t\tList<Integer> collect2 = Stream.iterate(1, x -> x + 2).skip(1).limit(5).collect(Collectors.toList());\n\n\t\tSystem.out.println("流合并：" + newList);\n\t\tSystem.out.println("limit：" + collect);\n\t\tSystem.out.println("skip：" + collect2);\n\t}\n}\n\n\n运行结果：\n\n> 流合并：[a, b, c, d, e, f, g]\n> \n> limit：[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n> \n> skip：[3, 5, 7, 9, 11]\n\n结束语\n\n好，以上就是全部内容，能坚持看到这里，你一定很有收获，那么动一动拿offer的小手，点个赞再走吧，听说这么做的人2021年都交了好运！',normalizedContent:'# java8 stream\n\n\n\n * 1 stream概述\n * 2 stream的创建\n * 3 stream的使用\n   * 3.1 遍历/匹配(foreach/find/match)\n   * 3.2 筛选(filter)\n   * 3.3 聚合(max/min/count)\n   * 3.4 映射(map/flatmap)\n   * 3.5 归约(reduce)\n   * 3.6 收集(collect)\n   * 3.7 排序(sorted)\n   * 3.8 提取/组合\n\n\n\n先贴上几个案例，水平高超的同学可以挑战一下：\n\n 1. 从员工集合中筛选出salary大于8000的员工，并放置到新的集合里。\n 2. 统计员工的最高薪资、平均薪资、薪资之和。\n 3. 将员工按薪资从高到低排序，同样薪资者年龄小者在前。\n 4. 将员工按性别分类，将员工按性别和地区分类，将员工按薪资是否高于8000分为两部分。\n\n用传统的迭代处理也不是很难，但代码就显得冗余了，跟stream相比高下立判。\n\n\n# 1 stream概述\n\njava 8 是一个非常成功的版本，这个版本新增的stream，配合同版本出现的 lambda ，给我们操作集合（collection）提供了极大的便利。\n\n那么什么是stream？\n\n提示\n\nstream将要处理的元素集合看作一种流，在流的过程中，借助stream api对流中的元素进行操作，比如：筛选、排序、聚合等。\n\nstream可以由数组或集合创建，对流的操作分为两种：\n\n 1. 中间操作，每次返回一个新的流，可以有多个。\n 2. 终端操作，每个流只能进行一次终端操作，终端操作结束后流无法再次使用。终端操作会产生一个新的集合或值。\n\n另外，stream有几个特性：\n\n 1. stream不存储数据，而是按照特定的规则对数据进行计算，一般会输出结果。\n 2. stream不会改变数据源，通常情况下会产生一个新的集合或一个值。\n 3. stream具有延迟执行特性，只有调用终端操作时，中间操作才会执行。\n\n\n# 2 stream的创建\n\nstream可以通过集合数组创建。\n\n1、通过 java.util.collection.stream() 方法用集合创建流\n\nlist<string> list = arrays.aslist("a", "b", "c");\n// 创建一个顺序流\nstream<string> stream = list.stream();\n// 创建一个并行流\nstream<string> parallelstream = list.parallelstream();\n\n\n2、使用java.util.arrays.stream(t[] array)方法用数组创建流\n\nint[] array={1,3,5,6,8};\nintstream stream = arrays.stream(array);\n\n\n3、使用stream的静态方法：of()、iterate()、generate()\n\nstream<integer> stream = stream.of(1, 2, 3, 4, 5, 6);\n\nstream<integer> stream2 = stream.iterate(0, (x) -> x + 3).limit(4);\nstream2.foreach(system.out::println);\n\nstream<double> stream3 = stream.generate(math::random).limit(3);\nstream3.foreach(system.out::println);\n\n\n输出结果：\n\n> 0 3 6 9\n> \n> 0.6796156909271994\n> \n> 0.1914314208854283\n> \n> 0.8116932592396652\n\nstream和parallelstream的简单区分： stream是顺序流，由主线程按顺序对流执行操作，而parallelstream是并行流，内部以多线程并行执行的方式对流进行操作，但前提是流中的数据处理没有顺序要求。例如筛选集合中的奇数，两者的处理不同之处：\n\n\n\n如果流中的数据量足够大，并行流可以加快处速度。\n\n除了直接创建并行流，还可以通过parallel()把顺序流转换成并行流：\n\noptional<integer> findfirst = list.stream().parallel().filter(x->x>6).findfirst();\n\n\n\n# 3 stream的使用\n\n在使用stream之前，先理解一个概念：optional 。\n\n提示\n\noptional类是一个可以为null的容器对象。如果值存在则ispresent()方法会返回true，调用get()方法会返回该对象。 更详细说明请见：菜鸟教程java 8 optional类\n\n接下来，大批代码向你袭来！我将用20个案例将stream的使用整得明明白白，只要跟着敲一遍代码，就能很好地掌握。\n\n\n\n案例使用的员工类\n\n这是后面案例中使用的员工类：\n\nlist<person> personlist = new arraylist<person>();\npersonlist.add(new person("tom", 8900, "male", "new york"));\npersonlist.add(new person("jack", 7000, "male", "washington"));\npersonlist.add(new person("lily", 7800, "female", "washington"));\npersonlist.add(new person("anni", 8200, "female", "new york"));\npersonlist.add(new person("owen", 9500, "male", "new york"));\npersonlist.add(new person("alisa", 7900, "female", "new york"));\n\nclass person {\n\tprivate string name;  // 姓名\n\tprivate int salary; // 薪资\n\tprivate int age; // 年龄\n\tprivate string sex; //性别\n\tprivate string area;  // 地区\n\n\t// 构造方法\n\tpublic person(string name, int salary, int age,string sex,string area) {\n\t\tthis.name = name;\n\t\tthis.salary = salary;\n\t\tthis.age = age;\n\t\tthis.sex = sex;\n\t\tthis.area = area;\n\t}\n\t// 省略了get和set，请自行添加\n\n}\n\n\n\n# 3.1 遍历/匹配(foreach/find/match)\n\nstream也是支持类似集合的遍历和匹配元素的，只是stream中的元素是以optional类型存在的。stream的遍历、匹配非常简单。\n\n\n\n// import已省略，请自行添加，后面代码亦是\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n        list<integer> list = arrays.aslist(7, 6, 9, 3, 8, 2, 1);\n\n        // 遍历输出符合条件的元素\n        list.stream().filter(x -> x > 6).foreach(system.out::println);\n        // 匹配第一个\n        optional<integer> findfirst = list.stream().filter(x -> x > 6).findfirst();\n        // 匹配任意（适用于并行流）\n        optional<integer> findany = list.parallelstream().filter(x -> x > 6).findany();\n        // 是否包含符合特定条件的元素\n        boolean anymatch = list.stream().anymatch(x -> x > 6);\n        system.out.println("匹配第一个值：" + findfirst.get());\n        system.out.println("匹配任意一个值：" + findany.get());\n        system.out.println("是否存在大于6的值：" + anymatch);\n    }\n}\n\n\n\n# 3.2 筛选(filter)\n\n筛选，是按照一定的规则校验流中的元素，将符合条件的元素提取到新的流中的操作。\n\n\n\n案例一：筛选出integer集合中大于7的元素，并打印出来\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<integer> list = arrays.aslist(6, 7, 3, 8, 1, 2, 9);\n\t\tstream<integer> stream = list.stream();\n\t\tstream.filter(x -> x > 7).foreach(system.out::println);\n\t}\n}\n\n\n预期结果：\n\n> 8\n> \n> 9\n\n案例二： 筛选员工中工资高于8000的人，并形成新的集合。 形成新集合依赖collect（收集），后文有详细介绍。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<person> personlist = new arraylist<person>();\n\t\tpersonlist.add(new person("tom", 8900, 23, "male", "new york"));\n\t\tpersonlist.add(new person("jack", 7000, 25, "male", "washington"));\n\t\tpersonlist.add(new person("lily", 7800, 21, "female", "washington"));\n\t\tpersonlist.add(new person("anni", 8200, 24, "female", "new york"));\n\t\tpersonlist.add(new person("owen", 9500, 25, "male", "new york"));\n\t\tpersonlist.add(new person("alisa", 7900, 26, "female", "new york"));\n\n\t\tlist<string> fiterlist = personlist.stream().filter(x -> x.getsalary() > 8000).map(person::getname)\n\t\t\t\t.collect(collectors.tolist());\n\t\tsystem.out.print("高于8000的员工姓名：" + fiterlist);\n\t}\n}\n\n\n运行结果：\n\n> 高于8000的员工姓名：[tom, anni, owen]\n\n\n# 3.3 聚合(max/min/count)\n\nmax、min、count这些字眼你一定不陌生，没错，在mysql中我们常用它们进行数据统计。java stream中也引入了这些概念和用法，极大地方便了我们对集合、数组的数据统计工作。\n\n\n\n案例一：获取string集合中最长的元素。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<string> list = arrays.aslist("adnm", "admmt", "pot", "xbangd", "weoujgsd");\n\n\t\toptional<string> max = list.stream().max(comparator.comparing(string::length));\n\t\tsystem.out.println("最长的字符串：" + max.get());\n\t}\n}\n\n\n输出结果：\n\n> 最长的字符串：weoujgsd\n\n案例二：获取integer集合中的最大值。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<integer> list = arrays.aslist(7, 6, 9, 4, 11, 6);\n\n\t\t// 自然排序\n\t\toptional<integer> max = list.stream().max(integer::compareto);\n\t\t// 自定义排序\n\t\toptional<integer> max2 = list.stream().max(new comparator<integer>() {\n\t\t\t@override\n\t\t\tpublic int compare(integer o1, integer o2) {\n\t\t\t\treturn o1.compareto(o2);\n\t\t\t}\n\t\t});\n\t\tsystem.out.println("自然排序的最大值：" + max.get());\n\t\tsystem.out.println("自定义排序的最大值：" + max2.get());\n\t}\n}\n\n\n输出结果：\n\n> 自然排序的最大值：11\n> \n> 自定义排序的最大值：11\n\n案例三：获取员工工资最高的人。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<person> personlist = new arraylist<person>();\n\t\tpersonlist.add(new person("tom", 8900, 23, "male", "new york"));\n\t\tpersonlist.add(new person("jack", 7000, 25, "male", "washington"));\n\t\tpersonlist.add(new person("lily", 7800, 21, "female", "washington"));\n\t\tpersonlist.add(new person("anni", 8200, 24, "female", "new york"));\n\t\tpersonlist.add(new person("owen", 9500, 25, "male", "new york"));\n\t\tpersonlist.add(new person("alisa", 7900, 26, "female", "new york"));\n\n\t\toptional<person> max = personlist.stream().max(comparator.comparingint(person::getsalary));\n\t\tsystem.out.println("员工工资最大值：" + max.get().getsalary());\n\t}\n}\n\n\n输出结果：\n\n> 员工工资最大值：9500\n\n案例四：计算integer集合中大于6的元素的个数。\n\nimport java.util.arrays;\nimport java.util.list;\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<integer> list = arrays.aslist(7, 6, 4, 8, 2, 11, 9);\n\n\t\tlong count = list.stream().filter(x -> x > 6).count();\n\t\tsystem.out.println("list中大于6的元素个数：" + count);\n\t}\n}\n\n\n输出结果：\n\n> list中大于6的元素个数：4\n\n\n# 3.4 映射(map/flatmap)\n\n映射，可以将一个流的元素按照一定的映射规则映射到另一个流中。分为map和flatmap：\n\n * map：接收一个函数作为参数，该函数会被应用到每个元素上，并将其映射成一个新的元素。\n * flatmap：接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流。\n\n\n\n\n\n案例一：英文字符串数组的元素全部改为大写。整数数组每个元素+3。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tstring[] strarr = { "abcd", "bcdd", "defde", "ftr" };\n\t\tlist<string> strlist = arrays.stream(strarr).map(string::touppercase).collect(collectors.tolist());\n\n\t\tlist<integer> intlist = arrays.aslist(1, 3, 5, 7, 9, 11);\n\t\tlist<integer> intlistnew = intlist.stream().map(x -> x + 3).collect(collectors.tolist());\n\n\t\tsystem.out.println("每个元素大写：" + strlist);\n\t\tsystem.out.println("每个元素+3：" + intlistnew);\n\t}\n}\n\n\n输出结果：\n\n> 每个元素大写：[abcd, bcdd, defde, ftr]\n> \n> 每个元素+3：[4, 6, 8, 10, 12, 14]\n\n案例二：将员工的薪资全部增加1000。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<person> personlist = new arraylist<person>();\n\t\tpersonlist.add(new person("tom", 8900, 23, "male", "new york"));\n\t\tpersonlist.add(new person("jack", 7000, 25, "male", "washington"));\n\t\tpersonlist.add(new person("lily", 7800, 21, "female", "washington"));\n\t\tpersonlist.add(new person("anni", 8200, 24, "female", "new york"));\n\t\tpersonlist.add(new person("owen", 9500, 25, "male", "new york"));\n\t\tpersonlist.add(new person("alisa", 7900, 26, "female", "new york"));\n\n\t\t// 不改变原来员工集合的方式\n\t\tlist<person> personlistnew = personlist.stream().map(person -> {\n\t\t\tperson personnew = new person(person.getname(), 0, 0, null, null);\n\t\t\tpersonnew.setsalary(person.getsalary() + 10000);\n\t\t\treturn personnew;\n\t\t}).collect(collectors.tolist());\n\t\tsystem.out.println("一次改动前：" + personlist.get(0).getname() + "--\x3e" + personlist.get(0).getsalary());\n\t\tsystem.out.println("一次改动后：" + personlistnew.get(0).getname() + "--\x3e" + personlistnew.get(0).getsalary());\n\n\t\t// 改变原来员工集合的方式\n\t\tlist<person> personlistnew2 = personlist.stream().map(person -> {\n\t\t\tperson.setsalary(person.getsalary() + 10000);\n\t\t\treturn person;\n\t\t}).collect(collectors.tolist());\n\t\tsystem.out.println("二次改动前：" + personlist.get(0).getname() + "--\x3e" + personlistnew.get(0).getsalary());\n\t\tsystem.out.println("二次改动后：" + personlistnew2.get(0).getname() + "--\x3e" + personlistnew.get(0).getsalary());\n\t}\n}\n\n\n输出结果：\n\n> 一次改动前：tom–>8900\n> \n> 一次改动后：tom–>18900\n> \n> 二次改动前：tom–>18900\n> \n> 二次改动后：tom–>18900\n\n案例三：将两个字符数组合并成一个新的字符数组。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<string> list = arrays.aslist("m,k,l,a", "1,3,5,7");\n\t\tlist<string> listnew = list.stream().flatmap(s -> {\n\t\t\t// 将每个元素转换成一个stream\n\t\t\tstring[] split = s.split(",");\n\t\t\tstream<string> s2 = arrays.stream(split);\n\t\t\treturn s2;\n\t\t}).collect(collectors.tolist());\n\n\t\tsystem.out.println("处理前的集合：" + list);\n\t\tsystem.out.println("处理后的集合：" + listnew);\n\t}\n}\n\n\n输出结果：\n\n> 处理前的集合：[m-k-l-a, 1-3-5]\n> \n> 处理后的集合：[m, k, l, a, 1, 3, 5]\n\n\n# 3.5 归约(reduce)\n\n归约，也称缩减，顾名思义，是把一个流缩减成一个值，能实现对集合求和、求乘积和求最值操作。\n\n案例一：求integer集合的元素之和、乘积和最大值。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<integer> list = arrays.aslist(1, 3, 2, 8, 11, 4);\n\t\t// 求和方式1\n\t\toptional<integer> sum = list.stream().reduce((x, y) -> x + y);\n\t\t// 求和方式2\n\t\toptional<integer> sum2 = list.stream().reduce(integer::sum);\n\t\t// 求和方式3\n\t\tinteger sum3 = list.stream().reduce(0, integer::sum);\n\t\t\n\t\t// 求乘积\n\t\toptional<integer> product = list.stream().reduce((x, y) -> x * y);\n\n\t\t// 求最大值方式1\n\t\toptional<integer> max = list.stream().reduce((x, y) -> x > y ? x : y);\n\t\t// 求最大值写法2\n\t\tinteger max2 = list.stream().reduce(1, integer::max);\n\n\t\tsystem.out.println("list求和：" + sum.get() + "," + sum2.get() + "," + sum3);\n\t\tsystem.out.println("list求积：" + product.get());\n\t\tsystem.out.println("list求和：" + max.get() + "," + max2);\n\t}\n}\n\n\n输出结果：\n\n> list求和：29,29,29\n> \n> list求积：2112\n> \n> list求和：11,11\n\n案例二：求所有员工的工资之和和最高工资。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<person> personlist = new arraylist<person>();\n\t\tpersonlist.add(new person("tom", 8900, 23, "male", "new york"));\n\t\tpersonlist.add(new person("jack", 7000, 25, "male", "washington"));\n\t\tpersonlist.add(new person("lily", 7800, 21, "female", "washington"));\n\t\tpersonlist.add(new person("anni", 8200, 24, "female", "new york"));\n\t\tpersonlist.add(new person("owen", 9500, 25, "male", "new york"));\n\t\tpersonlist.add(new person("alisa", 7900, 26, "female", "new york"));\n\n\t\t// 求工资之和方式1：\n\t\toptional<integer> sumsalary = personlist.stream().map(person::getsalary).reduce(integer::sum);\n\t\t// 求工资之和方式2：\n\t\tinteger sumsalary2 = personlist.stream().reduce(0, (sum, p) -> sum += p.getsalary(),\n\t\t\t\t(sum1, sum2) -> sum1 + sum2);\n\t\t// 求工资之和方式3：\n\t\tinteger sumsalary3 = personlist.stream().reduce(0, (sum, p) -> sum += p.getsalary(), integer::sum);\n\n\t\t// 求最高工资方式1：\n\t\tinteger maxsalary = personlist.stream().reduce(0, (max, p) -> max > p.getsalary() ? max : p.getsalary(),\n\t\t\t\tinteger::max);\n\t\t// 求最高工资方式2：\n\t\tinteger maxsalary2 = personlist.stream().reduce(0, (max, p) -> max > p.getsalary() ? max : p.getsalary(),\n\t\t\t\t(max1, max2) -> max1 > max2 ? max1 : max2);\n\n\t\tsystem.out.println("工资之和：" + sumsalary.get() + "," + sumsalary2 + "," + sumsalary3);\n\t\tsystem.out.println("最高工资：" + maxsalary + "," + maxsalary2);\n\t}\n}\n\n\n输出结果：\n\n> 工资之和：49300,49300,49300\n> \n> 最高工资：9500,9500\n\n\n# 3.6 收集(collect)\n\ncollect，收集，可以说是内容最繁多、功能最丰富的部分了。从字面上去理解，就是把一个流收集起来，最终可以是收集成一个值也可以收集成一个新的集合。\n\n提示\n\ncollect主要依赖java.util.stream.collectors类内置的静态方法。\n\n# 3.6.1 归集(tolist/toset/tomap)\n\n因为流不存储数据，那么在流中的数据完成处理后，需要将流中的数据重新归集到新的集合里。tolist、toset和tomap比较常用，另外还有tocollection、toconcurrentmap等复杂一些的用法。\n\n下面用一个案例演示tolist、toset和tomap：\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<integer> list = arrays.aslist(1, 6, 3, 4, 6, 7, 9, 6, 20);\n\t\tlist<integer> listnew = list.stream().filter(x -> x % 2 == 0).collect(collectors.tolist());\n\t\tset<integer> set = list.stream().filter(x -> x % 2 == 0).collect(collectors.toset());\n\n\t\tlist<person> personlist = new arraylist<person>();\n\t\tpersonlist.add(new person("tom", 8900, 23, "male", "new york"));\n\t\tpersonlist.add(new person("jack", 7000, 25, "male", "washington"));\n\t\tpersonlist.add(new person("lily", 7800, 21, "female", "washington"));\n\t\tpersonlist.add(new person("anni", 8200, 24, "female", "new york"));\n\t\t\n\t\tmap<?, person> map = personlist.stream().filter(p -> p.getsalary() > 8000)\n\t\t\t\t.collect(collectors.tomap(person::getname, p -> p));\n\t\tsystem.out.println("tolist:" + listnew);\n\t\tsystem.out.println("toset:" + set);\n\t\tsystem.out.println("tomap:" + map);\n\t}\n}\n\n\n运行结果：\n\n> tolist：[6, 4, 6, 6, 20]\n> \n> toset：[4, 20, 6]\n> \n> tomap：{tom=mutest.person@5fd0d5ae, anni=mutest.person@2d98a335}\n\n# 3.6.2 统计(count/averaging)\n\ncollectors提供了一系列用于数据统计的静态方法：\n\n * 计数：count\n\n * 平均值：averagingint、averaginglong、averagingdouble\n\n * 最值：maxby、minby\n\n * 求和：summingint、summinglong、summingdouble\n\n * 统计以上所有：summarizingint、summarizinglong、summarizingdouble\n\n案例：统计员工人数、平均工资、工资总额、最高工资。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<person> personlist = new arraylist<person>();\n\t\tpersonlist.add(new person("tom", 8900, 23, "male", "new york"));\n\t\tpersonlist.add(new person("jack", 7000, 25, "male", "washington"));\n\t\tpersonlist.add(new person("lily", 7800, 21, "female", "washington"));\n\n\t\t// 求总数\n\t\tlong count = personlist.stream().collect(collectors.counting());\n\t\t// 求平均工资\n\t\tdouble average = personlist.stream().collect(collectors.averagingdouble(person::getsalary));\n\t\t// 求最高工资\n\t\toptional<integer> max = personlist.stream().map(person::getsalary).collect(collectors.maxby(integer::compare));\n\t\t// 求工资之和\n\t\tinteger sum = personlist.stream().collect(collectors.summingint(person::getsalary));\n\t\t// 一次性统计所有信息\n\t\tdoublesummarystatistics collect = personlist.stream().collect(collectors.summarizingdouble(person::getsalary));\n\n\t\tsystem.out.println("员工总数：" + count);\n\t\tsystem.out.println("员工平均工资：" + average);\n\t\tsystem.out.println("员工工资总和：" + sum);\n\t\tsystem.out.println("员工工资所有统计：" + collect);\n\t}\n}\n\n\n运行结果：\n\n> 员工总数：3\n> \n> 员工平均工资：7900.0\n> \n> 员工工资总和：23700\n> \n> 员工工资所有统计：doublesummarystatistics{count=3, sum=23700.000000,min=7000.000000, average=7900.000000, max=8900.000000}\n\n# 3.6.3 分组(partitioningby/groupingby)\n\n * 分区：将stream按条件分为两个map，比如员工按薪资是否高于8000分为两部分。\n * 分组：将集合分为多个map，比如员工按性别分组。有单级分组和多级分组。\n\n\n\n案例：将员工按薪资是否高于8000分为两部分；将员工按性别和地区分组\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<person> personlist = new arraylist<person>();\n\t\tpersonlist.add(new person("tom", 8900, "male", "new york"));\n\t\tpersonlist.add(new person("jack", 7000, "male", "washington"));\n\t\tpersonlist.add(new person("lily", 7800, "female", "washington"));\n\t\tpersonlist.add(new person("anni", 8200, "female", "new york"));\n\t\tpersonlist.add(new person("owen", 9500, "male", "new york"));\n\t\tpersonlist.add(new person("alisa", 7900, "female", "new york"));\n\n\t\t// 将员工按薪资是否高于8000分组\n        map<boolean, list<person>> part = personlist.stream().collect(collectors.partitioningby(x -> x.getsalary() > 8000));\n        // 将员工按性别分组\n        map<string, list<person>> group = personlist.stream().collect(collectors.groupingby(person::getsex));\n        // 将员工先按性别分组，再按地区分组\n        map<string, map<string, list<person>>> group2 = personlist.stream().collect(collectors.groupingby(person::getsex, collectors.groupingby(person::getarea)));\n        system.out.println("员工按薪资是否大于8000分组情况：" + part);\n        system.out.println("员工按性别分组情况：" + group);\n        system.out.println("员工按性别、地区：" + group2);\n\t}\n}\n\n\n输出结果：\n\n员工按薪资是否大于8000分组情况：{false=[mutest.person@2d98a335, mutest.person@16b98e56, mutest.person@7ef20235], true=[mutest.person@27d6c5e0, mutest.person@4f3f5b24, mutest.person@15aeb7ab]}\n员工按性别分组情况：{female=[mutest.person@16b98e56, mutest.person@4f3f5b24, mutest.person@7ef20235], male=[mutest.person@27d6c5e0, mutest.person@2d98a335, mutest.person@15aeb7ab]}\n员工按性别、地区：{female={new york=[mutest.person@4f3f5b24, mutest.person@7ef20235], washington=[mutest.person@16b98e56]}, male={new york=[mutest.person@27d6c5e0, mutest.person@15aeb7ab], washington=[mutest.person@2d98a335]}}\n\n\n# 3.6.4 接合(joining)\n\njoining可以将stream中的元素用特定的连接符（没有的话，则直接连接）连接成一个字符串。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<person> personlist = new arraylist<person>();\n\t\tpersonlist.add(new person("tom", 8900, 23, "male", "new york"));\n\t\tpersonlist.add(new person("jack", 7000, 25, "male", "washington"));\n\t\tpersonlist.add(new person("lily", 7800, 21, "female", "washington"));\n\n\t\tstring names = personlist.stream().map(p -> p.getname()).collect(collectors.joining(","));\n\t\tsystem.out.println("所有员工的姓名：" + names);\n\t\tlist<string> list = arrays.aslist("a", "b", "c");\n\t\tstring string = list.stream().collect(collectors.joining("-"));\n\t\tsystem.out.println("拼接后的字符串：" + string);\n\t}\n}\n\n\n运行结果：\n\n> 所有员工的姓名：tom,jack,lily\n> \n> 拼接后的字符串：a-b-c\n\n# 3.6.5 归约(reducing)\n\ncollectors类提供的reducing方法，相比于stream本身的reduce方法，增加了对自定义归约的支持。\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<person> personlist = new arraylist<person>();\n\t\tpersonlist.add(new person("tom", 8900, 23, "male", "new york"));\n\t\tpersonlist.add(new person("jack", 7000, 25, "male", "washington"));\n\t\tpersonlist.add(new person("lily", 7800, 21, "female", "washington"));\n\n\t\t// 每个员工减去起征点后的薪资之和（这个例子并不严谨，但一时没想到好的例子）\n\t\tinteger sum = personlist.stream().collect(collectors.reducing(0, person::getsalary, (i, j) -> (i + j - 5000)));\n\t\tsystem.out.println("员工扣税薪资总和：" + sum);\n\n\t\t// stream的reduce\n\t\toptional<integer> sum2 = personlist.stream().map(person::getsalary).reduce(integer::sum);\n\t\tsystem.out.println("员工薪资总和：" + sum2.get());\n\t}\n}\n\n\n运行结果：\n\n> 员工扣税薪资总和：8700\n> \n> 员工薪资总和：23700\n\n\n# 3.7 排序(sorted)\n\nsorted，中间操作。有两种排序：\n\n * sorted()：自然排序，流中元素需实现comparable接口\n * sorted(comparator com)：comparator排序器自定义排序\n\n案例：将员工按工资由高到低（工资一样则按年龄由大到小）排序\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tlist<person> personlist = new arraylist<person>();\n\n\t\tpersonlist.add(new person("sherry", 9000, 24, "female", "new york"));\n\t\tpersonlist.add(new person("tom", 8900, 22, "male", "washington"));\n\t\tpersonlist.add(new person("jack", 9000, 25, "male", "washington"));\n\t\tpersonlist.add(new person("lily", 8800, 26, "male", "new york"));\n\t\tpersonlist.add(new person("alisa", 9000, 26, "female", "new york"));\n\n\t\t// 按工资升序排序（自然排序）\n\t\tlist<string> newlist = personlist.stream().sorted(comparator.comparing(person::getsalary)).map(person::getname)\n\t\t\t\t.collect(collectors.tolist());\n\t\t// 按工资倒序排序\n\t\tlist<string> newlist2 = personlist.stream().sorted(comparator.comparing(person::getsalary).reversed())\n\t\t\t\t.map(person::getname).collect(collectors.tolist());\n\t\t// 先按工资再按年龄升序排序\n\t\tlist<string> newlist3 = personlist.stream()\n\t\t\t\t.sorted(comparator.comparing(person::getsalary).thencomparing(person::getage)).map(person::getname)\n\t\t\t\t.collect(collectors.tolist());\n\t\t// 先按工资再按年龄自定义排序（降序）\n\t\tlist<string> newlist4 = personlist.stream().sorted((p1, p2) -> {\n\t\t\tif (p1.getsalary() == p2.getsalary()) {\n\t\t\t\treturn p2.getage() - p1.getage();\n\t\t\t} else {\n\t\t\t\treturn p2.getsalary() - p1.getsalary();\n\t\t\t}\n\t\t}).map(person::getname).collect(collectors.tolist());\n\n\t\tsystem.out.println("按工资升序排序：" + newlist);\n\t\tsystem.out.println("按工资降序排序：" + newlist2);\n\t\tsystem.out.println("先按工资再按年龄升序排序：" + newlist3);\n\t\tsystem.out.println("先按工资再按年龄自定义降序排序：" + newlist4);\n\t}\n}\n\n\n运行结果：\n\n> 按工资升序排序：[lily, tom, sherry, jack, alisa]\n> \n> 按工资降序排序：[sherry, jack, alisa, tom, lily]\n> \n> 先按工资再按年龄升序排序：[lily, tom, sherry, jack, alisa]\n> \n> 先按工资再按年龄自定义降序排序：[alisa, jack, sherry, tom, lily]\n\n\n# 3.8 提取/组合\n\n流也可以进行合并、去重、限制、跳过等操作。\n\n\n\n\n\n\n\npublic class streamtest {\n\tpublic static void main(string[] args) {\n\t\tstring[] arr1 = { "a", "b", "c", "d" };\n\t\tstring[] arr2 = { "d", "e", "f", "g" };\n\n\t\tstream<string> stream1 = stream.of(arr1);\n\t\tstream<string> stream2 = stream.of(arr2);\n\t\t// concat:合并两个流 distinct：去重\n\t\tlist<string> newlist = stream.concat(stream1, stream2).distinct().collect(collectors.tolist());\n\t\t// limit：限制从流中获得前n个数据\n\t\tlist<integer> collect = stream.iterate(1, x -> x + 2).limit(10).collect(collectors.tolist());\n\t\t// skip：跳过前n个数据\n\t\tlist<integer> collect2 = stream.iterate(1, x -> x + 2).skip(1).limit(5).collect(collectors.tolist());\n\n\t\tsystem.out.println("流合并：" + newlist);\n\t\tsystem.out.println("limit：" + collect);\n\t\tsystem.out.println("skip：" + collect2);\n\t}\n}\n\n\n运行结果：\n\n> 流合并：[a, b, c, d, e, f, g]\n> \n> limit：[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n> \n> skip：[3, 5, 7, 9, 11]\n\n结束语\n\n好，以上就是全部内容，能坚持看到这里，你一定很有收获，那么动一动拿offer的小手，点个赞再走吧，听说这么做的人2021年都交了好运！',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"什么是 Kubernetes",frontmatter:{title:"什么是 Kubernetes",date:"2022-02-09T13:37:49.000Z",permalink:"/pages/e51b3b/"},regularPath:"/22.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/01.%E4%BB%80%E4%B9%88%E6%98%AF%20Kubernetes.html",relativePath:"22.Kubernetes/01.Kubernetes 入门/01.什么是 Kubernetes.md",key:"v-a0a5c11c",path:"/pages/e51b3b/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:21},{level:2,title:"特点",slug:"特点",normalizedTitle:"特点",charIndex:284},{level:2,title:"从传统到容器化部署",slug:"从传统到容器化部署",normalizedTitle:"从传统到容器化部署",charIndex:380},{level:3,title:"传统的部署方式",slug:"传统的部署方式",normalizedTitle:"传统的部署方式",charIndex:396},{level:3,title:"容器化部署的优势",slug:"容器化部署的优势",normalizedTitle:"容器化部署的优势",charIndex:530},{level:2,title:"为什么需要 Kubernetes",slug:"为什么需要-kubernetes",normalizedTitle:"为什么需要 kubernetes",charIndex:870}],headersStr:"概述 特点 从传统到容器化部署 传统的部署方式 容器化部署的优势 为什么需要 Kubernetes",content:"# 什么是 Kubernetes\n\n\n# 概述\n\n\n\nKubernetes 是 Google 2014 年创建管理的，是 Google 10 多年大规模容器管理技术 Borg 的开源版本。\n\nKubernetes 是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。使用 Kubernetes 我们可以：\n\n * 快速部署应用\n * 快速扩展应用\n * 无缝对接新的应用功能\n * 节省资源，优化硬件资源的使用\n\nKubernetes 的目标是促进完善组件和工具的生态系统，以减轻应用程序在公有云或私有云中运行的负担。\n\n\n# 特点\n\n * 可移植： 支持公有云，私有云，混合云，多重云（多个公共云）\n * 可扩展： 模块化，插件化，可挂载，可组合\n * 自动化： 自动部署，自动重启，自动复制，自动伸缩/扩展\n\n\n# 从传统到容器化部署\n\n\n\n\n# 传统的部署方式\n\n传统的应用部署方式是通过插件或脚本来安装应用。这样做的缺点是应用的运行、配置、管理、所有生存周期将与当前操作系统绑定，这样做并不利于应用的升级更新/回滚等操作，当然也可以通过创建虚机的方式来实现某些功能，但是虚拟机非常重，并不利于可移植性。\n\n\n# 容器化部署的优势\n\n * 快速创建/部署应用： 与虚拟机相比，容器镜像的创建更加容易。\n * 持续开发、集成和部署： 提供可靠且频繁的容器镜像构建/部署，并使用快速和简单的回滚(由于镜像不可变性)。\n * 开发和运行相分离： 在 build 或者 release 阶段创建容器镜像，使得应用和基础设施解耦。\n * 开发，测试和生产环境一致性： 在本地或外网（生产环境）运行的一致性。\n * 云平台或其他操作系统： 可以在 Ubuntu、RHEL、CoreOS、on-prem、Google Container Engine 或其它任何环境中运行。\n * 分布式，弹性，微服务化： 应用程序分为更小的、独立的部件，可以动态部署和管理。\n * 资源隔离\n * 资源利用更高效\n\n\n# 为什么需要 Kubernetes\n\n可以在物理或虚拟机的 Kubernetes 集群上运行容器化应用，Kubernetes 能提供一个以 “容器为中心的基础架构”，满足在生产环境中运行应用的一些常见需求，如：\n\n * 多个进程协同工作\n * 存储系统挂载\n * 应用健康检查\n * 应用实例的复制\n * 自动伸缩/扩展\n * 注册与发现\n * 负载均衡\n * 滚动更新\n * 资源监控\n * 日志访问\n * 调试应用程序\n * 提供认证和授权",normalizedContent:"# 什么是 kubernetes\n\n\n# 概述\n\n\n\nkubernetes 是 google 2014 年创建管理的，是 google 10 多年大规模容器管理技术 borg 的开源版本。\n\nkubernetes 是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。使用 kubernetes 我们可以：\n\n * 快速部署应用\n * 快速扩展应用\n * 无缝对接新的应用功能\n * 节省资源，优化硬件资源的使用\n\nkubernetes 的目标是促进完善组件和工具的生态系统，以减轻应用程序在公有云或私有云中运行的负担。\n\n\n# 特点\n\n * 可移植： 支持公有云，私有云，混合云，多重云（多个公共云）\n * 可扩展： 模块化，插件化，可挂载，可组合\n * 自动化： 自动部署，自动重启，自动复制，自动伸缩/扩展\n\n\n# 从传统到容器化部署\n\n\n\n\n# 传统的部署方式\n\n传统的应用部署方式是通过插件或脚本来安装应用。这样做的缺点是应用的运行、配置、管理、所有生存周期将与当前操作系统绑定，这样做并不利于应用的升级更新/回滚等操作，当然也可以通过创建虚机的方式来实现某些功能，但是虚拟机非常重，并不利于可移植性。\n\n\n# 容器化部署的优势\n\n * 快速创建/部署应用： 与虚拟机相比，容器镜像的创建更加容易。\n * 持续开发、集成和部署： 提供可靠且频繁的容器镜像构建/部署，并使用快速和简单的回滚(由于镜像不可变性)。\n * 开发和运行相分离： 在 build 或者 release 阶段创建容器镜像，使得应用和基础设施解耦。\n * 开发，测试和生产环境一致性： 在本地或外网（生产环境）运行的一致性。\n * 云平台或其他操作系统： 可以在 ubuntu、rhel、coreos、on-prem、google container engine 或其它任何环境中运行。\n * 分布式，弹性，微服务化： 应用程序分为更小的、独立的部件，可以动态部署和管理。\n * 资源隔离\n * 资源利用更高效\n\n\n# 为什么需要 kubernetes\n\n可以在物理或虚拟机的 kubernetes 集群上运行容器化应用，kubernetes 能提供一个以 “容器为中心的基础架构”，满足在生产环境中运行应用的一些常见需求，如：\n\n * 多个进程协同工作\n * 存储系统挂载\n * 应用健康检查\n * 应用实例的复制\n * 自动伸缩/扩展\n * 注册与发现\n * 负载均衡\n * 滚动更新\n * 资源监控\n * 日志访问\n * 调试应用程序\n * 提供认证和授权",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Kubernetes 安装前的准备",frontmatter:{title:"Kubernetes 安装前的准备",date:"2022-02-09T13:37:49.000Z",permalink:"/pages/43a41d/"},regularPath:"/22.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/02.Kubernetes%20%E5%AE%89%E8%A3%85%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87.html",relativePath:"22.Kubernetes/01.Kubernetes 入门/02.Kubernetes 安装前的准备.md",key:"v-229229ee",path:"/pages/43a41d/",headers:[{level:2,title:"前提条件",slug:"前提条件",normalizedTitle:"前提条件",charIndex:58},{level:2,title:"部署目标",slug:"部署目标",normalizedTitle:"部署目标",charIndex:277},{level:2,title:"环境介绍",slug:"环境介绍",normalizedTitle:"环境介绍",charIndex:454},{level:2,title:"修改主机名",slug:"修改主机名",normalizedTitle:"修改主机名",charIndex:867},{level:2,title:"修改/etc/hosts文件",slug:"修改-etc-hosts文件",normalizedTitle:"修改/etc/hosts文件",charIndex:1044},{level:2,title:"配置时间同步",slug:"配置时间同步",normalizedTitle:"配置时间同步",charIndex:1170},{level:2,title:"关闭swap",slug:"关闭swap",normalizedTitle:"关闭swap",charIndex:1299},{level:2,title:"允许 iptables 检查桥接流量",slug:"允许-iptables-检查桥接流量",normalizedTitle:"允许 iptables 检查桥接流量",charIndex:1651},{level:2,title:"检查所需端口",slug:"检查所需端口",normalizedTitle:"检查所需端口",charIndex:2122},{level:3,title:"控制平面节点",slug:"控制平面节点",normalizedTitle:"控制平面节点",charIndex:2133},{level:3,title:"工作节点",slug:"工作节点",normalizedTitle:"工作节点",charIndex:2532},{level:2,title:"安装Docker",slug:"安装docker",normalizedTitle:"安装docker",charIndex:2881}],headersStr:"前提条件 部署目标 环境介绍 修改主机名 修改/etc/hosts文件 配置时间同步 关闭swap 允许 iptables 检查桥接流量 检查所需端口 控制平面节点 工作节点 安装Docker",content:'# Kubernetes 安装前的准备\n\n官方文档\n\n安装 kubeadm 使用 kubeadm 创建集群\n\n\n# 前提条件\n\n本次部署教程采用 Ubuntu 版本安装 kubernetes 集群环境，集群节点为 1 主 2 从模式，服务器（或虚拟机）基本要求，如下：\n\n * OS：Ubuntu 18.04.5 LTS\n * CPU：最低要求，1 CPU 2 核\n * 内存：最低要求，2GB\n * 磁盘：最低要求，20GB\n * 网络：可以访问外网，且集群中的所有机器的网络可以相互通信\n\n基于CentOS 7 部署K8S教程\n\n请点击这里\n\n\n# 部署目标\n\n如下图，给出了本章要部署的目标集群的基本环境，它拥有一个Master节点和两个Node节点。各Node主机的配置方式基本相同。\n\n\n\n各主机上采用的容器运行时环境为Docker，为Pod提供网络功能的CNI是calico，它运行为托管于Kubernetes之上的Pod对象，另外，基础附件还包括CoreDNS用于名称解析和服务发现。\n\n\n# 环境介绍\n\n操作系统、容器引擎及Kubernetes的相关版本如下：\n\n * OS: Ubuntu 18.04.5 LTS\n * Container runtime: Docker CE 20.10.0\n * Kubernetes: v1.20\n\n创建三台虚拟机，分别命名如下：\n\n主机名           IP地址             角色       OS               配置\nk8s-master1   192.168.31.101   master   Ubuntu 18.04.5   2核2G\nk8s-node1     192.168.31.102   worker   Ubuntu 18.04.5   2核2G\nk8s-node2     192.168.31.103   worker   Ubuntu 18.04.5   2核2G\n\n提示\n\n无特殊说明以下操作在所有节点执行\n\n\n# 修改主机名\n\n在同一局域网中主机名不应该相同，所以我们需要做修改。\n\n#master节点:\nhostnamectl set-hostname k8s-master\n#node1节点：\nhostnamectl set-hostname k8s-node1\n#node2节点:\nhostnamectl set-hostname k8s-node2\n\n\n\n# 修改/etc/hosts文件\n\ncat >> /etc/hosts << EOF\n192.168.31.101 k8s-master\n192.168.31.102 k8s-node1\n192.168.31.103 k8s-node2\nEOF\n\n\n\n# 配置时间同步\n\n使用chrony同步时间，配置master节点与网络NTP服务器同步时间，所有node节点与master节点同步时间。\n\nsudo apt install ntpdate  \nsudo ntpdate ntp.aliyun.com\n\n\n\n# 关闭swap\n\nswapoff -a\nyes | cp /etc/fstab /etc/fstab_bak\ncat /etc/fstab_bak |grep -v swap > /etc/fstab\n\n\n使用free -h命令检查配置是否生效\n\n[root@k8s-master etc]# free -h   \ntotal        used        free      shared  buff/cache   available\nMem:           1.8G        142M        1.5G        8.5M        122M        1.5G\nSwap:            0B          0B          0B\n\n\n\n# 允许 iptables 检查桥接流量\n\n确保 br_netfilter 模块被加载。这一操作可以通过运行 lsmod | grep br_netfilter 来完成。若要显式加载该模块，可执行 sudo modprobe br_netfilter。\n\n为了让你的 Linux 节点上的 iptables 能够正确地查看桥接流量，你需要确保在你的 sysctl 配置中将 net.bridge.bridge-nf-call-iptables 设置为 1。例如：\n\ncat <<EOF | sudo tee /etc/modules-load.d/k8s.conf\nbr_netfilter\nEOF\n\n\ncat <<EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\n\n\nsysctl --system\n\n\n更多的相关细节可查看网络插件需求页面。\n\n\n# 检查所需端口\n\n\n# 控制平面节点\n\n协议    方向   端口范围        作用                        使用者\nTCP   入站   6443        Kubernetes API 服务器        所有组件\nTCP   入站   2379-2380   etcd 服务器客户端 API           kube-apiserver, etcd\nTCP   入站   10250       Kubelet API               kubelet 自身、控制平面组件\nTCP   入站   10251       kube-scheduler            kube-scheduler 自身\nTCP   入站   10252       kube-controller-manager   kube-controller-manager 自身\n\n\n# 工作节点\n\n协议    方向   端口范围          作用             使用者\nTCP   入站   10250         Kubelet API    kubelet 自身、控制平面组件\nTCP   入站   30000-32767   NodePort 服务†   所有组件\n\n† NodePort 服务 的默认端口范围。\n\n使用 * 标记的任意端口号都可以被覆盖，所以你需要保证所定制的端口是开放的。\n\n虽然控制平面节点已经包含了 etcd 的端口，你也可以使用自定义的外部 etcd 集群，或是指定自定义端口。\n\n你使用的 Pod 网络插件 (见下) 也可能需要某些特定端口开启。由于各个 Pod 网络插件都有所不同， 请参阅他们各自文档中对端口的要求。\n\n\n# 安装Docker\n\n在每个节点上，根据安装 Docker 引擎 为你的 Linux 发行版安装 Docker。 你可以在此文件中找到最新的经过验证的 Docker 版本 依赖关系。\n\nStep 1 : 安装必要的一些系统工具\n\nsudo apt-get update\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common\n\n\nStep 2 : 安装GPG证书\n\nsudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n\n\nStep 3 : 写入软件源信息\n\nsudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"\n\n\nStep 4 : 更新并安装v19.03.0版的Docker-CE\n\nsudo apt-get install docker-ce=5:19.03.0~3-0~ubuntu-bionic docker-ce-cli=5:19.03.0~3-0~ubuntu-bionic containerd.io -y\n\n\nStep 5 ：配置 Docker 守护程序，尤其是使用 systemd 来管理容器的 cgroup，配置镜像加速\n\ncat <<EOF | sudo tee /etc/docker/daemon.json\n{\n  "exec-opts": ["native.cgroupdriver=systemd"],\n  "log-driver": "json-file",\n  "log-opts": {\n    "max-size": "100m"\n  },\n  "storage-driver": "overlay2",\n  "registry-mirrors": ["https://registry.cn-beijing.aliyuncs.com"]\n}\nEOF\n\n\nStep 6 ：重新启动 Docker 并在启动时启用\n\nsudo systemctl enable docker\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n\n\nStep 7 ： 建立 docker 用户组\n\n默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。\n\n切换到普通用户，将当前用户加入 docker 组：\n\n$ sudo usermod -aG docker $USER\n\n\n退出当前终端并重新登录，进行如下测试。',normalizedContent:'# kubernetes 安装前的准备\n\n官方文档\n\n安装 kubeadm 使用 kubeadm 创建集群\n\n\n# 前提条件\n\n本次部署教程采用 ubuntu 版本安装 kubernetes 集群环境，集群节点为 1 主 2 从模式，服务器（或虚拟机）基本要求，如下：\n\n * os：ubuntu 18.04.5 lts\n * cpu：最低要求，1 cpu 2 核\n * 内存：最低要求，2gb\n * 磁盘：最低要求，20gb\n * 网络：可以访问外网，且集群中的所有机器的网络可以相互通信\n\n基于centos 7 部署k8s教程\n\n请点击这里\n\n\n# 部署目标\n\n如下图，给出了本章要部署的目标集群的基本环境，它拥有一个master节点和两个node节点。各node主机的配置方式基本相同。\n\n\n\n各主机上采用的容器运行时环境为docker，为pod提供网络功能的cni是calico，它运行为托管于kubernetes之上的pod对象，另外，基础附件还包括coredns用于名称解析和服务发现。\n\n\n# 环境介绍\n\n操作系统、容器引擎及kubernetes的相关版本如下：\n\n * os: ubuntu 18.04.5 lts\n * container runtime: docker ce 20.10.0\n * kubernetes: v1.20\n\n创建三台虚拟机，分别命名如下：\n\n主机名           ip地址             角色       os               配置\nk8s-master1   192.168.31.101   master   ubuntu 18.04.5   2核2g\nk8s-node1     192.168.31.102   worker   ubuntu 18.04.5   2核2g\nk8s-node2     192.168.31.103   worker   ubuntu 18.04.5   2核2g\n\n提示\n\n无特殊说明以下操作在所有节点执行\n\n\n# 修改主机名\n\n在同一局域网中主机名不应该相同，所以我们需要做修改。\n\n#master节点:\nhostnamectl set-hostname k8s-master\n#node1节点：\nhostnamectl set-hostname k8s-node1\n#node2节点:\nhostnamectl set-hostname k8s-node2\n\n\n\n# 修改/etc/hosts文件\n\ncat >> /etc/hosts << eof\n192.168.31.101 k8s-master\n192.168.31.102 k8s-node1\n192.168.31.103 k8s-node2\neof\n\n\n\n# 配置时间同步\n\n使用chrony同步时间，配置master节点与网络ntp服务器同步时间，所有node节点与master节点同步时间。\n\nsudo apt install ntpdate  \nsudo ntpdate ntp.aliyun.com\n\n\n\n# 关闭swap\n\nswapoff -a\nyes | cp /etc/fstab /etc/fstab_bak\ncat /etc/fstab_bak |grep -v swap > /etc/fstab\n\n\n使用free -h命令检查配置是否生效\n\n[root@k8s-master etc]# free -h   \ntotal        used        free      shared  buff/cache   available\nmem:           1.8g        142m        1.5g        8.5m        122m        1.5g\nswap:            0b          0b          0b\n\n\n\n# 允许 iptables 检查桥接流量\n\n确保 br_netfilter 模块被加载。这一操作可以通过运行 lsmod | grep br_netfilter 来完成。若要显式加载该模块，可执行 sudo modprobe br_netfilter。\n\n为了让你的 linux 节点上的 iptables 能够正确地查看桥接流量，你需要确保在你的 sysctl 配置中将 net.bridge.bridge-nf-call-iptables 设置为 1。例如：\n\ncat <<eof | sudo tee /etc/modules-load.d/k8s.conf\nbr_netfilter\neof\n\n\ncat <<eof | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\neof\n\n\nsysctl --system\n\n\n更多的相关细节可查看网络插件需求页面。\n\n\n# 检查所需端口\n\n\n# 控制平面节点\n\n协议    方向   端口范围        作用                        使用者\ntcp   入站   6443        kubernetes api 服务器        所有组件\ntcp   入站   2379-2380   etcd 服务器客户端 api           kube-apiserver, etcd\ntcp   入站   10250       kubelet api               kubelet 自身、控制平面组件\ntcp   入站   10251       kube-scheduler            kube-scheduler 自身\ntcp   入站   10252       kube-controller-manager   kube-controller-manager 自身\n\n\n# 工作节点\n\n协议    方向   端口范围          作用             使用者\ntcp   入站   10250         kubelet api    kubelet 自身、控制平面组件\ntcp   入站   30000-32767   nodeport 服务†   所有组件\n\n† nodeport 服务 的默认端口范围。\n\n使用 * 标记的任意端口号都可以被覆盖，所以你需要保证所定制的端口是开放的。\n\n虽然控制平面节点已经包含了 etcd 的端口，你也可以使用自定义的外部 etcd 集群，或是指定自定义端口。\n\n你使用的 pod 网络插件 (见下) 也可能需要某些特定端口开启。由于各个 pod 网络插件都有所不同， 请参阅他们各自文档中对端口的要求。\n\n\n# 安装docker\n\n在每个节点上，根据安装 docker 引擎 为你的 linux 发行版安装 docker。 你可以在此文件中找到最新的经过验证的 docker 版本 依赖关系。\n\nstep 1 : 安装必要的一些系统工具\n\nsudo apt-get update\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common\n\n\nstep 2 : 安装gpg证书\n\nsudo curl -fssl https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n\n\nstep 3 : 写入软件源信息\n\nsudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"\n\n\nstep 4 : 更新并安装v19.03.0版的docker-ce\n\nsudo apt-get install docker-ce=5:19.03.0~3-0~ubuntu-bionic docker-ce-cli=5:19.03.0~3-0~ubuntu-bionic containerd.io -y\n\n\nstep 5 ：配置 docker 守护程序，尤其是使用 systemd 来管理容器的 cgroup，配置镜像加速\n\ncat <<eof | sudo tee /etc/docker/daemon.json\n{\n  "exec-opts": ["native.cgroupdriver=systemd"],\n  "log-driver": "json-file",\n  "log-opts": {\n    "max-size": "100m"\n  },\n  "storage-driver": "overlay2",\n  "registry-mirrors": ["https://registry.cn-beijing.aliyuncs.com"]\n}\neof\n\n\nstep 6 ：重新启动 docker 并在启动时启用\n\nsudo systemctl enable docker\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n\n\nstep 7 ： 建立 docker 用户组\n\n默认情况下，docker 命令会使用 unix socket 与 docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 docker 引擎的 unix socket。出于安全考虑，一般 linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。\n\n切换到普通用户，将当前用户加入 docker 组：\n\n$ sudo usermod -ag docker $user\n\n\n退出当前终端并重新登录，进行如下测试。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"安装 kubeadm",frontmatter:{title:"安装 kubeadm",date:"2022-02-09T13:37:49.000Z",permalink:"/pages/248b3e/"},regularPath:"/22.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/03.%E5%AE%89%E8%A3%85%20kubeadm.html",relativePath:"22.Kubernetes/01.Kubernetes 入门/03.安装 kubeadm.md",key:"v-0792908f",path:"/pages/248b3e/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:39},{level:2,title:"配置软件源",slug:"配置软件源",normalizedTitle:"配置软件源",charIndex:97},{level:2,title:"安装 kubeadm，kubelet，kubectl",slug:"安装-kubeadm-kubelet-kubectl",normalizedTitle:"安装 kubeadm，kubelet，kubectl",charIndex:432},{level:2,title:"设置 kubelet 自启动，并启动 kubelet",slug:"设置-kubelet-自启动-并启动-kubelet",normalizedTitle:"设置 kubelet 自启动，并启动 kubelet",charIndex:778}],headersStr:"概述 配置软件源 安装 kubeadm，kubelet，kubectl 设置 kubelet 自启动，并启动 kubelet",content:'# 安装 kubeadm\n\n提示\n\n无特殊说明以下操作在所有节点执行\n\n\n# 概述\n\nkubeadm 是 kubernetes 的集群安装工具，能够快速安装 kubernetes 集群\n\n\n# 配置软件源\n\n国外教程添加 Google 源（国内屏蔽了）。我们添加阿里源。\n\n1）由于是在非标准的存储库内下载 Kubernetes，所以，需要确保软件被授权。这里，我们通过添加签名密钥完成授权：\n\nsudo curl -fsSL https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -\n\n\n2）然后，添加阿里的源，即可：\n\nsudo apt-add-repository "deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main"\n\n\n3）更新\n\nsudo apt update\n\n\n\n# 安装 kubeadm，kubelet，kubectl\n\n在所有节点上安装指定版本 kubelet、kubeadm 和 kubectl, 注意 kubeadm 的版本号\n\nsudo apt install kubelet=1.20.0-00 kubeadm=1.20.0-00 kubectl=1.20.0-00 -y\n\n\n * kubeadm：用于初始化 Kubernetes 集群\n * kubectl：Kubernetes 的命令行工具，主要作用是部署和管理应用，查看各种资源，创建，删除和更新组件\n * kubelet：主要负责启动 Pod 和容器\n\n注意\n\n安装完成后，要确保kubeadm等程序文件的版本，这将也是后面初始化Kubernetes集群时需要明确指定的版本号。\n\n\n# 设置 kubelet 自启动，并启动 kubelet\n\nsudo systemctl enable kubelet && sudo systemctl start kubelet\n',normalizedContent:'# 安装 kubeadm\n\n提示\n\n无特殊说明以下操作在所有节点执行\n\n\n# 概述\n\nkubeadm 是 kubernetes 的集群安装工具，能够快速安装 kubernetes 集群\n\n\n# 配置软件源\n\n国外教程添加 google 源（国内屏蔽了）。我们添加阿里源。\n\n1）由于是在非标准的存储库内下载 kubernetes，所以，需要确保软件被授权。这里，我们通过添加签名密钥完成授权：\n\nsudo curl -fssl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -\n\n\n2）然后，添加阿里的源，即可：\n\nsudo apt-add-repository "deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main"\n\n\n3）更新\n\nsudo apt update\n\n\n\n# 安装 kubeadm，kubelet，kubectl\n\n在所有节点上安装指定版本 kubelet、kubeadm 和 kubectl, 注意 kubeadm 的版本号\n\nsudo apt install kubelet=1.20.0-00 kubeadm=1.20.0-00 kubectl=1.20.0-00 -y\n\n\n * kubeadm：用于初始化 kubernetes 集群\n * kubectl：kubernetes 的命令行工具，主要作用是部署和管理应用，查看各种资源，创建，删除和更新组件\n * kubelet：主要负责启动 pod 和容器\n\n注意\n\n安装完成后，要确保kubeadm等程序文件的版本，这将也是后面初始化kubernetes集群时需要明确指定的版本号。\n\n\n# 设置 kubelet 自启动，并启动 kubelet\n\nsudo systemctl enable kubelet && sudo systemctl start kubelet\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"配置 kubeadm",frontmatter:{title:"配置 kubeadm",date:"2022-02-09T13:37:49.000Z",permalink:"/pages/f792df/"},regularPath:"/22.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/04.%E9%85%8D%E7%BD%AE%20kubeadm.html",relativePath:"22.Kubernetes/01.Kubernetes 入门/04.配置 kubeadm.md",key:"v-0ba2fba8",path:"/pages/f792df/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:36},{level:2,title:"通过配置文件方式",slug:"通过配置文件方式",normalizedTitle:"通过配置文件方式",charIndex:650},{level:2,title:"查看和拉取镜像",slug:"查看和拉取镜像",normalizedTitle:"查看和拉取镜像",charIndex:1920},{level:2,title:"通过命令行方式",slug:"通过命令行方式",normalizedTitle:"通过命令行方式",charIndex:3048}],headersStr:"概述 通过配置文件方式 查看和拉取镜像 通过命令行方式",content:'# 配置 kubeadm\n\n注意\n\n仅在Master节点上操作\n\n\n# 概述\n\nkubeadm已经进入GA阶段，其控制面初始化和加入节点步骤都支持大量可定制内容，因此kubeadm还提供了配置文件功能用于复杂定制。同时，kubeadm将配置文件以ConfigMap的形式保存到集群中，便于后续的查询和升级工作。kubeadm config子命令提供了对这一组功能的支持：\n\n * kubeadm config upload from-file：由配置文件上传到集群中生成ConfigMap。\n * kubeadm config upload from-flags：由配置参数生成ConfigMap。\n * kubeadm config view：查看当前集群中的配置值。\n * kubeadm config print init-defaults：输出kubeadm init默认参数文件的内容。\n * kubeadm config print join-defaults：输出kubeadm join默认参数文件的内容。\n * kubeadm config migrate：在新旧版本之间进行配置转换。\n * kubeadm config images list：列出所需的镜像列表。\n * kubeadm config images pull：拉取镜像到本地。\n\n官方文档\n\nkubeadm init 打印配置\n\n有两种方式可以初始化Master节点，分别是命令行方式和配置文件方式，这里推荐使用通过配置文件方式\n\n\n# 通过配置文件方式\n\n导出配置文件\n\nkubeadm config print init-defaults > init-config.yaml\n\n\n修改配置为如下内容\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n \n \n\n\n\n\napiVersion: kubeadm.k8s.io/v1beta2\nbootstrapTokens:\n  - groups:\n      - system:bootstrappers:kubeadm:default-node-token\n    token: abcdef.0123456789abcdef\n    ttl: 24h0m0s\n    usages:\n      - signing\n      - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  # 修改为主节点 IP\n  advertiseAddress: 192.168.31.101\n  bindPort: 6443\nnodeRegistration:\n  criSocket: /var/run/dockershim.sock\n  name: k8s-master\n  taints:\n    - effect: NoSchedule\n      key: node-role.kubernetes.io/master\n---\napiServer:\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta2\n# 控制平面的接入端点，我们这里选择适配到192.168.31.101这一IP上\ncontrolPlaneEndpoint: "192.168.31.101:6443"\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrollerManager: {}\ndns:\n  type: CoreDNS\netcd:\n  local:\n    dataDir: /var/lib/etcd\n# 国内不能访问 Google，修改为阿里云\nimageRepository: registry.aliyuncs.com/google_containers\nkind: ClusterConfiguration\n# 版本号要与部署的目标版本保持一致\nkubernetesVersion: v1.20.0\nnetworking:\n  dnsDomain: cluster.local\n  # 配置成 Calico 的默认网段\n  podSubnet: 10.244.0.0/16\n  serviceSubnet: 10.96.0.0/12\nscheduler: {}\n\n\n将上面的内容保存为init-config.yaml备用。\n\n\n# 查看和拉取镜像\n\n查看所需镜像列表\n\nkubeadm config images list --config init-config.yaml\n\n\n拉取镜像\n\nkubeadm config images pull --config init-config.yaml\n\n\n查看镜像\n\nroot@k8s-master:/home/heyuqiang# docker images \nREPOSITORY                                                        TAG        IMAGE ID       CREATED         SIZE\nregistry.aliyuncs.com/google_containers/kube-proxy                v1.20.0    10cc881966cf   5 months ago    118MB\nregistry.aliyuncs.com/google_containers/kube-scheduler            v1.20.0    3138b6e3d471   5 months ago    46.4MB\nregistry.aliyuncs.com/google_containers/kube-apiserver            v1.20.0    ca9843d3b545   5 months ago    122MB\nregistry.aliyuncs.com/google_containers/kube-controller-manager   v1.20.0    b9fa1895dcaa   5 months ago    116MB\nregistry.aliyuncs.com/google_containers/etcd                      3.4.13-0   0369cf4303ff   8 months ago    253MB\nregistry.aliyuncs.com/google_containers/coredns                   1.7.0      bfe3a36ebd25   11 months ago   45.2MB\nregistry.aliyuncs.com/google_containers/pause                     3.2        80d28bedfe5d   15 months ago   683kB\n\n\n在镜像下载完成之后，就可以进行安装了。\n\n\n# 通过命令行方式\n\n# kubeadm init \n--apiserver-advertise-address=192.168.31.101\n--image-repository registry.aliyuncs.com/google_containers \n--kubernetes-version=v1.20.0 \n--service-cidr=10.96.0.0/12 \n--pod-network-cidr=10.244.0.0/16 \n--token-ttl=0\n\n\n命令中的各选项简单说明如下：\n\n * --apiserver-advertise-address：apiserver通告给其它组件的IP地址，一般应该为Master节点用于集群内部通信的IP地址\n\n * --image-repository：指定要使用的镜像仓库，默认为gcr.io\n\n * kubernetes-version：kubernetes程序组件的版本号，必须要与前面安装的版本一致\n\n * --service-cidr：集群内部虚拟网络，Pod统一访问入口；service的网络地址范围，其值为CIDR格式的网络地址，默认地址为10.96.0.0/12\n\n * --pod-network-cidr：Pod网络的地址范围，其值为CIDR格式的网络地址，通常，Flannel网络插件的默认为10.244.0.0/16，Project Calico插件的默认值为192.168.0.0/16；本章使用的是calico插件，所以下面部署CNI网络插件时需要与这里设置的保持一致。\n\n * --token-ttl：共享令牌（token）的过期时长，默认为24小时，0表示永不过期；为防止不安全存储等原因导致的令牌泄露危及集群安全，建议为其设定过期时长。未设定该选项时，在token过期后，若期望再向集群中加入其它节点，可以使用如下命令重新创建 token，并生成节点加入命令。\n   \n   kubeadm token create --print-join-command\n   ',normalizedContent:'# 配置 kubeadm\n\n注意\n\n仅在master节点上操作\n\n\n# 概述\n\nkubeadm已经进入ga阶段，其控制面初始化和加入节点步骤都支持大量可定制内容，因此kubeadm还提供了配置文件功能用于复杂定制。同时，kubeadm将配置文件以configmap的形式保存到集群中，便于后续的查询和升级工作。kubeadm config子命令提供了对这一组功能的支持：\n\n * kubeadm config upload from-file：由配置文件上传到集群中生成configmap。\n * kubeadm config upload from-flags：由配置参数生成configmap。\n * kubeadm config view：查看当前集群中的配置值。\n * kubeadm config print init-defaults：输出kubeadm init默认参数文件的内容。\n * kubeadm config print join-defaults：输出kubeadm join默认参数文件的内容。\n * kubeadm config migrate：在新旧版本之间进行配置转换。\n * kubeadm config images list：列出所需的镜像列表。\n * kubeadm config images pull：拉取镜像到本地。\n\n官方文档\n\nkubeadm init 打印配置\n\n有两种方式可以初始化master节点，分别是命令行方式和配置文件方式，这里推荐使用通过配置文件方式\n\n\n# 通过配置文件方式\n\n导出配置文件\n\nkubeadm config print init-defaults > init-config.yaml\n\n\n修改配置为如下内容\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n \n \n\n\n\n\napiversion: kubeadm.k8s.io/v1beta2\nbootstraptokens:\n  - groups:\n      - system:bootstrappers:kubeadm:default-node-token\n    token: abcdef.0123456789abcdef\n    ttl: 24h0m0s\n    usages:\n      - signing\n      - authentication\nkind: initconfiguration\nlocalapiendpoint:\n  # 修改为主节点 ip\n  advertiseaddress: 192.168.31.101\n  bindport: 6443\nnoderegistration:\n  crisocket: /var/run/dockershim.sock\n  name: k8s-master\n  taints:\n    - effect: noschedule\n      key: node-role.kubernetes.io/master\n---\napiserver:\n  timeoutforcontrolplane: 4m0s\napiversion: kubeadm.k8s.io/v1beta2\n# 控制平面的接入端点，我们这里选择适配到192.168.31.101这一ip上\ncontrolplaneendpoint: "192.168.31.101:6443"\ncertificatesdir: /etc/kubernetes/pki\nclustername: kubernetes\ncontrollermanager: {}\ndns:\n  type: coredns\netcd:\n  local:\n    datadir: /var/lib/etcd\n# 国内不能访问 google，修改为阿里云\nimagerepository: registry.aliyuncs.com/google_containers\nkind: clusterconfiguration\n# 版本号要与部署的目标版本保持一致\nkubernetesversion: v1.20.0\nnetworking:\n  dnsdomain: cluster.local\n  # 配置成 calico 的默认网段\n  podsubnet: 10.244.0.0/16\n  servicesubnet: 10.96.0.0/12\nscheduler: {}\n\n\n将上面的内容保存为init-config.yaml备用。\n\n\n# 查看和拉取镜像\n\n查看所需镜像列表\n\nkubeadm config images list --config init-config.yaml\n\n\n拉取镜像\n\nkubeadm config images pull --config init-config.yaml\n\n\n查看镜像\n\nroot@k8s-master:/home/heyuqiang# docker images \nrepository                                                        tag        image id       created         size\nregistry.aliyuncs.com/google_containers/kube-proxy                v1.20.0    10cc881966cf   5 months ago    118mb\nregistry.aliyuncs.com/google_containers/kube-scheduler            v1.20.0    3138b6e3d471   5 months ago    46.4mb\nregistry.aliyuncs.com/google_containers/kube-apiserver            v1.20.0    ca9843d3b545   5 months ago    122mb\nregistry.aliyuncs.com/google_containers/kube-controller-manager   v1.20.0    b9fa1895dcaa   5 months ago    116mb\nregistry.aliyuncs.com/google_containers/etcd                      3.4.13-0   0369cf4303ff   8 months ago    253mb\nregistry.aliyuncs.com/google_containers/coredns                   1.7.0      bfe3a36ebd25   11 months ago   45.2mb\nregistry.aliyuncs.com/google_containers/pause                     3.2        80d28bedfe5d   15 months ago   683kb\n\n\n在镜像下载完成之后，就可以进行安装了。\n\n\n# 通过命令行方式\n\n# kubeadm init \n--apiserver-advertise-address=192.168.31.101\n--image-repository registry.aliyuncs.com/google_containers \n--kubernetes-version=v1.20.0 \n--service-cidr=10.96.0.0/12 \n--pod-network-cidr=10.244.0.0/16 \n--token-ttl=0\n\n\n命令中的各选项简单说明如下：\n\n * --apiserver-advertise-address：apiserver通告给其它组件的ip地址，一般应该为master节点用于集群内部通信的ip地址\n\n * --image-repository：指定要使用的镜像仓库，默认为gcr.io\n\n * kubernetes-version：kubernetes程序组件的版本号，必须要与前面安装的版本一致\n\n * --service-cidr：集群内部虚拟网络，pod统一访问入口；service的网络地址范围，其值为cidr格式的网络地址，默认地址为10.96.0.0/12\n\n * --pod-network-cidr：pod网络的地址范围，其值为cidr格式的网络地址，通常，flannel网络插件的默认为10.244.0.0/16，project calico插件的默认值为192.168.0.0/16；本章使用的是calico插件，所以下面部署cni网络插件时需要与这里设置的保持一致。\n\n * --token-ttl：共享令牌（token）的过期时长，默认为24小时，0表示永不过期；为防止不安全存储等原因导致的令牌泄露危及集群安全，建议为其设定过期时长。未设定该选项时，在token过期后，若期望再向集群中加入其它节点，可以使用如下命令重新创建 token，并生成节点加入命令。\n   \n   kubeadm token create --print-join-command\n   ',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"使用 kubeadm 部署 Master 节点",frontmatter:{title:"使用 kubeadm 部署 Master 节点",date:"2022-02-09T13:37:49.000Z",permalink:"/pages/91b30f/"},regularPath:"/22.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/05.%E4%BD%BF%E7%94%A8%20kubeadm%20%E9%83%A8%E7%BD%B2%20Master%20%E8%8A%82%E7%82%B9.html",relativePath:"22.Kubernetes/01.Kubernetes 入门/05.使用 kubeadm 部署 Master 节点.md",key:"v-89e63956",path:"/pages/91b30f/",headers:[{level:2,title:"运行 kubeadm init 命令安装 Master",slug:"运行-kubeadm-init-命令安装-master",normalizedTitle:"运行 kubeadm init 命令安装 master",charIndex:49},{level:2,title:"配置 kubectl",slug:"配置-kubectl",normalizedTitle:"配置 kubectl",charIndex:7970},{level:2,title:"验证是否成功",slug:"验证是否成功",normalizedTitle:"验证是否成功",charIndex:8645},{level:2,title:"kubeadm init 的执行过程",slug:"kubeadm-init-的执行过程",normalizedTitle:"kubeadm init 的执行过程",charIndex:11618}],headersStr:"运行 kubeadm init 命令安装 Master 配置 kubectl 验证是否成功 kubeadm init 的执行过程",content:'# 使用 kubeadm 部署 Master 节点\n\n注意\n\n仅在Master节点上操作\n\n\n# 运行 kubeadm init 命令安装 Master\n\n至此，准备工作已经就绪。执行 kubeadm init 命令即可一键安装 Kubernetes 的 Master。\n\n在开始之前需要注意：kubeadm 的安装过程不涉及网络插件（CNI）的初始化，因此 kubeadm 初步安装完成的集群不具备网络功能，任何 Pod 包括自带的 CoreDNS 都无法正常工作。而网络插件的安装往往对 kubeadm init 命令的参数有一定的要求。例如，安装 Calico 插件时需要指定 --pod-network-cidr=192.168.0.0/16。\n\n接下来使用 kubeadm init 命令，使用前面创建的配置文件进行集群的初始化：\n\nkubeadm init --config=init-config.yaml | tee kubeadm-init.log\n\n\n追加的 tee kubeadm-init.log 用以输出日志。运行后，控制台将输出如下内容：\n\nroot@k8s-master:/home/heyuqiang# kubeadm init --config=init-config.yaml | tee kubeadm-init.log\n[init] Using Kubernetes version: v1.20.0\n[preflight] Running pre-flight checks\n        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.0. Latest validated version: 19.03\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using \'kubeadm config images pull\'\n[certs] Using certificateDir folder "/etc/kubernetes/pki"\n[certs] Generating "ca" certificate and key\n[certs] Generating "apiserver" certificate and key\n[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.31.101]\n[certs] Generating "apiserver-kubelet-client" certificate and key\n[certs] Generating "front-proxy-ca" certificate and key\n[certs] Generating "front-proxy-client" certificate and key\n[certs] Generating "etcd/ca" certificate and key\n[certs] Generating "etcd/server" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.31.101 127.0.0.1 ::1]\n[certs] Generating "etcd/peer" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.31.101 127.0.0.1 ::1]\n[certs] Generating "etcd/healthcheck-client" certificate and key\n[certs] Generating "apiserver-etcd-client" certificate and key\n[certs] Generating "sa" key and public key\n[kubeconfig] Using kubeconfig folder "/etc/kubernetes"\n[kubeconfig] Writing "admin.conf" kubeconfig file\n[kubeconfig] Writing "kubelet.conf" kubeconfig file\n[kubeconfig] Writing "controller-manager.conf" kubeconfig file\n[kubeconfig] Writing "scheduler.conf" kubeconfig file\n[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"\n[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"\n[kubelet-start] Starting the kubelet\n[control-plane] Using manifest folder "/etc/kubernetes/manifests"\n[control-plane] Creating static Pod manifest for "kube-apiserver"\n[control-plane] Creating static Pod manifest for "kube-controller-manager"\n[control-plane] Creating static Pod manifest for "kube-scheduler"\n[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s\n[apiclient] All control plane components are healthy after 24.004707 seconds\n[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace\n[kubelet] Creating a ConfigMap "kubelet-config-1.20" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node k8s-master as control-plane by adding the labels "node-role.kubernetes.io/master=\'\'" and "node-role.kubernetes.io/control-plane=\'\' (deprecated)"\n[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[bootstrap-token] Using token: abcdef.0123456789abcdef\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace\n[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun "kubectl apply -f [podnetwork].yaml" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of control-plane nodes by copying certificate authorities\nand service account keys on each node and then running the following as root:\n\n  kubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \\\n    --control-plane \n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \n\n\n这里有一个警告，意思是说Docker的版本不在Kubernetes的可用列表中，不过经过测试不影响使用，强迫症可以选择降级Docker版本\n\n[preflight] Running pre-flight checks\n        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.0. Latest validated version: 19.03\n\n\n这里来说明一下上图返回的信息中我们需要注意的地方：\n\n# 下面是成功完成第一个控制平面节点初始化的提示信息及后续需要完成的步骤\nYour Kubernetes control-plane has initialized successfully!\n\n# 要开始使用群集，您需要以普通用户身份运行以下命令：\nTo start using your cluster, you need to run the following as a regular user:\n\n  # Setup 1 : 复制Kubernetes集群管理员认证到Kubernetes集群时使用的kubeconfig配置文件：\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n# 我们也可以不做上述设定，而使用环境变量KUBECONFIG为kubelet等指定默认使用的kubeconfig：\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\n# Setup 2 : 管理员需要使用网络插件为Kubernetes集群部署Pod网络，具体选用的插件取决于管理员：\nYou should now deploy a pod network to the cluster.\nRun "kubectl apply -f [podnetwork].yaml" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\n# Setup 3 : 使用root用户身份运行一下命令，可以通过证书认证方式添加控制节点：\nYou can now join any number of control-plane nodes by copying certificate authorities\nand service account keys on each node and then running the following as root:\n\n  kubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \\\n    --control-plane \n\n# Setup 4 : 使用root用户身份运行一下命令，可以通过证书认证方式添加任意数量的工作节点：\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \n\n\n注意\n\n记录下初始化结果中的kubeadm join命令，部署worker节点时会用到\n\n提示\n\n如果安装 kubernetes 版本和下载的镜像版本不统一则会出现 timed out waiting for the condition 错误。中途失败或是想修改配置可以使用 kubeadm reset 命令重置配置，再做初始化操作即可。\n\n\n# 配置 kubectl\n\nkubectl 是管理 Kubernetes Cluster 的命令行工具，前面我们已经在所有的节点安装了 kubectl。Master 初始化完成后需要做一些配置工作，然后 kubectl 就能使用了。 依照 kubeadm init 输出的最后提示，推荐用 Linux 普通用户执行 kubectl。\n\n切换到普通用户ubuntu，保存集群安全配置文件到当前用户.kube目录\n\nsu - ubuntu\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n#启用 kubectl 命令自动补全功能（注销重新登录生效）\necho "source <(kubectl completion bash)" >> ~/.bashrc\n\n\n需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的 .kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。 如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。 配置完成后 ubuntu 用户就可以使用 kubectl 命令管理集群了。\n\n\n# 验证是否成功\n\n确认各个组件都处于healthy状态。 查看节点状态\n\n[ubuntu@k8s-master ~]$ kubectl get nodes\nNAME         STATUS     ROLES    AGE   VERSION\nk8s-master   Ready   master   23m   v1.20.0\n\n\n可以看到，当前只存在1个master节点，并且这个节点的状态是 NotReady。 使用 kubectl describe 命令来查看这个节点（Node）对象的详细信息、状态和事件（Event）：\n\nubuntu@k8s-master:~/.kube$ kubectl describe node k8s-master\n......\nEvents:\n  Type    Reason                   Age                From        Message\n  ----    ------                   ----               ----        -------\n  Normal  NodeHasSufficientMemory  43m (x7 over 43m)  kubelet     Node k8s-master status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    43m (x7 over 43m)  kubelet     Node k8s-master status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     43m (x7 over 43m)  kubelet     Node k8s-master status is now: NodeHasSufficientPID\n  Normal  Starting                 42m                kubelet     Starting kubelet.\n  Normal  NodeHasSufficientMemory  42m                kubelet     Node k8s-master status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    42m                kubelet     Node k8s-master status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     42m                kubelet     Node k8s-master status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  42m                kubelet     Updated Node Allocatable limit across pods\n  Normal  Starting                 42m                kube-proxy  Starting kube-proxy.\n\n\n通过 kubectl describe 指令的输出，我们可以看到 NodeNotReady 的原因在于，我们尚未部署任何网络插件，kube-proxy 等组件还处于 starting 状态。\n\n另外，我们还可以通过 kubectl 检查这个节点上各个系统 Pod 的状态，其中，kube-system 是 Kubernetes 项目预留的系统 Pod 的工作空间（Namespace，注意它并不是 Linux Namespace，它只是 Kubernetes 划分不同工作空间的单位）：\n\nkubectl get pod -n kube-system -o wide\n\n\nubuntu@k8s-master:~$ kubectl get pod -n kube-system -o wide\nNAME                                      READY   STATUS    RESTARTS   AGE   IP           NODE              NOMINATED NODE   READINESS GATES\ncoredns-7f89b7bc75-d4drw                  0/1     Pending   0          44m   <none>       <none>            <none>           <none>\ncoredns-7f89b7bc75-xzjfp                  0/1     Pending   0          44m   <none>       <none>            <none>           <none>\netcd-k8s-master                      1/1     Running   0          44m   192.168.31.101   k8s-master   <none>           <none>\nkube-apiserver-k8s-master            1/1     Running   0          44m   192.168.31.101   k8s-master   <none>           <none>\nkube-controller-manager-k8s-master   1/1     Running   0          44m   192.168.31.101   k8s-master   <none>           <none>\nkube-proxy-55q6f                          1/1     Running   0          44m   192.168.31.101   k8s-master   <none>           <none>\nkube-scheduler-k8s-master            1/1     Running   0          44m   192.168.31.101   k8s-master   <none>           <none>\n\n\n可以看到，CoreDNS 依赖于网络的 Pod 都处于 Pending 状态，即调度失败。这当然是符合预期的：因为这个 Master 节点的网络尚未就绪。\n\n至此主节点配置完成。\n\n\n# kubeadm init 的执行过程\n\n * init：指定版本进行初始化操作\n * preflight：初始化前的检查和下载所需要的 Docker 镜像文件\n * kubelet-start：生成 kubelet 的配置文件 var/lib/kubelet/config.yaml，没有这个文件 kubelet 无法启动，所以初始化之前的 kubelet 实际上启动不会成功\n * certificates：生成 Kubernetes 使用的证书，存放在 /etc/kubernetes/pki 目录中\n * kubeconfig：生成 KubeConfig 文件，存放在 /etc/kubernetes 目录中，组件之间通信需要使用对应文件\n * control-plane：使用 /etc/kubernetes/manifest 目录下的 YAML 文件，安装 Master 组件\n * etcd：使用 /etc/kubernetes/manifest/etcd.yaml 安装 Etcd 服务\n * wait-control-plane：等待 control-plan 部署的 Master 组件启动\n * apiclient：检查 Master 组件服务状态。\n * uploadconfig：更新配置\n * kubelet：使用 configMap 配置 kubelet\n * patchnode：更新 CNI 信息到 Node 上，通过注释的方式记录\n * mark-control-plane：为当前节点打标签，打了角色 Master，和不可调度标签，这样默认就不会使用 Master 节点来运行 Pod\n * bootstrap-token：生成 token 记录下来，后边使用 kubeadm join 往集群中添加节点时会用到\n * addons：安装附加组件 CoreDNS 和 kube-proxy',normalizedContent:'# 使用 kubeadm 部署 master 节点\n\n注意\n\n仅在master节点上操作\n\n\n# 运行 kubeadm init 命令安装 master\n\n至此，准备工作已经就绪。执行 kubeadm init 命令即可一键安装 kubernetes 的 master。\n\n在开始之前需要注意：kubeadm 的安装过程不涉及网络插件（cni）的初始化，因此 kubeadm 初步安装完成的集群不具备网络功能，任何 pod 包括自带的 coredns 都无法正常工作。而网络插件的安装往往对 kubeadm init 命令的参数有一定的要求。例如，安装 calico 插件时需要指定 --pod-network-cidr=192.168.0.0/16。\n\n接下来使用 kubeadm init 命令，使用前面创建的配置文件进行集群的初始化：\n\nkubeadm init --config=init-config.yaml | tee kubeadm-init.log\n\n\n追加的 tee kubeadm-init.log 用以输出日志。运行后，控制台将输出如下内容：\n\nroot@k8s-master:/home/heyuqiang# kubeadm init --config=init-config.yaml | tee kubeadm-init.log\n[init] using kubernetes version: v1.20.0\n[preflight] running pre-flight checks\n        [warning systemverification]: this docker version is not on the list of validated versions: 20.10.0. latest validated version: 19.03\n[preflight] pulling images required for setting up a kubernetes cluster\n[preflight] this might take a minute or two, depending on the speed of your internet connection\n[preflight] you can also perform this action in beforehand using \'kubeadm config images pull\'\n[certs] using certificatedir folder "/etc/kubernetes/pki"\n[certs] generating "ca" certificate and key\n[certs] generating "apiserver" certificate and key\n[certs] apiserver serving cert is signed for dns names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and ips [10.96.0.1 192.168.31.101]\n[certs] generating "apiserver-kubelet-client" certificate and key\n[certs] generating "front-proxy-ca" certificate and key\n[certs] generating "front-proxy-client" certificate and key\n[certs] generating "etcd/ca" certificate and key\n[certs] generating "etcd/server" certificate and key\n[certs] etcd/server serving cert is signed for dns names [k8s-master localhost] and ips [192.168.31.101 127.0.0.1 ::1]\n[certs] generating "etcd/peer" certificate and key\n[certs] etcd/peer serving cert is signed for dns names [k8s-master localhost] and ips [192.168.31.101 127.0.0.1 ::1]\n[certs] generating "etcd/healthcheck-client" certificate and key\n[certs] generating "apiserver-etcd-client" certificate and key\n[certs] generating "sa" key and public key\n[kubeconfig] using kubeconfig folder "/etc/kubernetes"\n[kubeconfig] writing "admin.conf" kubeconfig file\n[kubeconfig] writing "kubelet.conf" kubeconfig file\n[kubeconfig] writing "controller-manager.conf" kubeconfig file\n[kubeconfig] writing "scheduler.conf" kubeconfig file\n[kubelet-start] writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"\n[kubelet-start] writing kubelet configuration to file "/var/lib/kubelet/config.yaml"\n[kubelet-start] starting the kubelet\n[control-plane] using manifest folder "/etc/kubernetes/manifests"\n[control-plane] creating static pod manifest for "kube-apiserver"\n[control-plane] creating static pod manifest for "kube-controller-manager"\n[control-plane] creating static pod manifest for "kube-scheduler"\n[etcd] creating static pod manifest for local etcd in "/etc/kubernetes/manifests"\n[wait-control-plane] waiting for the kubelet to boot up the control plane as static pods from directory "/etc/kubernetes/manifests". this can take up to 4m0s\n[apiclient] all control plane components are healthy after 24.004707 seconds\n[upload-config] storing the configuration used in configmap "kubeadm-config" in the "kube-system" namespace\n[kubelet] creating a configmap "kubelet-config-1.20" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] skipping phase. please see --upload-certs\n[mark-control-plane] marking the node k8s-master as control-plane by adding the labels "node-role.kubernetes.io/master=\'\'" and "node-role.kubernetes.io/control-plane=\'\' (deprecated)"\n[mark-control-plane] marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:noschedule]\n[bootstrap-token] using token: abcdef.0123456789abcdef\n[bootstrap-token] configuring bootstrap tokens, cluster-info configmap, rbac roles\n[bootstrap-token] configured rbac rules to allow node bootstrap tokens to get nodes\n[bootstrap-token] configured rbac rules to allow node bootstrap tokens to post csrs in order for nodes to get long term certificate credentials\n[bootstrap-token] configured rbac rules to allow the csrapprover controller automatically approve csrs from a node bootstrap token\n[bootstrap-token] configured rbac rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] creating the "cluster-info" configmap in the "kube-public" namespace\n[kubelet-finalize] updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key\n[addons] applied essential addon: coredns\n[addons] applied essential addon: kube-proxy\n\nyour kubernetes control-plane has initialized successfully!\n\nto start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $home/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $home/.kube/config\n  sudo chown $(id -u):$(id -g) $home/.kube/config\n\nalternatively, if you are the root user, you can run:\n\n  export kubeconfig=/etc/kubernetes/admin.conf\n\nyou should now deploy a pod network to the cluster.\nrun "kubectl apply -f [podnetwork].yaml" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nyou can now join any number of control-plane nodes by copying certificate authorities\nand service account keys on each node and then running the following as root:\n\n  kubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \\\n    --control-plane \n\nthen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \n\n\n这里有一个警告，意思是说docker的版本不在kubernetes的可用列表中，不过经过测试不影响使用，强迫症可以选择降级docker版本\n\n[preflight] running pre-flight checks\n        [warning systemverification]: this docker version is not on the list of validated versions: 20.10.0. latest validated version: 19.03\n\n\n这里来说明一下上图返回的信息中我们需要注意的地方：\n\n# 下面是成功完成第一个控制平面节点初始化的提示信息及后续需要完成的步骤\nyour kubernetes control-plane has initialized successfully!\n\n# 要开始使用群集，您需要以普通用户身份运行以下命令：\nto start using your cluster, you need to run the following as a regular user:\n\n  # setup 1 : 复制kubernetes集群管理员认证到kubernetes集群时使用的kubeconfig配置文件：\n  mkdir -p $home/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $home/.kube/config\n  sudo chown $(id -u):$(id -g) $home/.kube/config\n\n# 我们也可以不做上述设定，而使用环境变量kubeconfig为kubelet等指定默认使用的kubeconfig：\nalternatively, if you are the root user, you can run:\n\n  export kubeconfig=/etc/kubernetes/admin.conf\n\n# setup 2 : 管理员需要使用网络插件为kubernetes集群部署pod网络，具体选用的插件取决于管理员：\nyou should now deploy a pod network to the cluster.\nrun "kubectl apply -f [podnetwork].yaml" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\n# setup 3 : 使用root用户身份运行一下命令，可以通过证书认证方式添加控制节点：\nyou can now join any number of control-plane nodes by copying certificate authorities\nand service account keys on each node and then running the following as root:\n\n  kubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \\\n    --control-plane \n\n# setup 4 : 使用root用户身份运行一下命令，可以通过证书认证方式添加任意数量的工作节点：\nthen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \n\n\n注意\n\n记录下初始化结果中的kubeadm join命令，部署worker节点时会用到\n\n提示\n\n如果安装 kubernetes 版本和下载的镜像版本不统一则会出现 timed out waiting for the condition 错误。中途失败或是想修改配置可以使用 kubeadm reset 命令重置配置，再做初始化操作即可。\n\n\n# 配置 kubectl\n\nkubectl 是管理 kubernetes cluster 的命令行工具，前面我们已经在所有的节点安装了 kubectl。master 初始化完成后需要做一些配置工作，然后 kubectl 就能使用了。 依照 kubeadm init 输出的最后提示，推荐用 linux 普通用户执行 kubectl。\n\n切换到普通用户ubuntu，保存集群安全配置文件到当前用户.kube目录\n\nsu - ubuntu\nmkdir -p $home/.kube\nsudo cp -i /etc/kubernetes/admin.conf $home/.kube/config\nsudo chown $(id -u):$(id -g) $home/.kube/config\n\n#启用 kubectl 命令自动补全功能（注销重新登录生效）\necho "source <(kubectl completion bash)" >> ~/.bashrc\n\n\n需要这些配置命令的原因是：kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 kubernetes 集群的安全配置文件，保存到当前用户的 .kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 kubernetes 集群。 如果不这么做的话，我们每次都需要通过 export kubeconfig 环境变量告诉 kubectl 这个安全配置文件的位置。 配置完成后 ubuntu 用户就可以使用 kubectl 命令管理集群了。\n\n\n# 验证是否成功\n\n确认各个组件都处于healthy状态。 查看节点状态\n\n[ubuntu@k8s-master ~]$ kubectl get nodes\nname         status     roles    age   version\nk8s-master   ready   master   23m   v1.20.0\n\n\n可以看到，当前只存在1个master节点，并且这个节点的状态是 notready。 使用 kubectl describe 命令来查看这个节点（node）对象的详细信息、状态和事件（event）：\n\nubuntu@k8s-master:~/.kube$ kubectl describe node k8s-master\n......\nevents:\n  type    reason                   age                from        message\n  ----    ------                   ----               ----        -------\n  normal  nodehassufficientmemory  43m (x7 over 43m)  kubelet     node k8s-master status is now: nodehassufficientmemory\n  normal  nodehasnodiskpressure    43m (x7 over 43m)  kubelet     node k8s-master status is now: nodehasnodiskpressure\n  normal  nodehassufficientpid     43m (x7 over 43m)  kubelet     node k8s-master status is now: nodehassufficientpid\n  normal  starting                 42m                kubelet     starting kubelet.\n  normal  nodehassufficientmemory  42m                kubelet     node k8s-master status is now: nodehassufficientmemory\n  normal  nodehasnodiskpressure    42m                kubelet     node k8s-master status is now: nodehasnodiskpressure\n  normal  nodehassufficientpid     42m                kubelet     node k8s-master status is now: nodehassufficientpid\n  normal  nodeallocatableenforced  42m                kubelet     updated node allocatable limit across pods\n  normal  starting                 42m                kube-proxy  starting kube-proxy.\n\n\n通过 kubectl describe 指令的输出，我们可以看到 nodenotready 的原因在于，我们尚未部署任何网络插件，kube-proxy 等组件还处于 starting 状态。\n\n另外，我们还可以通过 kubectl 检查这个节点上各个系统 pod 的状态，其中，kube-system 是 kubernetes 项目预留的系统 pod 的工作空间（namespace，注意它并不是 linux namespace，它只是 kubernetes 划分不同工作空间的单位）：\n\nkubectl get pod -n kube-system -o wide\n\n\nubuntu@k8s-master:~$ kubectl get pod -n kube-system -o wide\nname                                      ready   status    restarts   age   ip           node              nominated node   readiness gates\ncoredns-7f89b7bc75-d4drw                  0/1     pending   0          44m   <none>       <none>            <none>           <none>\ncoredns-7f89b7bc75-xzjfp                  0/1     pending   0          44m   <none>       <none>            <none>           <none>\netcd-k8s-master                      1/1     running   0          44m   192.168.31.101   k8s-master   <none>           <none>\nkube-apiserver-k8s-master            1/1     running   0          44m   192.168.31.101   k8s-master   <none>           <none>\nkube-controller-manager-k8s-master   1/1     running   0          44m   192.168.31.101   k8s-master   <none>           <none>\nkube-proxy-55q6f                          1/1     running   0          44m   192.168.31.101   k8s-master   <none>           <none>\nkube-scheduler-k8s-master            1/1     running   0          44m   192.168.31.101   k8s-master   <none>           <none>\n\n\n可以看到，coredns 依赖于网络的 pod 都处于 pending 状态，即调度失败。这当然是符合预期的：因为这个 master 节点的网络尚未就绪。\n\n至此主节点配置完成。\n\n\n# kubeadm init 的执行过程\n\n * init：指定版本进行初始化操作\n * preflight：初始化前的检查和下载所需要的 docker 镜像文件\n * kubelet-start：生成 kubelet 的配置文件 var/lib/kubelet/config.yaml，没有这个文件 kubelet 无法启动，所以初始化之前的 kubelet 实际上启动不会成功\n * certificates：生成 kubernetes 使用的证书，存放在 /etc/kubernetes/pki 目录中\n * kubeconfig：生成 kubeconfig 文件，存放在 /etc/kubernetes 目录中，组件之间通信需要使用对应文件\n * control-plane：使用 /etc/kubernetes/manifest 目录下的 yaml 文件，安装 master 组件\n * etcd：使用 /etc/kubernetes/manifest/etcd.yaml 安装 etcd 服务\n * wait-control-plane：等待 control-plan 部署的 master 组件启动\n * apiclient：检查 master 组件服务状态。\n * uploadconfig：更新配置\n * kubelet：使用 configmap 配置 kubelet\n * patchnode：更新 cni 信息到 node 上，通过注释的方式记录\n * mark-control-plane：为当前节点打标签，打了角色 master，和不可调度标签，这样默认就不会使用 master 节点来运行 pod\n * bootstrap-token：生成 token 记录下来，后边使用 kubeadm join 往集群中添加节点时会用到\n * addons：安装附加组件 coredns 和 kube-proxy',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"安装网络插件",frontmatter:{title:"安装网络插件",date:"2022-02-09T13:37:49.000Z",permalink:"/pages/1d4070/"},regularPath:"/22.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/06.%E5%AE%89%E8%A3%85%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6.html",relativePath:"22.Kubernetes/01.Kubernetes 入门/06.安装网络插件.md",key:"v-080f0c7c",path:"/pages/1d4070/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:43},{level:2,title:"什么是 CNI",slug:"什么是-cni",normalizedTitle:"什么是 cni",charIndex:295},{level:2,title:"Kubernetes 中的 CNI 插件",slug:"kubernetes-中的-cni-插件",normalizedTitle:"kubernetes 中的 cni 插件",charIndex:471},{level:2,title:"什么是 Calico",slug:"什么是-calico",normalizedTitle:"什么是 calico",charIndex:938},{level:2,title:"安装网络插件 Calico",slug:"安装网络插件-calico",normalizedTitle:"安装网络插件 calico",charIndex:1148},{level:2,title:"下载配置文件",slug:"下载配置文件",normalizedTitle:"下载配置文件",charIndex:1288},{level:2,title:"修改配置",slug:"修改配置",normalizedTitle:"修改配置",charIndex:1359},{level:2,title:"应用配置清单并部署网络插件",slug:"应用配置清单并部署网络插件",normalizedTitle:"应用配置清单并部署网络插件",charIndex:1689},{level:2,title:"确认安装是否成功",slug:"确认安装是否成功",normalizedTitle:"确认安装是否成功",charIndex:3722},{level:2,title:"解决 ImagePullBackOff",slug:"解决-imagepullbackoff",normalizedTitle:"解决 imagepullbackoff",charIndex:4233}],headersStr:"概述 什么是 CNI Kubernetes 中的 CNI 插件 什么是 Calico 安装网络插件 Calico 下载配置文件 修改配置 应用配置清单并部署网络插件 确认安装是否成功 解决 ImagePullBackOff",content:'# 安装网络插件\n\n注意\n\n仅 Master 节点安装即可，工作节点无需安装\n\n\n# 概述\n\n容器网络是容器选择连接到其他容器、主机和外部网络的机制。容器的 runtime 提供了各种网络模式，每种模式都会产生不同的体验。例如，Docker 默认情况下可以为容器配置以下网络：\n\n * none： 将容器添加到一个容器专门的网络堆栈中，没有对外连接。\n * host： 将容器添加到主机的网络堆栈中，没有隔离。\n * default bridge： 默认网络模式。每个容器可以通过 IP 地址相互连接。\n * 自定义网桥： 用户定义的网桥，具有更多的灵活性、隔离性和其他便利功能。\n\n\n# 什么是 CNI\n\nCNI(Container Network Interface) 是一个标准的，通用的接口。在容器平台，Docker，Kubernetes，Mesos 容器网络解决方案 flannel，calico，weave。只要提供一个标准的接口，就能为同样满足该协议的所有容器平台提供网络功能，而 CNI 正是这样的一个标准接口协议。\n\n\n# Kubernetes 中的 CNI 插件\n\nCNI 的初衷是创建一个框架，用于在配置或销毁容器时动态配置适当的网络配置和资源。插件负责为接口配置和管理 IP 地址，并且通常提供与 IP 管理、每个容器的 IP 分配、以及多主机连接相关的功能。容器运行时会调用网络插件，从而在容器启动时分配 IP 地址并配置网络，并在删除容器时再次调用它以清理这些资源。\n\n运行时或协调器决定了容器应该加入哪个网络以及它需要调用哪个插件。然后，插件会将接口添加到容器网络命名空间中，作为一个 veth 对的一侧。接着，它会在主机上进行更改，包括将 veth 的其他部分连接到网桥。再之后，它会通过调用单独的 IPAM（IP地址管理）插件来分配 IP 地址并设置路由。\n\n在 Kubernetes 中，kubelet 可以在适当的时间调用它找到的插件，为通过 kubelet 启动的 pod进行自动的网络配置。\n\nKubernetes 中可选的 CNI 插件如下：\n\n * Flannel\n * Calico\n * Canal\n * Weave\n\n\n# 什么是 Calico\n\nCalico 为容器和虚拟机提供了安全的网络连接解决方案，并经过了大规模生产验证（在公有云和跨数千个集群节点中），可与 Kubernetes，OpenShift，Docker，Mesos，DC / OS 和 OpenStack 集成。\n\nCalico 还提供网络安全规则的动态实施。使用 Calico 的简单策略语言，您可以实现对容器，虚拟机工作负载和裸机主机端点之间通信的细粒度控制。\n\n\n# 安装网络插件 Calico\n\n提示\n\n截止到文章发表日期 2021 年 05 月 18 日，Calico 官方版本为 v3.18.3\n\n参考官方文档安装：https://docs.projectcalico.org/getting-started/kubernetes/\n\n\n# 下载配置文件\n\nwget https://docs.projectcalico.org/manifests/calico.yaml\n\n\n\n# 修改配置\n\n下载完后还需要修改里面定义Pod网络（CALICO_IPV4POOL_CIDR），与前面kubeadm init命令中使用的--pod-network-cidr选项指定的一样，大概在calico.yaml文件的第3684行左右（不同版本位置可能不一样）\n\n# 找到如下内容\n            # - name: CALICO_IPV4POOL_CIDR\n            #   value: "192.168.0.0/16"\n# 去掉注释并替换为          \n            - name: CALICO_IPV4POOL_CIDR\n              value: "10.244.0.0/16"\n\n\n\n# 应用配置清单并部署网络插件\n\n在 Master 节点操作即可\n\nkubectl apply -f calico.yaml\n\n\n安装时显示如下输出\n\nubuntu@k8s-master:~$ kubectl apply -f calico.yaml\nconfigmap/calico-config created\ncustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\nclusterrole.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrole.rbac.authorization.k8s.io/calico-node created\nclusterrolebinding.rbac.authorization.k8s.io/calico-node created\ndaemonset.apps/calico-node created\nserviceaccount/calico-node created\ndeployment.apps/calico-kube-controllers created\nserviceaccount/calico-kube-controllers created\npoddisruptionbudget.policy/calico-kube-controllers created\n\n\n\n# 确认安装是否成功\n\nkubectl get pod -n kube-system -l k8s-app=calico-node\n\n# 需要等待所有状态为 Running，注意时间可能较久，3 - 5 分钟的样子\nNAME                READY   STATUS    RESTARTS   AGE\ncalico-node-r52c6   1/1     Running   0          114s\n\n\n再次查看master节点状态已经为ready状态：\n\nkubectl get nodes\n\n# 控制台输出\nNAME              STATUS   ROLES                  AGE   VERSION\nk8s-master   Ready    control-plane,master   16h   v1.20.0\n\n\n至此，Kubernetes 的 Master 节点就部署完成了。如果你只需要一个单节点的 Kubernetes，现在你就可以使用了。不过，在默认情况下，Kubernetes 的 Master 节点是不能运行用户 Pod 的。\n\n\n# 解决 ImagePullBackOff\n\n在使用 watch kubectl get pods --all-namespaces 命令观察 Pods 状态时如果出现 ImagePullBackOff 无法 Running 的情况，请尝试使用如下步骤处理：\n\n * Master 中删除 Nodes：kubectl delete nodes <NAME>\n * Slave 中重置配置：kubeadm reset\n * Slave 重启计算机：reboot\n * Slave 重新加入集群：kubeadm join',normalizedContent:'# 安装网络插件\n\n注意\n\n仅 master 节点安装即可，工作节点无需安装\n\n\n# 概述\n\n容器网络是容器选择连接到其他容器、主机和外部网络的机制。容器的 runtime 提供了各种网络模式，每种模式都会产生不同的体验。例如，docker 默认情况下可以为容器配置以下网络：\n\n * none： 将容器添加到一个容器专门的网络堆栈中，没有对外连接。\n * host： 将容器添加到主机的网络堆栈中，没有隔离。\n * default bridge： 默认网络模式。每个容器可以通过 ip 地址相互连接。\n * 自定义网桥： 用户定义的网桥，具有更多的灵活性、隔离性和其他便利功能。\n\n\n# 什么是 cni\n\ncni(container network interface) 是一个标准的，通用的接口。在容器平台，docker，kubernetes，mesos 容器网络解决方案 flannel，calico，weave。只要提供一个标准的接口，就能为同样满足该协议的所有容器平台提供网络功能，而 cni 正是这样的一个标准接口协议。\n\n\n# kubernetes 中的 cni 插件\n\ncni 的初衷是创建一个框架，用于在配置或销毁容器时动态配置适当的网络配置和资源。插件负责为接口配置和管理 ip 地址，并且通常提供与 ip 管理、每个容器的 ip 分配、以及多主机连接相关的功能。容器运行时会调用网络插件，从而在容器启动时分配 ip 地址并配置网络，并在删除容器时再次调用它以清理这些资源。\n\n运行时或协调器决定了容器应该加入哪个网络以及它需要调用哪个插件。然后，插件会将接口添加到容器网络命名空间中，作为一个 veth 对的一侧。接着，它会在主机上进行更改，包括将 veth 的其他部分连接到网桥。再之后，它会通过调用单独的 ipam（ip地址管理）插件来分配 ip 地址并设置路由。\n\n在 kubernetes 中，kubelet 可以在适当的时间调用它找到的插件，为通过 kubelet 启动的 pod进行自动的网络配置。\n\nkubernetes 中可选的 cni 插件如下：\n\n * flannel\n * calico\n * canal\n * weave\n\n\n# 什么是 calico\n\ncalico 为容器和虚拟机提供了安全的网络连接解决方案，并经过了大规模生产验证（在公有云和跨数千个集群节点中），可与 kubernetes，openshift，docker，mesos，dc / os 和 openstack 集成。\n\ncalico 还提供网络安全规则的动态实施。使用 calico 的简单策略语言，您可以实现对容器，虚拟机工作负载和裸机主机端点之间通信的细粒度控制。\n\n\n# 安装网络插件 calico\n\n提示\n\n截止到文章发表日期 2021 年 05 月 18 日，calico 官方版本为 v3.18.3\n\n参考官方文档安装：https://docs.projectcalico.org/getting-started/kubernetes/\n\n\n# 下载配置文件\n\nwget https://docs.projectcalico.org/manifests/calico.yaml\n\n\n\n# 修改配置\n\n下载完后还需要修改里面定义pod网络（calico_ipv4pool_cidr），与前面kubeadm init命令中使用的--pod-network-cidr选项指定的一样，大概在calico.yaml文件的第3684行左右（不同版本位置可能不一样）\n\n# 找到如下内容\n            # - name: calico_ipv4pool_cidr\n            #   value: "192.168.0.0/16"\n# 去掉注释并替换为          \n            - name: calico_ipv4pool_cidr\n              value: "10.244.0.0/16"\n\n\n\n# 应用配置清单并部署网络插件\n\n在 master 节点操作即可\n\nkubectl apply -f calico.yaml\n\n\n安装时显示如下输出\n\nubuntu@k8s-master:~$ kubectl apply -f calico.yaml\nconfigmap/calico-config created\ncustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\nclusterrole.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrole.rbac.authorization.k8s.io/calico-node created\nclusterrolebinding.rbac.authorization.k8s.io/calico-node created\ndaemonset.apps/calico-node created\nserviceaccount/calico-node created\ndeployment.apps/calico-kube-controllers created\nserviceaccount/calico-kube-controllers created\npoddisruptionbudget.policy/calico-kube-controllers created\n\n\n\n# 确认安装是否成功\n\nkubectl get pod -n kube-system -l k8s-app=calico-node\n\n# 需要等待所有状态为 running，注意时间可能较久，3 - 5 分钟的样子\nname                ready   status    restarts   age\ncalico-node-r52c6   1/1     running   0          114s\n\n\n再次查看master节点状态已经为ready状态：\n\nkubectl get nodes\n\n# 控制台输出\nname              status   roles                  age   version\nk8s-master   ready    control-plane,master   16h   v1.20.0\n\n\n至此，kubernetes 的 master 节点就部署完成了。如果你只需要一个单节点的 kubernetes，现在你就可以使用了。不过，在默认情况下，kubernetes 的 master 节点是不能运行用户 pod 的。\n\n\n# 解决 imagepullbackoff\n\n在使用 watch kubectl get pods --all-namespaces 命令观察 pods 状态时如果出现 imagepullbackoff 无法 running 的情况，请尝试使用如下步骤处理：\n\n * master 中删除 nodes：kubectl delete nodes <name>\n * slave 中重置配置：kubeadm reset\n * slave 重启计算机：reboot\n * slave 重新加入集群：kubeadm join',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"使用 kubeadm 配置 Worker 节点",frontmatter:{title:"使用 kubeadm 配置 Worker 节点",date:"2022-02-09T13:37:49.000Z",permalink:"/pages/b23692/"},regularPath:"/22.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/07.%E4%BD%BF%E7%94%A8%20kubeadm%20%E9%85%8D%E7%BD%AE%20Worker%20%E8%8A%82%E7%82%B9.html",relativePath:"22.Kubernetes/01.Kubernetes 入门/07.使用 kubeadm 配置 Worker 节点.md",key:"v-5840b4cb",path:"/pages/b23692/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:30},{level:2,title:"加入集群",slug:"加入集群",normalizedTitle:"加入集群",charIndex:304},{level:2,title:"验证是否成功",slug:"验证是否成功",normalizedTitle:"验证是否成功",charIndex:2259},{level:2,title:"配置kubectl命令自动补全功能",slug:"配置kubectl命令自动补全功能",normalizedTitle:"配置kubectl命令自动补全功能",charIndex:2602},{level:2,title:"测试集群各个组件",slug:"测试集群各个组件",normalizedTitle:"测试集群各个组件",charIndex:9430},{level:2,title:"验证kube-proxy",slug:"验证kube-proxy",normalizedTitle:"验证kube-proxy",charIndex:10289},{level:2,title:"停止服务",slug:"停止服务",normalizedTitle:"停止服务",charIndex:11737},{level:2,title:"Pod调度到Master节点",slug:"pod调度到master节点",normalizedTitle:"pod调度到master节点",charIndex:11912},{level:2,title:"kube-proxy开启ipvs",slug:"kube-proxy开启ipvs",normalizedTitle:"kube-proxy开启ipvs",charIndex:12484},{level:2,title:"移除节点和集群",slug:"移除节点和集群",normalizedTitle:"移除节点和集群",charIndex:14420}],headersStr:"概述 加入集群 验证是否成功 配置kubectl命令自动补全功能 测试集群各个组件 验证kube-proxy 停止服务 Pod调度到Master节点 kube-proxy开启ipvs 移除节点和集群",content:'# 使用 kubeadm 配置 Worker 节点\n\n\n# 概述\n\n部署worker节点 Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod。 在 k8s-node1 和 k8s-node2 上分别执行如下命令，将其注册到 Cluster 中：\n\n注意\n\n切换到 Worker 节点，执行如下命令加入集群，注意使用root用户执行命令\n\n\n# 加入集群\n\n执行以下命令将节点接入集群\n\nkubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \n\n\n如果执行kubeadm init时没有记录下加入集群的命令，可以通过以下命令重新创建 kubeadm token create --print-join-command 在k8s-node1上执行kubeadm join ：\n\nroot@k8s-node1:/home/heyuqiang# kubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n>     --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \n[preflight] Running pre-flight checks\n        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.0. Latest validated version: 19.03\n[preflight] Reading configuration from the cluster...\n[preflight] FYI: You can look at this config file with \'kubectl -n kube-system get cm kubeadm-config -o yaml\'\n[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"\n[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"\n[kubelet-start] Starting the kubelet\n[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...\n\nThis node has joined the cluster:\n* Certificate signing request was sent to apiserver and a response was received.\n* The Kubelet was informed of the new secure connection details.\n\nRun \'kubectl get nodes\' on the control-plane to see this node join the cluster.\n\n\n重复执行以上操作将k8s-node2也加进去（注意重新执行kubeadm token create --print-join-command）。 然后根据提示，我们可以通过 kubectl get nodes 查看节点的状态：\n\n说明：\n\n * token\n   * 可以通过安装 master 时的日志查看 token 信息\n   * 可以通过 kubeadm token list 命令打印出 token 信息\n   * 如果 token 过期，可以使用 kubeadm token create 命令创建新的 token\n * discovery-token-ca-cert-hash\n   * 可以通过安装 master 时的日志查看 sha256 信息\n   * 可以通过 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed \'s/^.* //\' 命令查看 sha256 信息\n\n\n# 验证是否成功\n\n回到 k8s-master 服务器,可以看到 node 成功加入 master\n\nubuntu@k8s-master:~$ kubectl get nodes \nNAME              STATUS   ROLES                  AGE     VERSION\nk8s-master   Ready    control-plane,master   16h     v1.20.0\nk8s-node1    Ready    <none>                 2m35s   v1.20.0\nk8s-node2    Ready    <none>                 80s     v1.20.0\n\n\n\n# 配置kubectl命令自动补全功能\n\necho "source <(kubectl completion bash)" >> ~/.profile\n\n\n提示\n\n如果 node 节点加入 master 时配置有问题可以在 node 节点上使用 kubeadm reset 重置配置再使用 kubeadm join 命令重新加入即可。希望在 master 节点删除 node ，可以使用 kubeadm delete nodes 删除。\n\nnodes状态全部为ready，由于每个节点都需要启动若干组件，如果node节点的状态是 NotReady，可以查看所有节点pod状态，确保所有pod成功拉取到镜像并处于running状态：\n\n[ubuntu@k8s-master ~]$ kubectl get pod --all-namespaces -o wide\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP              NODE              NOMINATED NODE   READINESS GATES\nkube-system   calico-kube-controllers-6d7b4db76c-76gqb   1/1     Running   0          11m     10.244.98.194   k8s-master   <none>           <none>\nkube-system   calico-node-bnsp4                          1/1     Running   0          3m37s   10.17.8.82      k8s-node2    <none>           <none>\nkube-system   calico-node-cptwc                          1/1     Running   0          4m52s   192.168.31.102      k8s-node1    <none>           <none>\nkube-system   calico-node-r52c6                          1/1     Running   0          11m     192.168.31.101      k8s-master   <none>           <none>\nkube-system   coredns-7f89b7bc75-d4drw                   1/1     Running   0          16h     10.244.98.195   k8s-master   <none>           <none>\nkube-system   coredns-7f89b7bc75-xzjfp                   1/1     Running   0          16h     10.244.98.193   k8s-master   <none>           <none>\nkube-system   etcd-k8s-master                       1/1     Running   0          16h     192.168.31.101      k8s-master   <none>           <none>\nkube-system   kube-apiserver-k8s-master             1/1     Running   0          16h     192.168.31.101      k8s-master   <none>           <none>\nkube-system   kube-controller-manager-k8s-master    1/1     Running   0          16h     192.168.31.101      k8s-master   <none>           <none>\nkube-system   kube-proxy-2cffv                           1/1     Running   0          3m37s   10.17.8.82      k8s-node2    <none>           <none>\nkube-system   kube-proxy-55q6f                           1/1     Running   0          16h     192.168.31.101      k8s-master   <none>           <none>\nkube-system   kube-proxy-kg6m7                           1/1     Running   0          4m52s   192.168.31.102      k8s-node1    <none>           <none>\nkube-system   kube-scheduler-k8s-master             1/1     Running   0          16h     192.168.31.101      k8s-master   <none>           <none>\n\n\n这时，所有的节点都已经 Ready，Kubernetes Cluster 创建成功，一切准备就绪。 如果pod状态为Pending、ContainerCreating、ImagePullBackOff 都表明 Pod 没有就绪，Running 才是就绪状态。 如果有pod提示Init:ImagePullBackOff，说明这个pod的镜像在对应节点上拉取失败，我们可以通过 kubectl describe pod 查看 Pod 具体情况，以确认拉取失败的镜像：\n\n[ubuntu@k8s-master ~]$ kubectl describe pod calico-node-lh5j2 --namespace=kube-system\n......\nEvents:\n  Type     Reason     Age    From               Message\n  ----     ------     ----   ----               -------\n  Normal   Scheduled  4m56s  default-scheduler  Successfully assigned kube-system/calico-node-bnsp4 to k8s-node2\n  Normal   Pulling    4m45s  kubelet            Pulling image "docker.io/calico/cni:v3.19.0"\n  Normal   Pulled     4m7s   kubelet            Successfully pulled image "docker.io/calico/cni:v3.19.0" in 38.679699261s\n  Normal   Created    4m6s   kubelet            Created container upgrade-ipam\n  Normal   Started    4m6s   kubelet            Started container upgrade-ipam\n  Normal   Pulled     4m5s   kubelet            Container image "docker.io/calico/cni:v3.19.0" already present on machine\n  Normal   Created    4m4s   kubelet            Created container install-cni\n  Normal   Started    4m4s   kubelet            Started container install-cni\n  Normal   Pulling    3m56s  kubelet            Pulling image "docker.io/calico/pod2daemon-flexvol:v3.19.0"\n  Normal   Pulled     2m29s  kubelet            Successfully pulled image "docker.io/calico/pod2daemon-flexvol:v3.19.0" in 1m26.543216669s\n  Normal   Created    2m28s  kubelet            Created container flexvol-driver\n  Normal   Started    2m27s  kubelet            Started container flexvol-driver\n  Normal   Pulling    2m26s  kubelet            Pulling image "docker.io/calico/node:v3.19.0"\n  Normal   Pulled     101s   kubelet            Successfully pulled image "docker.io/calico/node:v3.19.0" in 44.442030701s\n  Normal   Created    100s   kubelet            Created container calico-node\n  Normal   Started    100s   kubelet            Started container calico-node\n  Warning  Unhealthy  96s    kubelet            Readiness probe failed: 2021-05-21 01:38:10.203 [INFO][149] confd/health.go 180: Number of node(s) with BGP peering established = 0\ncalico/node is not ready: BIRD is not ready: BGP not established with 192.168.31.101,192.168.31.102\n\n\n查看master节点下载了哪些镜像\n\n[ubuntu@k8s-master ~]$ docker images\nREPOSITORY                                                        TAG        IMAGE ID       CREATED         SIZE\ncalico/node                                                       v3.19.0    b0744cc52c19   3 weeks ago     153MB\ncalico/pod2daemon-flexvol                                         v3.19.0    a5decf77918d   3 weeks ago     21.7MB\ncalico/cni                                                        v3.19.0    3d17cd6307a4   3 weeks ago     146MB\ncalico/kube-controllers                                           v3.19.0    c51610d08fdf   3 weeks ago     60.6MB\nregistry.aliyuncs.com/google_containers/kube-proxy                v1.20.0    10cc881966cf   5 months ago    118MB\nregistry.aliyuncs.com/google_containers/kube-apiserver            v1.20.0    ca9843d3b545   5 months ago    122MB\nregistry.aliyuncs.com/google_containers/kube-scheduler            v1.20.0    3138b6e3d471   5 months ago    46.4MB\nregistry.aliyuncs.com/google_containers/kube-controller-manager   v1.20.0    b9fa1895dcaa   5 months ago    116MB\nregistry.aliyuncs.com/google_containers/etcd                      3.4.13-0   0369cf4303ff   8 months ago    253MB\nregistry.aliyuncs.com/google_containers/coredns                   1.7.0      bfe3a36ebd25   11 months ago   45.2MB\nregistry.aliyuncs.com/google_containers/pause                     3.2        80d28bedfe5d   15 months ago   683kB\n\n\n查看worker节点下载了哪些镜像：\n\nubuntu@k8s-node1:~$ docker images\nREPOSITORY                                           TAG       IMAGE ID       CREATED         SIZE\ncalico/node                                          v3.19.0   b0744cc52c19   3 weeks ago     153MB\ncalico/pod2daemon-flexvol                            v3.19.0   a5decf77918d   3 weeks ago     21.7MB\ncalico/cni                                           v3.19.0   3d17cd6307a4   3 weeks ago     146MB\nregistry.aliyuncs.com/google_containers/kube-proxy   v1.20.0   10cc881966cf   5 months ago    118MB\nregistry.aliyuncs.com/google_containers/pause        3.2       80d28bedfe5d   15 months ago   683kB\n\n\n\n# 测试集群各个组件\n\n首先验证kube-apiserver, kube-controller-manager, kube-scheduler, pod network 是否正常： 部署一个 Nginx Deployment，包含2个Pod 参考：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\n\n[ubuntu@k8s-master ~]$ kubectl create deployment nginx --image=nginx:alpine\ndeployment.apps/nginx created\n\n[ubuntu@k8s-master ~]$ kubectl scale deployment nginx --replicas=2\ndeployment.apps/nginx scaled\n\n\n验证Nginx Pod是否正确运行，并且会分配10.244.开头的集群IP\n\n[ubuntu@k8s-master ~]$ kubectl get pods -l app=nginx -o wide      \nNAME                     READY   STATUS    RESTARTS   AGE   IP              NODE             NOMINATED NODE   READINESS GATES\nnginx-565785f75c-6bjgm   1/1     Running   0          45s   10.244.86.1     k8s-node1   <none>           <none>\nnginx-565785f75c-qdnbg   1/1     Running   0          22s   10.244.156.65   k8s-node2   <none>           <none>\n\n\n\n# 验证kube-proxy\n\n以 NodePort 方式对外提供服务 参考：https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/\n\n[ubuntu@k8s-master ~]$ kubectl expose deployment nginx --port=80 --type=NodePort\nservice/nginx exposed\n\n[ubuntu@k8s-master ~]$ kubectl get services nginx\nNAME    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nnginx   NodePort   10.108.73.221   <none>        80:31622/TCP   25s\n\n\n可以通过任意 NodeIP:Port 在集群外部访问这个服务：\n\n[ubuntu@k8s-master ~]$ curl 192.168.31.101:31622\n[ubuntu@k8s-master ~]$ curl 192.168.31.102:31622\n[ubuntu@k8s-master ~]$ curl 192.168.31.103:31622\n\n\n最后验证一下dns, pod network是否正常： 运行Busybox并进入交互模式\n\n[ubuntu@k8s-master ~]$ kubectl run -it curl --image=radial/busyboxplus:curl\nIf you don\'t see a command prompt, try pressing enter.\n\n\n输入nslookup nginx查看是否可以正确解析出集群内的IP，以验证DNS是否正常\n\n[ root@curl:/ ]$ nslookup nginx\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      nginx\nAddress 1: 10.108.73.221 nginx.default.svc.cluster.local\n\n\n通过服务名进行访问，验证kube-proxy是否正常\n\n[ root@curl:/ ]$ curl http://nginx/\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n......\n</body>\n</html>\n# 分别访问一下2个Pod的内网IP，验证跨Node的网络通信是否正常\n[ root@curl:/ ]$ curl 10.244.86.1\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n......\n</body>\n</html>\n[ root@curl:/ ]$ curl 10.244.156.65\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n......\n</body>\n</html>\n\n\n\n# 停止服务\n\n[ubuntu@k8s-master ~]$ kubectl delete deployment nginx\ndeployment.apps "nginx" deleted\n\n[ubuntu@k8s-master ~]$ kubectl delete service nginx\nservice "nginx" deleted\n\n\n\n# Pod调度到Master节点\n\n出于安全考虑，默认配置下Kubernetes不会将Pod调度到Master节点。查看Taints字段默认配置：\n\n[ubuntu@k8s-master ~]$ kubectl describe node k8s-master \n......\nTaints:             node-role.kubernetes.io/master:NoSchedule\n\n\n如果希望将k8s-master也当作Node节点使用，可以执行如下命令,其中k8s-master是主机节点hostname：\n\nkubectl taint node k8s-master node-role.kubernetes.io/master-\n\n\n修改后Taints字段状态：\n\n[ubuntu@k8s-master ~]$ kubectl describe node k8s-master                             \n......\nTaints:             <none>\n\n\n如果要恢复Master Only状态，执行如下命令：\n\nkubectl taint node k8s-master node-role.kubernetes.io/master=:NoSchedule\n\n\n\n# kube-proxy开启ipvs\n\n修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”：\n\n[ubuntu@k8s-master ~]$ kubectl edit cm kube-proxy -n kube-system\nconfigmap/kube-proxy edited\n之后重启各个节点上的kube-proxy pod\n\n[ubuntu@k8s-master ~]$ kubectl get pod -n kube-system | grep kube-proxy | awk \'{system("kubectl delete pod "$1" -n kube-system")}\'\npod "kube-proxy-2w9sh" deleted\npod "kube-proxy-gw4lx" deleted\npod "kube-proxy-thv4c" deleted\n[ubuntu@k8s-master ~]$ kubectl get pod -n kube-system | grep kube-proxy\nkube-proxy-6qlgv                        1/1     Running   0          65s\nkube-proxy-fdtjd                        1/1     Running   0          47s\nkube-proxy-m8zkx                        1/1     Running   0          52s\n[ubuntu@k8s-master ~]$\n\n\n查看日志：\n\n[ubuntu@k8s-master ~]$ kubectl logs kube-proxy-6qlgv -n kube-system\nI1213 09:50:15.414493       1 server_others.go:189] Using ipvs Proxier.\nW1213 09:50:15.414908       1 proxier.go:365] IPVS scheduler not specified, use rr by default\nI1213 09:50:15.415021       1 server_others.go:216] Tearing down inactive rules.\nI1213 09:50:15.461658       1 server.go:464] Version: v1.13.0\nI1213 09:50:15.467827       1 conntrack.go:52] Setting nf_conntrack_max to 131072\nI1213 09:50:15.467997       1 config.go:202] Starting service config controller\nI1213 09:50:15.468010       1 controller_utils.go:1027] Waiting for caches to sync for service config controller\nI1213 09:50:15.468092       1 config.go:102] Starting endpoints config controller\nI1213 09:50:15.468100       1 controller_utils.go:1027] Waiting for caches to sync for endpoints config controller\nI1213 09:50:15.568766       1 controller_utils.go:1034] Caches are synced for endpoints config controller\nI1213 09:50:15.568950       1 controller_utils.go:1034] Caches are synced for service config controller\n[ubuntu@k8s-master ~]$\n\n\n日志中打印出了Using ipvs Proxier，说明ipvs模式已经开启。\n\n至此，1个Master，并附带2个Node的kubernetes集群基础设置已经部署完成，并且其核心功能可以正常使用。\n\n\n# 移除节点和集群\n\nkubernetes集群移除节点，以移除k8s-node2节点为例，在Master节点上运行：\n\nkubectl drain k8s-node2 --delete-local-data --force --ignore-daemonsets\nkubectl delete node k8s-node2\n\n\n上面两条命令执行完成后，在k8s-node2节点执行清理命令，重置kubeadm的安装状态：\n\nkubeadm reset\n\n\n在master上删除node并不会清理k8s-node2运行的容器，需要在删除节点上面手动运行清理命令。 如果你想重新配置集群，使用新的参数重新运行kubeadm init或者kubeadm join即可。\n\n至此3个节点的集群搭建完成，后续可以继续添加node节点，或者部署dashboard、helm包管理工具、EFK日志系统、Prometheus Operator监控系统、rook+ceph存储系统等组件。',normalizedContent:'# 使用 kubeadm 配置 worker 节点\n\n\n# 概述\n\n部署worker节点 kubernetes 的 worker 节点跟 master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 pod。 在 k8s-node1 和 k8s-node2 上分别执行如下命令，将其注册到 cluster 中：\n\n注意\n\n切换到 worker 节点，执行如下命令加入集群，注意使用root用户执行命令\n\n\n# 加入集群\n\n执行以下命令将节点接入集群\n\nkubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n    --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \n\n\n如果执行kubeadm init时没有记录下加入集群的命令，可以通过以下命令重新创建 kubeadm token create --print-join-command 在k8s-node1上执行kubeadm join ：\n\nroot@k8s-node1:/home/heyuqiang# kubeadm join 192.168.31.101:6443 --token abcdef.0123456789abcdef \\\n>     --discovery-token-ca-cert-hash sha256:cff67f08d748fb39c91199ce5515817dfffc5e6dd8a94448693fd3cbb6885eb3 \n[preflight] running pre-flight checks\n        [warning systemverification]: this docker version is not on the list of validated versions: 20.10.0. latest validated version: 19.03\n[preflight] reading configuration from the cluster...\n[preflight] fyi: you can look at this config file with \'kubectl -n kube-system get cm kubeadm-config -o yaml\'\n[kubelet-start] writing kubelet configuration to file "/var/lib/kubelet/config.yaml"\n[kubelet-start] writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"\n[kubelet-start] starting the kubelet\n[kubelet-start] waiting for the kubelet to perform the tls bootstrap...\n\nthis node has joined the cluster:\n* certificate signing request was sent to apiserver and a response was received.\n* the kubelet was informed of the new secure connection details.\n\nrun \'kubectl get nodes\' on the control-plane to see this node join the cluster.\n\n\n重复执行以上操作将k8s-node2也加进去（注意重新执行kubeadm token create --print-join-command）。 然后根据提示，我们可以通过 kubectl get nodes 查看节点的状态：\n\n说明：\n\n * token\n   * 可以通过安装 master 时的日志查看 token 信息\n   * 可以通过 kubeadm token list 命令打印出 token 信息\n   * 如果 token 过期，可以使用 kubeadm token create 命令创建新的 token\n * discovery-token-ca-cert-hash\n   * 可以通过安装 master 时的日志查看 sha256 信息\n   * 可以通过 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed \'s/^.* //\' 命令查看 sha256 信息\n\n\n# 验证是否成功\n\n回到 k8s-master 服务器,可以看到 node 成功加入 master\n\nubuntu@k8s-master:~$ kubectl get nodes \nname              status   roles                  age     version\nk8s-master   ready    control-plane,master   16h     v1.20.0\nk8s-node1    ready    <none>                 2m35s   v1.20.0\nk8s-node2    ready    <none>                 80s     v1.20.0\n\n\n\n# 配置kubectl命令自动补全功能\n\necho "source <(kubectl completion bash)" >> ~/.profile\n\n\n提示\n\n如果 node 节点加入 master 时配置有问题可以在 node 节点上使用 kubeadm reset 重置配置再使用 kubeadm join 命令重新加入即可。希望在 master 节点删除 node ，可以使用 kubeadm delete nodes 删除。\n\nnodes状态全部为ready，由于每个节点都需要启动若干组件，如果node节点的状态是 notready，可以查看所有节点pod状态，确保所有pod成功拉取到镜像并处于running状态：\n\n[ubuntu@k8s-master ~]$ kubectl get pod --all-namespaces -o wide\nnamespace     name                                       ready   status    restarts   age     ip              node              nominated node   readiness gates\nkube-system   calico-kube-controllers-6d7b4db76c-76gqb   1/1     running   0          11m     10.244.98.194   k8s-master   <none>           <none>\nkube-system   calico-node-bnsp4                          1/1     running   0          3m37s   10.17.8.82      k8s-node2    <none>           <none>\nkube-system   calico-node-cptwc                          1/1     running   0          4m52s   192.168.31.102      k8s-node1    <none>           <none>\nkube-system   calico-node-r52c6                          1/1     running   0          11m     192.168.31.101      k8s-master   <none>           <none>\nkube-system   coredns-7f89b7bc75-d4drw                   1/1     running   0          16h     10.244.98.195   k8s-master   <none>           <none>\nkube-system   coredns-7f89b7bc75-xzjfp                   1/1     running   0          16h     10.244.98.193   k8s-master   <none>           <none>\nkube-system   etcd-k8s-master                       1/1     running   0          16h     192.168.31.101      k8s-master   <none>           <none>\nkube-system   kube-apiserver-k8s-master             1/1     running   0          16h     192.168.31.101      k8s-master   <none>           <none>\nkube-system   kube-controller-manager-k8s-master    1/1     running   0          16h     192.168.31.101      k8s-master   <none>           <none>\nkube-system   kube-proxy-2cffv                           1/1     running   0          3m37s   10.17.8.82      k8s-node2    <none>           <none>\nkube-system   kube-proxy-55q6f                           1/1     running   0          16h     192.168.31.101      k8s-master   <none>           <none>\nkube-system   kube-proxy-kg6m7                           1/1     running   0          4m52s   192.168.31.102      k8s-node1    <none>           <none>\nkube-system   kube-scheduler-k8s-master             1/1     running   0          16h     192.168.31.101      k8s-master   <none>           <none>\n\n\n这时，所有的节点都已经 ready，kubernetes cluster 创建成功，一切准备就绪。 如果pod状态为pending、containercreating、imagepullbackoff 都表明 pod 没有就绪，running 才是就绪状态。 如果有pod提示init:imagepullbackoff，说明这个pod的镜像在对应节点上拉取失败，我们可以通过 kubectl describe pod 查看 pod 具体情况，以确认拉取失败的镜像：\n\n[ubuntu@k8s-master ~]$ kubectl describe pod calico-node-lh5j2 --namespace=kube-system\n......\nevents:\n  type     reason     age    from               message\n  ----     ------     ----   ----               -------\n  normal   scheduled  4m56s  default-scheduler  successfully assigned kube-system/calico-node-bnsp4 to k8s-node2\n  normal   pulling    4m45s  kubelet            pulling image "docker.io/calico/cni:v3.19.0"\n  normal   pulled     4m7s   kubelet            successfully pulled image "docker.io/calico/cni:v3.19.0" in 38.679699261s\n  normal   created    4m6s   kubelet            created container upgrade-ipam\n  normal   started    4m6s   kubelet            started container upgrade-ipam\n  normal   pulled     4m5s   kubelet            container image "docker.io/calico/cni:v3.19.0" already present on machine\n  normal   created    4m4s   kubelet            created container install-cni\n  normal   started    4m4s   kubelet            started container install-cni\n  normal   pulling    3m56s  kubelet            pulling image "docker.io/calico/pod2daemon-flexvol:v3.19.0"\n  normal   pulled     2m29s  kubelet            successfully pulled image "docker.io/calico/pod2daemon-flexvol:v3.19.0" in 1m26.543216669s\n  normal   created    2m28s  kubelet            created container flexvol-driver\n  normal   started    2m27s  kubelet            started container flexvol-driver\n  normal   pulling    2m26s  kubelet            pulling image "docker.io/calico/node:v3.19.0"\n  normal   pulled     101s   kubelet            successfully pulled image "docker.io/calico/node:v3.19.0" in 44.442030701s\n  normal   created    100s   kubelet            created container calico-node\n  normal   started    100s   kubelet            started container calico-node\n  warning  unhealthy  96s    kubelet            readiness probe failed: 2021-05-21 01:38:10.203 [info][149] confd/health.go 180: number of node(s) with bgp peering established = 0\ncalico/node is not ready: bird is not ready: bgp not established with 192.168.31.101,192.168.31.102\n\n\n查看master节点下载了哪些镜像\n\n[ubuntu@k8s-master ~]$ docker images\nrepository                                                        tag        image id       created         size\ncalico/node                                                       v3.19.0    b0744cc52c19   3 weeks ago     153mb\ncalico/pod2daemon-flexvol                                         v3.19.0    a5decf77918d   3 weeks ago     21.7mb\ncalico/cni                                                        v3.19.0    3d17cd6307a4   3 weeks ago     146mb\ncalico/kube-controllers                                           v3.19.0    c51610d08fdf   3 weeks ago     60.6mb\nregistry.aliyuncs.com/google_containers/kube-proxy                v1.20.0    10cc881966cf   5 months ago    118mb\nregistry.aliyuncs.com/google_containers/kube-apiserver            v1.20.0    ca9843d3b545   5 months ago    122mb\nregistry.aliyuncs.com/google_containers/kube-scheduler            v1.20.0    3138b6e3d471   5 months ago    46.4mb\nregistry.aliyuncs.com/google_containers/kube-controller-manager   v1.20.0    b9fa1895dcaa   5 months ago    116mb\nregistry.aliyuncs.com/google_containers/etcd                      3.4.13-0   0369cf4303ff   8 months ago    253mb\nregistry.aliyuncs.com/google_containers/coredns                   1.7.0      bfe3a36ebd25   11 months ago   45.2mb\nregistry.aliyuncs.com/google_containers/pause                     3.2        80d28bedfe5d   15 months ago   683kb\n\n\n查看worker节点下载了哪些镜像：\n\nubuntu@k8s-node1:~$ docker images\nrepository                                           tag       image id       created         size\ncalico/node                                          v3.19.0   b0744cc52c19   3 weeks ago     153mb\ncalico/pod2daemon-flexvol                            v3.19.0   a5decf77918d   3 weeks ago     21.7mb\ncalico/cni                                           v3.19.0   3d17cd6307a4   3 weeks ago     146mb\nregistry.aliyuncs.com/google_containers/kube-proxy   v1.20.0   10cc881966cf   5 months ago    118mb\nregistry.aliyuncs.com/google_containers/pause        3.2       80d28bedfe5d   15 months ago   683kb\n\n\n\n# 测试集群各个组件\n\n首先验证kube-apiserver, kube-controller-manager, kube-scheduler, pod network 是否正常： 部署一个 nginx deployment，包含2个pod 参考：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\n\n[ubuntu@k8s-master ~]$ kubectl create deployment nginx --image=nginx:alpine\ndeployment.apps/nginx created\n\n[ubuntu@k8s-master ~]$ kubectl scale deployment nginx --replicas=2\ndeployment.apps/nginx scaled\n\n\n验证nginx pod是否正确运行，并且会分配10.244.开头的集群ip\n\n[ubuntu@k8s-master ~]$ kubectl get pods -l app=nginx -o wide      \nname                     ready   status    restarts   age   ip              node             nominated node   readiness gates\nnginx-565785f75c-6bjgm   1/1     running   0          45s   10.244.86.1     k8s-node1   <none>           <none>\nnginx-565785f75c-qdnbg   1/1     running   0          22s   10.244.156.65   k8s-node2   <none>           <none>\n\n\n\n# 验证kube-proxy\n\n以 nodeport 方式对外提供服务 参考：https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/\n\n[ubuntu@k8s-master ~]$ kubectl expose deployment nginx --port=80 --type=nodeport\nservice/nginx exposed\n\n[ubuntu@k8s-master ~]$ kubectl get services nginx\nname    type       cluster-ip      external-ip   port(s)        age\nnginx   nodeport   10.108.73.221   <none>        80:31622/tcp   25s\n\n\n可以通过任意 nodeip:port 在集群外部访问这个服务：\n\n[ubuntu@k8s-master ~]$ curl 192.168.31.101:31622\n[ubuntu@k8s-master ~]$ curl 192.168.31.102:31622\n[ubuntu@k8s-master ~]$ curl 192.168.31.103:31622\n\n\n最后验证一下dns, pod network是否正常： 运行busybox并进入交互模式\n\n[ubuntu@k8s-master ~]$ kubectl run -it curl --image=radial/busyboxplus:curl\nif you don\'t see a command prompt, try pressing enter.\n\n\n输入nslookup nginx查看是否可以正确解析出集群内的ip，以验证dns是否正常\n\n[ root@curl:/ ]$ nslookup nginx\nserver:    10.96.0.10\naddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n\nname:      nginx\naddress 1: 10.108.73.221 nginx.default.svc.cluster.local\n\n\n通过服务名进行访问，验证kube-proxy是否正常\n\n[ root@curl:/ ]$ curl http://nginx/\n<!doctype html>\n<html>\n<head>\n<title>welcome to nginx!</title>\n......\n</body>\n</html>\n# 分别访问一下2个pod的内网ip，验证跨node的网络通信是否正常\n[ root@curl:/ ]$ curl 10.244.86.1\n<!doctype html>\n<html>\n<head>\n<title>welcome to nginx!</title>\n......\n</body>\n</html>\n[ root@curl:/ ]$ curl 10.244.156.65\n<!doctype html>\n<html>\n<head>\n<title>welcome to nginx!</title>\n......\n</body>\n</html>\n\n\n\n# 停止服务\n\n[ubuntu@k8s-master ~]$ kubectl delete deployment nginx\ndeployment.apps "nginx" deleted\n\n[ubuntu@k8s-master ~]$ kubectl delete service nginx\nservice "nginx" deleted\n\n\n\n# pod调度到master节点\n\n出于安全考虑，默认配置下kubernetes不会将pod调度到master节点。查看taints字段默认配置：\n\n[ubuntu@k8s-master ~]$ kubectl describe node k8s-master \n......\ntaints:             node-role.kubernetes.io/master:noschedule\n\n\n如果希望将k8s-master也当作node节点使用，可以执行如下命令,其中k8s-master是主机节点hostname：\n\nkubectl taint node k8s-master node-role.kubernetes.io/master-\n\n\n修改后taints字段状态：\n\n[ubuntu@k8s-master ~]$ kubectl describe node k8s-master                             \n......\ntaints:             <none>\n\n\n如果要恢复master only状态，执行如下命令：\n\nkubectl taint node k8s-master node-role.kubernetes.io/master=:noschedule\n\n\n\n# kube-proxy开启ipvs\n\n修改configmap的kube-system/kube-proxy中的config.conf，mode: “ipvs”：\n\n[ubuntu@k8s-master ~]$ kubectl edit cm kube-proxy -n kube-system\nconfigmap/kube-proxy edited\n之后重启各个节点上的kube-proxy pod\n\n[ubuntu@k8s-master ~]$ kubectl get pod -n kube-system | grep kube-proxy | awk \'{system("kubectl delete pod "$1" -n kube-system")}\'\npod "kube-proxy-2w9sh" deleted\npod "kube-proxy-gw4lx" deleted\npod "kube-proxy-thv4c" deleted\n[ubuntu@k8s-master ~]$ kubectl get pod -n kube-system | grep kube-proxy\nkube-proxy-6qlgv                        1/1     running   0          65s\nkube-proxy-fdtjd                        1/1     running   0          47s\nkube-proxy-m8zkx                        1/1     running   0          52s\n[ubuntu@k8s-master ~]$\n\n\n查看日志：\n\n[ubuntu@k8s-master ~]$ kubectl logs kube-proxy-6qlgv -n kube-system\ni1213 09:50:15.414493       1 server_others.go:189] using ipvs proxier.\nw1213 09:50:15.414908       1 proxier.go:365] ipvs scheduler not specified, use rr by default\ni1213 09:50:15.415021       1 server_others.go:216] tearing down inactive rules.\ni1213 09:50:15.461658       1 server.go:464] version: v1.13.0\ni1213 09:50:15.467827       1 conntrack.go:52] setting nf_conntrack_max to 131072\ni1213 09:50:15.467997       1 config.go:202] starting service config controller\ni1213 09:50:15.468010       1 controller_utils.go:1027] waiting for caches to sync for service config controller\ni1213 09:50:15.468092       1 config.go:102] starting endpoints config controller\ni1213 09:50:15.468100       1 controller_utils.go:1027] waiting for caches to sync for endpoints config controller\ni1213 09:50:15.568766       1 controller_utils.go:1034] caches are synced for endpoints config controller\ni1213 09:50:15.568950       1 controller_utils.go:1034] caches are synced for service config controller\n[ubuntu@k8s-master ~]$\n\n\n日志中打印出了using ipvs proxier，说明ipvs模式已经开启。\n\n至此，1个master，并附带2个node的kubernetes集群基础设置已经部署完成，并且其核心功能可以正常使用。\n\n\n# 移除节点和集群\n\nkubernetes集群移除节点，以移除k8s-node2节点为例，在master节点上运行：\n\nkubectl drain k8s-node2 --delete-local-data --force --ignore-daemonsets\nkubectl delete node k8s-node2\n\n\n上面两条命令执行完成后，在k8s-node2节点执行清理命令，重置kubeadm的安装状态：\n\nkubeadm reset\n\n\n在master上删除node并不会清理k8s-node2运行的容器，需要在删除节点上面手动运行清理命令。 如果你想重新配置集群，使用新的参数重新运行kubeadm init或者kubeadm join即可。\n\n至此3个节点的集群搭建完成，后续可以继续添加node节点，或者部署dashboard、helm包管理工具、efk日志系统、prometheus operator监控系统、rook+ceph存储系统等组件。',charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"概念总结",frontmatter:{title:"概念总结",date:"2022-02-09T13:37:49.000Z",permalink:"/pages/cfa4e2/"},regularPath:"/22.Kubernetes/01.Kubernetes%20%E5%85%A5%E9%97%A8/08.%E6%A6%82%E5%BF%B5%E6%80%BB%E7%BB%93.html",relativePath:"22.Kubernetes/01.Kubernetes 入门/08.概念总结.md",key:"v-4de06ab6",path:"/pages/cfa4e2/",headers:[{level:2,title:"什么是 Kubernetes",slug:"什么是-kubernetes",normalizedTitle:"什么是 kubernetes",charIndex:11},{level:2,title:"Kubernetes Master",slug:"kubernetes-master",normalizedTitle:"kubernetes master",charIndex:756},{level:2,title:"Kubernetes Node",slug:"kubernetes-node",normalizedTitle:"kubernetes node",charIndex:865},{level:2,title:"Kubernetes 架构",slug:"kubernetes-架构",normalizedTitle:"kubernetes 架构",charIndex:1538}],headersStr:"什么是 Kubernetes Kubernetes Master Kubernetes Node Kubernetes 架构",content:"# 概念总结\n\n\n# 什么是 Kubernetes\n\nKubernetes 是一个开源的 Docker 容器编排系统，它可以调度计算集群的节点，动态管理上面的作业，保证它们按用户期望的状态运行。通过使用「labels」和「pods」的概念，Kubernetes 将应用按逻辑单元进行分组，方便管理和服务发现。\n\n\n\n * pods： 是一组紧密关联的容器集合，它们共享 IPC(进程间通信)、Network(网络) 和 UTS namespace(UTS 命名空间是 Linux 命名空间的一个子系统，主要作用是完成对容器 Hostname 和 Domain 的隔离，同时保存内核名称、版本、以及底层体系结构类型等信息)，是 Kubernetes 调度的基本单位。\n * labels： 键值对(key/value)标签，可以被关联到如 Pod 这样的对象上，主要作用是给用户一个直观的感受，比如这个 Pod 是用来放置数据库的\n * GUI： 用户图形界面，可以是 Web 用户界面，比如使用 kubernetes-dashboard 组件，用户可以通过 Dashboard 在 Kubernetes 集群中部署容器化的应用，可以查看集群中应用的运行情况，同时也能够基于 Dashboard 创建或修改部署、任务、服务等 Kubernetes 的资源。通过部署向导，用户能够对部署进行扩缩容，进行滚动更新、重启 Pod 和部署新应用。当然，通过 Dashboard 也能够查看 Kubernetes 资源的状态\n * kubectl： 用于管理 Kubernetes 集群的命令行工具\n * kube-apiserver： 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制\n * Kubernetes Master： Kubernetes 集群主节点，主要由 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 四个模块组成\n * Kubernetes Node： Kubernetes 集群子节点，主要由 kubelet、kube-proxy、runtime 三个模块组成\n * Image Registry： 镜像仓库，比如：Ducker HUB 或 Docker 私服\n\n\n# Kubernetes Master\n\n\n\n * kube-apiserver： 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制\n * kube-scheduler： 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上\n * kube-controller-manager： 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等\n * etcd： CoreOS 基于 Raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）\n\n\n# Kubernetes Node\n\n\n\n * runtime： 负责镜像管理以及 Pod 和容器的真正运行（CRI，Container Runtime Interface），默认的容器运行时为 Docker，还支持 RKT 容器\n * kubelet： 负责维持容器的生命周期，同时也负责 Volume（CVI，Container Volume Interface）和网络（CNI，Container Network Interface）的管理\n * kube-proxy： 负责为 Service 提供 cluster 内部的服务发现和负载均衡\n\n\n# Kubernetes 架构\n\n\n\n",normalizedContent:"# 概念总结\n\n\n# 什么是 kubernetes\n\nkubernetes 是一个开源的 docker 容器编排系统，它可以调度计算集群的节点，动态管理上面的作业，保证它们按用户期望的状态运行。通过使用「labels」和「pods」的概念，kubernetes 将应用按逻辑单元进行分组，方便管理和服务发现。\n\n\n\n * pods： 是一组紧密关联的容器集合，它们共享 ipc(进程间通信)、network(网络) 和 uts namespace(uts 命名空间是 linux 命名空间的一个子系统，主要作用是完成对容器 hostname 和 domain 的隔离，同时保存内核名称、版本、以及底层体系结构类型等信息)，是 kubernetes 调度的基本单位。\n * labels： 键值对(key/value)标签，可以被关联到如 pod 这样的对象上，主要作用是给用户一个直观的感受，比如这个 pod 是用来放置数据库的\n * gui： 用户图形界面，可以是 web 用户界面，比如使用 kubernetes-dashboard 组件，用户可以通过 dashboard 在 kubernetes 集群中部署容器化的应用，可以查看集群中应用的运行情况，同时也能够基于 dashboard 创建或修改部署、任务、服务等 kubernetes 的资源。通过部署向导，用户能够对部署进行扩缩容，进行滚动更新、重启 pod 和部署新应用。当然，通过 dashboard 也能够查看 kubernetes 资源的状态\n * kubectl： 用于管理 kubernetes 集群的命令行工具\n * kube-apiserver： 提供了资源操作的唯一入口，并提供认证、授权、访问控制、api 注册和发现等机制\n * kubernetes master： kubernetes 集群主节点，主要由 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 四个模块组成\n * kubernetes node： kubernetes 集群子节点，主要由 kubelet、kube-proxy、runtime 三个模块组成\n * image registry： 镜像仓库，比如：ducker hub 或 docker 私服\n\n\n# kubernetes master\n\n\n\n * kube-apiserver： 提供了资源操作的唯一入口，并提供认证、授权、访问控制、api 注册和发现等机制\n * kube-scheduler： 负责资源的调度，按照预定的调度策略将 pod 调度到相应的机器上\n * kube-controller-manager： 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等\n * etcd： coreos 基于 raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）\n\n\n# kubernetes node\n\n\n\n * runtime： 负责镜像管理以及 pod 和容器的真正运行（cri，container runtime interface），默认的容器运行时为 docker，还支持 rkt 容器\n * kubelet： 负责维持容器的生命周期，同时也负责 volume（cvi，container volume interface）和网络（cni，container network interface）的管理\n * kube-proxy： 负责为 service 提供 cluster 内部的服务发现和负载均衡\n\n\n# kubernetes 架构\n\n\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"Kubernetes Dashboard",frontmatter:{title:"Kubernetes Dashboard",date:"2022-02-09T13:37:49.000Z",permalink:"/pages/11af34/"},regularPath:"/22.Kubernetes/02.Kubernetes%20%E5%AE%9E%E8%B7%B5/01.Kubernetes%20Dashboard.html",relativePath:"22.Kubernetes/02.Kubernetes 实践/01.Kubernetes Dashboard.md",key:"v-eb9fcf56",path:"/pages/11af34/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:27},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:88},{level:2,title:"访问",slug:"访问",normalizedTitle:"访问",charIndex:263},{level:2,title:"登录",slug:"登录",normalizedTitle:"登录",charIndex:9137},{level:2,title:"清理",slug:"清理",normalizedTitle:"清理",charIndex:10944}],headersStr:"概述 安装 访问 登录 清理",content:'# Kubernetes Dashboard\n\n\n# 概述\n\nKubernetes Dashboard 是 Kubernetes 集群的 Web UI，用于管理集群。\n\n\n# 安装\n\nGitHub 地址：Kubernetes Dashboard\n\n下载配置文件\n\nwget https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml\n\n\n默认情况下，Service类型为ClusterIP，只能用于集群内部访问，这里指定使用"NodePort"类型，并使用固定端口30001，将其发布集群外部访问\n\n首先拷贝官方源文件，得到kubernetes-dashboard.yaml\n\ncp recommended.yaml kubernetes-dashboard.yaml\n\n\n修改kubernetes-dashboard.yaml配置如下:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Copyright 2017 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the "License");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an "AS IS" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kubernetes-dashboard\n\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\n\n---\n\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  # 修改类型为 NodePort 访问\n  type: NodePort\n  ports:\n    - port: 443\n      targetPort: 8443\n      # 设置端口号为 30001\n      nodePort: 30001\n  selector:\n    k8s-app: kubernetes-dashboard\n\n---\n\napiVersion: v1\nkind: Secret\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-certs\n  namespace: kubernetes-dashboard\ntype: Opaque\n\n---\n\napiVersion: v1\nkind: Secret\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-csrf\n  namespace: kubernetes-dashboard\ntype: Opaque\ndata:\n  csrf: ""\n\n---\n\napiVersion: v1\nkind: Secret\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-key-holder\n  namespace: kubernetes-dashboard\ntype: Opaque\n\n---\n\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-settings\n  namespace: kubernetes-dashboard\n\n---\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nrules:\n  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.\n  - apiGroups: [""]\n    resources: ["secrets"]\n    resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs", "kubernetes-dashboard-csrf"]\n    verbs: ["get", "update", "delete"]\n    # Allow Dashboard to get and update \'kubernetes-dashboard-settings\' config map.\n  - apiGroups: [""]\n    resources: ["configmaps"]\n    resourceNames: ["kubernetes-dashboard-settings"]\n    verbs: ["get", "update"]\n    # Allow Dashboard to get metrics.\n  - apiGroups: [""]\n    resources: ["services"]\n    resourceNames: ["heapster", "dashboard-metrics-scraper"]\n    verbs: ["proxy"]\n  - apiGroups: [""]\n    resources: ["services/proxy"]\n    resourceNames: ["heapster", "http:heapster:", "https:heapster:", "dashboard-metrics-scraper", "http:dashboard-metrics-scraper"]\n    verbs: ["get"]\n\n---\n\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\nrules:\n  # Allow Metrics Scraper to get metrics from the Metrics server\n  - apiGroups: ["metrics.k8s.io"]\n    resources: ["pods", "nodes"]\n    verbs: ["get", "list", "watch"]\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: kubernetes-dashboard\nsubjects:\n  - kind: ServiceAccount\n    name: kubernetes-dashboard\n    namespace: kubernetes-dashboard\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubernetes-dashboard\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kubernetes-dashboard\nsubjects:\n  - kind: ServiceAccount\n    name: kubernetes-dashboard\n    namespace: kubernetes-dashboard\n\n---\n\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard\n    spec:\n      containers:\n        - name: kubernetes-dashboard\n          # 修改镜像地址为阿里云\n          image: registry.aliyuncs.com/google_containers/dashboard:v2.2.0\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8443\n              protocol: TCP\n          args:\n            - --auto-generate-certificates\n            - --namespace=kubernetes-dashboard\n            # Uncomment the following line to manually specify Kubernetes API server Host\n            # If not specified, Dashboard will attempt to auto discover the API server and connect\n            # to it. Uncomment only if the default does not work.\n            # - --apiserver-host=http://my-address:port\n          volumeMounts:\n            - name: kubernetes-dashboard-certs\n              mountPath: /certs\n              # Create on-disk volume to store exec logs\n            - mountPath: /tmp\n              name: tmp-volume\n          livenessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /\n              port: 8443\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            runAsUser: 1001\n            runAsGroup: 2001\n      volumes:\n        - name: kubernetes-dashboard-certs\n          secret:\n            secretName: kubernetes-dashboard-certs\n        - name: tmp-volume\n          emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n      nodeSelector:\n        "kubernetes.io/os": linux\n      # Comment the following tolerations if Dashboard must not be deployed on master\n      tolerations:\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n\n---\n\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: dashboard-metrics-scraper\n  name: dashboard-metrics-scraper\n  namespace: kubernetes-dashboard\nspec:\n  ports:\n    - port: 8000\n      targetPort: 8000\n  selector:\n    k8s-app: dashboard-metrics-scraper\n\n---\n\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    k8s-app: dashboard-metrics-scraper\n  name: dashboard-metrics-scraper\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: dashboard-metrics-scraper\n  template:\n    metadata:\n      labels:\n        k8s-app: dashboard-metrics-scraper\n      annotations:\n        seccomp.security.alpha.kubernetes.io/pod: \'runtime/default\'\n    spec:\n      containers:\n        - name: dashboard-metrics-scraper\n          image: kubernetesui/metrics-scraper:v1.0.6\n          ports:\n            - containerPort: 8000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              scheme: HTTP\n              path: /\n              port: 8000\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          volumeMounts:\n            - mountPath: /tmp\n              name: tmp-volume\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            runAsUser: 1001\n            runAsGroup: 2001\n      serviceAccountName: kubernetes-dashboard\n      nodeSelector:\n        "kubernetes.io/os": linux\n      # Comment the following tolerations if Dashboard must not be deployed on master\n      tolerations:\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n      volumes:\n        - name: tmp-volume\n          emptyDir: {}\n\n\n部署kubernetes-dashboard\n\n# 部署\nkubectl create -f kubernetes-dashboard.yaml\n\n# 查看\nkubectl -n kubernetes-dashboard get pods\nkubectl -n kubernetes-dashboard get service kubernetes-dashboard\nkubectl -n kubernetes-dashboard describe service kubernetes-dashboard\n\n# 验证pod是否正常运行\nkubectl get pod -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard\nNAME                                    READY   STATUS    RESTARTS   AGE\nkubernetes-dashboard-5dbf55bd9d-9f97k   1/1     Running   0          68s\n\n\n从上面的反馈结果来看，pod运行正常，并且已经就绪。\n\n\n# 访问\n\nDashboard已经暴露在端口30001(HTTPS)上,现在可以用浏览器输入地址：https://<node-ip>:<nodePort>访问 Dashboard，因为证书原因除火狐浏览器外其它浏览器无法直接打开页面\n\nChrome 浏览器显示如下\n\n\n\nFirefox 浏览器显示如下\n\n\n\n点击 接受风险并继续 即可显示欢迎界面\n\n\n\n\n# 登录\n\n我们采用 Token 方式登录\n\n我们首先在kubernetes-dashboard命名空间中创建名为admin-user的ServiceAccount。\n\ncat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\nEOF\n\n\n为admin-user用户创建一个ClusterRoleBinding\n\ncat <<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\nEOF\n\n\n获得一个Bearer Token\n\nkubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"\n\n\n应该打印出如下内容：\n\neyJhbGciOiJSUzI1NiIsImtpZCI6ImRnV2dKVHNrZUliRU1fRmlsZHhMcGt3VmVmVm5PYW5BOFIxYVc3NHRfVm8ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTh2djg5Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyMjFhYWE5Zi1jNjBlLTRlNmYtYTY4NC02NTM4YWU3MjYwM2UiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.IsGz8pcjnsWyaHm21okD3iN1UEblYs0237Xp6KTJbR-UDpmECRwLUeLHceEjIjtrv55jAS2PB2hKe0LQ1FYHeEKsgw8OiEhpHmM06Ua59ehQiT2PYbLS022-g63bIbKdOWT7Dkd212iHCNQMfoho2pzqULsyfLMublw47R6xEReeira4kQORrEyXKGt8-xYiYCtXTtWoXWYmpp7N2dey9Dnb5WIFqVTaAomZFvUUpWnP2skdrGt_cE4N3BEOF_5L2D-V_HDs7cmhl6OdMfdGj3W8KzaCSnrWz_QhsYqAzyTF8RBFz7t-KzkYZVLz8Zfqwq4G5Oc-KH6xJR8-1wK2LQ\n\n\n现在复制Bearer Token并将其粘贴到登录屏幕上的输入 token *字段中。单击登录按钮即可。您现在以管理员身份登录。\n\n工作量状态\n\n\n\nNodes\n\n\n\n\n# 清理\n\n移除adminServiceAccount和ClusterRoleBinding\n\nkubectl -n kubernetes-dashboard delete serviceaccount admin-user\nkubectl -n kubernetes-dashboard delete clusterrolebinding admin-user\n',normalizedContent:'# kubernetes dashboard\n\n\n# 概述\n\nkubernetes dashboard 是 kubernetes 集群的 web ui，用于管理集群。\n\n\n# 安装\n\ngithub 地址：kubernetes dashboard\n\n下载配置文件\n\nwget https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml\n\n\n默认情况下，service类型为clusterip，只能用于集群内部访问，这里指定使用"nodeport"类型，并使用固定端口30001，将其发布集群外部访问\n\n首先拷贝官方源文件，得到kubernetes-dashboard.yaml\n\ncp recommended.yaml kubernetes-dashboard.yaml\n\n\n修改kubernetes-dashboard.yaml配置如下:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# copyright 2017 the kubernetes authors.\n#\n# licensed under the apache license, version 2.0 (the "license");\n# you may not use this file except in compliance with the license.\n# you may obtain a copy of the license at\n#\n#     http://www.apache.org/licenses/license-2.0\n#\n# unless required by applicable law or agreed to in writing, software\n# distributed under the license is distributed on an "as is" basis,\n# without warranties or conditions of any kind, either express or implied.\n# see the license for the specific language governing permissions and\n# limitations under the license.\n\napiversion: v1\nkind: namespace\nmetadata:\n  name: kubernetes-dashboard\n\n---\n\napiversion: v1\nkind: serviceaccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\n\n---\n\nkind: service\napiversion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  # 修改类型为 nodeport 访问\n  type: nodeport\n  ports:\n    - port: 443\n      targetport: 8443\n      # 设置端口号为 30001\n      nodeport: 30001\n  selector:\n    k8s-app: kubernetes-dashboard\n\n---\n\napiversion: v1\nkind: secret\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-certs\n  namespace: kubernetes-dashboard\ntype: opaque\n\n---\n\napiversion: v1\nkind: secret\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-csrf\n  namespace: kubernetes-dashboard\ntype: opaque\ndata:\n  csrf: ""\n\n---\n\napiversion: v1\nkind: secret\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-key-holder\n  namespace: kubernetes-dashboard\ntype: opaque\n\n---\n\nkind: configmap\napiversion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-settings\n  namespace: kubernetes-dashboard\n\n---\n\nkind: role\napiversion: rbac.authorization.k8s.io/v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nrules:\n  # allow dashboard to get, update and delete dashboard exclusive secrets.\n  - apigroups: [""]\n    resources: ["secrets"]\n    resourcenames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs", "kubernetes-dashboard-csrf"]\n    verbs: ["get", "update", "delete"]\n    # allow dashboard to get and update \'kubernetes-dashboard-settings\' config map.\n  - apigroups: [""]\n    resources: ["configmaps"]\n    resourcenames: ["kubernetes-dashboard-settings"]\n    verbs: ["get", "update"]\n    # allow dashboard to get metrics.\n  - apigroups: [""]\n    resources: ["services"]\n    resourcenames: ["heapster", "dashboard-metrics-scraper"]\n    verbs: ["proxy"]\n  - apigroups: [""]\n    resources: ["services/proxy"]\n    resourcenames: ["heapster", "http:heapster:", "https:heapster:", "dashboard-metrics-scraper", "http:dashboard-metrics-scraper"]\n    verbs: ["get"]\n\n---\n\nkind: clusterrole\napiversion: rbac.authorization.k8s.io/v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\nrules:\n  # allow metrics scraper to get metrics from the metrics server\n  - apigroups: ["metrics.k8s.io"]\n    resources: ["pods", "nodes"]\n    verbs: ["get", "list", "watch"]\n\n---\n\napiversion: rbac.authorization.k8s.io/v1\nkind: rolebinding\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nroleref:\n  apigroup: rbac.authorization.k8s.io\n  kind: role\n  name: kubernetes-dashboard\nsubjects:\n  - kind: serviceaccount\n    name: kubernetes-dashboard\n    namespace: kubernetes-dashboard\n\n---\n\napiversion: rbac.authorization.k8s.io/v1\nkind: clusterrolebinding\nmetadata:\n  name: kubernetes-dashboard\nroleref:\n  apigroup: rbac.authorization.k8s.io\n  kind: clusterrole\n  name: kubernetes-dashboard\nsubjects:\n  - kind: serviceaccount\n    name: kubernetes-dashboard\n    namespace: kubernetes-dashboard\n\n---\n\nkind: deployment\napiversion: apps/v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  revisionhistorylimit: 10\n  selector:\n    matchlabels:\n      k8s-app: kubernetes-dashboard\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard\n    spec:\n      containers:\n        - name: kubernetes-dashboard\n          # 修改镜像地址为阿里云\n          image: registry.aliyuncs.com/google_containers/dashboard:v2.2.0\n          imagepullpolicy: always\n          ports:\n            - containerport: 8443\n              protocol: tcp\n          args:\n            - --auto-generate-certificates\n            - --namespace=kubernetes-dashboard\n            # uncomment the following line to manually specify kubernetes api server host\n            # if not specified, dashboard will attempt to auto discover the api server and connect\n            # to it. uncomment only if the default does not work.\n            # - --apiserver-host=http://my-address:port\n          volumemounts:\n            - name: kubernetes-dashboard-certs\n              mountpath: /certs\n              # create on-disk volume to store exec logs\n            - mountpath: /tmp\n              name: tmp-volume\n          livenessprobe:\n            httpget:\n              scheme: https\n              path: /\n              port: 8443\n            initialdelayseconds: 30\n            timeoutseconds: 30\n          securitycontext:\n            allowprivilegeescalation: false\n            readonlyrootfilesystem: true\n            runasuser: 1001\n            runasgroup: 2001\n      volumes:\n        - name: kubernetes-dashboard-certs\n          secret:\n            secretname: kubernetes-dashboard-certs\n        - name: tmp-volume\n          emptydir: {}\n      serviceaccountname: kubernetes-dashboard\n      nodeselector:\n        "kubernetes.io/os": linux\n      # comment the following tolerations if dashboard must not be deployed on master\n      tolerations:\n        - key: node-role.kubernetes.io/master\n          effect: noschedule\n\n---\n\nkind: service\napiversion: v1\nmetadata:\n  labels:\n    k8s-app: dashboard-metrics-scraper\n  name: dashboard-metrics-scraper\n  namespace: kubernetes-dashboard\nspec:\n  ports:\n    - port: 8000\n      targetport: 8000\n  selector:\n    k8s-app: dashboard-metrics-scraper\n\n---\n\nkind: deployment\napiversion: apps/v1\nmetadata:\n  labels:\n    k8s-app: dashboard-metrics-scraper\n  name: dashboard-metrics-scraper\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  revisionhistorylimit: 10\n  selector:\n    matchlabels:\n      k8s-app: dashboard-metrics-scraper\n  template:\n    metadata:\n      labels:\n        k8s-app: dashboard-metrics-scraper\n      annotations:\n        seccomp.security.alpha.kubernetes.io/pod: \'runtime/default\'\n    spec:\n      containers:\n        - name: dashboard-metrics-scraper\n          image: kubernetesui/metrics-scraper:v1.0.6\n          ports:\n            - containerport: 8000\n              protocol: tcp\n          livenessprobe:\n            httpget:\n              scheme: http\n              path: /\n              port: 8000\n            initialdelayseconds: 30\n            timeoutseconds: 30\n          volumemounts:\n            - mountpath: /tmp\n              name: tmp-volume\n          securitycontext:\n            allowprivilegeescalation: false\n            readonlyrootfilesystem: true\n            runasuser: 1001\n            runasgroup: 2001\n      serviceaccountname: kubernetes-dashboard\n      nodeselector:\n        "kubernetes.io/os": linux\n      # comment the following tolerations if dashboard must not be deployed on master\n      tolerations:\n        - key: node-role.kubernetes.io/master\n          effect: noschedule\n      volumes:\n        - name: tmp-volume\n          emptydir: {}\n\n\n部署kubernetes-dashboard\n\n# 部署\nkubectl create -f kubernetes-dashboard.yaml\n\n# 查看\nkubectl -n kubernetes-dashboard get pods\nkubectl -n kubernetes-dashboard get service kubernetes-dashboard\nkubectl -n kubernetes-dashboard describe service kubernetes-dashboard\n\n# 验证pod是否正常运行\nkubectl get pod -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard\nname                                    ready   status    restarts   age\nkubernetes-dashboard-5dbf55bd9d-9f97k   1/1     running   0          68s\n\n\n从上面的反馈结果来看，pod运行正常，并且已经就绪。\n\n\n# 访问\n\ndashboard已经暴露在端口30001(https)上,现在可以用浏览器输入地址：https://<node-ip>:<nodeport>访问 dashboard，因为证书原因除火狐浏览器外其它浏览器无法直接打开页面\n\nchrome 浏览器显示如下\n\n\n\nfirefox 浏览器显示如下\n\n\n\n点击 接受风险并继续 即可显示欢迎界面\n\n\n\n\n# 登录\n\n我们采用 token 方式登录\n\n我们首先在kubernetes-dashboard命名空间中创建名为admin-user的serviceaccount。\n\ncat <<eof | kubectl apply -f -\napiversion: v1\nkind: serviceaccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\neof\n\n\n为admin-user用户创建一个clusterrolebinding\n\ncat <<eof | kubectl apply -f -\napiversion: rbac.authorization.k8s.io/v1\nkind: clusterrolebinding\nmetadata:\n  name: admin-user\nroleref:\n  apigroup: rbac.authorization.k8s.io\n  kind: clusterrole\n  name: cluster-admin\nsubjects:\n- kind: serviceaccount\n  name: admin-user\n  namespace: kubernetes-dashboard\neof\n\n\n获得一个bearer token\n\nkubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"\n\n\n应该打印出如下内容：\n\neyjhbgcioijsuzi1niisimtpzci6imrnv2dkvhnrzuliru1frmlszhhmcgt3vmvmvm5pyw5bofixyvc3nhrfvm8ifq.eyjpc3mioijrdwjlcm5ldgvzl3nlcnzpy2vhy2nvdw50iiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9uyw1lc3bhy2uioijrdwjlcm5ldgvzlwrhc2hib2fyzcisimt1ymvybmv0zxmuaw8vc2vydmljzwfjy291bnqvc2vjcmv0lm5hbwuioijhzg1pbi11c2vylxrva2vulth2djg5iiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9zzxj2awnllwfjy291bnqubmftzsi6imfkbwlulxvzzxiilcjrdwjlcm5ldgvzlmlvl3nlcnzpy2vhy2nvdw50l3nlcnzpy2utywnjb3vudc51awqioiiymjfhywe5zi1jnjblltrlnmytyty4nc02ntm4ywu3mjywm2uilcjzdwiioijzexn0zw06c2vydmljzwfjy291bnq6a3vizxjuzxrlcy1kyxnoym9hcmq6ywrtaw4tdxnlcij9.isgz8pcjnswyahm21okd3in1ueblys0237xp6ktjbr-udpmecrwluelhceejijtrv55jas2pb2hke0lq1fyheeksgw8oiehphmm06ua59ehqit2pybls022-g63bibkdowt7dkd212ihcnqmfoho2pzqulsyflmublw47r6xereeira4kqorreyxkgt8-xyiyctxttwoxwympp7n2dey9dnb5wifqvtaaomzfvuupwnp2skdrgt_ce4n3beof_5l2d-v_hds7cmhl6odmfdgj3w8kzacsnrwz_qhsyqazytf8rbfz7t-kzkyzvlz8zfqwq4g5oc-kh6xjr8-1wk2lq\n\n\n现在复制bearer token并将其粘贴到登录屏幕上的输入 token *字段中。单击登录按钮即可。您现在以管理员身份登录。\n\n工作量状态\n\n\n\nnodes\n\n\n\n\n# 清理\n\n移除adminserviceaccount和clusterrolebinding\n\nkubectl -n kubernetes-dashboard delete serviceaccount admin-user\nkubectl -n kubernetes-dashboard delete clusterrolebinding admin-user\n',charsets:{cjk:!0},lastUpdated:"2022/02/09, 14:16:28",lastUpdatedTimestamp:1644387388e3},{title:"整合云平台相关配置（内部）",frontmatter:{title:"整合云平台相关配置（内部）",date:"2022-02-09T13:37:49.000Z",permalink:"/pages/f61a3d/"},regularPath:"/22.Kubernetes/03.%E5%86%85%E9%83%A8%E5%B9%B3%E5%8F%B0%E5%AF%B9%E6%8E%A5/01.%E6%95%B4%E5%90%88%E4%BA%91%E5%B9%B3%E5%8F%B0%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%EF%BC%88%E5%86%85%E9%83%A8%EF%BC%89.html",relativePath:"22.Kubernetes/03.内部平台对接/01.整合云平台相关配置（内部）.md",key:"v-183d693e",path:"/pages/f61a3d/",headers:[{level:2,title:"Kubernetes 调整 nodePort 端口范围",slug:"kubernetes-调整-nodeport-端口范围",normalizedTitle:"kubernetes 调整 nodeport 端口范围",charIndex:66},{level:2,title:"自动清理完成的 Job",slug:"自动清理完成的-job",normalizedTitle:"自动清理完成的 job",charIndex:2883}],headersStr:"Kubernetes 调整 nodePort 端口范围 自动清理完成的 Job",content:"# 整合云平台相关配置\n\n提示\n\n本小节仅为智慧教学云平台对接k8s做的定制优化配置，与学习k8s及部署无关，仅共内部使用\n\n\n# Kubernetes 调整 nodePort 端口范围\n\nsudo vim /etc/kubernetes/manifests/kube-apiserver.yaml\n\n\n添加--service-node-port-range=1-65535到/etc/kubernetes/manifests/kube-apiserver.yaml直接保存即可生效\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.31.101:6443\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --advertise-address=192.168.31.101\n    - --service-node-port-range=1-65535\n    - --allow-privileged=true\n    - --authorization-mode=Node,RBAC\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --enable-admission-plugins=NodeRestriction\n    - --enable-bootstrap-token-auth=true\n    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt\n    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt\n    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key\n    - --etcd-servers=https://127.0.0.1:2379\n    - --insecure-port=0\n    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt\n    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key\n    - --requestheader-allowed-names=front-proxy-client\n    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n    - --requestheader-extra-headers-prefix=X-Remote-Extra-\n    - --requestheader-group-headers=X-Remote-Group\n    - --requestheader-username-headers=X-Remote-User\n    - --secure-port=6443\n    - --service-account-issuer=https://kubernetes.default.svc.cluster.local\n    - --service-account-key-file=/etc/kubernetes/pki/sa.pub\n    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key\n    - --service-cluster-ip-range=10.96.0.0/12\n    - --feature-gates=TTLAfterFinished=true\n    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n    image: registry.aliyuncs.com/google_containers/kube-apiserver:v1.20.0\n    imagePullPolicy: IfNotPresent\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 192.168.31.101\n        path: /livez\n        port: 6443\n        scheme: HTTPS\n      initialDelaySeconds: 10\n      periodSeconds: 10\n      timeoutSeconds: 15\n    name: kube-apiserver\n    readinessProbe:\n      failureThreshold: 3\n      httpGet:\n        host: 192.168.31.101\n        path: /readyz\n        port: 6443\n        scheme: HTTPS\n      periodSeconds: 1\n      timeoutSeconds: 15\n    resources:\n\n\n\n# 自动清理完成的 Job\n\nk8s之前的job完成后，如果不用cronjob管理，都不会自动删除，该job对象和其相关的pod对象都会保存以便查看记录。\n\n然而在1.12版本之后，k8s提出了通过TTL自动删除Job的特性，当前仅对job生效，对 Complete 和 Failed 状态的Job都会自动删除，以后会逐步对所有的其他资源对象生效。\n\n关于spec.ttlSecondsAfterFinished属性的三种含义\n\nttlSecondsAfterFinished: 0 #在job执行完时马上删除\nttlSecondsAfterFinished: 100 #在job执行完后，等待100s再删除\n\n\n若不设置ttlSecondsAfterFinished这个属性则不会自动删除\n\n操作实践\n\n这个特性现在在v1.12版本是alpha阶段，而且默认关闭的，需要手动开启。\n\n需要修改的组件包括apiserver、controller和scheduler。\n\n修改/etc/kubernetes/manifests下面对应的三个.yaml静态文件，加入- --feature-gates=TTLAfterFinished=true命令，然后令对应的pod重新运行即可。\n\n例如修改后的kube-scheduler.yaml的spec部分如下，kube-apiserver.yaml和kube-controller-manager.yaml也在spec部分加入- --feature-gates=TTLAfterFinished=true即可。\n\nsudo vim /etc/kubernetes/manifests/kube-scheduler.yaml\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\napiVersion: v1\n...\nspec:\n  containers:\n  - command:\n    - kube-scheduler\n    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n    - --bind-address=127.0.0.1\n    - --kubeconfig=/etc/kubernetes/scheduler.conf\n    - --leader-elect=true\n    - --port=0\n    - --feature-gates=TTLAfterFinished=true\n ...\n",normalizedContent:"# 整合云平台相关配置\n\n提示\n\n本小节仅为智慧教学云平台对接k8s做的定制优化配置，与学习k8s及部署无关，仅共内部使用\n\n\n# kubernetes 调整 nodeport 端口范围\n\nsudo vim /etc/kubernetes/manifests/kube-apiserver.yaml\n\n\n添加--service-node-port-range=1-65535到/etc/kubernetes/manifests/kube-apiserver.yaml直接保存即可生效\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\napiversion: v1\nkind: pod\nmetadata:\n  annotations:\n    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.31.101:6443\n  creationtimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --advertise-address=192.168.31.101\n    - --service-node-port-range=1-65535\n    - --allow-privileged=true\n    - --authorization-mode=node,rbac\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --enable-admission-plugins=noderestriction\n    - --enable-bootstrap-token-auth=true\n    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt\n    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt\n    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key\n    - --etcd-servers=https://127.0.0.1:2379\n    - --insecure-port=0\n    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n    - --kubelet-preferred-address-types=internalip,externalip,hostname\n    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt\n    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key\n    - --requestheader-allowed-names=front-proxy-client\n    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n    - --requestheader-extra-headers-prefix=x-remote-extra-\n    - --requestheader-group-headers=x-remote-group\n    - --requestheader-username-headers=x-remote-user\n    - --secure-port=6443\n    - --service-account-issuer=https://kubernetes.default.svc.cluster.local\n    - --service-account-key-file=/etc/kubernetes/pki/sa.pub\n    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key\n    - --service-cluster-ip-range=10.96.0.0/12\n    - --feature-gates=ttlafterfinished=true\n    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n    image: registry.aliyuncs.com/google_containers/kube-apiserver:v1.20.0\n    imagepullpolicy: ifnotpresent\n    livenessprobe:\n      failurethreshold: 8\n      httpget:\n        host: 192.168.31.101\n        path: /livez\n        port: 6443\n        scheme: https\n      initialdelayseconds: 10\n      periodseconds: 10\n      timeoutseconds: 15\n    name: kube-apiserver\n    readinessprobe:\n      failurethreshold: 3\n      httpget:\n        host: 192.168.31.101\n        path: /readyz\n        port: 6443\n        scheme: https\n      periodseconds: 1\n      timeoutseconds: 15\n    resources:\n\n\n\n# 自动清理完成的 job\n\nk8s之前的job完成后，如果不用cronjob管理，都不会自动删除，该job对象和其相关的pod对象都会保存以便查看记录。\n\n然而在1.12版本之后，k8s提出了通过ttl自动删除job的特性，当前仅对job生效，对 complete 和 failed 状态的job都会自动删除，以后会逐步对所有的其他资源对象生效。\n\n关于spec.ttlsecondsafterfinished属性的三种含义\n\nttlsecondsafterfinished: 0 #在job执行完时马上删除\nttlsecondsafterfinished: 100 #在job执行完后，等待100s再删除\n\n\n若不设置ttlsecondsafterfinished这个属性则不会自动删除\n\n操作实践\n\n这个特性现在在v1.12版本是alpha阶段，而且默认关闭的，需要手动开启。\n\n需要修改的组件包括apiserver、controller和scheduler。\n\n修改/etc/kubernetes/manifests下面对应的三个.yaml静态文件，加入- --feature-gates=ttlafterfinished=true命令，然后令对应的pod重新运行即可。\n\n例如修改后的kube-scheduler.yaml的spec部分如下，kube-apiserver.yaml和kube-controller-manager.yaml也在spec部分加入- --feature-gates=ttlafterfinished=true即可。\n\nsudo vim /etc/kubernetes/manifests/kube-scheduler.yaml\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\napiversion: v1\n...\nspec:\n  containers:\n  - command:\n    - kube-scheduler\n    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n    - --bind-address=127.0.0.1\n    - --kubeconfig=/etc/kubernetes/scheduler.conf\n    - --leader-elect=true\n    - --port=0\n    - --feature-gates=ttlafterfinished=true\n ...\n",charsets:{cjk:!0},lastUpdated:"2022/02/09, 13:47:26",lastUpdatedTimestamp:1644385646e3},{title:"支持这个项目",frontmatter:{title:"支持这个项目",date:"2020-05-12T15:09:57.000Z",permalink:"/pages/1b12ed",sidebar:!1,article:!1},regularPath:"/99.%E6%94%AF%E6%8C%81/01.%E6%94%AF%E6%8C%81.html",relativePath:"99.支持/01.支持.md",key:"v-09f5c492",path:"/pages/1b12ed/",headers:[{level:3,title:"用爱发电",slug:"用爱发电",normalizedTitle:"用爱发电",charIndex:2},{level:2,title:"致谢",slug:"致谢",normalizedTitle:"致谢",charIndex:186}],headersStr:"用爱发电 致谢",content:"# 用爱发电\n\n本站全部文档由作者工作和学习中收集整理，如果您正在使用这个项目并感觉良好，或者是想支持我继续开发，您可以通过如下任意方式支持我：\n\n 1. Star并分享 主题项目 🚀\n 2. 保留主题 footer(页脚) 的主题链接 😄\n 3. 通过以下二维码 一次性捐款。 我多半会买一杯 咖啡 茶。🍵\n\n谢谢！ ❤️\n\n微信   支付宝\n     \n\n\n# 致谢\n\n感谢给予支持的朋友，您的支持是我前进的动力 🎉",normalizedContent:"# 用爱发电\n\n本站全部文档由作者工作和学习中收集整理，如果您正在使用这个项目并感觉良好，或者是想支持我继续开发，您可以通过如下任意方式支持我：\n\n 1. star并分享 主题项目 🚀\n 2. 保留主题 footer(页脚) 的主题链接 😄\n 3. 通过以下二维码 一次性捐款。 我多半会买一杯 咖啡 茶。🍵\n\n谢谢！ ❤️\n\n微信   支付宝\n     \n\n\n# 致谢\n\n感谢给予支持的朋友，您的支持是我前进的动力 🎉",charsets:{cjk:!0},lastUpdated:"2022/02/09, 17:09:54",lastUpdatedTimestamp:1644397794e3},{title:"博客文章",frontmatter:{archivesPage:!0,title:"博客文章",permalink:"/blog/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-0a72df0a",path:"/blog/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/02/08, 17:44:18",lastUpdatedTimestamp:1644313458e3},{title:"Home",frontmatter:{home:!0,heroImage:"/img/logo.png",heroText:"Java全栈",tagline:"🚀有道无术，术尚可求，有术无道，止于术",actionText:"互联网 Java 全栈工程师 →",actionLink:"/pages/250f21/",bannerBg:"none",features:[{title:"热门框架源码学习",details:"掌握 Intellij IDEA、Maven、Bootstrap、Spring、Spring MVC、MyBatis、HttpClient 等工具与框架的使用，并开始引入架构的概念，为“微服务架构”阶段打下坚实的基础"},{title:"自动化工具专题",details:"Maven项目工具，Git分布式版本控制，Sonar代码质量检测平台，Jenkins DevOps自动化"},{title:"微服务架构专题",details:"从底层 Linux 的安装到最终 DevOps 的所需技能。包括但不限于 Spring Boot、Spring Cloud、Spring Cloud Alibaba、Dubbo、Zookeeper、Redis、ELK、RabbitMQ、Ubuntu、Docker、Kubernetes、Jenkins 等全栈技能"}],postList:"none"},regularPath:"/",relativePath:"index.md",key:"v-363a61d8",path:"/",headers:[{level:2,title:"🎖特别用户",slug:"🎖特别用户",normalizedTitle:"🎖特别用户",charIndex:14},{level:2,title:"⚡️未来...",slug:"⚡️未来",normalizedTitle:"⚡️未来...",charIndex:646}],headersStr:"🎖特别用户 ⚡️未来...",content:"支持这个项目\n\n\n\n\n\n# 🎖特别用户\n\nOpenHarmony\n\n开放原子开源基金会\n\nMyBatis-Plus官网\n\n🚀为简化开发而生\n\nDeepin 社区\n\nDeepin 应用开发技术分享、DTK开发经验等\n\nVForm官网\n\n低代码表单优选方案，拖拽式设计，一键生成源码\n\n- name: OpenHarmony\n  desc: 开放原子开源基金会\n  link: https://docs.openharmony.cn/pages/000000/\n  bgColor: '#f1f1f1'\n  textColor: '#2A3344'\n- name: MyBatis-Plus官网\n  desc: 🚀为简化开发而生\n  link: https://baomidou.com/\n  bgColor: '#f1f1f1'\n  textColor: '#2A3344'\n- name: Deepin 社区\n  desc: Deepin 应用开发技术分享、DTK开发经验等\n  link: https://docs.deepin.org\n  bgColor: '#f1f1f1'\n  textColor: '#2A3344'\n- name: VForm官网\n  desc: 低代码表单优选方案，拖拽式设计，一键生成源码\n  link: http://www.vform666.com\n  bgColor: '#f1f1f1'\n  textColor: '#2A3344'\n\n\n\n\n# ⚡️未来...\n\n提示\n\n期待 VuePress v2.0 以及 VitePress 的正式发布...\n\n届时，VuePress 1.x 编译慢的缺点将得到极大的改善。我将会视情况把主题升级至 VuePress v2.0 或 VitePress。还希望大家多多 💖支持 哟，持续关注吧~\n\n\n\n\n\n\nHappy Wish\nNew You\nYear Luck\n2022 Tomorrow\n\n",normalizedContent:"支持这个项目\n\n\n\n\n\n# 🎖特别用户\n\nopenharmony\n\n开放原子开源基金会\n\nmybatis-plus官网\n\n🚀为简化开发而生\n\ndeepin 社区\n\ndeepin 应用开发技术分享、dtk开发经验等\n\nvform官网\n\n低代码表单优选方案，拖拽式设计，一键生成源码\n\n- name: openharmony\n  desc: 开放原子开源基金会\n  link: https://docs.openharmony.cn/pages/000000/\n  bgcolor: '#f1f1f1'\n  textcolor: '#2a3344'\n- name: mybatis-plus官网\n  desc: 🚀为简化开发而生\n  link: https://baomidou.com/\n  bgcolor: '#f1f1f1'\n  textcolor: '#2a3344'\n- name: deepin 社区\n  desc: deepin 应用开发技术分享、dtk开发经验等\n  link: https://docs.deepin.org\n  bgcolor: '#f1f1f1'\n  textcolor: '#2a3344'\n- name: vform官网\n  desc: 低代码表单优选方案，拖拽式设计，一键生成源码\n  link: http://www.vform666.com\n  bgcolor: '#f1f1f1'\n  textcolor: '#2a3344'\n\n\n\n\n# ⚡️未来...\n\n提示\n\n期待 vuepress v2.0 以及 vitepress 的正式发布...\n\n届时，vuepress 1.x 编译慢的缺点将得到极大的改善。我将会视情况把主题升级至 vuepress v2.0 或 vitepress。还希望大家多多 💖支持 哟，持续关注吧~\n\n\n\n\n\n\nhappy wish\nnew you\nyear luck\n2022 tomorrow\n\n",charsets:{cjk:!0},lastUpdated:"2022/02/10, 09:38:09",lastUpdatedTimestamp:1644457089e3},{title:"常见问题与解决方案",frontmatter:{title:"常见问题与解决方案",date:"2022-02-09T10:51:04.000Z",permalink:"/pages/7b6be8/"},regularPath:"/04.Ubuntu/02.Ubuntu%20%E4%BD%BF%E7%94%A8/05.%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.html",relativePath:"04.Ubuntu/02.Ubuntu 使用/05.常见问题与解决方案.md",key:"v-250b49a5",path:"/pages/7b6be8/",headers:[{level:2,title:"VirtualBox中Ubuntu桥接DHCP始终取到相同IP地址问题",slug:"virtualbox中ubuntu桥接dhcp始终取到相同ip地址问题",normalizedTitle:"virtualbox中ubuntu桥接dhcp始终取到相同ip地址问题",charIndex:16},{level:2,title:"vi/vim模式下的粘贴",slug:"vi-vim模式下的粘贴",normalizedTitle:"vi/vim模式下的粘贴",charIndex:442}],headersStr:"VirtualBox中Ubuntu桥接DHCP始终取到相同IP地址问题 vi/vim模式下的粘贴",content:"# 常见问题与解决方案\n\n\n# VirtualBox中Ubuntu桥接DHCP始终取到相同IP地址问题\n\n如题，尽管在创建/克隆了Ubuntu后更改了MAC物理地址，但系统启动后始终得到相同的IP地址。百度了很多没得到解，后来谷歌看了些老外的帖子，意外发现有个文件 /etc/machine-id，里面存放了机器ID，并尝试修改成不同值后，DHCP能得到不同IP地址了。\n\n/etc/machine-id 是一个只读文件，权限是444（即,r--r--r--）。几种方法任选其一：\n\n * 直接使用root强制保存。\n * 先chmod修改其权限，让它可写；然后修改它；再将权限改回。\n * sudo将它删除，然后创建一个并写入一些字符；再改回权限。\n\nmachine-id中的值是一串字母数字组合，应该是固定长度。\n\n如果只修改其中一个字符（即，格式仍合法），则系统重启后，该值不会改变；如果胡乱修改一下，使它缺几位，多几位，非法字符等，则系统重启后，该值会自动重新分配。\n\n\n# vi/vim模式下的粘贴\n\n因为linux系统和win系统的差异性，有时候在win环境运行的python代码会放在Linux系统上执行，这个时候就需要把win系统上IDE上的代码copy下来，在Linux文件vi/wim模式下进行粘贴，但是会发现粘贴过来的代码多出了很多空格\n\n这是因为拷贝的文本中已经有表示缩进的空格或者制表符的话，它们也会被当成字符串，而被缩进，从而形成如上图所示的样式\n\n解决办法：\n\n 1. 在拷贝前输入:set paste (这样的话，vim就不会启动自动缩进，而只是纯拷贝粘贴）\n 2. 拷贝完成之后，输入:set nopaste (关闭paste)",normalizedContent:"# 常见问题与解决方案\n\n\n# virtualbox中ubuntu桥接dhcp始终取到相同ip地址问题\n\n如题，尽管在创建/克隆了ubuntu后更改了mac物理地址，但系统启动后始终得到相同的ip地址。百度了很多没得到解，后来谷歌看了些老外的帖子，意外发现有个文件 /etc/machine-id，里面存放了机器id，并尝试修改成不同值后，dhcp能得到不同ip地址了。\n\n/etc/machine-id 是一个只读文件，权限是444（即,r--r--r--）。几种方法任选其一：\n\n * 直接使用root强制保存。\n * 先chmod修改其权限，让它可写；然后修改它；再将权限改回。\n * sudo将它删除，然后创建一个并写入一些字符；再改回权限。\n\nmachine-id中的值是一串字母数字组合，应该是固定长度。\n\n如果只修改其中一个字符（即，格式仍合法），则系统重启后，该值不会改变；如果胡乱修改一下，使它缺几位，多几位，非法字符等，则系统重启后，该值会自动重新分配。\n\n\n# vi/vim模式下的粘贴\n\n因为linux系统和win系统的差异性，有时候在win环境运行的python代码会放在linux系统上执行，这个时候就需要把win系统上ide上的代码copy下来，在linux文件vi/wim模式下进行粘贴，但是会发现粘贴过来的代码多出了很多空格\n\n这是因为拷贝的文本中已经有表示缩进的空格或者制表符的话，它们也会被当成字符串，而被缩进，从而形成如上图所示的样式\n\n解决办法：\n\n 1. 在拷贝前输入:set paste (这样的话，vim就不会启动自动缩进，而只是纯拷贝粘贴）\n 2. 拷贝完成之后，输入:set nopaste (关闭paste)",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3},{title:"Ubuntu 配置静态ip和动态ip",frontmatter:{title:"Ubuntu 配置静态ip和动态ip",date:"2022-02-09T10:56:45.000Z",permalink:"/pages/63ed35/"},regularPath:"/04.Ubuntu/02.Ubuntu%20%E4%BD%BF%E7%94%A8/04.Ubuntu%20%E9%85%8D%E7%BD%AE%E9%9D%99%E6%80%81ip%E5%92%8C%E5%8A%A8%E6%80%81ip.html",relativePath:"04.Ubuntu/02.Ubuntu 使用/04.Ubuntu 配置静态ip和动态ip.md",key:"v-2617300a",path:"/pages/63ed35/",headers:[{level:2,title:"ubuntu18.04 配置静态ip",slug:"ubuntu18-04-配置静态ip",normalizedTitle:"ubuntu18.04 配置静态ip",charIndex:31},{level:2,title:"ubuntu18.04 配置动态ip",slug:"ubuntu18-04-配置动态ip",normalizedTitle:"ubuntu18.04 配置动态ip",charIndex:1713}],headersStr:"ubuntu18.04 配置静态ip ubuntu18.04 配置动态ip",content:"# Ubuntu 18.04 配置静态ip和动态ip\n\n\n# ubuntu18.04 配置静态ip\n\nubuntu18.04 采用的是netplan来管理network。首先我们进入到/etc/netplan目录下，查找默认的网络配置文件，文件后最为.yaml。我这里默认的配置文件名为：00-installer-config.yaml\n\n\n\n\n \n\n\nroot@heyuqiang:/etc/netplan# cd /etc/netplan\nroot@heyuqiang:/etc/netplan# ls\n00-installer-config.yaml\n\n\n编辑网络配置文件之前，先查看自己的网卡名称，我的是ens160。\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nroot@heyuqiang:/etc/netplan# ifconfig\nens160: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n        inet 192.168.1.100  netmask 255.255.0.0  broadcast 10.17.255.255\n        inet6 fe80::20c:29ff:fe85:5736  prefixlen 64  scopeid 0x20<link>\n        ether 00:0c:29:85:57:36  txqueuelen 1000  (Ethernet)\n        RX packets 11068  bytes 7835851 (7.8 MB)\n        RX errors 0  dropped 496  overruns 0  frame 0\n        TX packets 1626  bytes 137807 (137.8 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nlo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536\n        inet 127.0.0.1  netmask 255.0.0.0\n        inet6 ::1  prefixlen 128  scopeid 0x10<host>\n        loop  txqueuelen 1000  (Local Loopback)\n        RX packets 106  bytes 8598 (8.5 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 106  bytes 8598 (8.5 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\n\n编辑网络配置文件00-installer-config.yaml，内容如下：\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n# Let NetworkManager manage all devices on this system\nnetwork:\n  version: 2\n  ethernets:\n     ens160: #配置的网卡名称,使用ifconfig -a查看得到\n       dhcp4: no #dhcp4关闭\n       addresses: [192.168.1.100/24] #设置本机IP及掩码\n       gateway4: 192.168.1.1 #设置网关\n       nameservers:\n         addresses: [192.168.1.1] #设置DNS\n\n\n使用命令，使静态ip生效。\n\nsudo netplan apply\n\n\n编辑网络配置文件之前，使用ifconfig命令查看配置情况，如果配置成功上图中ip会变成自己设置的ip。\n\n\n# ubuntu18.04 配置动态ip\n\n修改网络配置文件的内容如下：\n\n\n\n\n\n\n \n\n\n\n# This is the network config written by 'subiquity'\nnetwork:\n  ethernets:\n    ens160:\n      dhcp4: true\n  version: 2\n\n\n使用命令，使动态ip生效。\n\nsudo netplan apply\n\n\n之后再使用ifconfig命令查看配置情况，如果配置成功上图中ip会变成动态的ip。",normalizedContent:"# ubuntu 18.04 配置静态ip和动态ip\n\n\n# ubuntu18.04 配置静态ip\n\nubuntu18.04 采用的是netplan来管理network。首先我们进入到/etc/netplan目录下，查找默认的网络配置文件，文件后最为.yaml。我这里默认的配置文件名为：00-installer-config.yaml\n\n\n\n\n \n\n\nroot@heyuqiang:/etc/netplan# cd /etc/netplan\nroot@heyuqiang:/etc/netplan# ls\n00-installer-config.yaml\n\n\n编辑网络配置文件之前，先查看自己的网卡名称，我的是ens160。\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nroot@heyuqiang:/etc/netplan# ifconfig\nens160: flags=4163<up,broadcast,running,multicast>  mtu 1500\n        inet 192.168.1.100  netmask 255.255.0.0  broadcast 10.17.255.255\n        inet6 fe80::20c:29ff:fe85:5736  prefixlen 64  scopeid 0x20<link>\n        ether 00:0c:29:85:57:36  txqueuelen 1000  (ethernet)\n        rx packets 11068  bytes 7835851 (7.8 mb)\n        rx errors 0  dropped 496  overruns 0  frame 0\n        tx packets 1626  bytes 137807 (137.8 kb)\n        tx errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nlo: flags=73<up,loopback,running>  mtu 65536\n        inet 127.0.0.1  netmask 255.0.0.0\n        inet6 ::1  prefixlen 128  scopeid 0x10<host>\n        loop  txqueuelen 1000  (local loopback)\n        rx packets 106  bytes 8598 (8.5 kb)\n        rx errors 0  dropped 0  overruns 0  frame 0\n        tx packets 106  bytes 8598 (8.5 kb)\n        tx errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\n\n编辑网络配置文件00-installer-config.yaml，内容如下：\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n# let networkmanager manage all devices on this system\nnetwork:\n  version: 2\n  ethernets:\n     ens160: #配置的网卡名称,使用ifconfig -a查看得到\n       dhcp4: no #dhcp4关闭\n       addresses: [192.168.1.100/24] #设置本机ip及掩码\n       gateway4: 192.168.1.1 #设置网关\n       nameservers:\n         addresses: [192.168.1.1] #设置dns\n\n\n使用命令，使静态ip生效。\n\nsudo netplan apply\n\n\n编辑网络配置文件之前，使用ifconfig命令查看配置情况，如果配置成功上图中ip会变成自己设置的ip。\n\n\n# ubuntu18.04 配置动态ip\n\n修改网络配置文件的内容如下：\n\n\n\n\n\n\n \n\n\n\n# this is the network config written by 'subiquity'\nnetwork:\n  ethernets:\n    ens160:\n      dhcp4: true\n  version: 2\n\n\n使用命令，使动态ip生效。\n\nsudo netplan apply\n\n\n之后再使用ifconfig命令查看配置情况，如果配置成功上图中ip会变成动态的ip。",charsets:{cjk:!0},lastUpdated:"2022/02/09, 11:02:52",lastUpdatedTimestamp:1644375772e3}],themeConfig:{nav:[{text:"首页",link:"/"},{text:"指南",items:[{text:"基础",items:[{text:"基础设施即服务",link:"/pages/250f21/"},{text:"平台即服务",link:"/pages/f04654/"},{text:"微服务解决复杂问题",link:"/pages/58bcba/"},{text:"Spring Cloud",link:"/pages/b05793/"},{text:"Spring Cloud Alibaba",link:"/pages/1946f3/"}]},{text:"高级",items:[{text:"Docker",link:"/pages/aeaf26/"},{text:"Kubernetes",link:"/pages/059418/"}]},{text:"专题",items:[{text:"Spring Security OAuth2",link:"/pages/fbef9a/"},{text:"企业级 DevOps CI/CD实践",link:"/pages/72ccd6/"},{text:"Simplify Docs",link:"/pages/d49c54/"}]}]},{text:"在线文档",items:[{text:"前端文档",items:[{text:"Element - 网站快速成型工具",link:"https://element.eleme.cn/#/zh-CN"},{text:"Lodash Documentation",link:"https://lodash.com/docs/#defaultsDeep"},{text:"Nuxt.js - Vue.js 通用应用框架",link:"https://zh.nuxtjs.org"},{text:"Vue.js",link:"https://cn.vuejs.org/v2/api/"},{text:"Vue CLI",link:"https://cli.vuejs.org/zh/"},{text:"Vuex",link:"https://vuex.vuejs.org/zh/api/#vuex-store"},{text:"VuePress 中文文档",link:"https://www.vuepress.cn"}]},{text:"后端文档",items:[{text:"Google Guava官方教程",link:"https://ifeve.com/google-guava/"},{text:"Docker Documentation",link:"https://docs.docker.com/"}]}]},{text:"在线工具",items:[{text:"在线编辑",items:[{text:"在线白板",link:"https://excalidraw.com/"},{text:"JSON 编辑器",link:"https://www.bejson.com/jsoneditoronline"},{text:"MD 表格生成",link:"https://tableconvert.com/"},{text:"CRON 表达式",link:"http://cron.qqe2.com/"},{text:"代码格式化",link:"https://tool.oschina.net/codeformat/html"},{text:"二维码生成器",link:"https://cli.im/"},{text:"在线编码转换",link:"http://tool.chinaz.com/tools/native_ascii.aspx"}]},{text:"在线服务",items:[{text:"流程图思维导图",link:"https://www.processon.com"},{text:"画图工具",link:"https://excalidraw.com"}]},{text:"开源镜像",items:[{text:"OPSX镜像站",link:"https://developer.aliyun.com/mirror/"},{text:"AZURE镜像站",link:"https://mirror.azure.cn/"},{text:"Docker Hub",link:"https://hub.docker.com/"}]}]},{text:"💖支持",link:"/pages/1b12ed/"},{text:"返回旧版",link:"https://www.pigcs.com/"}],sidebarDepth:2,logo:"/img/logo.png",repo:"xugaoyi/vuepress-theme-vdoing",searchMaxSuggestions:10,lastUpdated:"上次更新",sidebar:{"/01.指南/":[{title:"基础",collapsable:!1,children:[["01.基础/01.基础设施即服务.md","基础设施即服务","/pages/250f21/"],["01.基础/02.平台即服务.md","平台即服务","/pages/f04654/"],["01.基础/03.微服务解决复杂问题.md","微服务解决复杂问题","/pages/58bcba/"],["01.基础/04.Spring Cloud.md","Spring Cloud","/pages/b05793/"],["01.基础/05.Spring Cloud Alibaba.md","Spring Cloud Alibaba","/pages/1946f3/"]]},{title:"高级",collapsable:!1,children:[["02.高级/01.Docker.md","Docker","/pages/aeaf26/"],["02.高级/02.Kubernetes.md","Kubernetes","/pages/059418/"]]},{title:"专题",collapsable:!1,children:[["03.专题/01.Spring Security OAuth2.md","Spring Security OAuth2","/pages/fbef9a/"],["03.专题/02.企业级 DevOps 实践.md","企业级 DevOps CI/CD实践","/pages/72ccd6/"]]}],catalogue:{},"/02.Linux/":[{title:"Linux",collapsable:!1,children:[["01.Linux/01.Linux 简介.md","Linux 简介","/pages/202ee1/"],["01.Linux/02.Linux 与 Windows 比较.md","Linux 与 Windows 比较","/pages/715f2d/"],["01.Linux/03.Linux 远程控制管理.md","Linux 远程控制管理","/pages/56abb4/"],["01.Linux/04.Linux 的目录结构.md","Linux 的目录结构","/pages/1e9034/"],["01.Linux/05.Linux 操作文件目录.md","Linux 操作文件目录","/pages/8ad474/"],["01.Linux/06.Linux 系统管理命令.md","Linux 系统管理命令","/pages/6f8d47/"],["01.Linux/07.Linux 开关机命令.md","Linux 开关机命令","/pages/ef1cfd/"],["01.Linux/08.Linux 压缩命令.md","Linux 压缩命令","/pages/657e76/"],["01.Linux/09.Linux 编辑器.md","Linux 编辑器","/pages/287d9f/"],["01.Linux/10.Linux 软件包管理.md","Linux 软件包管理","/pages/0d3526/"],["01.Linux/11.Linux 用户和组管理.md","Linux 用户和组管理","/pages/037bdb/"],["01.Linux/12.Linux 文件权限管理.md","Linux 文件权限管理","/pages/c29423/"],["01.Linux/13.Linux LVM 磁盘扩容.md","Linux LVM 磁盘扩容","/pages/6afa9f/"]]}],"/03.CentOS/":[{title:"入门",collapsable:!1,children:[["01.入门/01.什么是 CentOS.md","什么是 CentOS","/pages/8eb38e/"],["01.入门/02.CentOS 7 中使用阿里云的yum源.md","CentOS 7 中使用阿里云的yum源","/pages/4d8bf7/"],["01.入门/03.CentOS 7 设置 SSH 通过密钥登录.md","CentOS 7 设置 SSH 通过密钥登录","/pages/801448/"],["01.入门/04.CentOS 7 时间与网络时间同步.md","CentOS 7 时间与网络时间同步","/pages/72cb23/"],["01.入门/05.CentOS 7 安装Jdk并配置环境变量.md","CentOS 7 安装Jdk并配置环境变量","/pages/b65cee/"],["01.入门/06.CentOS 7 配置静态ip和动态ip.md","CentOS 7 配置静态ip和动态ip","/pages/8a00d6/"],["01.入门/07.CentOS 7 安装 Nginx.md","CentOS 7 安装 Nginx","/pages/2bd107/"],["01.入门/08.CentOS 7 安装 Tomcat.md","CentOS 7 安装 Tomcat","/pages/b0c71d/"],["01.入门/09.CentOS 7 安装 MySQL.md","CentOS 7 安装 MySQL","/pages/3748a2/"],["01.入门/10.CentOS 7 安装 MongoDB.md","CentOS 7 安装 MongoDB","/pages/866b2e/"],["01.入门/11.CentOS 7 安装 NodeJS.md","CentOS 7 安装 NodeJS","/pages/39ed97/"],["01.入门/12.CentOS 7 安装 Gitlab.md","CentOS 7 安装 Gitlab","/pages/5cddbb/"],["01.入门/13.CentOS 7 安装 SonarQube.md","CentOS 7 安装 SonarQube","/pages/1dfc9f/"],["01.入门/14.CentOS 7 安装 Jenkins.md","CentOS 7 安装 Jenkins","/pages/893d19/"],["01.入门/15.CentOS 7 安装 Redis.md","CentOS 7 安装 Redis","/pages/54c917/"],["01.入门/16.CentOS 7 安装 TigerVNC Server.md","CentOS 7 安装 TigerVNC Server","/pages/8c6aad/"],["01.入门/17.CentOS 7 安装 NFS.md","CentOS 7 安装 NFS","/pages/50595e/"]]},{title:"高级",collapsable:!1,children:[["02.高级/01.CentOS7 LVM操作实战.md","CentOS7 LVM操作实战","/pages/619aa3/"]]},{title:"实战",collapsable:!1,children:[["03.实战/01.部署后台portal-web项目.md","部署后台portal-web项目","/pages/1b51c8/"],["03.实战/02.部署前端ITP项目.md","部署前端ITP项目","/pages/7ba23c/"]]}],"/04.Ubuntu/":[{title:"Ubuntu",collapsable:!1,children:[["01.Ubuntu/01.什么是 Ubuntu.md","什么是 Ubuntu","/pages/49420c/"]]},{title:"Ubuntu 使用",collapsable:!1,children:[["02.Ubuntu 使用/01.Ubuntu 安装教程.md","Ubuntu 安装教程","/pages/075d45/"],["02.Ubuntu 使用/02.Ubuntu 更换国内源.md","Ubuntu 更换国内源","/pages/5d8a7e/"],["02.Ubuntu 使用/03.Ubuntu 开发环境变量配置.md","Ubuntu 开发环境变量配置","/pages/65555a/"],["02.Ubuntu 使用/04.Ubuntu 配置静态ip和动态ip.md","Ubuntu 配置静态ip和动态ip","/pages/63ed35/"],["02.Ubuntu 使用/05.常见问题与解决方案.md","常见问题与解决方案","/pages/7b6be8/"]]}],"/05.MacOS/":[{title:"macOS",collapsable:!1,children:[["01.macOS/01.macOS开发环境安装.md","macOS开发环境安装","/pages/d6e0ac/"],["01.macOS/02.macOS终端利器iTerm2.md","macOS终端利器iTerm2","/pages/3b1a68/"],["01.macOS/03.macOS自带终端美化.md","macOS自带终端美化","/pages/078f4d/"],["01.macOS/04.macOS常用软件.md","macOS常用软件","/pages/9ee77d/"]]}],"/06.GitLab/":[{title:"GitLab",collapsable:!1,children:[["01.GitLab/01.什么是 Git.md","什么是 Git","/pages/22c758/"],["01.GitLab/02.安装 Git.md","安装 Git","/pages/75dfd9/"],["01.GitLab/03.Git 的一般工作流程.md","Git 的一般工作流程","/pages/8383bb/"],["01.GitLab/04.Git 的基本操作.md","Git 的基本操作","/pages/20e9b1/"],["01.GitLab/05.TortoiseGit 简化 Git 操作.md","TortoiseGit 简化 Git 操作","/pages/7eca40/"],["01.GitLab/06.什么是 GitLab.md","什么是 GitLab","/pages/89c106/"],["01.GitLab/07.基于 Docker 安装 GitLab.md","基于 Docker 安装 GitLab","/pages/ffc177/"],["01.GitLab/08.GitLab 的基本设置.md","GitLab 的基本设置","/pages/655544/"],["01.GitLab/09.GitLab 的账户管理.md","GitLab 的账户管理","/pages/0ba162/"],["01.GitLab/10.GitLab 创建第一个项目.md","GitLab 创建第一个项目","/pages/aa27ff/"],["01.GitLab/11.GitLab 使用 SSH 免密登录.md","GitLab 使用 SSH 免密登录","/pages/2b230d/"]]}],"/07.Nexus/":[{title:"Nexus",collapsable:!1,children:[["01.Nexus/01.什么是 Nexus.md","什么是 Nexus","/pages/ceff17/"],["01.Nexus/02.基于 Docker 安装 Nexus.md","基于 Docker 安装 Nexus","/pages/dc57a4/"],["01.Nexus/03.Nexus配置阿里云代理仓库.md","Nexus配置阿里云代理仓库","/pages/159c7d/"],["01.Nexus/04.Maven 仓库介绍.md","Maven 仓库介绍","/pages/ee14ce/"],["01.Nexus/05.在项目中使用 Maven 私服.md","在项目中使用 Maven 私服","/pages/433dea/"],["01.Nexus/06.Maven pom文件标签大全详解.md","Maven pom文件标签大全详解","/pages/d1f496/"],["01.Nexus/07.将项目推送到 Maven 中央仓库实践.md","将项目推送到 Maven 中央仓库实践","/pages/f28aa7/"]]}],"/08.Registry/":[{title:"Registry",collapsable:!1,children:[["01.Registry/01.安装 Docker Registry 私服.md","安装 Docker Registry 私服","/pages/654ea2/"],["01.Registry/02.配置 Docker Registry 客户端.md","配置 Docker Registry 客户端","/pages/87fa6f/"],["01.Registry/03.部署 Harbor 企业级Registry服务器.md","部署 Harbor 企业级Registry服务器","/pages/22b02b/"],["01.Registry/04.部署 Docker 可视化管理工具Portainer.md","部署 Docker 可视化管理工具Portainer","/pages/7ad507/"]]}],"/09.Redis/":[{title:"Redis",collapsable:!1,children:[["01.Redis/01.Redis 简介.md","Redis 简介","/pages/5b81e8/"],["01.Redis/02.基于源码编译安装 Redis.md","基于源码编译安装 Redis","/pages/3f32a2/"],["01.Redis/03.基于 Docker 安装 Redis.md","基于 Docker 安装 Redis","/pages/7a3c10/"]]}],"/10.Nginx/":[{title:"Nginx",collapsable:!1,children:[["01.Nginx/01.什么是 Nginx.md","什么是 Nginx","/pages/8c9288/"],["01.Nginx/02.Nginx 安装SSL证书.md","Nginx 安装SSL证书","/pages/df904c/"],["01.Nginx/03.Nginx 开启gzip压缩.md","Nginx 开启gzip压缩","/pages/73e9ae/"],["01.Nginx/04.Nginx location正则写法.md","Nginx location正则写法","/pages/9d4443/"]]}],"/11.MongoDB/":[{title:"MongoDB",collapsable:!1,children:[["01.MongoDB/01.什么是MongoDB.md","什么是MongoDB","/pages/035a1b/"],["01.MongoDB/02.CentOS7 离线安装MongoDB.md","CentOS7 离线安装MongoDB","/pages/25c76e/"]]}],"/12.MinIO/":[{title:"Minio服务器",collapsable:!1,children:[["01.Minio服务器/01.Minio快速入门指南.md","Minio快速入门指南","/pages/76dc6a/"],["01.Minio服务器/02.Minio Docker快速入门.md","Minio Docker快速入门","/pages/0cd85f/"],["01.Minio服务器/03.Minio纠删码快速入门.md","Minio纠删码快速入门","/pages/6bd2f8/"],["01.Minio服务器/04.分布式Minio快速入门.md","分布式Minio快速入门","/pages/8153b1/"],["01.Minio服务器/05.如何使用 TLS 保护对 MinIO 服务器的访问.md","如何使用 TLS 保护对 MinIO 服务器的访问","/pages/98cf59/"],["01.Minio服务器/06.Minio存储桶通知指南.md","Minio存储桶通知指南","/pages/50e89b/"],["01.Minio服务器/07.Minio服务限制租户.md","Minio服务限制租户","/pages/f9e3a2/"],["01.Minio服务器/08.Minio Server配置指南.md","Minio Server配置指南","/pages/625c17/"],["01.Minio服务器/09.Minio多租户（Multi-tenant）部署指南.md","Minio多租户（Multi-tenant）部署指南","/pages/9b5f74/"]]},{title:"Minio客户端快速入门指南",collapsable:!1,children:[["02.Minio客户端快速入门指南/01.Minio客户端快速入门指南.md","Minio客户端快速入门指南","/pages/17c975/"],["02.Minio客户端快速入门指南/02.Minio Client完全指南.md","Minio Client完全指南","/pages/538f88/"]]},{title:"Minio SDKs",collapsable:!1,children:[["03.Minio SDKs/01.Java Client快速入门指南.md","Java Client快速入门指南","/pages/53a165/"],["03.Minio SDKs/02.Java Client API参考文档.md","Java Client API参考文档","/pages/b8b311/"]]},{title:"实战秘籍",collapsable:!1,children:[["04.实战秘籍/01.使用Certbot生成Let's Encrypt证书.md","使用Certbot生成Let's Encrypt证书","/pages/4a395a/"],["04.实战秘籍/02.使用 MinIO 服务器设置 Nginx 代理.md","使用 MinIO 服务器设置 Nginx 代理","/pages/7acf5a/"]]},{title:"Minio部署",collapsable:!1,children:[["05.Minio部署/01.Minio部署快速入门.md","Minio部署快速入门","/pages/542516/"],["05.Minio部署/02.使用Docker Swarm部署Minio.md","使用Docker Swarm部署Minio","/pages/b1ea49/"],["05.Minio部署/03.使用Kubernetes部署Minio.md","使用Kubernetes部署Minio","/pages/01eafc/"],["05.Minio部署/04.在 Docker Compose 上部署 MinIO.md","在 Docker Compose 上部署 MinIO","/pages/46c251/"]]}],"/13.RabbitMQ/":[{title:"快速入门",collapsable:!1,children:[["01.快速入门/01.RabbitMQ 简介.md","RabbitMQ 简介","/pages/d710e9/"],["01.快速入门/02.RabbitMQ 快速入门.md","RabbitMQ 快速入门","/pages/1a684e/"],["01.快速入门/03.RabbitMQ 消息模型.md","RabbitMQ 消息模型","/pages/05a61a/"],["01.快速入门/04.Spring整合RabbitMQ.md","Spring整合RabbitMQ","/pages/36085c/"]]},{title:"高级用法",collapsable:!1,children:[["02.高级用法/01.TTL队列_死信队列.md","TTL队列_死信队列","/pages/c5c502/"]]}],"/14.微服务/":[{title:"微服务简介",collapsable:!1,children:[["01.微服务简介/01.构建单体应用模型.md","构建单体应用模型","/pages/593778/"],["01.微服务简介/02.走向单体地狱.md","走向单体地狱","/pages/72d00f/"],["01.微服务简介/03.微服务-解决复杂问题.md","微服务-解决复杂问题","/pages/0b2d6e/"],["01.微服务简介/04.微服务的优点.md","微服务的优点","/pages/47dac2/"],["01.微服务简介/05.微服务的缺点.md","微服务的缺点","/pages/fb92c3/"]]}],"/15.Spring Cloud Gateway/":[{title:"Spring Cloud Gateway",collapsable:!1,children:[["01.Spring Cloud Gateway/01.Spring Cloud 之 Gateway.md","Spring Cloud 之 Gateway","/pages/aa8849/"],["01.Spring Cloud Gateway/02.Spring Cloud Gateway 配置大全.md","Spring Cloud Gateway 配置大全","/pages/f0610e/"],["01.Spring Cloud Gateway/03.Spring Cloud Gateway 中文官网文档.md","Spring Cloud Gateway 中文官网文档","/pages/f7b8bf/"]]}],"/16.Spring Cloud Alibaba/":[{title:"Spring Cloud Alibaba",collapsable:!1,children:[["01.Spring Cloud Alibaba/01.简介.md","简介","/pages/b62716/"],["01.Spring Cloud Alibaba/02.服务注册与发现.md","服务注册与发现","/pages/e48516/"],["01.Spring Cloud Alibaba/03.Nacos discovery.md","Nacos discovery","/pages/01bdf6/"],["01.Spring Cloud Alibaba/04.Nacos config.md","Nacos config","/pages/3a1e7d/"]]}],"/17.Docker/":[{title:"Docker",collapsable:!1,children:[["01.Docker/01.什么是 Docker.md","什么是 Docker","/pages/555c55/"],["01.Docker/02.为什么要使用 Docker.md","为什么要使用 Docker","/pages/d1d9db/"]]},{title:"Docker 基本概念",collapsable:!1,children:[["02.Docker 基本概念/01.Docker 基本概念.md","Docker 基本概念","/pages/ad1da1/"],["02.Docker 基本概念/02.Docker 引擎.md","Docker 引擎","/pages/77a885/"],["02.Docker 基本概念/03.Docker 系统架构.md","Docker 系统架构","/pages/f566e4/"],["02.Docker 基本概念/04.Docker 镜像.md","Docker 镜像","/pages/546146/"],["02.Docker 基本概念/05.Docker 容器.md","Docker 容器","/pages/c9e8a8/"],["02.Docker 基本概念/06.Docker 仓库.md","Docker 仓库","/pages/f00cd0/"]]},{title:"安装 Docker",collapsable:!1,children:[["03.安装 Docker/01.安装 Docker.md","安装 Docker","/pages/11ef44/"],["03.安装 Docker/02.Ubuntu 安装 Docker.md","Ubuntu 安装 Docker","/pages/748872/"],["03.安装 Docker/03.CentOS 安装 Docker.md","CentOS 安装 Docker","/pages/102284/"],["03.安装 Docker/04.macOS 安装 Docker.md","macOS 安装 Docker","/pages/847488/"],["03.安装 Docker/05.Windows 安装 Docker.md","Windows 安装 Docker","/pages/c4f755/"],["03.安装 Docker/06.Docker 镜像加速器.md","Docker 镜像加速器","/pages/87062e/"]]},{title:"Docker 镜像",collapsable:!1,children:[["04.Docker 镜像/01.使用 Docker 镜像.md","使用 Docker 镜像","/pages/13e1d1/"],["04.Docker 镜像/02.Docker 获取镜像.md","Docker 获取镜像","/pages/840a1a/"],["04.Docker 镜像/03.Docker 列出镜像.md","Docker 列出镜像","/pages/be83bd/"],["04.Docker 镜像/04.Docker 删除本地镜像.md","Docker 删除本地镜像","/pages/058894/"],["04.Docker 镜像/05.Docker 镜像导入导出.md","Docker 镜像导入导出","/pages/79f5f4/"],["04.Docker 镜像/06.利用 commit 理解镜像构成.md","利用 commit 理解镜像构成","/pages/134f38/"],["04.Docker 镜像/07.使用 Dockerfile 定制镜像.md","使用 Dockerfile 定制镜像","/pages/ffea77/"],["04.Docker 镜像/08.Dockerfile 指令详解.md","Dockerfile 指令详解","/pages/6d2add/"],["04.Docker 镜像/09.Dockerfile 多阶段构建.md","Dockerfile 多阶段构建","/pages/f99491/"],["04.Docker 镜像/10.镜像的实现原理.md","镜像的实现原理","/pages/3937d4/"]]},{title:"Docker 容器",collapsable:!1,children:[["05.Docker 容器/01.操作 Docker 容器.md","操作 Docker 容器","/pages/7e034a/"],["05.Docker 容器/02.Docker 启动容器.md","Docker 启动容器","/pages/f5a508/"],["05.Docker 容器/03.Docker 守护态运行.md","Docker 守护态运行","/pages/4c0f60/"],["05.Docker 容器/04.Docker 终止容器.md","Docker 终止容器","/pages/a777e4/"],["05.Docker 容器/05.Docker 进入容器.md","Docker 进入容器","/pages/9efa8d/"],["05.Docker 容器/06.Docker 导出和导入容器.md","Docker 导出和导入容器","/pages/d3a79f/"],["05.Docker 容器/07.Docker 删除容器.md","Docker 删除容器","/pages/8b2f36/"]]},{title:"访问 Docker 仓库",collapsable:!1,children:[["06.访问 Docker 仓库/01.访问 Docker 仓库.md","访问 Docker 仓库","/pages/c0886b/"],["06.访问 Docker 仓库/02.Docker Hub.md","Docker Hub","/pages/ebce48/"],["06.访问 Docker 仓库/03.Docker 私有仓库.md","Docker 私有仓库","/pages/92e266/"],["06.访问 Docker 仓库/04.Docker 私有仓库高级配置.md","Docker 私有仓库高级配置","/pages/141d21/"]]},{title:"Docker 数据管理",collapsable:!1,children:[["07.Docker 数据管理/01.Docker 数据管理.md","Docker 数据管理","/pages/bcfae7/"],["07.Docker 数据管理/02.Docker 数据卷.md","Docker 数据卷","/pages/5f0882/"],["07.Docker 数据管理/03.监听主机目录.md","监听主机目录","/pages/565ee4/"]]},{title:"Docker 三剑客 Compose",collapsable:!1,children:[["08.Docker 三剑客 Compose/01.什么是 Docker Compose.md","什么是 Docker Compose","/pages/e17a65/"],["08.Docker 三剑客 Compose/02.Docker Compose 安装与卸载.md","Docker Compose 安装与卸载","/pages/f7fbae/"],["08.Docker 三剑客 Compose/03.Docker Compose 入门.md","Docker Compose 入门","/pages/a48959/"],["08.Docker 三剑客 Compose/04.Docker Compose 命令说明.md","Docker Compose 命令说明","/pages/cddd90/"],["08.Docker 三剑客 Compose/05.Docker Compose 模板文件.md","Docker Compose 模板文件","/pages/5e9e65/"],["08.Docker 三剑客 Compose/06.Docker Compose 实战 WordPress.md","Docker Compose 实战 WordPress","/pages/7285c6/"],["08.Docker 三剑客 Compose/07.Docker Compose 实战 Tomcat.md","Docker Compose 实战 Tomcat","/pages/8a9ceb/"],["08.Docker 三剑客 Compose/08.Docker Compose 实战 MySQL.md","Docker Compose 实战 MySQL","/pages/853cbd/"],["08.Docker 三剑客 Compose/09.Docker Compose 常用命令.md","Docker Compose 常用命令","/pages/f508e3/"],["08.Docker 三剑客 Compose/10.YAML 配置文件语言.md","YAML 配置文件语言","/pages/bd41de/"],["08.Docker 三剑客 Compose/11.附：为什么说 JSON 不适合做配置文件？.md","附：为什么说 JSON 不适合做配置文件？","/pages/0c8168/"]]},{title:"常见问题",collapsable:!1,children:[["09.常见问题/01.docker磁盘空间不足解决办法.md","docker磁盘空间不足解决办法","/pages/be0094/"]]}],"/18.Kubernetes/":[{title:"Kubernetes 入门",collapsable:!1,children:[["01.Kubernetes 入门/01.什么是 Kubernetes.md","什么是 Kubernetes","/pages/419ea5/"],["01.Kubernetes 入门/02.Kubernetes 安装前的准备.md","Kubernetes 安装前的准备","/pages/b2e7fa/"],["01.Kubernetes 入门/03.安装 kubeadm.md","安装 kubeadm","/pages/babf51/"],["01.Kubernetes 入门/05.配置 kubeadm.md","配置 kubeadm","/pages/14ea2d/"],["01.Kubernetes 入门/06.使用 kubeadm 部署 Master 节点.md","使用 kubeadm 部署 Master 节点","/pages/9b17d0/"],["01.Kubernetes 入门/07.安装网络插件.md","安装网络插件","/pages/9477b8/"],["01.Kubernetes 入门/08.使用 kubeadm 配置 Worker 节点.md","使用 kubeadm 配置 Worker 节点","/pages/f6617c/"],["01.Kubernetes 入门/09.概念总结.md","概念总结","/pages/f4b696/"]]},{title:"Kubernetes 实践",collapsable:!1,children:[["02.Kubernetes 实践/01.Kubernetes Dashboard.md","Kubernetes Dashboard","/pages/e68987/"],["02.Kubernetes 实践/02.NFS 高可用方案.md","NFS 高可用方案","/pages/1b012a/"],["02.Kubernetes 实践/03.离线部署 Kubernetess + KubeSphere.md","离线部署 Kubernetess + KubeSphere","/pages/8a1473/"]]}],"/19.Spring Security OAuth2/":[{title:"Spring Security OAuth2",collapsable:!1,children:[["01.Spring Security OAuth2/01.介绍.md","介绍","/pages/1b4b4e/"],["01.Spring Security OAuth2/02.为什么需要 OAuth2.md","为什么需要 OAuth2","/pages/17b300/"],["01.Spring Security OAuth2/03.开放平台.md","开放平台","/pages/5cb943/"],["01.Spring Security OAuth2/04.令牌的访问与刷新.md","令牌的访问与刷新","/pages/37f648/"],["01.Spring Security OAuth2/05.客户端授权模式.md","客户端授权模式","/pages/fbe541/"]]},{title:"创建案例工程",collapsable:!1,children:[["02.创建案例工程/01.创建案例工程项目.md","创建案例工程项目","/pages/00c356/"],["02.创建案例工程/02.创建统一的依赖管理模块.md","创建统一的依赖管理模块","/pages/c3686c/"]]},{title:"创建认证服务器",collapsable:!1,children:[["03.创建认证服务器/01.授权服务器配置.md","授权服务器配置","/pages/5eb137/"],["03.创建认证服务器/02.基于内存存储令牌.md","基于内存存储令牌","/pages/2279cf/"],["03.创建认证服务器/03.基于 JDBC 存储令牌.md","基于 JDBC 存储令牌","/pages/9b9a7b/"],["03.创建认证服务器/04.RBAC 基于角色的权限控制.md","RBAC 基于角色的权限控制","/pages/cee981/"],["03.创建认证服务器/05.基于 RBAC 的自定义认证.md","基于 RBAC 的自定义认证","/pages/4bff79/"],["03.创建认证服务器/06.JWT令牌.md","JWT令牌","/pages/8dc90c/"]]},{title:"创建资源服务器",collapsable:!1,children:[["04.创建资源服务器/01.资源服务器配置.md","资源服务器配置","/pages/3414a7/"]]},{title:"分布式系统认证方案",collapsable:!1,children:[["05.分布式系统认证方案/01.什么是分布式系统.md","什么是分布式系统","/pages/d62ff2/"],["05.分布式系统认证方案/02.分布式认证需求.md","分布式认证需求","/pages/16b31e/"],["05.分布式系统认证方案/03.分布式认证方案.md","分布式认证方案","/pages/d6bec0/"],["05.分布式系统认证方案/04.分布式认证实战.md","分布式认证实战","/pages/7b210e/"]]}],"/20.DevOps/":[{title:"入门",collapsable:!1,children:[["01.入门/01.软件工程介绍.md","软件工程介绍","/pages/11f8a5/"],["01.入门/02.持续继承流程说明.md","持续继承流程说明","/pages/5b4493/"],["01.入门/03.Gitlab代码托管服务器安装.md","Gitlab代码托管服务器安装","/pages/eaeddc/"],["01.入门/04.Jenkins自动化服务器安装.md","Jenkins自动化服务器安装","/pages/605fde/"],["01.入门/05.Jenkins插件管理.md","Jenkins插件管理","/pages/6519b5/"],["01.入门/06.Jenkins用户权限管理.md","Jenkins用户权限管理","/pages/61d667/"],["01.入门/07.Jenkins凭证管理.md","Jenkins凭证管理","/pages/a1fe86/"],["01.入门/08.Maven安装和配置.md","Maven安装和配置","/pages/c15beb/"],["01.入门/09.Tomcat安装和配置.md","Tomcat安装和配置","/pages/9d384b/"]]},{title:"实战",collapsable:!1,children:[["02.实战/01.Jenkins构建Maven项目.md","Jenkins构建Maven项目","/pages/5448da/"],["02.实战/02.Jenkins常用构建触发器.md","Jenkins常用构建触发器","/pages/bf228f/"],["02.实战/03.Jenkins参数化构建.md","Jenkins参数化构建","/pages/991f2e/"],["02.实战/04.Jenkins配置邮箱服务器.md","Jenkins配置邮箱服务器","/pages/bd7581/"],["02.实战/05.SonarQube自动代码审查工具.md","SonarQube自动代码审查工具","/pages/e2748f/"],["02.实战/06.Jenkins整合SonarQube实现代码审查.md","Jenkins整合SonarQube实现代码审查","/pages/848aec/"],["02.实战/07.微服务持续集成(上).md","微服务持续集成(上)","/pages/8e8067/"],["02.实战/08.微服务持续集成(下).md","微服务持续集成(下)","/pages/b7044d/"]]}],"/21.Simplify/":[{title:"Simplify Docs",collapsable:!1,children:[["01.Simplify Docs/01.Lombok使用指南.md","Lombok使用指南","/pages/d49c54/"],["01.Simplify Docs/02.Spring Boot整合Swagger.md","Spring Boot整合Swagger","/pages/ff62ec/"],["01.Simplify Docs/03.axios中文文档.md","axios中文文档","/pages/92c82f/"],["01.Simplify Docs/04.vue-axios.md","vue-axios","/pages/305234/"],["01.Simplify Docs/05.优化Docker中的Spring Boot应用.md","优化Docker中的Spring Boot应用","/pages/59f664/"],["01.Simplify Docs/06.自己制作一个java11的docker镜像.md","自己制作一个java11的docker镜像","/pages/46a509/"],["01.Simplify Docs/07.Spring Boot 新特性 Layered Jar.md","Spring Boot 2.3 新特性 Layered Jar","/pages/97d41d/"],["01.Simplify Docs/08.Docker客户端使用TLS保护连接远程Docker服务.md","Docker客户端使用TLS保护连接远程Docker服务","/pages/a60efa/"],["01.Simplify Docs/09.docker-maven-plugin.md","docker-maven-plugin","/pages/779e01/"],["01.Simplify Docs/10.Inner注解使用说明.md","Inner注解使用说明","/pages/9dfd5c/"],["01.Simplify Docs/11.Java8 Stream.md","Java8 Stream","/pages/5f71ee/"]]}],"/22.Kubernetes/":[{title:"Kubernetes 入门",collapsable:!1,children:[["01.Kubernetes 入门/01.什么是 Kubernetes.md","什么是 Kubernetes","/pages/e51b3b/"],["01.Kubernetes 入门/02.Kubernetes 安装前的准备.md","Kubernetes 安装前的准备","/pages/43a41d/"],["01.Kubernetes 入门/03.安装 kubeadm.md","安装 kubeadm","/pages/248b3e/"],["01.Kubernetes 入门/04.配置 kubeadm.md","配置 kubeadm","/pages/f792df/"],["01.Kubernetes 入门/05.使用 kubeadm 部署 Master 节点.md","使用 kubeadm 部署 Master 节点","/pages/91b30f/"],["01.Kubernetes 入门/06.安装网络插件.md","安装网络插件","/pages/1d4070/"],["01.Kubernetes 入门/07.使用 kubeadm 配置 Worker 节点.md","使用 kubeadm 配置 Worker 节点","/pages/b23692/"],["01.Kubernetes 入门/08.概念总结.md","概念总结","/pages/cfa4e2/"]]},{title:"Kubernetes 实践",collapsable:!1,children:[["02.Kubernetes 实践/01.Kubernetes Dashboard.md","Kubernetes Dashboard","/pages/11af34/"]]},{title:"内部平台对接",collapsable:!1,children:[["03.内部平台对接/01.整合云平台相关配置（内部）.md","整合云平台相关配置（内部）","/pages/f61a3d/"]]}],"/99.支持/":[["01.支持.md","支持这个项目","/pages/1b12ed"]]},updateBar:{showToArticle:!1},category:!1,tag:!1,author:{name:"heyuqiang",href:"https://github.com/heyuqiang"},social:{icons:[{iconClass:"icon-youjian",title:"发邮件",link:"mailto:785056500@qq.com"},{iconClass:"icon-github",title:"GitHub",link:"https://github.com/heyuqiang"},{iconClass:"icon-erji",title:"听音乐",link:"https://music.163.com/#/playlist?id=755597173"}]},footer:{createYear:2019,copyrightInfo:"PigxCloud, Inc."}}};t(151),t(207),t(139);var cc=t(217),lc=t(218),dc=(t(371),t(232),t(43));var uc={computed:{$filterPosts:function(){return this.$site.pages.filter((function(n){var e=n.frontmatter,t=e.pageComponent,r=e.article,o=e.home;return!(t||!1===r||!0===o)}))},$sortPosts:function(){return(n=this.$filterPosts).sort((function(n,e){var t=n.frontmatter.sticky,r=e.frontmatter.sticky;return t&&r?t==r?Object(dc.a)(n,e):t-r:t&&!r?-1:!t&&r?1:Object(dc.a)(n,e)})),n;var n},$sortPostsByDate:function(){return(n=this.$filterPosts).sort((function(n,e){return Object(dc.a)(n,e)})),n;var n},$groupPosts:function(){return function(n){for(var e={},t={},r=function(r,o){var i=n[r].frontmatter,a=i.categories,s=i.tags;"array"===Object(dc.n)(a)&&a.forEach((function(t){t&&(e[t]||(e[t]=[]),e[t].push(n[r]))})),"array"===Object(dc.n)(s)&&s.forEach((function(e){e&&(t[e]||(t[e]=[]),t[e].push(n[r]))}))},o=0,i=n.length;o<i;o++)r(o);return{categories:e,tags:t}}(this.$sortPosts)},$categoriesAndTags:function(){return function(n){var e=[],t=[];for(var r in n.categories)e.push({key:r,length:n.categories[r].length});for(var o in n.tags)t.push({key:o,length:n.tags[o].length});return{categories:e,tags:t}}(this.$groupPosts)}}};Po.component(cc.default),Po.component(lc.default);function pc(n){return n.toString().padStart(2,"0")}t(375);Po.component("Badge",(function(){return Promise.all([t.e(0),t.e(4)]).then(t.bind(null,716))})),Po.component("CodeBlock",(function(){return Promise.resolve().then(t.bind(null,217))})),Po.component("CodeGroup",(function(){return Promise.resolve().then(t.bind(null,218))}));t(376),t(377);var mc=[function(n){n.Vue,n.options;var e=n.router;n.siteData;n.isServer||e.afterEach((function(){var n;n=function(){setTimeout((function(){var n,e;void 0===window._AdBlockInit&&(n=document.getElementsByClassName("wwads-cn"),e=document.querySelector(".wwads-content"),n[0]&&!e&&(n[0].innerHTML="<style>.wwads-horizontal,.wwads-vertical{background-color:#f4f8fa;padding:5px;min-height:120px;margin-top:20px;box-sizing:border-box;border-radius:3px;font-family:sans-serif;display:flex;min-width:150px;position:relative;overflow:hidden;}.wwads-horizontal{flex-wrap:wrap;justify-content:center}.wwads-vertical{flex-direction:column;align-items:center;padding-bottom:32px}.wwads-horizontal a,.wwads-vertical a{text-decoration:none}.wwads-horizontal .wwads-img,.wwads-vertical .wwads-img{margin:5px}.wwads-horizontal .wwads-content,.wwads-vertical .wwads-content{margin:5px}.wwads-horizontal .wwads-content{flex:130px}.wwads-vertical .wwads-content{margin-top:10px}.wwads-horizontal .wwads-text,.wwads-content .wwads-text{font-size:14px;line-height:1.4;color:#0e1011;-webkit-font-smoothing:antialiased}.wwads-horizontal .wwads-poweredby,.wwads-vertical .wwads-poweredby{display:block;font-size:11px;color:#a6b7bf;margin-top:1em}.wwads-vertical .wwads-poweredby{position:absolute;left:10px;bottom:10px}.wwads-horizontal .wwads-poweredby span,.wwads-vertical .wwads-poweredby span{transition:all 0.2s ease-in-out;margin-left:-1em}.wwads-horizontal .wwads-poweredby span:first-child,.wwads-vertical .wwads-poweredby span:first-child{opacity:0}.wwads-horizontal:hover .wwads-poweredby span,.wwads-vertical:hover .wwads-poweredby span{opacity:1;margin-left:0}.wwads-horizontal .wwads-hide,.wwads-vertical .wwads-hide{position:absolute;right:-23px;bottom:-23px;width:46px;height:46px;border-radius:23px;transition:all 0.3s ease-in-out;cursor:pointer;}.wwads-horizontal .wwads-hide:hover,.wwads-vertical .wwads-hide:hover{background:rgb(0 0 0 /0.05)}.wwads-horizontal .wwads-hide svg,.wwads-vertical .wwads-hide svg{position:absolute;left:10px;top:10px;fill:#a6b7bf}.wwads-horizontal .wwads-hide:hover svg,.wwads-vertical .wwads-hide:hover svg{fill:#3E4546}</style><a href='https://wwads.cn/page/whitelist-wwads' class='wwads-img' target='_blank' rel='nofollow'><img src='https://cdn.jsdelivr.net/gh/xugaoyi/image_store@master/blog/wwads.2a3pidhlh4ys.webp' width='130'></a><div class='wwads-content'><a href='https://wwads.cn/page/whitelist-wwads' class='wwads-text' target='_blank' rel='nofollow'>为了本站的长期运营，请将我们的网站加入广告拦截器的白名单，感谢您的支持！<span style='color: #11a8cd'>如何添加白名单?</span></a><a href='https://wwads.cn/page/end-user-privacy' class='wwads-poweredby' title='万维广告 ～ 让广告更优雅，且有用' target='_blank'><span>万维</span><span>广告</span></a></div><a class='wwads-hide' onclick='parentNode.remove()' title='隐藏广告'><svg xmlns='http://www.w3.org/2000/svg' width='6' height='7'><path d='M.879.672L3 2.793 5.121.672a.5.5 0 11.707.707L3.708 3.5l2.12 2.121a.5.5 0 11-.707.707l-2.12-2.12-2.122 2.12a.5.5 0 11-.707-.707l2.121-2.12L.172 1.378A.5.5 0 01.879.672z'></path></svg></a>"))}),3e3)},"complete"===document.readyState||"interactive"===document.readyState?setTimeout(n,1):document.addEventListener("DOMContentLoaded",n),setTimeout((function(){var n=document.querySelector(".pageB");if(n){var e=n.querySelector(".wwads-hide");e&&(e.onclick=function(){n.style.display="none"}),n.style.display="flex"}}),0)}))},function(n){var e=n.Vue,t=(n.options,n.router,n.siteData);t.pages.map((function(n){var e=n.frontmatter,r=e.date,o=e.author;"string"==typeof r&&"Z"===r.charAt(r.length-1)&&(n.frontmatter.date=function(n){n instanceof Date||(n=new Date(n));return"".concat(n.getUTCFullYear(),"-").concat(pc(n.getUTCMonth()+1),"-").concat(pc(n.getUTCDate())," ").concat(pc(n.getUTCHours()),":").concat(pc(n.getUTCMinutes()),":").concat(pc(n.getUTCSeconds()))}(r)),o?n.author=o:t.themeConfig.author&&(n.author=t.themeConfig.author)})),e.mixin(uc)},{},function(n){n.Vue.mixin({computed:{$dataBlock:function(){return this.$options.__data__block__}}})},{},{},function(){"undefined"!=typeof window&&function(n,e,t){function r(n){var t=e.createElement("div");t.className="heart",o.push({el:t,x:n.clientX-5,y:n.clientY-5,scale:1,alpha:1,color:"#11a8cd"}),e.body.appendChild(t)}var o=[];n.requestAnimationFrame=n.requestAnimationFrame||n.webkitRequestAnimationFrame||n.mozRequestAnimationFrame||n.oRequestAnimationFrame||n.msRequestAnimationFrame||function(n){setTimeout(n,1e3/60)},function(n){var t=e.createElement("style");t.type="text/css";try{t.appendChild(e.createTextNode(n))}catch(e){t.styleSheet.cssText=n}e.getElementsByTagName("head")[0].appendChild(t)}(".heart{width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;}.heart:after{top: -5px;}.heart:before{left: -5px;}"),function(){var e="function"==typeof n.onclick&&n.onclick;n.onclick=function(n){var t=!0;n.path&&n.path.forEach((function(n){1===n.nodeType&&"string"==typeof n.className&&n.className.indexOf("theme-vdoing-content")>-1&&(t=!1)})),t&&(e&&e(),r(n))}}(),function n(){for(var t=0;t<o.length;t++)o[t].alpha<=0?(e.body.removeChild(o[t].el),o.splice(t,1)):(o[t].y--,o[t].scale+=.004,o[t].alpha-=.013,o[t].el.style.cssText="left:"+o[t].x+"px;top:"+o[t].y+"px;opacity:"+o[t].alpha+";transform:scale("+o[t].scale+","+o[t].scale+") rotate(45deg);background:"+o[t].color+";z-index:99999");requestAnimationFrame(n)}()}(window,document)}],gc=[];t(210);function hc(n,e){return(hc=Object.setPrototypeOf||function(n,e){return n.__proto__=e,n})(n,e)}t(211),t(212);function bc(n){return(bc=Object.setPrototypeOf?Object.getPrototypeOf:function(n){return n.__proto__||Object.getPrototypeOf(n)})(n)}function fc(n,e){if(e&&("object"===Ta(e)||"function"==typeof e))return e;if(void 0!==e)throw new TypeError("Derived constructors may only return object or undefined");return function(n){if(void 0===n)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return n}(n)}function vc(n){var e=function(){if("undefined"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(n){return!1}}();return function(){var t,r=bc(n);if(e){var o=bc(this).constructor;t=Reflect.construct(r,arguments,o)}else t=r.apply(this,arguments);return fc(this,t)}}var kc=function(n){!function(n,e){if("function"!=typeof e&&null!==e)throw new TypeError("Super expression must either be null or a function");n.prototype=Object.create(e&&e.prototype,{constructor:{value:n,writable:!0,configurable:!0}}),Object.defineProperty(n,"prototype",{writable:!1}),e&&hc(n,e)}(t,n);var e=vc(t);function t(){return ls(this,t),e.apply(this,arguments)}return us(t)}(function(){function n(){ls(this,n),this.store=new Po({data:{state:{}}})}return us(n,[{key:"$get",value:function(n){return this.store.state[n]}},{key:"$set",value:function(n,e){Po.set(this.store.state,n,e)}},{key:"$emit",value:function(){var n;(n=this.store).$emit.apply(n,arguments)}},{key:"$on",value:function(){var n;(n=this.store).$on.apply(n,arguments)}}]),n}());Object.assign(kc.prototype,{getPageAsyncComponent:$a,getLayoutAsyncComponent:Ga,getAsyncComponent:Fa,getVueComponent:Ha});var yc={install:function(n){var e=new kc;n.$vuepress=e,n.prototype.$vuepress=e}};function xc(n){n.beforeEach((function(e,t,r){if(wc(n,e.path))r();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){var o=e.path.replace(/\/$/,"")+".html";wc(n,o)?r(o):r()}else r();else{var i=e.path+"/",a=e.path+".html";wc(n,a)?r(a):wc(n,i)?r(i):r()}}))}function wc(n,e){var t=e.toLowerCase();return n.options.routes.some((function(n){return n.path.toLowerCase()===t}))}var Sc={props:{pageKey:String,slotKey:{type:String,default:"default"}},render:function(n){var e=this.pageKey||this.$parent.$page.key;return Ja("pageKey",e),Po.component(e)||Po.component(e,$a(e)),Po.component(e)?n(e):n("")}},_c={functional:!0,props:{slotKey:String,required:!0},render:function(n,e){var t=e.props,r=e.slots;return n("div",{class:["content__".concat(t.slotKey)]},r()[t.slotKey])}},Ec={computed:{openInNewWindowTitle:function(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},Tc=(t(378),t(379),Object(oc.a)(Ec,(function(){var n=this.$createElement,e=this._self._c||n;return e("span",[e("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[e("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),e("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),e("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports);function Ic(){return(Ic=Object(r.a)(regeneratorRuntime.mark((function n(e){var t,r,o,i;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return t="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:sc.routerBase||sc.base,xc(r=new _a({base:t,mode:"history",fallback:!1,routes:ac,scrollBehavior:function(n,e,t){return t||(n.hash?!Po.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})}})),o={},n.prev=4,n.next=7,Promise.all(mc.filter((function(n){return"function"==typeof n})).map((function(n){return n({Vue:Po,options:o,router:r,siteData:sc,isServer:e})})));case 7:n.next=12;break;case 9:n.prev=9,n.t0=n.catch(4),console.error(n.t0);case 12:return i=new Po(Object.assign(o,{router:r,render:function(n){return n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},gc.map((function(e){return n(e)})))])}})),n.abrupt("return",{app:i,router:r});case 14:case"end":return n.stop()}}),n,null,[[4,9]])})))).apply(this,arguments)}Po.config.productionTip=!1,Po.use(_a),Po.use(yc),Po.mixin(function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:Po;Ea(e),t.$vuepress.$set("siteData",e);var r=n(t.$vuepress.$get("siteData")),o=new r,i=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(o)),a={};return Object.keys(i).reduce((function(n,e){return e.startsWith("$")&&(n[e]=i[e].get),n}),a),{computed:a}}((function(n){return function(){function e(){ls(this,e)}return us(e,[{key:"setPage",value:function(n){this.__page=n}},{key:"$site",get:function(){return n}},{key:"$themeConfig",get:function(){return this.$site.themeConfig}},{key:"$frontmatter",get:function(){return this.$page.frontmatter}},{key:"$localeConfig",get:function(){var n,e,t=this.$site.locales,r=void 0===t?{}:t;for(var o in r)"/"===o?e=r[o]:0===this.$page.path.indexOf(o)&&(n=r[o]);return n||e||{}}},{key:"$siteTitle",get:function(){return this.$localeConfig.title||this.$site.title||""}},{key:"$canonicalUrl",get:function(){var n=this.$page.frontmatter.canonicalUrl;return"string"==typeof n&&n}},{key:"$title",get:function(){var n=this.$page,e=this.$page.frontmatter.metaTitle;if("string"==typeof e)return e;var t=this.$siteTitle,r=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?r?r+" | "+t:t:r||"VuePress"}},{key:"$description",get:function(){var n=function(n){if(n){var e=n.filter((function(n){return"description"===n.name}))[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}},{key:"$lang",get:function(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}},{key:"$localePath",get:function(){return this.$localeConfig.path||"/"}},{key:"$themeLocaleConfig",get:function(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}},{key:"$page",get:function(){return this.__page?this.__page:function(n,e){for(var t=0;t<n.length;t++){var r=n[t];if(r.path.toLowerCase()===e.toLowerCase())return r}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}}]),e}()}),sc)),Po.component("Content",Sc),Po.component("ContentSlotsDistributor",_c),Po.component("OutboundLink",Tc),Po.component("ClientOnly",{functional:!0,render:function(n,e){var t=e.parent,r=e.children;if(t._isMounted)return r;t.$once("hook:mounted",(function(){t.$forceUpdate()}))}}),Po.component("Layout",Ga("Layout")),Po.component("NotFound",Ga("NotFound")),Po.prototype.$withBase=function(n){var e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.9.2",hash:"5547af4"},function(n){return Ic.apply(this,arguments)}(!1).then((function(n){var e=n.app;n.router.onReady((function(){e.$mount("#app")}))}))}]);